{"meta":{"title":"Mengmeng Kuang's Blog!","subtitle":null,"description":"I want to have my cake, and eat it.","author":"Mengmeng Kuang","url":"http://home.meng.uno"},"pages":[{"title":"","date":"2021-01-02T04:13:38.000Z","updated":"2021-01-02T04:13:38.000Z","comments":true,"path":"hexo-admin-ehc-images.json","permalink":"http://home.meng.uno/hexo-admin-ehc-images.json","excerpt":"[{\"name\":\"DASHENG-900x700-1.png\",\"date\":1518959551960},{\"name\":\"DASHENG-900x700-2.png\",\"date\":1518959599098},{\"name\":\"DASHENG-900x700-3.png\",\"date\":1518959609913},{\"name\":\"DASHENG-900x700-4.png\",\"date\":1518959618105}]","text":"[{\"name\":\"DASHENG-900x700-1.png\",\"date\":1518959551960},{\"name\":\"DASHENG-900x700-2.png\",\"date\":1518959599098},{\"name\":\"DASHENG-900x700-3.png\",\"date\":1518959609913},{\"name\":\"DASHENG-900x700-4.png\",\"date\":1518959618105}]"},{"title":"关于我","date":"2020-11-27T06:19:35.000Z","updated":"2018-02-11T06:17:40.000Z","comments":true,"path":"about/index.html","permalink":"http://home.meng.uno/about/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-11-27T06:19:35.000Z","updated":"2018-02-11T06:17:12.000Z","comments":true,"path":"categories/index.html","permalink":"http://home.meng.uno/categories/index.html","excerpt":"","text":""},{"title":"留言板","date":"2020-11-27T06:19:35.000Z","updated":"2018-02-11T06:17:58.000Z","comments":true,"path":"comments/index.html","permalink":"http://home.meng.uno/comments/index.html","excerpt":"","text":""},{"title":"仓库","date":"2020-11-27T06:19:35.000Z","updated":"2018-04-24T08:54:24.000Z","comments":true,"path":"repository/index.html","permalink":"http://home.meng.uno/repository/index.html","excerpt":"","text":""},{"title":"标签云","date":"2020-11-27T06:19:35.000Z","updated":"2018-02-11T06:17:27.000Z","comments":true,"path":"tags/index.html","permalink":"http://home.meng.uno/tags/index.html","excerpt":"","text":""},{"title":"我的作品","date":"2020-11-27T06:19:35.000Z","updated":"2018-03-11T04:57:40.000Z","comments":true,"path":"works/index.html","permalink":"http://home.meng.uno/works/index.html","excerpt":"","text":""}],"posts":[{"title":"Attention and Self Attention","slug":"self-attention","date":"2020-12-01T09:56:52.000Z","updated":"2020-12-02T01:43:44.000Z","comments":true,"path":"articles/9d18641a/","link":"","permalink":"http://home.meng.uno/articles/9d18641a/","excerpt":"Attention Seq2Seq结构在一些End2End问题上能取得非常好的结果，其结构一般由Encoder和Decoder组成，其运行流程如下视频所示。 然而，采用RNN的Seq2Seq结构因为RNN中存在长程梯度消失的问题，很难将较长输入的序列转化为定长的向量而保存所有的有效信息。所以随着句子的长度的增加，这种结构的效果会显著下降。为了解决这一由长序列到定长向量转化而造成的信息损失的瓶颈，Attention机制被引入了。Attention机制跟人类翻译文章时候的思路有些类似，即将注意力关注于我们翻译部分对应的上下文。此机制打破了只能利用Encoder最终单一向量结果的限制，从而使模型可","text":"Attention Seq2Seq结构在一些End2End问题上能取得非常好的结果，其结构一般由Encoder和Decoder组成，其运行流程如下视频所示。 然而，采用RNN的Seq2Seq结构因为RNN中存在长程梯度消失的问题，很难将较长输入的序列转化为定长的向量而保存所有的有效信息。所以随着句子的长度的增加，这种结构的效果会显著下降。为了解决这一由长序列到定长向量转化而造成的信息损失的瓶颈，Attention机制被引入了。Attention机制跟人类翻译文章时候的思路有些类似，即将注意力关注于我们翻译部分对应的上下文。此机制打破了只能利用Encoder最终单一向量结果的限制，从而使模型可以集中在所有对于下一个目标单词重要的输入信息上，使模型效果得到极大的改善。还有一个优点是，我们通过观察Attention权重矩阵的变化，可以更好地知道哪部分翻译对应哪部分源文字，有助于更好的理解模型工作机制。 Attention机制实际上是对Query, Key, Value 的运算，其整体流程可以用下图来表示。 其中： 阶段一是Query与Key进行某种F()运算； 阶段二是使用softmax对结果归一化； 阶段三是上阶段输出值和Value做运算。 F()运算可以是：$$Q^{T}K$$, $$Q^{T}W_{a}K$$ 或者 $$v_{a}^{T} tanh(W_{a} concat(Q, K))$$。 Self Attention 如果Attention机制中的Key, Value以及Query都来着同一个分布，我们称之为Self-Attention。 接下来，结合网上的一个例子，我们从数据输入、计算Key, Value和Query、再到具体的三阶段计算来认识Self-Attention。 数据准备 我们准备了三个输入，每个输入维度为4。如下： 123 Input 1: [1, 0, 1, 0] Input 2: [0, 2, 0, 2]Input 3: [1, 1, 1, 1] 计算Key,Value以及Query 假设我们希望Key, Value, Query的尺寸为3。而由于现在每个输入的尺寸均为4，这意味着每组权重的形状都必须为4×3。 计算Key 我们假定Key的权重为： 1234 [[0, 0, 1],[1, 1, 0],[0, 1, 0],[1, 1, 0]] 则通过矩阵乘法，我们可以得到Key为： 1234 [0, 0, 1][1, 0, 1, 0] [1, 1, 0] [0, 1, 1][0, 2, 0, 2] x [0, 1, 0] = [4, 4, 0][1, 1, 1, 1] [1, 1, 0] [2, 3, 1] 此过程如下图所示： 计算Value 同理，Value权重及计算如下： 1234 [0, 2, 0][1, 0, 1, 0] [0, 3, 0] [1, 2, 3] [0, 2, 0, 2] x [1, 0, 3] = [2, 8, 0][1, 1, 1, 1] [1, 1, 0] [2, 6, 3] 此过程也有图片展示: 计算Query 同样的方式可以计算Query： 1234 [1, 0, 1][1, 0, 1, 0] [1, 0, 0] [1, 0, 2][0, 2, 0, 2] x [0, 0, 1] = [2, 2, 2][1, 1, 1, 1] [0, 1, 1] [2, 1, 3] 动图表示此过程如下： 至此，我们可以得到来自同一分布的Key, Value以及Query。 三阶段计算 F()运算 在此，我们选择点乘作为我们的F()。对于第一个Query，我们可以得到其注意力得分： 123 [0, 4, 2][1, 0, 2] x [1, 4, 3] = [2, 4, 4] [1, 0, 1] 过程如下： softmax归一化 以上视频展示了softmax的过程，softmax本身是这样一个公式： 得到Attention Value 针对某一个Query，其经过以上两步之后的输出与Value分别想乘并结果想加，可以得到对应位置的Attention Value，在本实例中得到第一个值的过程大致如下： 分别得到三个Query对应的三个Attention Value的过程如下： 至此，Self-Attention的全流程以及实例完毕。 Applications Attention机制引入以来，主要用在以下三个场景中。 Sequence to sequence 引入了Attention的Seq2Seq结构如上图，Encoder不再是只保存最终的状态，而是保存整个Attention Value矩阵，在Decoder时，可以更好的得到所需要的结果。 Transformer Transformer是另一个使用到Attention的模型，更具体的说是Multi-head Attention。其结构图如下所示： 其中，Multi-head Attention是通过h个不同的线性变换对Query，Key以及Value进行投影，最后将不同的Attention结果拼接起来。其与加了mask的Attention原理如下图。 BERT BERT的网络架构是完全重用Transformer的Encoder部分，一个创新亮点就是训练中使用的目标函数，也就是Loss function的定义。这个创新虽小但是很关键。BERT的Loss function由两个子任务的Loss相加而得。第一个任务就是把一个句中的几个字mask掉，然后让模型去预测这几个字，看它的正确率。第二个Loss来源于预测两个句子是不是上下句。这是为了让模型拥有句子层面的语义相关性的判断。为了增加鲁棒性，这两个句子都有可能不是整句，而是片段。 除此之外，BERT的Input Embedding也是一大亮点，其考虑到Attention并没有考虑的位置信息，增加了Segment Embeddings和Position Embeddings，他们和Token Embeddings的和作为Input Embeddings。 The link of this page is http://home.meng.uno/articles/9d18641a/ . Welcome to reproduce it!","categories":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://home.meng.uno/categories/Natural-Language-Processing/"}],"tags":[{"name":"Attention","slug":"Attention","permalink":"http://home.meng.uno/tags/Attention/"},{"name":"Self-Attention","slug":"Self-Attention","permalink":"http://home.meng.uno/tags/Self-Attention/"},{"name":"Transformer","slug":"Transformer","permalink":"http://home.meng.uno/tags/Transformer/"},{"name":"BERT","slug":"BERT","permalink":"http://home.meng.uno/tags/BERT/"},{"name":"Seq2Seq","slug":"Seq2Seq","permalink":"http://home.meng.uno/tags/Seq2Seq/"}]},{"title":"MPhil历程以及两年的香港生活总结","slug":"mphil-application","date":"2020-09-30T05:49:24.000Z","updated":"2021-04-18T15:05:01.000Z","comments":true,"path":"articles/bf7d2a9d/","link":"","permalink":"http://home.meng.uno/articles/bf7d2a9d/","excerpt":"时间线 * 申请：入学前一年4 - 5月到入学当年的4月30日（秋季学期）或者8月31日（春季学期）。 * 报道：9月1日或者1月1日。 * Probation：要求是入学一年，但是很灵活，一般导师会要求投出去第一篇论文之后。 * 毕业：在当年9.23日之前提交final thesis则当年底毕业典礼领取毕业证，否则推迟到下一年领取毕业证。 大家主要还是会关心按时毕业的问题，所以在此我将我的毕业流程（我按时毕业）列举如下，大家可以参考。 1. MPhil Probation Talk：2020年4月20日。 2. 提交“Notice of Intent to Submit a","text":"时间线 申请：入学前一年4 - 5月到入学当年的4月30日（秋季学期）或者8月31日（春季学期）。 报道：9月1日或者1月1日。 Probation：要求是入学一年，但是很灵活，一般导师会要求投出去第一篇论文之后。 毕业：在当年9.23日之前提交final thesis则当年底毕业典礼领取毕业证，否则推迟到下一年领取毕业证。 大家主要还是会关心按时毕业的问题，所以在此我将我的毕业流程（我按时毕业）列举如下，大家可以参考。 MPhil Probation Talk：2020年4月20日。 提交“Notice of Intent to Submit a Thesis”：4月20日（要求是在论文初稿提交前3个月提交这个Notice，但是很灵活不用在意时间要求）。 提交论文初稿：6月24日（提交之前要找导师修改，以及让导师签字允许你提交初稿，还有重复率的问题，这些都是导师决定）。 答辩：8月13日（当你提交初稿之后一周，会收到学院发的邮件确定大致答辩日期，我的预计答辩日期是8月18日，但是具体时间和导师商量）。 答辩结果：8月17日（会附上各个reviewer的意见，说是将论文送到研究生院做最后决定）。 研究生院检查结果：8月27日（会附带一个checklist，告诉你需要提交哪些文件到学院）。 提交电子论文到图书馆：8月31日。 提交装订好的Thesis：9月3日（需要附带一份导师同意提交final bound thesis的同意书）。 毕业信：9月8日（上面会写清你的学位和学业完成时间以及毕业典礼时间即毕业证时间）。 申请 香港的MPhil和PhD一样需要先联系好导师，当导师确定要你，而你已经提交申请之后，大概两周可以来offer。 感想 本科毕业苦于英语不行，托福只考了82/120分，当时也是觉得来香港读MPhil作为申请美国PhD的跳板，于是就接受了导师的MPhil Offer。读MPhil期间，多次有机会转PhD，我都谢绝了，现在想来甚是后悔。 虽然网上都说MPhil可以作为申请美国PhD的跳板，但是我仔细调查发现，那些毕业的MPhil一大部分转本校PhD，一部分直接去工作，还有很少的一部分申请了其他学校的PhD，而这些学校也没有比香港的大学好很多，极少那种读了MPhil就申请美国牛校PhD的例子，现在发现自己当初有多么幼稚，不该相信网上的传言。 结合我自己的经历，我觉得那些（1）英语都过关（TOEFL100+，GRE320+），同时（2）有一部分研究经历，但是本科申美国牛校PhD失败了的那些大佬可能来香港读MPhil之后能申请到美国牛校，这两个条件任何一个不满足，可能MPhil的跳板作用就没有那么明显了。我就是同时不满足这两个条件，MPhil第一年结束根本不知道研究啥，没有任何研究产出，正是因为没有产出，所以更得把时间放在研究上，更没有时间准备英语。这就导致MPhil第一年结束我还是英语不行，科研不行，申美国PhD还是无望。我觉得如果有同学在这个时间点和我情况类似，那基本考虑转博吧，这是最好的打算。我当时没认清自己，还是觉得MPhil毕业了我再考英语，再申请。事实是，慌慌张张毕业了，又得找工作，根本没心情和时间准备英语。 确实，美国去不了，我也可以考虑欧洲啊。但是情况还是和我想的不一样，欧洲不要英语成绩（因为在以英语为授课语言的大学上过学），但是主要靠套词来申请PhD。而且有个更重要的问题，欧洲很多老师没钱，不招学生或者在MPhil毕业的时间点（9月）之前已经招满了！ 最后的打算只能是先工作啦。现在在广州微信做搜索算法，先暂时赚点以后的读博生活费啦。 香港生活Tips Outlets： 东荟城（东涌地铁站）； 海怡广场（海怡半岛地铁站）； 佛罗伦萨小镇（葵兴地铁站）。 化妆品/护肤品： DFS，CDF，连卡佛（贵点），其他大商场的专柜正品； Sasa、卓悦、龙丰等都是水货。 Sogo店庆： 大概5 - 6月， 11 - 12月 大商场： 海港城（尖沙咀）； 时代广场（铜锣湾）； ifcmall（中环）； 朗豪坊（旺角）； 新城市广场（旺角东）； 又一城（九龙塘）等。 开仓： 时代广场一座16楼； Sogo（铜锣湾）16楼； 海港城九仓电讯中心5楼。 奶茶店： 旺角基本都有，主推KOI（红茶玛奇朵）、comebuy tea等。 其他Tips 香港隔三差五都有品牌打折，一次不要买太多，(例如CK，Tommy去年一折，Aigle去年三折）。 香港用品比较贵（例如被子，餐具，书本等），可以淘宝，淘宝里面有些东西可以满199免运费，也有天猫超市满250左右免运费，也可以找第三方转运公司。 不要太相信卖保险的同学说的话，即使是学长学姐，不要相信代购。 租房多看看群，多看几家，不用找中介，一般5000小房（4-6平米）完全OK，第一年申宿舍基本都可以中。 出门必须带身份证！ 记得薅信用卡羊毛。 Ins在香港相对比较多人用，香港人一般使用WhatsApp聊天（不良分子也多）。 The link of this page is http://home.meng.uno/articles/bf7d2a9d/ . Welcome to reproduce it!","categories":[{"name":"Note","slug":"Note","permalink":"http://home.meng.uno/categories/Note/"}],"tags":[{"name":"MPhil","slug":"MPhil","permalink":"http://home.meng.uno/tags/MPhil/"},{"name":"留学申请","slug":"留学申请","permalink":"http://home.meng.uno/tags/留学申请/"},{"name":"按时毕业","slug":"按时毕业","permalink":"http://home.meng.uno/tags/按时毕业/"},{"name":"香港大学","slug":"香港大学","permalink":"http://home.meng.uno/tags/香港大学/"},{"name":"香港生活Tips","slug":"香港生活Tips","permalink":"http://home.meng.uno/tags/香港生活Tips/"}]},{"title":"2021届自己参加的秋招笔试题记录","slug":"2021-autumn","date":"2020-08-27T12:44:57.000Z","updated":"2020-12-02T09:19:51.000Z","comments":true,"path":"articles/fdc308a8/","link":"","permalink":"http://home.meng.uno/articles/fdc308a8/","excerpt":"2021届秋招正如火如荼地进行着，截止目前，我已经参加了绝大部分互联网企业的秋招，我想将自己写过的代码保存一下，供以后再来回味。 代码基本都是用Python写的，有些并没有100%AC。 不匹配括号数量 Input: 包含若干(, ), [, ]的字符串。 Output: 不能配对的括号的数量。 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def BracketsMatch(text, flag): arr = [] if flag == 1: left = '('","text":"2021届秋招正如火如荼地进行着，截止目前，我已经参加了绝大部分互联网企业的秋招，我想将自己写过的代码保存一下，供以后再来回味。 代码基本都是用Python写的，有些并没有100%AC。 不匹配括号数量 Input: 包含若干(, ), [, ]的字符串。 Output: 不能配对的括号的数量。 代码 12345678910111213141516171819202122 def BracketsMatch(text, flag): arr = [] if flag == 1: left = &apos;(&apos; right = &apos;)&apos; else: left = &apos;[&apos; right = &apos;]&apos; for c in text: if c == left: arr.append(c) elif c == right: if len(arr) == 0: arr.append(c) else: if arr[-1] == left: arr.pop() else: arr.append(c) return len(arr)text = input().strip()print(BracketsMatch(text, 0) + BracketsMatch(text, 1)) 分析 针对任意一对括号，可以用栈来存储无法匹配的括号。同时这种写法可以拓展到其他需要前后配对的字符上，例如{和}或者『和』等。 前m个值最大选项的序号 Input: 第一行n和m，n代表有几个物品，m代表取价值前m大；以后n行输入n个物品的重量和体积。 Output: 从小到大输出价值前m大的物品编号。 *物品价值 = 重量 + 2 × 体积 代码 12345678910111213141516171819202122232425 line1 = input().split(&apos; &apos;)n = int(line1[0])m = int(line1[1])matrix = [[0 for i in range(4)] for j in range(n)]for i in range(n): line_n = input().split(&apos; &apos;) matrix[i][0] = float(line_n[0]) matrix[i][1] = float(line_n[1]) matrix[i][2] = float(line_n[0]) + 2 * float(line_n[1]) matrix[i][3] = i + 1matrix = sorted(matrix, key = lambda item:item[2])[::-1]max_m = int(matrix[0][2])result_m = [[0 for i in range(n + 1)] for j in range(max_m + 1)]for i in range(len(matrix)): result_m[int(matrix[i][2])][int(matrix[i][3])] = 1result = []for i in range(max_m, -1, -1): for j in range(n + 1): if result_m[i][j] == 1: result.append(j)result = sorted(result[0 : m])strs = &apos;&apos;for i in range(m): strs += str(result[i]) + &apos; &apos;print(strs.strip()) 分析 类似于Excel之中排序，主排序标准和次排序标准。 0/1背包 0/1背包作为经典的动态规划问题，经常考察，但是只要掌握状态转移方程，动态规划基本上很少的代码就可以写出来。 代码 1234567891011121314 def Package(W, V, max_w): len_w = len(W) dp = [[0 for i in range(max_w + 1)] for j in range(len_w)] for i in range(len_w): for j in range(max_w + 1): if W[i] &gt; j: dp[i][j] = dp[i - 1][j] else: dp[i][j] = max(dp[i - 1][j - W[i]] + V[i], dp[i - 1][j]) return dp[-1][-1]W = [10, 20, 20, 30]V = [12, 18, 17, 32]max_w = 50print(Package(W, V, max_w)) 根据输入操作数字串 Input: 一行n和m，代表长度为n的值为下标的数字串；m行不同的操作。（1：将串首元素移动到最尾；2：交换第“2 × i + 1”和第“2 × i + 2”号元素，其中i从1开始。） Output: 操作结束的数字串 代码 123456789101112131415161718192021222324252627282930 line1 = input().split(&apos; &apos;)n = int(line1[0])m = int(line1[1])line2 = input().split(&apos; &apos;)line = []for i in range(n): line.append(i + 1)def OP1(line): new_line = [] for i in range(1, len(line)): new_line.append(line[i]) new_line.append(line[0]) return new_linedef OP2(line): for i in range(0, len(line), 2): t = line[i] line[i] = line[i + 1] line[i + 1] = t return linefor i in range(m): if int(line2[i]) == 1: line = OP1(line) elif int(line2[i]) == 2: line = OP2(line) else: line = lineret = &apos;&apos;for i in range(n): ret += str(line[i]) + &apos; &apos;print(ret) 分析 这种题目就是按照题目要求，写出不同操作的函数，然后调用就OK啦。在今年的秋招中，出现过很多次类似的题目，虽然操作不尽相同。 计算面积 求y = 0, x = C, x = D 与y = Ax^2 + x + B相交区域的面积。 Input: A，B，C和D。 Output: 面积（保留6位小数） 代码 1234567891011 n = int(input())ret = []def Area(a, b, c, d): right = (float(a) / 3) * d * d * d + 0.5 * d * d + float(b) * float(d) left = (float(a) / 3) * c * c * c + 0.5 * c * c + float(b) * float(c) return right - leftfor i in range(n): line = input().split(&apos; &apos;) ret.append(Area(int(line[0]), int(line[1]), int(line[2]), int(line[3])))for i in range(n): print(&quot;%.6f&quot; % ret[i]) 分析 这种题乍一看没有思路，需要结合数学知识，求面积就是求一定范围内的积分。当函数变化时，也是同样的做法。 全组合 Input: 一个正整数n。 Ouput: 从1到n的全组合，并且每种组合从组合中选一个代表的总可能数。 代码 1234567891011121314 n = int(input())ret = 0def C(n, i): ret = 1 d = 1 while i &gt; 0: ret *= n % (pow(10, 9) + 7) n -= 1 d *= i % (pow(10, 9) + 7) i -= 1 return int(ret/ d) % (pow(10, 9) + 7)for i in range(1, n + 1): ret += (i * C(n, i)) % (pow(10, 9) + 7)print(ret % (pow(10, 9) + 7)) 分析 全组合可以通过转化成2进制，然后数1的个数来确定，也可以使用从1到n组合求和来做。 求图中链接相同元素的节点对的数量 Input: 节点数n，边数m；以后m行，m条边。 Output: 链接相同元素的节点对的数量 代码 1234567891011121314151617181920 line = input().split(&apos; &apos;)n = int(line[0])m = int(line[1])matrix = [[0 for i in range(n + 1)] for j in range(n + 1)]for i in range(m): tmp = input().split(&apos; &apos;) matrix[int(tmp[0])][int(tmp[1])] = 1 matrix[int(tmp[1])][int(tmp[0])] = 1def Equal(list1, list2, n): flag = True for i in range(1, n + 1): if list1[i] != list2[i]: flag = False return flagret = 0for i in range(1, n + 1): for j in range(i + 1, n + 1): if Equal(matrix[i], matrix[j], n): ret += 1print(ret) 分析 我这里用了一种最笨的方法，将图存成邻接矩阵，然后两行完全相同（不包含全为0），则代表他们链接的节点相同。 蛇形输出 蛇形输出或者蛇形赋值在很多地方都出现了，在此提供一个模板。 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 def Right(i, j, right_end, bottom_end, ret, matrix): if j == right_end and i &lt; bottom_end: return Down(i, j, right_end, bottm_end, ret, matrix) elif j == right_end and i == bottom_end: return else: ret.append(matrix[i][j + 1]) if i == bottom_end: return RightUp(i, j + 1, right_end, bottom_end, ret, matrix) else: return LeftDown(i, j + 1, right_end, bottom_end, ret, matrix)def Down(i, j, right_end, bottom_end, ret, matrix): if i == bottom_end and j == right_end: return elif i == bottom_end and j &lt; right_end: Right(i, j, right_end, bottom_end, ret, matrix) else: ret.append(matrix[i + 1][j]) if j == right_end: return LeftDown(i + 1, j, right_end, bottom_end, ret, matrix) else: return RightUp(i + 1, j, right_end, bottom_end, ret, matrix)def LeftDown(i, j, right_end, bottom_end, ret, matrix): if i == bottom_end and j == right_end: return elif i == bottom_end: return Right(i, j, right_end, bottom_end, ret, matrix) elif j == 0: return Down(i, j, right_end, bottom_end, ret, matrix) else: ret.append(matrix[i + 1][j - 1]) return LeftDown(i + 1, j - 1, right_end, bottom_end, ret, matrix)def RightUp(i, j, right_end, bottom_end, ret, matrix): if i == bottom_end and j == right_end: return elif j == right_end: return Down(i, j, right_end, bottom_end, ret, matrix) elif i == 0: return Right(i, j, right_end, bottom_end, ret, matrix) else: ret.append(matrix[i - 1][j + 1]) return RightUp(i - 1, j + 1, right_end, bottom_end, ret, matrix)n = int(input())matrix = [[str(j) + &apos;+&apos; + str(i) for i in range(n)] for j in range(n)] ret = []ret.append(matrix[0][0])Down(0, 0, n - 1, n - 1, ret, matrix)print(ret) 子串长度 Input: 只包含小写字母的字符串。 Output: 包含连续两个“abcde”或者不包含“abcde”的子串最长长度。 代码 123456789101112131415161718 strs = input()chars = &apos;abcde&apos;dp = [0 for i in range(len(strs) + 1)]for i in range(len(strs)): if strs[i] not in chars: dp[i + 1] = dp[i] + 1 else: if i == 0: dp[i + 1] = dp[i] elif strs[i] == strs[i - 1]: dp[i + 1] = dp[i - 1] + 2 else: dp[i + 1] = 0max = 0for i in range(len(strs) + 1): if dp[i] &gt; max: max = dp[i]print(max) 字符串分割相等 Input: &quot;,&quot;分隔的两个字符串。 Output: 如果两个字符串相同或者分隔成子串相同，则输出“true”，否则输出“false”。 代码 123456789101112131415161718 line = input().split(&apos;,&apos;)str1 = line[0]str2 = line[1]def Equal(str1, str2): if len(str1) != len(str2): return False elif len(str1) == 1 and str1 != str2: return False elif str1 == str2: return True elif len(str1) % 2 == 1: return Equal(str1[0: int(len(str1) / 2)], str2[int(len(str2) / 2) - 1:]) and Equal(str2[0: int(len(str2) / 2) - 1], str1[int(len(str1) / 2):]) or (Equal(str1[0: int(len(str1) / 2) - 1], str2[int(len(str2) / 2):]) and Equal(str2[0: int(len(str2) / 2)], str1[int(len(str1) / 2) - 1:])) else: return Equal(str1[0: int(len(str1) / 2)], str2[int(len(str2) / 2):]) and Equal(str2[0: int(len(str2) / 2)], str1[int(len(str1) / 2):]) if Equal(str1, str2): print(&apos;true&apos;)else: print(&apos;false&apos;) 分析 递归的思想，如果当前串相同则返回True，否则将串二等分，看子串是否相等。 杨辉三角 Input: 行数n；每一行有2 × i - 1个数字，构成一个三角形。 Output: 从第一行走到最后一行最大的和。 代码 1234567891011121314 n = int(input().strip())matrix = [[0 for i in range(2 * n + 1)] for j in range(n + 1)]for i in range(1, n + 1): line = input().split(&apos; &apos;) for j in range(n + 1 - i, n + 1 - i + len(line)): matrix[i][j] = int(line[j - n + i - 1])dp = [[0 for i in range(2 * n + 1)] for j in range(n + 1)]for i in range(1, n + 1): for j in range(1, 2 * n): if i == 1: dp[i][j] = matrix[i][j] else: dp[i][j] = max(dp[i - 1][j - 1], dp[i - 1][j], dp[i - 1][j + 1]) + matrix[i][j]print(max(dp[-1])) 分析 比较经典的动态规划题。动态规划与递归的区别在于动态规划利用“最优子结构”这个特性，所以下一个状态由上一个状态推出，写代码时是从头往后，而递归则是从后往前。 回字形写入斐波拉契数列 Input: 矩阵的边长n。 Output: 回字形写入斐波拉契数列的n × n矩阵。 代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 def Trans(matrix): if len(matrix) == 0: return new_matrix = [] tmp_line = [] for j in range(len(matrix[0]) - 1, -1, -1): tmp_line = [] for i in range(len(matrix)): tmp_line.append(matrix[i][j]) new_matrix.append(tmp_line) return new_matrixret = []def ReadLine(matrix): if len(matrix) &gt; 1: ret.append(matrix[0]) return ReadLine(Trans(matrix[1:])) elif len(matrix) == 1: ret.append(matrix[0]) else: returnn = int(input()) matrix = [[&apos;&apos; for i in range(n)] for j in range(n)]for i in range(n): for j in range(n): matrix[i][j] = str(i) + &apos;+&apos; + str(j)ReadLine(matrix) m = n * ndef FB(m): fb_arr = [] fb_arr.append(0) for i in range(1, m + 1): if i == 1 or i == 2: fb_arr.append(1) else: fb_arr.append(fb_arr[i - 2] + fb_arr[i - 1]) return fb_arrfb_arr = FB(m)[::-1]ret_matrix = [[0 for i in range(n)] for j in range(n)]for i in range(len(ret)): for j in range(len(ret[i])): tmp = ret[i][j].split(&apos;+&apos;) t = int(tmp[0]) k = int(tmp[1]) ret_matrix[t][k] = fb_arr[0] fb_arr = fb_arr[1:]print(ret_matrix) 分析 回字形输出，等价于先输出矩阵第一行，然后将矩阵旋转（这个时候之前的最右列就变成了第一行）再输出，往复到矩阵为空。 前K大的数 求一堆数中前K大的数。 一般可以使用小顶堆来实现。用数组保存堆的时候：左子树 = 父节点 × 2；右子树 = 父节点 × 2 + 1。 代码 12345678910111213141516171819202122232425262728293031323334 k = 4k_tree = [-1 for i in range(0, k + 1)]def FindLeft(root): if root * 2 &lt;= k: return root * 2 return -1def FindRight(root): if root * 2 &lt;= k - 1: return root * 2 + 1 return -1def BuildTree(val, root): if val &lt;= k_tree[root]: return else: swap(val, root)def swap(val, root): if FindLeft(root) != -1 and val &lt;= k_tree[FindLeft(root)]: k_tree[root] = val elif FindRight(root) != -1 and val &gt;= k_tree[FindRight(root)]: k_tree[root] = k_tree[FindRight(root)] k_tree[FindRight(root)] = val swap(k_tree[root], root) swap(val, FindRight(root)) elif FindLeft(root) != -1 and FindRight(root) != -1 and val &gt; k_tree[FindLeft(root)] and val &lt; k_tree[FindRight(root)]: k_tree[root] = k_tree[FindLeft(root)] k_tree[FindLeft(root)] = val swap(val, FindLeft(root)) elif FindLeft(root) != -1 and FindRight(root) == -1 and val &gt;= k_tree[FindLeft(root)]: k_tree[root] = k_tree[FindLeft(root)] k_tree[FindLeft(root)] = val swap(val, FindLeft(root))for i in [12,32,53,45,6,7,6,8,9,9,324,6,5,4]: BuildTree(i, 1) print(k_tree) 两个有序的整数列表合并 简单版的归并排序。 代码 12345678910111213141516171819202122 def Sort(list1, list2): ret_list = [] len1 = len(list1) len2 = len(list2) idx1 = idx2 = 0 while idx1 &lt; len1 or idx2 &lt; len2: if idx1 &gt;= len1: ret_list.append(list2[idx2]) idx2 += 1 elif idx2 &gt;= len2: ret_list.append(list1[idx1]) idx1 += 1 elif list1[idx1] &lt; list2[idx2]: ret_list.append(list1[idx1]) idx1 += 1 else: ret_list.append(list2[idx2]) idx2 += 1 return ret_listlist1 = [1,2,5,6,7,32,44]list2 = [32,43,45,56,767]print(Sort(list1, list2)) 快速排序 快速排序作为排序的代表，原型或者其变型经常被考察。 代码 12345678910111213141516171819 def quick_sort(array, left, right): if left &gt;= right: return low = left high = right key = array[left] while left &lt; right: if left &lt; right and array[right] &gt; key: right -= 1 array[left] = array[right] if left &lt; right and array[left] &lt;= key: left += 1 array[right] = array[left] array[left] = key quick_sort(array, low, left - 1) quick_sort(array, left + 1, high)array = [1,4,2,3,45,46,6,57,68,79,98,223]quick_sort(array, 0, len(array) - 1)print(array) 矩阵查找 一个行、列都递增的矩阵中查找某个数。 代码 123456789101112131415161718 def solution(matrix, target): if not matrix: return False if not matrix[0]: return False if target &gt; matrix[-1][-1] or target &lt; matrix[0][0]: return False j = 0 i = len(matrix) - 1 while True: if i &lt; 0 or j &lt; 0 or i &gt;= len(matrix) or j &gt;= len(matrix[0]): return False if matrix[i][j] &gt; target: i -= 1 elif matrix[i][j] &lt; target: j += 1 else: return True 连续不为0的区间和最大 Input: 数组长度n；数组的n个值；m个不同分割下标。 Output: 分别输出m个以对应下标分割的子区间的连续不为0的最大和。 代码 12345678910111213141516171819202122232425 n = int(input())line2 = input().split(&apos; &apos;)line3 = input().split(&apos; &apos;)line = [0 for i in range(n + 1)]for i in range(n): line[i + 1] = int(line2[i])def BIG(sep, line): line[sep] = 0 ret_sep = [] tmp = 0 for i in range(n + 1): if line[i] == 0: ret_sep.append(tmp) tmp = 0 else: tmp += line[i] ret_sep.append(tmp) max = 0 for item in ret_sep: if item &gt; max: max = item return maxfor i in range(n): sep = int(line3[i]) print(BIG(sep, line)) 矩阵中“CHINA”的个数 Input: 矩阵大小n，以及n × n的字符矩阵。 Output: &quot;CHINA&quot;的个数。 代码 12345678910111213141516171819202122232425262728 n = int(input())n_lines = []for i in range(n): n_lines.append(input()) china_num = 0arr_i = [1, -1, 0, 0]arr_j = [0, 0, 1, -1]def DFS(i, j, len): if len == 5: china_num += 1 else: for t in range(4): new_i = i + arr_i[t] new_j = j + arr_j[t] if new_i &gt;= 0 and new_j &gt;= 0 and new_i &lt; n and new_j &lt; n: if len == 1 and n_lines[new_i][new_j] == &apos;H&apos;: DFS(new_i, new_j, len + 1) elif len == 2 and n_lines[new_i][new_j] == &apos;I&apos;: DFS(new_i, new_j, len + 1) elif len == 3 and n_lines[new_i][new_j] == &apos;N&apos;: DFS(new_i, new_j, len + 1) elif len == 4 and n_lines[new_i][new_j] == &apos;A&apos;: DFS(new_i, new_j, len + 1)for i in range(n): for j in range(len(n_lines[i])): if n_lines[i][j] == &apos;C&apos;: DFS(i, j, 1)print(china_num) 分析 在矩阵中可以上下左右组合的字符串或者递增子序列之类的问题，都可以使用DFS来做。 字符串中“Good”的数量 Input: 字符串。 Output: Good的数量 代码 1234567891011121314151617181920212223242526 line = input().strip()mark = [0 for i in range(len(line))]ret = 0for i in range(len(line)): if line[i] == &apos;G&apos;: mark[i] = 1 j = i o_count = 0 d_count = 0 while j &lt; len(line) and o_count &lt; 2: if line[j] == &apos;o&apos; and mark[j] == 0: o_count += 1 mark[j] = 1 j += 1 if o_count == 2: while j &lt; len(line) and d_count &lt; 1: if line[j] == &apos;d&apos; and mark[j] == 0: d_count += 1 j += 1 if d_count == 1: ret += 1 else: break else: breakprint(ret) 分析 字符串中存在某种子序列，做法大致相同。 矩阵的递增序列 Input: n × n矩阵。 Output: 最长递增序列的长度 代码 12345678910111213141516171819202122232425 n = int(input())matrix = [[0 for i in range(n)] for j in range(n)]for i in range(n): line = input().strip().split(&apos; &apos;) for j in range(n): matrix[i][j] = int(line[j])dfs = [[0 for i in range(n)] for j in range(n)]x_d = [0, 0, 1, -1]y_d = [1, -1, 0, 0]def DFS(x, y, dfs): if dfs[x][y] != 0: return dfs[x][y] for i in range(len(x_d)): new_x = x + x_d[i] new_y = y + y_d[i] if new_x &gt;= 0 and new_y &gt;= 0 and new_x &lt; n and new_y &lt; n: if matrix[new_x][new_y] &gt; matrix[x][y]: dfs[x][y] = max(DFS(new_x, new_y, dfs) + 1, dfs[x][y]) else: dfs[x][y] = max(1, dfs[x][y]) return dfs[x][y]for i in range(n): for j in range(n): dfs[i][j] = DFS(i, j, dfs)print(max(max(dfs))) 分析 一个很明显的思路就是从某点出发，DFS查找最长的递增路径，但是考虑到某些节点可能在之前已经遍历过，可以使用其保存的信息。 不重叠区间 Input: n个不同区间。 Output: 去掉多少区间之后，剩下的区间构成不重叠区间。 代码 1234567891011121314 interval_list = []n = int(input())for i in range(n): line = input().strip().split(&apos; &apos;) interval_list.append([int(line[0]), int(line[1])])interval_list = sorted(interval_list, key = lambda item:item[1])dp = [1 for i in range(n)]for i in range(n): for j in range(i - 1, -1, -1): if interval_list[i][0] &gt;= interval_list[j][1]: dp[i] = max(dp[j] + 1, dp[i]) else: dp[i] = max(dp[i], dp[j])print(n - dp[-1]) 机器人行走 Input: 有t组测试，每一组机器人从(0, 0)位置朝向向上，向右为x轴正方向，向下为y轴正方向，指令有：向左转(L)，向右转®，前进N步或者到头(G N)以及打印当前位置§。 Output: 打印每个Case的编号以及执行P指令之后的输出。 代码 12345678910111213141516171819202122232425262728293031323334353637383940414243 t = int(input())cur_x = 0cur_y = 0cur_t = 1Turns = [&apos;L&apos;, &apos;U&apos;, &apos;R&apos;, &apos;D&apos;]step_x = [0, -1, 0, 1]step_y = [-1, 0, 1, 0]def Turn(cur_t, turn): if turn == &apos;R&apos;: cur_t += 1 % len(Turns) else: cur_t -= 1 % len(Turns) return cur_t def Walk(cur_x, cur_y, cur_t, step, n): if cur_x + step * step_x[cur_t] &gt;= 0 and cur_x + step * step_x[cur_t] &lt; n and cur_y + step * step_y[cur_t] &gt;= 0 and cur_y + step * step_y[cur_t] &lt; n: return cur_x + step * step_x[cur_t], cur_y + step * step_y[cur_t] elif cur_x + step * step_x[cur_t] &lt; 0: cur_x = 0 elif cur_y + step * step_y[cur_t] &lt; 0: cur_x = 0 elif cur_x + step * step_x[cur_t] &gt;= n: cur_x = n - 1 elif cur_y + step * step_y[cur_t] &gt;= n: cur_y = n - 1 return cur_x, cur_yret = &apos;&apos; for times in range(t): ret += &apos;Case #&apos; + str(times + 1) + &apos;:\\n&apos; n_m = input().strip().split(&apos; &apos;) n = int(n_m[0]) m = int(n_m[1]) cur_x = 0 cur_y = 0 cur_t = 1 for i in range(m): line = input().strip().split(&apos; &apos;) if line[0] == &apos;L&apos; or line[0] == &apos;R&apos;: cur_t = Turn(cur_t, line[0]) elif line[0] == &apos;G&apos;: cur_x, cur_y = Walk(cur_x, cur_y, cur_t, int(line[1]), n) elif line[0] == &apos;P&apos;: ret += str(cur_y) + &apos; &apos; + str(cur_x) + &apos;\\n&apos;print(ret) 将0/1表示的字符串根据莫斯密码转化成字符串 以‘1’表示摩斯密码中的‘·’，以‘111’表示摩斯密码中的‘-’。 以‘0’表示莫斯密码中的分割。 以‘000’表示不同字符摩斯密码的分割。 以‘0000000’表示不同单词的分割。 Input: 以0/1表示的字符串 Output: 转化后的字符串 代码 123456789101112131415161718192021222324252627282930 line = input().strip()ret = &apos;&apos;last = 0letter = &apos;ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890&apos;ms = [&apos;01&apos;, &apos;1000&apos;, &apos;1010&apos;, &apos;100&apos;, &apos;0&apos;, &apos;0010&apos;, &apos;110&apos;, &apos;0000&apos;, &apos;00&apos;, &apos;0111&apos;, &apos;101&apos;, &apos;0100&apos;, &apos;11&apos;, &apos;10&apos;, &apos;111&apos;, &apos;0110&apos;, &apos;1101&apos;, &apos;010&apos;, &apos;000&apos;, &apos;1&apos;, &apos;001&apos;, &apos;0001&apos;, &apos;011&apos;, &apos;1001&apos;, &apos;1011&apos;, &apos;1100&apos;, &apos;01111&apos;, &apos;00111&apos;, &apos;00011&apos;, &apos;00001&apos;, &apos;0000&apos;, &apos;10000&apos;, &apos;11000&apos;, &apos;11100&apos;, &apos;11110&apos;, &apos;11111&apos;]def DealC(line): chr = &apos;&apos; for i in line.split(&apos;0&apos;): if i == &apos;1&apos;: chr += &apos;0&apos; else: chr += &apos;1&apos; for i in range(len(ms)): if ms[i] == chr: return letter[i]def DealL(line): ret = &apos;&apos; lst = 0 for i in range(len(line) - 3): if line[i: i + 3] == &apos;000&apos;: ret += str(DealC(line[lst: i])) lst = i + 3 ret += str(DealC(line[lst:])) return retfor i in range(len(line) - 7): if line[i:i + 7] == &apos;0000000&apos;: ret += str(DealL(line[last: i])) + &apos; &apos; last = i + 7ret += str(DealL(line[last:]))print(ret) 数n中各位数字全排列能整除数m的个数 Input: 两个数字n和m。 Output: 数n中各位数字全排列能整除数m的个数。 代码 123456789101112131415161718192021222324252627 line = input().split(&apos; &apos;)n = int(line[0])m = int(line[1])num_list = [0 for i in range(10)]lens = 0while n &gt; 0: num_list[n % 10] += 1 n //= 10 lens += 1ret = 0 def IN(num, num_list): tmp_num_list = [0 for i in range(10)] while num &gt; 0: tmp_num_list[num % 10] += 1 num //= 10 if tmp_num_list[0] &lt;= num_list[0]: for i in range(1, 10): if tmp_num_list[i] != num_list[i]: return False return True else: return Falsefor i in range(1, int(pow(10, lens) / m)): if IN(m * i, num_list): print(m * i) ret += 1print(ret) 分析 我的思路是通过逆向观察m倍数需要的那些数字是否都在n里面来判断n的各位全排列能否得到m的倍数。当然，如果n中0的个数大于m的倍数中0的个数是不影响的。 最长回文子串 Input: 字符串 Output: 最长回文子串 代码 12345678910111213 line = input().strip()dp = [0] * len(line)for i in range(1, len(line)): if line[i] == line[i - dp[i - 1] - 1] and i - dp[i - 1] - 1 &gt;= 0: dp[i] = dp[i - 1] + 2 elif line[i] == line[i - 2] and dp[i - 1] == 0: dp[i] = 3 else: dp[i] = 0for i in range(len(dp)): if max(dp) == dp[i]: print(line[i + 1 - max(dp): i + 1]) 分析 简单的动态规划，但是要考虑两种情况：（1）回文串长度为偶数；（2）回文串长度为奇数即中间的字母不用回文。 大鱼吃小鱼 Input: 长度为n的整数串 Output: 每次大鱼吃小鱼，可以同时进行，输出大鱼吃小鱼的次数。 代码 123456789101112131415161718192021 n = int(input())line = input().split(&apos; &apos;)int_list = [int(line[i]) for i in range(len(line))]live = [1] * len(int_list)left = len(int_list)new_left = 65535ret = -1while left &lt; new_left: new_left = left key = int_list[0] for i in range(1, len(int_list)): new_k = int_list[i] if key &gt; int_list[i]: int_list[i] = -1 key = new_k ret += 1 for i in range(len(int_list)- 1, -1, -1): if int_list[i] == -1: int_list.pop(i) left = len(int_list)print(ret) 非递归版二叉树深度 代码 12345678910111213141516171819202122232425262728293031323334 num =int(input())class TreeNode: def __init__(self, val = None, left = None,right = None): self.val = val self.left = left self.right = rightdef Depth(root): if root == None: return 0 else: queue = [] depth = 0 queue.append(root) while len(queue): depth += 1 cur = len(queue) tmp = 0 while(tmp &lt; cur): tmp += 1 node = queue.pop(0) if node.left != None: queue.append(node.left) if node.right != None: queue.append(node.right) return depthtreenode = [None for i in range(num)]for i in range(num): treenode[i] = TreeNode(i)for i in range(num): if i * 2 + 1 &lt; num: treenode[i].left = treenode[i * 2 + 1] if i * 2 + 2 &lt; num: treenode[i].right = treenode[i * 2 + 2]print(Depth(treenode[0])) 分析 二叉树深度就是二叉树层数，所以可以使用层序遍历二叉树实现。 数组中出现超过一半的数字 代码 1234567891011121314 def majorityElement(nums: List[int]) -&gt; int: dp = [0] * len(nums) dp[0] = 1 votes = nums[0] for i in range(1, len(nums)): if dp[i - 1] == 0: votes = nums[i] dp[i] = 1 continue if votes == nums[i]: dp[i] = dp[i - 1] + 1 else: dp[i] = dp[i - 1] - 1 return votes 分析 每一步设定当前值为“大于一半的数”并将此时的值记为“+1”，往后遇到不同的值在该值上“-1”，遇到值为0，则将前面的序列舍弃，重新开始计数。最终在序列扫描一遍之后的假设“大于一半的数”的值即为最终结果。 最长不含重复字符的子字符串 描述 从字符串中找出一个最长的不包含重复字符的子字符串，计算该最长子字符串的长度。 Input: &quot;abcabcbb&quot; Output: 3 Reason: 因为无重复字符的最长子串是 “abc”，所以其长度为 3。 代码 12345678910111213141516171819 class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: if len(s) &lt;= 1: return len(s) maxs = 1 dp = [1 for i in range(len(s))] def Find_i(s, idx): if idx == 0: return 0 for i in range(idx - 1, -1, -1): if s[i] == s[idx]: return i return -1 for i in range(1, len(s)): if dp[i - 1] &lt; i - 1 - Find_i(s, i): dp[i] = dp[i - 1] + 1 else: dp[i] = i - Find_i(s, i) return max(dp) 分析 可以用动态规划来解，首先定义函数Find_i，用来找到当前i节点往前最近与之相同的字符的位置（没有则返回-1）。 然后，状态转移方程为： dp[i] = dp[i - 1] + 1，当dp[i - 1] &lt; i - Find_i(s, i)时，否则： dp[i] = i - Find_i(s, i)。 最后返回dp最大值，即为所求。 The link of this page is http://home.meng.uno/articles/fdc308a8/ . Welcome to reproduce it!","categories":[{"name":"Code","slug":"Code","permalink":"http://home.meng.uno/categories/Code/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://home.meng.uno/tags/算法/"},{"name":"2021秋招","slug":"2021秋招","permalink":"http://home.meng.uno/tags/2021秋招/"},{"name":"算法工程师","slug":"算法工程师","permalink":"http://home.meng.uno/tags/算法工程师/"},{"name":"AI工程师","slug":"AI工程师","permalink":"http://home.meng.uno/tags/AI工程师/"},{"name":"编程","slug":"编程","permalink":"http://home.meng.uno/tags/编程/"}]},{"title":"Loss Functions","slug":"loss-func","date":"2019-06-04T18:12:13.000Z","updated":"2021-01-04T18:49:56.000Z","comments":true,"path":"articles/cf69cf57/","link":"","permalink":"http://home.meng.uno/articles/cf69cf57/","excerpt":"背景 在优化算法中，用于评估候选解决方案的函数（即一组权重）称为目标函数（Objective Function）。我们可能会寻求最大化或最小化目标函数，这意味着我们正在搜索分别具有最高或最低得分的候选解决方案。通常，使用神经网络，我们试图将误差最小化。这样，目标函数通常被称为成本函数（Cost Function）或损失函数（Loss Function），而由损失函数计算出的值简称为“损失（Loss）”。 损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测","text":"背景 在优化算法中，用于评估候选解决方案的函数（即一组权重）称为目标函数（Objective Function）。我们可能会寻求最大化或最小化目标函数，这意味着我们正在搜索分别具有最高或最低得分的候选解决方案。通常，使用神经网络，我们试图将误差最小化。这样，目标函数通常被称为成本函数（Cost Function）或损失函数（Loss Function），而由损失函数计算出的值简称为“损失（Loss）”。 损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。 一般损失函数 0-1损失函数(zero-one loss) 形式 0-1损失函数的计算是预测值和目标值不相等为1， 否则为0，数学形式为： 当然，这个条件有点太严苛，通常也可以方宽条件到|Y - f(x)| &lt; T，即： 出处 感知机 平均绝对值损失函数（Mean Absolute Error, MAE） 形式 实现 123456 def mean_absolute_error(actual, predicted): sum_absolute_error = 0.0 for i in range(len(actual)): sum_absolute_error += abs(actual[i] - predicted[i]) mean_absolute_error_ = 1.0 / len(actual) * sum_absolute_error return mean_absolute_error_ 平均平方损失函数（Mean Squared Error, MSE） 形式 出处 回归问题 实现 123456 def mean_squared_error(actual, predicted): sum_squared_error = 0.0 for i in range(len(actual)): sum_squared_error += (actual[i] - predicted[i])**2.0 mean_squared_error_ = 1.0 / len(actual) * sum_squared_error return mean_squared_error_ 交叉熵损失函数（Cross-Entropy Loss or Log Loss） 形式 多个类别的交叉熵一般形式为： 对于二分类问题，因为i只有1个值，所以也写成： 出处 分类问题 实现 二分类交叉熵 123456 def binary_cross_entropy(actual, predicted): sum_score = 0.0 for i in range(len(actual)): sum_score += actual[i] * log(1e-15 + predicted[i]) mean_sum_score = 1.0 / len(actual) * sum_score return -mean_sum_score 多分类交叉熵 1234567 def categorical_cross_entropy(actual, predicted): sum_score = 0.0 for i in range(len(actual)): for j in range(len(actual[i])): sum_score += actual[i][j] * log(1e-15 + predicted[i][j]) mean_sum_score = 1.0 / len(actual) * sum_score return -mean_sum_score 合页损失函数（Hinge Loss） 形式 其标准形式为： 因为其在坐标系中的图形形状像书页，所以称之为合页损失。 出处 支持向量机（SVM） 感知机损失函数（Perceptron Loss） 形式 其可以看成是简化版的Hinge Loss。 出处 感知机（Perceptron） The link of this page is http://home.meng.uno/articles/cf69cf57/ . Welcome to reproduce it!","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://home.meng.uno/categories/Deep-Learning/"}],"tags":[{"name":"Loss","slug":"Loss","permalink":"http://home.meng.uno/tags/Loss/"}]},{"title":"Activation Functions","slug":"activation-func","date":"2019-05-04T15:18:29.000Z","updated":"2021-01-04T16:09:43.000Z","comments":true,"path":"articles/7af9d25d/","link":"","permalink":"http://home.meng.uno/articles/7af9d25d/","excerpt":"背景 在不使用激活函数的情况下，神经网络的每一层节点的输入都是上层输出的线性函数。很容易验证，无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron），此时的神经网络拟合能力相当有限。正因为这个原因，我们需要引入非线性函数作为激活函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。 激活函数的主要形式 在此从形式以及存在的问题方面介绍几种常用的激活函数，Sigmoid，tanh，ReLU（包括LeakyReLU，pReLU，ELU），Maxout，Softmax以及Softplus。 Si","text":"背景 在不使用激活函数的情况下，神经网络的每一层节点的输入都是上层输出的线性函数。很容易验证，无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron），此时的神经网络拟合能力相当有限。正因为这个原因，我们需要引入非线性函数作为激活函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。 激活函数的主要形式 在此从形式以及存在的问题方面介绍几种常用的激活函数，Sigmoid，tanh，ReLU（包括LeakyReLU，pReLU，ELU），Maxout，Softmax以及Softplus。 Sigmoid 形式 我们从Sigmoid（S形曲线）激活函数说起，刚开始认识Sigmoid可能是从Logistic Regression开始，它的形式如下： $$ f(z)=\\frac{1}{1+e^{-z}} $$ 对应的坐标系表示为： 问题 有饱和区域，是软饱和，在大的正数和负数作为输入的时候，梯度就会变成零，使得神经元基本不能更新。 只有正数输出（不是zero-centered），这就导致所谓的zigzag现象。 计算量较大 tanh 形式 其作为改进版的Sigmoid，数学形式及对应的曲线如下： 问题 tanh是zero-centered，但是还是会饱和，这就导致在输入值较大或者较小时，倒数趋近于0。 ReLU（Rectified Linear Unit） 形式 ReLU的形式比较简单，省去了exp的计算，因而计算量小，收敛较快。当输入大于0的时候返回原输入值，当输入小于0的时候返回0，即f(x) = max(0, x)。 问题 非zero-centered。 在负数区域被killed的现象叫做Dying ReLU。 针对ReLU的改进 为了解决Dying ReLU问题，有三种比较常用的解决方案。 LeakyReLU LeakyReLU的思想是直接将x ≤ 0的部分修改为较小的0.01x，于是整体的形式就变成了f(x) = max(0.01x, x)。 pReLU (Parametric ReLU) 其形式为f(x) = max(ax, x)，其中a通过学习得到，而不是固定的。 ELU（Exponential Linear Unit） 其形式表示为：f(x) = max(α(exp(x) - 1), x)。 Maxout Maxout是通过分段线性函数来拟合所有可能的凸函数来作为激活函数的，但是由于线性函数是可学习，所以实际上是可以学出来的激活函数。具体操作是对所有线性取最大，也就是把若干直线的交点作为分段的界，然后每一段取最大。 其形式为： Softmax 也称归一化指数函数，其一般形式为： Sigmoid函数如果用来分类的话，只能进行二分类，而Softmax函数可以看做是Sigmoid函数的一般化，可以进行多分类。 Softplus 其一般形式为： $$ f(x) = \\ln \\left(1+e^{x}\\right) $$ The link of this page is http://home.meng.uno/articles/7af9d25d/ . Welcome to reproduce it!","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://home.meng.uno/categories/Deep-Learning/"}],"tags":[{"name":"Activation","slug":"Activation","permalink":"http://home.meng.uno/tags/Activation/"},{"name":"Sigmoid","slug":"Sigmoid","permalink":"http://home.meng.uno/tags/Sigmoid/"},{"name":"Tanh","slug":"Tanh","permalink":"http://home.meng.uno/tags/Tanh/"},{"name":"ReLU","slug":"ReLU","permalink":"http://home.meng.uno/tags/ReLU/"},{"name":"LeakyReLU","slug":"LeakyReLU","permalink":"http://home.meng.uno/tags/LeakyReLU/"},{"name":"pReLU","slug":"pReLU","permalink":"http://home.meng.uno/tags/pReLU/"},{"name":"ELU","slug":"ELU","permalink":"http://home.meng.uno/tags/ELU/"},{"name":"Maxout","slug":"Maxout","permalink":"http://home.meng.uno/tags/Maxout/"},{"name":"Softmax","slug":"Softmax","permalink":"http://home.meng.uno/tags/Softmax/"},{"name":"Softplus","slug":"Softplus","permalink":"http://home.meng.uno/tags/Softplus/"}]},{"title":"机器学习&深度学习速查表","slug":"code_dl","date":"2019-04-10T06:36:02.000Z","updated":"2020-12-02T01:42:05.000Z","comments":true,"path":"articles/5b828fc2/","link":"","permalink":"http://home.meng.uno/articles/5b828fc2/","excerpt":"基础 神经网络 线性代数 Python基础 scipy科学计算 Spark 数据保存及可视化 numpy pandas bokeh 画图 matplotlib ggplot 机器学习 sk-learn Keras TensorFlow 算法 数据结构 复杂度 排序算法 The link of this page is http://home.meng.uno/articles/5b828fc2/ . Welcome to reproduce it!","text":"基础 神经网络 线性代数 Python基础 scipy科学计算 Spark 数据保存及可视化 numpy pandas bokeh 画图 matplotlib ggplot 机器学习 sk-learn Keras TensorFlow 算法 数据结构 复杂度 排序算法 The link of this page is http://home.meng.uno/articles/5b828fc2/ . Welcome to reproduce it!","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://home.meng.uno/categories/Deep-Learning/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://home.meng.uno/tags/Deep-Learning/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://home.meng.uno/tags/Machine-Learning/"},{"name":"Python","slug":"Python","permalink":"http://home.meng.uno/tags/Python/"}]},{"title":"Recurrent Neural Networks (RNNs)","slug":"rnn-lstm","date":"2019-01-03T16:09:04.000Z","updated":"2021-01-04T06:58:37.000Z","comments":true,"path":"articles/14232dff/","link":"","permalink":"http://home.meng.uno/articles/14232dff/","excerpt":"循环神经网络（RNNs） 背景 RNNs在多种与序列相关的任务中都起到了很好的作用，其也是在处理时序问题的最常用的一类模型，这都归因于其计算当前时间片的时候，除了像普通DNNs使用到当前的输入信息之外，还使用到了前一个时间片的信息，其一般结构如下图所示。 其按时间片展开的形式如下图。 因为RNNs的考虑序列化信息的特性，使得其在许多序列化任务中发挥比较不错的效果，例如语音识别，Seq2Seq，甚至OCR（Computer Vision领域）。 形式 其数学形式可以表示为： $$ h_{t}=\\sigma\\left(x_{t} \\times W_{x t}+h_{t-1} \\ti","text":"循环神经网络（RNNs） 背景 RNNs在多种与序列相关的任务中都起到了很好的作用，其也是在处理时序问题的最常用的一类模型，这都归因于其计算当前时间片的时候，除了像普通DNNs使用到当前的输入信息之外，还使用到了前一个时间片的信息，其一般结构如下图所示。 其按时间片展开的形式如下图。 因为RNNs的考虑序列化信息的特性，使得其在许多序列化任务中发挥比较不错的效果，例如语音识别，Seq2Seq，甚至OCR（Computer Vision领域）。 形式 其数学形式可以表示为： $$ h_{t}=\\sigma\\left(x_{t} \\times W_{x t}+h_{t-1} \\times W_{h t}+b\\right) $$ 其时间片t - 1的输出不仅作为该时刻的预测值，同时也作为t时间片的隐状态。 (当然，也可以将传递状态和输出分开，这时预测值的形式为：$$y_{t}=\\sigma(W_{t} \\times h_{t})$$) 反向传播 RNN中反向传播使用的是名为Back Propagation Through Time (BPTT)方法，损失loss对参数W的梯度等于loss在各时间步对w偏导数的之和。用公式表示如下。 同时还有： 与 详细的反向传播过程参考 RNN BPTT。 问题 在反向传播的时候，在求时间片t到i的偏导时，结合以上两个式子，可以得到： 当t - i比较大时，问题来了，当$$\\sigma^{\\prime} W^{h} &gt; 1$$时，连乘的结果会比较大（梯度爆炸），相反，如果$$\\sigma^{\\prime} W^{h} &lt; 1$$，则会得到一个很小的值（梯度消失）。这种现象叫做RNNs的“长期依赖”。 解决方案 针对“梯度爆炸” 一般是采用梯度裁剪的方法。 针对“梯度消失” LSTM等改进的循环神经网络。 针对此问题的拓展 在深度学习发展的进程中，梯度弥散是一直存在的问题，正是因为这一问题使得深度学习模型的深度无法太深。在这个问题上也有各种不同的比较好的解决方案，例如： LSTM中的遗忘门 ReLU系激活函数 Residual Connection (Skip Connection) Batch Normalization 长短期记忆网络（Long Short Term Memory Network, LSTM） LSTM提出的动机是为了解决上面提到的“长期依赖”问题。传统的RNNs节点输出仅由权值，偏置以及激活函数决定，也就是说RNNs是一个链式结构，每个时间片使用的是相同的参数。而LSTM之所以能够一定程度上解决“长期依赖”问题，是因为LSTM引入了门（gate）机制用于控制特征的流通和遗忘。其形象化的展示如下图。 相对于RNNs，LSTM增加了一个cell state。只看LSTM一个cell state部分，如下图。 其形式可表述为： $$ C_{t}=f_{t} \\times C_{t-1}+i_{t} \\times \\tilde{C}_{t} $$ 其中$$f_t$$和$$i_t$$分别控制遗忘门和输入门参与计算的比例（他们经过了Sigmoid函数，输出结果在0 - 1之间。） $$f_t$$的计算可以表示为： $$ f_{t}=\\sigma\\left(W_{f} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{f}\\right) $$ 在图示中的位置如下图： $$i_t$$的计算可以表示为： $$ i_{t}=\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right) $$ 在图示中的位置如下： 状态更新值$$\\tilde{C}_{t}$$的数学表达式以及图示如下： 最后，预测值$$y_t$$和生成下个时间片的输入$$h_t$$，我们需要如下计算： $$ o_{t}=\\sigma\\left(W_{o}\\left[h_{t-1}, x_{t}\\right]+b_{o}\\right) $$ $$ h_{t}=o_{t} * \\tanh \\left(C_{t}\\right) $$ History总结 截止此处，RNNs，LSTM的发展脉络可以由下图总结。 参数计算 LSTM中参数的计算公式如下： 4 * [(embedding_size + hidden_size) * hidden_size + hidden_size] 其他RNNs的变型 双向RNNs (Bi-RNNs) 有时候，为了更好地理解上下文和消除歧义，可能需要从将来的时间步中学习，于是有学者提出了双向的循环神经网络，其结构如下图所示。 原理上讲，所有的循环神经网络及其变型都可以有双向形式。而且对于多层的RNNs，双向形式还可以有两种不同的形式，如下图所示。 GRU Gated Recurrent Unit (GRU)将LSTM中的遗忘门和输入门合成了一个单一的“更新门”。同样还混合了细胞状态C和隐藏状态h，和其他一些改动。最终的模型比标准的LSTM模型要简单，也是非常流行的变体。其形式化结构和数学形式如下： Peephole Connection LSTM 其网络结构及数学形式如下： 实际上是为LSTM中的每个门的输入增加一个cell state的信号。 耦合输入和遗忘门 (Coupled Input and Forget Gate, CIFG) 相比于原来的LSTM，输入门和遗忘门的因子$$i_t$$与$$f_t$$值没有太大的相互关系，在此使$$i_t + f_t = 1$$，于是其形式变为了： The link of this page is http://home.meng.uno/articles/14232dff/ . Welcome to reproduce it!","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://home.meng.uno/categories/Deep-Learning/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://home.meng.uno/tags/RNN/"},{"name":"LSTM","slug":"LSTM","permalink":"http://home.meng.uno/tags/LSTM/"},{"name":"GRU","slug":"GRU","permalink":"http://home.meng.uno/tags/GRU/"},{"name":"Peephole","slug":"Peephole","permalink":"http://home.meng.uno/tags/Peephole/"}]},{"title":"误差(Error)，偏差(Bias)，方差(Variance)与噪声(Noise)","slug":"noises","date":"2019-01-01T15:03:57.000Z","updated":"2021-01-01T15:12:05.000Z","comments":true,"path":"articles/undefined/","link":"","permalink":"http://home.meng.uno/articles/undefined/","excerpt":"“什么是偏差、方差与噪声，以及他们是什么关系？”是机器学习中经常遇到的问题。机器学习模型的泛化性能由算法的能力、数据的充分性以及学习任务本身的难度共同决定。对于给定的学习任务，为了取得更好的泛化性能，则需要使模型预测结果与真实值之间的偏差较小（即能够充分拟合数据），并且使不同测试集输出的方差较小（即使得数据扰动所造成的影响小）。 偏差（Bias） Bias是用所有可能的训练数据集训练出的所有模型的输出的平均值与真实模型的输出值之间的差异。 其衡量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。 其计算可以由下式表示： $$ [h(x)-f(x)] $$ 其中，f","text":"“什么是偏差、方差与噪声，以及他们是什么关系？”是机器学习中经常遇到的问题。机器学习模型的泛化性能由算法的能力、数据的充分性以及学习任务本身的难度共同决定。对于给定的学习任务，为了取得更好的泛化性能，则需要使模型预测结果与真实值之间的偏差较小（即能够充分拟合数据），并且使不同测试集输出的方差较小（即使得数据扰动所造成的影响小）。 偏差（Bias） Bias是用所有可能的训练数据集训练出的所有模型的输出的平均值与真实模型的输出值之间的差异。 其衡量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。 其计算可以由下式表示： $$ [h(x)-f(x)] $$ 其中，f(x)代表真实值，而h(x)代表算法的预测结果。 方差（Variance） Variance是不同的训练数据集训练出的模型输出值之间的差异。 其度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。 其期望的计算可以用下式表示： $$E_{P}\\left[(h(\\mathrm{x})-\\bar{h}(\\mathrm{x}))^{2}\\right]$$ 其中，h(x)指的是某一次训练的输出值。 偏差与方差的关系 两者的关系可以由下图表示： 这幅图可以看出，两者可以组成四种关系，这四种关系也对应着机器学习模型的四种不同表现。 Low Bias, Low Variance: 好模型； High Bias, High Variance: 差模型； High Bias, Low Variance: 欠拟合； Low Bias, High Variance: 过拟合。 于是，从偏差、方差的角度解释过拟合、欠拟合： 欠拟合：模型不能适配训练样本，有一个很大的偏差。 过拟合：模型很好的适配训练样本，但在测试集上有一个很大的方差。 减小偏差、方差的方法 首先，第一个角度当然是从处理欠拟合和过拟合出发，这个角度的策略有很多，例如下面列举的几条。 欠拟合：寻找更好的特征以及增大特征的维度； 过拟合：增大训练集，减少数据维度，正则化，交叉验证等。 另一个角度就是集成学习，而集成学习有两个主要的分支Bagging和Boosting。 Bagging: 这种集成的思想本身就是取多个预测结果的众数，从一定程度上是为了减小方差； Boosting:这种集成思想是不断的调整学习的方向，自然是为了不断的减小偏差。 偏差-方差窘境（bias-variance dilemma） 能得到方差和偏差都很小的模型当然是最好的结果，但是事实是我们总是要取得两者的一个平衡，两者的关系可以用下图表示，这也被称为偏差-方差窘境。 随着模型复杂度的提升，Bias会不断减小，但是Variance会不断增大，泛化误差将会呈现出先减小后增大的趋势，这个泛化误差的最小值点当然是我们的追求，所以我们在训练一个机器学习模型的时候需要平衡偏差与方差。 噪声（Noise） 噪声的数学期望可以由下式表示： $$E_{D}\\left[\\left(u_{D}-y\\right)^{2}\\right]$$ 噪声的存在是机器学习算法所无法解决的问题，数据的质量决定了学习的上限。假设在数据已经给定的情况下，此时上限已定，我们要做的就是尽可能的接近这个上限。 怎么减小噪声 因为噪声是数据本身的问题，所以减小噪声自然要从提高数据质量上做文章。一般而言，我们假设数据或者噪声都服从正态分布，其中噪声服从以0为均值的正态分布。且数据由正式值和噪声的和组成。即： $$g_{t}=x_{t}+\\varepsilon_{t}$$ 数据平滑法 当我们将t时刻前后n个值求平均，作为当前时刻的平滑之后的值时，得： 因为根据我们的假设，噪声服从0为均值的正态分布，其期望为0： $$\\sum_{i=1}^{n}\\left(\\varepsilon_{t-i}+\\varepsilon_{t+i}\\right)+\\varepsilon_{t} = 0$$ 数据过滤法 既然数据服从正态分布，即： $$\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right)$$ 那么，根据数学知识我们知道，当数据距离期望越远的时候，其越容易成为噪声点，于是我们可以将值偏离期望一定值（例如三倍标准差）的数据过滤掉。具体代码可以参考下例： 123456789101112131415161718192021222324252627282930313233343536373839 class ReduceNoise(): def __init__(self, data): self.data = data self.len = len(data) self.dim = len(data[0]) def getAvg(self, mat): num = [0.0] * self.dim for i in range(self.dim): for j in mat: num[i] += j[i] num[i] = num[i] / self.len return num def getVar(self, average, mat): ListMat = [] for i in mat: ListMat.append(list(map(lambda x: x[0] - x[1], zip(average, i)))) num = [0] * self.dim for j in range(self.dim): for i in ListMat: num[j] += i[j] * i[j] num[j] /= self.len return num def getSD(self, mat): return list(map(lambda x:x**0.5,mat)) def reduce(self): average = self.getAvg(self.data) variance = self.getVar(average, self.data) sd = self.getSD(variance) section = list(map(lambda x: x[0] + 3*x[1], zip(average, sd))) noiseMat = [] nonoiseMat=[] for i in self.data: for j in range(self.dim): if i[j] &gt; section[j]: noiseMat.append(i) break if j == (self.dim - 1): nonoiseMat.append(i) return noiseMat, nonoiseMat 泛化误差与三者的关系 可以证明泛化误差与偏差、方差以及噪声的数学期望的关系为：","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://home.meng.uno/categories/Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://home.meng.uno/tags/机器学习/"},{"name":"噪声","slug":"噪声","permalink":"http://home.meng.uno/tags/噪声/"},{"name":"偏差","slug":"偏差","permalink":"http://home.meng.uno/tags/偏差/"},{"name":"方差","slug":"方差","permalink":"http://home.meng.uno/tags/方差/"}]},{"title":"FastSP: linear time calculation of alignment accuracy","slug":"fastsp","date":"2018-10-27T12:31:08.000Z","updated":"2020-12-02T01:46:33.000Z","comments":true,"path":"articles/bd4539a2/","link":"","permalink":"http://home.meng.uno/articles/bd4539a2/","excerpt":"A General Observation From the result, we could see that: 1 2 3 4 5 SP-Score: 19/26 = 0.7307… Modeler: 19/27 = 0.7037… SP-FN: (26 - 19)/26 = 0.2692…. SP-FP: (27 - 19)/27 = 0.2962…. TC: 3/8 = 0.375 Approach From the last section, we know our propose is to calculate: 1 2 3 4 5 1. Number of s","text":"A General Observation From the result, we could see that: 12345 SP-Score: 19/26 = 0.7307…Modeler: 19/27 = 0.7037…SP-FN: (26 - 19)/26 = 0.2692….SP-FP: (27 - 19)/27 = 0.2962….TC: 3/8 = 0.375 Approach From the last section, we know our propose is to calculate: 12345 1. Number of shared homologies.2. Number of homologies in the reference alignment.3. Number of homologies in the estimated alignment.4. Number of correctly aligned columns.5. Number of aligned columns in the reference alignment. At the same time, we have a general thinking which is: 123 2,3 —&gt; 15 is the easiest1 — &gt; 4 First of all, it gives these definitions: 123 Si represent the i-th sequence in the alignment.Ai represent the i-th alignment.Ni,j represent the j-th site in Si. I will give you an example: Then, we could know these (Explaining an example): Algorithm This section is for programming. Let’s look at some difinitions first: 1234567 n represent the number of sequences in the alignment.k represent the biggest length of sequences in the alignment.S[i,j] represent a n•k matrix which equals (a, b) means Ni,j appears in site a for the reference alignment and in site b for the estimated alignment.bx represent the number of non-gapped entries in the x-th site.mi represent the number of elements in the i-th equivalence class.Nx represent the number of homologies in the estimated alignment that are shared with the x-th site in the reference alignment. hi represent the number of homologous pairs in alignment Ai. Explaining them by this example: The pseudo code is like this: Make a mapping: The result could be: 123 SP-Score: N/h1Modeler: N/h2TC: cor_num/k Evaluation Calculating the matrix S: O(nk) Calculating combination number: O(1) Calculating each Nx: O(n) As for the FOR loop: O(nk) So: The time complexity is O(n•k). The space complexity is O(n•k). Comparing to the other programs: The link of this page is http://home.meng.uno/articles/bd4539a2/ . Welcome to reproduce it!","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"http://home.meng.uno/categories/Bioinformatics/"}],"tags":[{"name":"MSA","slug":"MSA","permalink":"http://home.meng.uno/tags/MSA/"},{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"http://home.meng.uno/tags/Bioinformatics/"},{"name":"TC-score","slug":"TC-score","permalink":"http://home.meng.uno/tags/TC-score/"},{"name":"SP-score","slug":"SP-score","permalink":"http://home.meng.uno/tags/SP-score/"}]},{"title":"Implementation details of TensorFlow","slug":"tensorflow","date":"2018-10-02T02:45:47.000Z","updated":"2020-12-02T02:08:06.000Z","comments":true,"path":"articles/717ad116/","link":"","permalink":"http://home.meng.uno/articles/717ad116/","excerpt":"Brief introduction to TensorFlow * Open source * A second-generation machine learning system * Developed by Google * Improving flexibility and portability, speed and scalability * A framework for implementing and executing machine learning algorithms * In the form of a tensor flowing over a Gr","text":"Brief introduction to TensorFlow Open source A second-generation machine learning system Developed by Google Improving flexibility and portability, speed and scalability A framework for implementing and executing machine learning algorithms In the form of a tensor flowing over a Graph Stars of open source Deep Learning platforms in GitHub The architecture of TensorFlow Front-end: Provide programming model, responsible for the construction of computational graphs, Python, C++ and other language support. Back-end: Provide the runtime environment, responsible for executing the calculation diagram, and using C++. Code directory organization structure graph: Calculate flow graph related operations, such as construct, partition, optimize, execute, etc. kernels: Opkernels, such as matmul, conv2d, argmax, batch_norm, etc. ops: basic operations, gradient operation, IO related ops, control flow and data flow operation. eigen3: eigen matrix operation library, TensorFlow foundation operations’ call. TensorFlow programming mode TensorFlow uses symbolic programming. Symbolic programming abstracts the calculation process into a graph, and all input nodes, operation nodes and output nodes are symbolized. Symbolic programming is more efficient in memory and computation. Symbolic programming programs either explicitly or implicitly contain compilation steps, wrapping previously defined computational diagrams into callable functions, whereas the actual calculation occurs after compilation. Basic concepts of TensorFlow Use Graph to represent the calculation process. Execution diagram in Session. Using Tensor to represent data. Using Variable to maintain state. Use Feed and Fetch to assign or extract data from any operation. Graph is a description of the computation process and needs to be run in Session. TensorFlow provides a Feed mechanism to import data from outside, in addition to using Variable and Constant to import data. Tensor, that is, any dimension of data, one-dimensional, two-dimensional, three-dimensional, four-dimensional data collectively known as tensor. TensorFlow refers to keeping data nodes unchanged and allowing data to flow. An Example: TensorFlow implementation process The construction of a graph Creating a graph to represent and train the neural network in this phase. The execution of the graph The training operations in the diagram is executed repeatedly at this stage. Brief summary of TensorFlow TensorFlow is a programming system that represents computation as a graph. The nodes in the graph are called ops (operation). A ops uses 0 or more Tensors to generate 0 or more Tensors by performing some operations. A Tensor is a multidimensional array. For example, you can represent a batch of images as a four-dimensional array [batch, height, width, channels], with floating-point values. TensorFlow uses the tensor data structure (which is actually a multidimensional data) to represent all the data and pass it between the nodes in the graph calculation. A tensor has a fixed type, level, and size, and you can refer to Rank, Shape, and Type for a deeper understanding of these concepts. The link of this page is http://home.meng.uno/articles/717ad116/ . Welcome to reproduce it!","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://home.meng.uno/categories/Deep-Learning/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://home.meng.uno/tags/Deep-Learning/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://home.meng.uno/tags/TensorFlow/"}]},{"title":"前端开发小细节","slug":"front","date":"2018-08-15T14:09:21.000Z","updated":"2021-01-06T18:37:43.000Z","comments":true,"path":"articles/2b207973/","link":"","permalink":"http://home.meng.uno/articles/2b207973/","excerpt":"Ajax页面跳转并传入数据 在暑假进行的项目 医疗文本处理平台 中，我需要将队友的Ajax写好的功能整合到我的界面上，因为他之前是一个界面，而我想做成像搜索引擎那种，在一个页面上输入搜索词，跳转到另一个页面显示结果（后跳转界面还可以继续通过Ajax获得新的搜索结果） 在我们把后台服务化后，前端跨平台化之前，我们还需要了解前台和后台之间怎么通讯。从现有的一些技术上来看，Ajax是比较受欢迎的。 Ajax AJAX 即 “Asynchronous JavaScript And XML”（异步 JavaScript 和 XML），是指一种创建交互式网页应用的网页开发技术。这个功能在之前的很多年来","text":"Ajax页面跳转并传入数据 在暑假进行的项目 医疗文本处理平台 中，我需要将队友的Ajax写好的功能整合到我的界面上，因为他之前是一个界面，而我想做成像搜索引擎那种，在一个页面上输入搜索词，跳转到另一个页面显示结果（后跳转界面还可以继续通过Ajax获得新的搜索结果） 在我们把后台服务化后，前端跨平台化之前，我们还需要了解前台和后台之间怎么通讯。从现有的一些技术上来看，Ajax是比较受欢迎的。 Ajax AJAX 即 “Asynchronous JavaScript And XML”（异步 JavaScript 和 XML），是指一种创建交互式网页应用的网页开发技术。这个功能在之前的很多年来一直被 Web 开发者所忽视，直到最近 Gmail、Google Suggest 和 Google Maps 的出现，才使人们开始意识到其重要性。通过在后台与服务器进行少量数据交换，AJAX 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。传统的网页如果需要更新内容，必须重载整个网页页面。 Ajax 请求 说起 Ajax，我们就需要用 JavaScript 向服务器发送一个 HTTP 请求。这个过程要从 XMLHttpRequest 开始说起，它是一个 JavaScript 对象。它最初由微软设计，随后被 Mozilla、Apple 和 Google 采纳。如今，该对象已经被 W3C 组织标准化。 如下的所示的是一个 Ajax 请求的示例代码： 12345678 var xhr = new XMLHttpRequest();xhr.onreadystatechange = function() &#123; if (xhr.readyState == XMLHttpRequest.DONE) &#123; alert(xhr.responseText); &#125;&#125;xhr.open('GET', 'http://example.com', true);xhr.send(null); 我们只需要简单的创建一个请求对象实例，打开一个 URL，然后发送这个请求。当传输完毕后，结果的 HTTP 状态以及返回的响应内容也可以从请求对象中获取。 而这个返回的内容可以是多种格式，如 XML 和 JSON，但是从近年的趋势来看，XML 基本上已经很少看到了。这里我们以 JSON 为主，来简单地介绍一下返回数据的解析。 JSON JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式。它基于 ECMAScript 的一个子集。 JSON采用完全独立于语言的文本格式，但是也使用了类似于 C 语言家族的习惯（包括 C、C++、C#、Java、JavaScript、Perl、Python等）。这些特性使 JSON 成为理想的数据交换语言。易于人阅读和编写，同时也易于机器解析和生成(一般用于提升网络传输速率)。 XML VS JSON JSON 格式的数据具有以下的一些特点： 容易阅读 解析速度更快 占用空间更少 如下所示的是一个简单的对比过程： 1 myJSON = &#123;&quot;age&quot; : 12, &quot;name&quot; : &quot;Danielle&quot;&#125; 如果我们要取出上面数值中的age，那么我们只需要这样做： 12 anObject = JSON.parse(myJSON);anObject.age === 12 // True 同样的，对于 XML 来说，我们有下面的格式: 1234 &lt;person&gt; &lt;age&gt;12&lt;/age&gt; &lt;name&gt;Danielle&lt;/name&gt;&lt;/person&gt; 而如果我们要取出上面数据中的age的值，他将是这样的： 1234 myObject = parseThatXMLPlease();thePeople = myObject.getChildren(&quot;person&quot;);thePerson = thePeople[0];thePerson.getChildren(&quot;age&quot;)[0].value() == &quot;12&quot; // True 对比一下，我们可以发现XML的数据不仅仅解析上比较麻烦，而且还繁琐。而且还有个问题，XML解析的时候，一般xml的起始语句前不能有空行。 JSON WEB Tokens JSON Web Token (JWT) 是一种基于 token 的认证方案。 在人们大规模地开始 Web 应用的时候，我们在授权的时候遇到了一些问题，而这些问题不是 Cookie 所能解决的。Cookie 存在一些明显的问题：不能支持跨域、并且不是无状态的、不能使用CDN、与系统耦合等等。除了解决上面的问题，它还可以提高性能等等。基于 Session 的授权机制需要服务端来保存这个状态，而使用 JWT 则可以跳过这个问题，并且使我们设计出来的 API 满足 RESTful 规范。即，我们 API 的状态应该是没有状态的。因此人们提出了 JWT 来解决这一系列的问题。 通过 JWT 我们可以更方便地写出适用于前端应用的认证方案，如登陆、注册这些功能。当我们使用 JWT 来实现我们的注册、登陆功能时，我们在登陆的时候将向我们的服务器发送我们的用户名和密码，服务器验证后将生成对应的 Token。在下次我们进行页面操作的时候，如访问 /Dashboard 时，发出的 HTTP 请求的 Header 中会包含这个 Token。服务器在接收到请求后，将对这个 Token 进行验证并判断这个 Token 是否已经过期了。 实例 在这里将前一个页面命名为A，后一个为B（什么文件格式不重要，只要是静态页面就成）。 A中的JavaScript代码 12345678910111213 &lt;script type=&quot;text/javascript&quot;&gt; function jumpOnClick(flag) &#123; url = &quot;section3_2.jsp?text=&quot; + encodeURIComponent(document.getElementById(&apos;search&apos;).value) + &quot;&amp;flag=&quot; + flag;if(document.getElementById(&apos;search&apos;).value.match(&quot;\\\\s+&quot;) || document.getElementById(&apos;search&apos;).value == null || document.getElementById(&apos;search&apos;).value == &quot;&quot;)&#123; alert(&quot;请输入症状或问题后点击相应查询按钮！&quot;); return; &#125; //网页跳转 location.href = url; window.event.returnValue=false; &#125; &lt;/script&gt; 在A中，我将调用放到了按钮的onclick中。 B中的JavaScript代码 123456789101112131415161718192021222324252627282930 &lt;script type=&quot;text/javascript&quot;&gt; function GetUrlParam() &#123; var url = document.location.toString(); var arrObj = url.split(&quot;?&quot;); var text,flag; if (arrObj.length &gt; 1) &#123; var arrPara = arrObj[1].split(&quot;&amp;&quot;); var arr; for (var i = 0; i &lt; arrPara.length; i++) &#123; arr = arrPara[i].split(&quot;=&quot;); if (arr != null &amp;&amp; arr[0] == &quot;text&quot;) &#123; text = decodeURIComponent(arr[1]); flag = 0; if(text == &quot;&quot; || text == null)&#123; return; &#125; var psel = document.getElementById(&quot;kw&quot;); psel.value = text; //设置 &#125;else if(arr != null &amp;&amp; arr[0] == &quot;flag&quot;)&#123; flag = arr[1]; &#125; &#125; if(flag == 1)&#123; searchOnClick(text); &#125;else if(flag == 2)&#123; search2OnClick(text); &#125; &#125; &#125;&lt;/script&gt; 在B中，我将调用放到了body的onload中。 至此完成上述功能，并且保证了在后跳入页面上刷新时，不会因为保留了跳入内容而无法刷新的情况。 H5网页失去焦点Title改变的方法 这里要分享的其实是一个API：visibilitychange 这个 API 本身非常简单，由以下三部分组成。 document.hidden：表示页面是否隐藏的布尔值。页面隐藏包括 页面在后台标签页中 或者 浏览器最小化 （注意，页面被其他软件遮盖并不算隐藏，比如打开的 sublime 遮住了浏览器）。 document.visibilityState：表示下面 4 个可能状态的值 hidden：页面在后台标签页中或者浏览器最小化 visible：页面在前台标签页中 prerender：页面在屏幕外执行预渲染处理 document.hidden 的值为 true unloaded：页面正在从内存中卸载 Visibilitychange事件：当文档从可见变为不可见或者从不可见变为可见时，会触发该事件。 这样，我们可以监听 Visibilitychange 事件，当该事件触发时，获取 document.hidden 的值，根据该值进行页面一些事件的处理。 123456789101112131415161718192021 &lt;!DOCTYPE html&gt;&lt;html lang=\"zh-CN\"&gt; &lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" /&gt; &lt;title&gt;这是原来的title&lt;/title&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1\"/&gt; &lt;script&gt; var tmptitle = document.title; document.addEventListener('visibilitychange', function() &#123; var isHidden = document.hidden; if (isHidden) &#123; document.title = '当焦点不在当前窗口时的网页标题'; &#125; else &#123; document.title = tmptitle; &#125; &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;/body&gt;&lt;/html&gt; JavaScript操作DOM 创建节点 除了可以使用createElement创建元素，也可以使用createTextNode创建文本节点。document.body指向的是&lt;body&gt;元素，document.documentElement则指向&lt;html&gt;元素。 123456 //创建节点 var createNode = document.createElement(\"div\"); var createTextNode = document.createTextNode(\"hello world\"); createNode.appendChild(createTextNode); document.body.appendChild(createNode); document.documentElement.appendChild(createNode); 插入节点 可以使用appendChild，insertBefore，insertBefore接收两个参数，第一个是插入的节点，第二个是参照节点，如insertBefore(a,b)，则a会插入在b的前面 123456 //插入节点 var createNode = document.createElement(\"div\");var createTextNode = document.createTextNode(\"hello world\");createNode.appendChild(createTextNode);var div1 = document.getElementById(\"div1\");document.body.insertBefore(createNode,div1); 替换和删除元素 从replaceChild和removeChild的字面意思看，就是删除子节点，因此调用者，需要包含子节点div1，不然调用会报错。返回的节点是替换的或删除的元素，被替换/删除的元素仍然存在，但document中已经没有他们的位置了。 1234 //替换元素 var replaceChild = document.body.replaceChild(createNode,div1);//删除元素 var removeChild = document.body.removeChild(div1); 节点的属性 firstChild:第一个子节点 lastChild:最后一个子节点 childNodes:子节点集合，获取其中子节点可 someNode.childNodes[index]或 someNode.childNodes.item(index) nextSibling:下一个兄弟节点 previousSibling：上一个兄弟节点 parentNode：父节点 123456 &lt;ul id=\"ul\"&gt;&lt;li&gt;sdsssssss&lt;/li&gt;&lt;li&gt;qqqq&lt;/li&gt;&lt;li&gt;wwww&lt;/li&gt;&lt;li&gt;eeee&lt;/li&gt;&lt;/ul&gt; 12345678910111213141516 //节点属性 var ul = document.getElementById(\"ul\"); var firstChild = ul.firstChild; console.log(firstChild.innerHTML); var lastChild = ul.lastChild; console.log(lastChild.innerHTML); var length = ul.childNodes.length; console.log(length); var secondChild = ul.childNodes.item(1); console.log(secondChild.innerHTML); var forthChild = ul.childNodes.item(2).nextSibling; console.log(forthChild.innerHTML); var thridChild = forthChild.previousSibling; console.log(thridChild.innerHTML); var parentNode = forthChild.parentNode; console.log(parentNode.innerHTML); 文档片段 好处在于减少dom的渲染次数，可以优化性能。 12345678910 //文本片段 var fragment = document.createDocumentFragment(); var ul = document.getElementById(\"ul\"); var li = null; for (var i = 4; i &gt;= 0; i--) &#123; li = document.createElement(\"li\"); li.appendChild(document.createTextNode(\"item \"+i)); fragment.appendChild(li); &#125; ul.appendChild(fragment); 克隆元素 someNode.cloneNode(true):深度克隆，会复制节点及整个子节点 someNode.cloneNode(false):浅克隆，会复制节点，但不复制子节点 123 //克隆var clone = ul.cloneNode(true);document.body.appendChild(clone); 需要注意的问题 childNodes.length存在跨浏览器的问题 可以看到有关列表的html片段没有用 123456 &lt;ul id=\"ul\"&gt;&lt;li&gt;sdsssssss&lt;/li&gt;&lt;li&gt;qqqq&lt;/li&gt;&lt;li&gt;wwww&lt;/li&gt;&lt;li&gt;eeee&lt;/li&gt;&lt;/ul&gt; 这种书写格式而是使用没有换行的格式书写，是因为在不同的浏览器中，获取ul.childNodes.length的结果有差异： 在ie中，ul.childNodes.length不会计算li之间的换行空格，从而得到数值为4 在ff、chrome,safari中，会有包含li之间的空白符的5个文本节点，因此ul.childNodes.length为9 若要解决跨浏览器问题，可以将li之间的换行去掉，改成一行书写格式。 cloneNode存在跨浏览器的问题 在IE中，通过cloneNode方法复制的元素，会复制事件处理程序，比如，var b = a.cloneNode(true).若a存在click,mouseover等事件监听，则b也会拥有这些事件监听。 在ff,chrome,safari中，通过cloneNode方法复制的元素，只会复制特性，其他一切都不会复制 因此，若要解决跨浏览器问题，在复制前，最好先移除事件处理程序。 JavaScript将页面导出为图片 2016-12-12那天心血来潮，突然想将我们组开发的网站上的“导出Excel”功能做一点拓展，于是就想能不能直接将网页表格导出为图片！ 在我的不懈搜索后（搜索过程中绝大部分博客上的博文要么相互抄袭要么没什么用），终于得到了“canvas2image.js”这个神奇的JavaScript脚本，具体使用办法见如下代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 &lt;!doctype html&gt;&lt;html&gt;&lt;script src=\"canvas2image.js\"&gt;&lt;/script&gt;&lt;body&gt; &lt;canvas id=\"cvs\"&gt;&lt;/canvas&gt; &lt;button id=\"save\"&gt;save&lt;/button&gt;&lt;script&gt; var canvas, ctx, bMouseIsDown = false, iLastX, iLastY, $save, $imgs, $convert, $imgW, $imgH, $sel; function init ()&#123; canvas = document.getElementById('cvs'); ctx = canvas.getContext('2d'); $save = document.getElementById('save'); $convert = document.getElementById('convert'); $sel = \"png\"; $imgs = document.getElementById('imgs'); $imgW = 1980; $imgH = 2000; bind(); draw(); &#125; function bind () &#123; canvas.onmousedown = function(e) &#123; bMouseIsDown = true; iLastX = e.clientX - canvas.offsetLeft + (window.pageXOffset||document.body.scrollLeft||document.documentElement.scrollLeft); iLastY = e.clientY - canvas.offsetTop + (window.pageYOffset||document.body.scrollTop||document.documentElement.scrollTop); &#125; canvas.onmouseup = function() &#123; bMouseIsDown = false; iLastX = -1; iLastY = -1; &#125; canvas.onmousemove = function(e) &#123; if (bMouseIsDown) &#123; var iX = e.clientX - canvas.offsetLeft + (window.pageXOffset||document.body.scrollLeft||document.documentElement.scrollLeft); var iY = e.clientY - canvas.offsetTop + (window.pageYOffset||document.body.scrollTop||document.documentElement.scrollTop); ctx.moveTo(iLastX, iLastY); ctx.lineTo(iX, iY); ctx.stroke(); iLastX = iX; iLastY = iY; &#125; &#125;; $save.onclick = function (e) &#123; var type = $sel.value, w = $imgW.value, h = $imgH.value; Canvas2Image.saveAsImage(canvas, w, h, type); &#125; &#125; onload = init;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; Web表格搜索 虽然表格的排列相当困难，但表格的搜索却非常容易。增加一个搜索输入，如果那里的值匹配到了任意一行的文本，则显示该行，并隐藏其他所有的行。使用jQuery来实现就像下面这么简单： 12345 var allRows = $(\"tr\");$(\"input#search\").on(\"keydown keyup\", function() &#123; allRows.hide(); $(\"tr:contains('\" + $(this).val() + \"')\").show();&#125;); 没有看错，就是这么简单，如果是在实际应用中，可以这样来写： 先声明一个按钮： 1 &lt;input type=\"search\" id=\"search\" placeholder=\"请输入内容……\"&gt; 在input框之后加入以下JavaScript代码： 123456789101112131415 &lt;script&gt;// Quick Table Search$('#search').keyup(function() &#123; var regex = new RegExp($('#search').val(), \"i\"); var rows = $('table tr:gt(0)'); rows.each(function (index) &#123; title = $(this).children(\"#title\").html() if (title.search(regex) != -1) &#123; $(this).show(); &#125; else &#123; $(this).hide(); &#125; &#125;);&#125;);&lt;/script&gt; 浮出层使用 进行《软件工程》大项目时，遇到想在主页上仿Google首页上的那种“一个输入框+两个按钮”，同时一个按钮搜索、一个按钮上传文件，于是到网上查了好久，都没有关于“一个按钮实现文件上传”的功能介绍，最后只能使用“通过一个浮出层弹出上传文件”这种方式，下面贴出JavaScript代码。 在按钮中使用时只需要这样使用就行： 1 &lt;a href=\"javascript:_iframe() %&gt;')\" class=\"button\"&gt;点击进入我的博客&lt;/a&gt; JavaScript代码如下： 123456789101112 &lt;script&gt; function _iframe() &#123; zeroModal.show(&#123; title: '我的博客', iframe: true, url: 'http://www.meng.uno', width: '60%', height: '60%', cancel: true &#125;); &#125; &lt;/script&gt; 多个input合并提交到后台 需求 我在做我们的《软件工程》作业时，遇到了这样一个问题：我们需要打开一个表，这个表的列数不确定，但要增加增加行的操作。 实现 于是，需求产生了，我需要将前端的多个input标签内容合并成一个字符串来进行提交，我看了几个比较牛的方法（json、ognl……）但是好像与我们的需求偏的有点远（如果可以实现，欢迎留言），最后，没办法只能自己想，由于我还是会一点JavaScript的，所以我就想用JavaScript实现，在尝试了很多次之后，终于成功了，在此先贴上代码。 123456789101112131415161718192021222324 &lt;script type=\"text/javascript\"&gt;function n(n)&#123; var num=\"\"; for(var i=0;i&lt;n;i++)&#123; num += document.getElementById(\"num\"+i).value; &#125;document.getElementById(\"result\").value = num;&#125;&lt;/script&gt;&lt;% int num=3;%&gt;&lt;form action=\"addAction\"&gt;&lt;input id=\"num\" name=\"num\" value=&lt;%=num %&gt; type=\"text\"&gt;&lt;% for(int i=0;i&lt;num;i++)&#123;%&gt;&lt;input id=\"num&lt;%=i %&gt;\" type=\"text\" onblur=\"n(&lt;%=num%&gt;)\"&gt;&lt;% &#125;%&gt;&lt;input name=\"str\" id=\"result\" type=\"hidden\" &gt;&lt;button &gt;提交&lt;/button&gt;&lt;/form&gt; 分析 最后来分析，到底是怎么实现的，其实道理特别简单，就是JavaScript获取input的个数，然后一个循环，将所有input合并，并且给到一个“hidden”的input里，在后台接收这个input就可以了。 Hexo搭建博客中的小技巧 Hexo + GitHub 可能是一组比较简单的搭建博客的方式了。在使用Hexo的这么长的时间里，我发现很多不舒适的问题，通过搜索与探索，基本解决，现在分享出来。 添加404页面 有人可能也有这个疑问，通过GitHub发布的静态网页，没有路由，怎么识别404错误呢？GitHub已经替我们想到了这些，我们只需要在我们的网站主目录添加404.html，就可以实现出错自动跳转到404界面了，关于404界面的书写，大家可以直接下载我的404页面：http://www.meng.uno/404.html 排除编译某文件 排除HTML等格式 在添加了404页面后，最简单的方法就是将404.html放到本地博客的public目录下。但是有个问题，也是最重要的，就是每次hexo clean之后，public/文件夹就会被删除，这样的话每次都需要粘贴进去，自然费时费力。 一个简单方法就是，将404.html等html文件，直接放到博客source或者主题source目录下，再hexo g生成，就自动生成了，但是又有个问题，虽然我们的html已经是完整的网页了，但是还是被hexo嵌入到系统的主题下。那怎么办呢？方法如下： 123 layout: falsetitle: &quot;404&quot;--- 将上述三行代码添加到不想被嵌入的HTML等文件的最前面。 排除.md格式 在不跟踪了HTML文件后，我们是不是又想用同样的方法，让hexo不要编译readme.md文件呢？ 从网上的信息看，大致有三种方法： 将Readme.md 改名Readme；（在GitHub显示上有些问题） 将Readme.md改名Read.mdown；（完美） 在_config.yml文件中添加skip_render:\\n - README.md。（这种方法也适用于其他文件） JavaScript加载图片 &lt;img src=&quot;xx.jpg&quot; /&gt;是每个前端开发都会的技能。然而，如果你想做到极致，事情还没有这么简单。 滚屏加载 这是最容易想到的点，也是一开始就准备做的。 随web体验的进步，滚屏加载代替分页加载的情形越来越多。也就是先预留图片位置，而不去加载图片，直到这个预留区域滚动到屏幕中，用户能看见了，才去加载图片。如此一来，有“按需加载”的意味，由于图片加载延后，不抢占带宽，在打开页面的首屏会快很多。我们称之为“懒加载(lazyload)”。 其实现也很简单，在html里面写&lt;img lazy-src=&quot;xx.jpg&quot; /&gt;，然后用js去判断这个节点是否出现在屏幕中，如果是，则取出lazy-src属性，赋值成&lt;img lazy-src=&quot;xx.jpg&quot; src=&quot;xx.jpg&quot;/&gt;，触发此节点onload，这就实现最简单的滚屏加载了。 特殊状态处理 特殊状态有两种：加载中与加载失败。这两种情况的处理逻辑相类似，拿加载中的逻辑做例子。 图片触发加载，到图片加载完成（或失败）之间，肯定会有一段时间。不做处理的话，用户在等待的过程中，就只能看到空白的区域，非常的奇怪。在低网速，以及用户非常快的拉滚动条的情形下，这种现象将更加明显。 那么在触发onload之前，就需要补一些逻辑，展示对应的loading图。 将需要处理的img节点作为参数，调用tempImg函数，克隆一个节点强行插在img之前，用于loading中的展示。 123456789101112 var tempImg = function(target)&#123; var w = target.width(); var h = target.height(); var tempDom = target.clone().addClass(&quot;lazy-loding&quot;).insertBefore(target); if(w/h == 1)&#123; tempDom[0].src = &quot;http://9.url.cn/edu/img/img-loading.png&quot;; &#125;else&#123; tempDom[0].src = &quot;http://9.url.cn/edu/img/img-loading2.png&quot;; &#125; target.hide();&#125; 上报监控 这一步在大型前端项目中非常重要，也是经常被忽略的地方。尽管需要简易的后台配合，但不算麻烦的上报监控，能让产品更加稳定和健壮。 我在两个地方用到了上报。其一是图片加载失败，触发onerror时，这样一来我们能知道每天图片拉取失败的量；其二是图片加载的时间，能够帮助我们分析cdn服务是否异常，分析全国慢速用户比例等等。 而所谓的上报其实就是一个http请求，我会大概把这些信息带上： 123456 log(&#123; &apos;type&apos;: &apos;error&apos;, &apos;msg&apos;: &apos;lazyload拉取图片失败上报 &apos;, &apos;url&apos;: window.location.href, &apos;pid&apos;: 414342 //产品对应的id&#125;); 居中截取 这是前端无可避免的一个问题，先来说下此问题的背景。 由于我们是先用一个空白的img标签占位，再去加载图片，如果图片的高度特别长（比如新浪长微博），加载完成时就会撑开节点，引起滚动条的跳动。由于移动端屏幕较PC窄，一个跳动就可能让你找不到前一秒正在浏览的内容，这种体验尤其严重。在移动端的web设计中，可以看到许多知名互联网公司的产品，也经常忽略这一点。 因此我们可以限定占位区域的size，以此区域来做居中截取。当占位区域与图片最终展示同宽同高时，就不会引起跳动，而且也保持了视觉的一致性。 其原理如下，先判断是竖向长型图，还是横向长型图，根据不同的情况，优先让宽或高填充满占位区域，然后通过不同的负margin去实现居中。 123456789101112131415 var calSize = function($img) &#123; var w = $img.width(), h = $img.height(), width = size[0], height = size[1]; if(w+h == 0) return; //如果是长型图，优先适配宽度，高度居中截取 if(w/h &gt; width/height)&#123; var newWidth = height * w / h; var margin = (width - newWidth)/2; $img.height(height).css(&#123;&quot;margin-left&quot;: margin&#125;); &#125;else&#123; var newHeight = width * h / w; var margin = (height - newHeight)/2; $img.width(width).css(&#123;&quot;margin-top&quot;: margin&#125;); &#125;&#125; 支持.webp .webp格式图片是Google开发的一种旨在加快图片加载速度的图片格式，压缩提交大概只有jpg的2/3。随chrome的比例越来越多，其实让更多用户体验到.webp也是一件好事。 那么问题来了，怎么去判断用户的浏览器是否支持webp呢？ 根据ua去判断是个好方法，但不太靠谱，因为chrome中其实也有设置，让它不能去支持webp，而且webkit本身就开源，会衍生出很多你不知道名字的浏览器。 最终我使用的是特性检测： 12345678910111213141516 if(!supportedWebPIsLoading) &#123; supportedWebPIsLoading = true; var images = &#123; basic: &quot;data:image/webp;base64,UklGRjIAAABXRUJQVlA4ICYAAACyAgCdASoCAAEALmk0mk0iIiIiIgBoSygABc6zbAAA/v56QAAAAA==&quot; &#125;, $img = new Image(); $img.onload = function () &#123; supportedWebPIsLoading = false; $.cookie.set(&quot;iswebp&quot; , +supportedWebP); &#125;; $img.onerror = function () &#123; supportedWebP = false; supportedWebPIsLoading = false; $.cookie.set(&quot;iswebp&quot; , +supportedWebP); &#125;; $img.src = images.basic;&#125; 我们会让浏览器试着加载一张非常小的base64格式的webp图片，如果能够正常加载，说明是支持webp的。 并且，会把测试记录在cookie里，所以第二次直接从cookie里读结果，基本不会影响性能。完成了最重要的检查，我们就可以放心让服务器返回不同格式的图片了。 The link of this page is http://home.meng.uno/articles/2b207973/ . Welcome to reproduce it!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://home.meng.uno/categories/Software-Engineering/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://home.meng.uno/tags/Java/"},{"name":"Web","slug":"Web","permalink":"http://home.meng.uno/tags/Web/"},{"name":"前端","slug":"前端","permalink":"http://home.meng.uno/tags/前端/"},{"name":"Ajax","slug":"Ajax","permalink":"http://home.meng.uno/tags/Ajax/"},{"name":"H5","slug":"H5","permalink":"http://home.meng.uno/tags/H5/"},{"name":"Title","slug":"Title","permalink":"http://home.meng.uno/tags/Title/"},{"name":"焦点","slug":"焦点","permalink":"http://home.meng.uno/tags/焦点/"},{"name":"DOM","slug":"DOM","permalink":"http://home.meng.uno/tags/DOM/"},{"name":"导出图片","slug":"导出图片","permalink":"http://home.meng.uno/tags/导出图片/"},{"name":"canvas","slug":"canvas","permalink":"http://home.meng.uno/tags/canvas/"},{"name":"表格搜索","slug":"表格搜索","permalink":"http://home.meng.uno/tags/表格搜索/"},{"name":"浮出层","slug":"浮出层","permalink":"http://home.meng.uno/tags/浮出层/"},{"name":"合并input","slug":"合并input","permalink":"http://home.meng.uno/tags/合并input/"},{"name":"Hexo","slug":"Hexo","permalink":"http://home.meng.uno/tags/Hexo/"},{"name":"博客","slug":"博客","permalink":"http://home.meng.uno/tags/博客/"},{"name":"加载图片","slug":"加载图片","permalink":"http://home.meng.uno/tags/加载图片/"}]},{"title":"Knowledge Graph in Search Engine","slug":"KnowledgeGraph","date":"2018-08-07T07:05:54.000Z","updated":"2021-01-04T05:31:28.000Z","comments":true,"path":"articles/349dc05d/","link":"","permalink":"http://home.meng.uno/articles/349dc05d/","excerpt":"简介 近两年来，随着Linking Open Data等项目的全面展开，语义Web数据源的数量激增，大量RDF数据被发布。互联网正从仅包含网页和网页之间超链接的文档万维网(Document Web)转变成包含大量描述各种实体和实体之间丰富关系的数据万维网(Data Web)。在这个背景下，Google、百度和搜狗等搜索引擎公司纷纷以此为基础构建知识图谱，分别命名为Knowledge Graph、知心和知立方，来改进搜索质量，从而拉开了语义搜索的序幕。下面将从以下几个方面来介绍知识图谱：知识图谱的表示和在搜索中的展现形式，知识图谱的构建和知识图谱在搜索中的应用等。 知识图谱的表示和在搜索中的展","text":"简介 近两年来，随着Linking Open Data等项目的全面展开，语义Web数据源的数量激增，大量RDF数据被发布。互联网正从仅包含网页和网页之间超链接的文档万维网(Document Web)转变成包含大量描述各种实体和实体之间丰富关系的数据万维网(Data Web)。在这个背景下，Google、百度和搜狗等搜索引擎公司纷纷以此为基础构建知识图谱，分别命名为Knowledge Graph、知心和知立方，来改进搜索质量，从而拉开了语义搜索的序幕。下面将从以下几个方面来介绍知识图谱：知识图谱的表示和在搜索中的展现形式，知识图谱的构建和知识图谱在搜索中的应用等。 知识图谱的表示和在搜索中的展现形式 知识图谱旨在描述真实世界中存在的各种实体或概念。其中，每个实体或概念用一个全局唯一确定的ID来标识，称为它们的标识符(identifier)。每个属性-值对(attribute-value pair，又称AVP)用来刻画实体的内在特性，而关系(relation)用来连接两个实体，刻画它们之间的关联。知识图谱亦可被看作是一张巨大的图，图中的节点表示实体或概念，而图中的边则由属性或关系构成。上述图模型可用W3C提出的资源描述框架RDF或属性图(property graph)来表示。知识图谱率先由Google提出，以提高其搜索的质量。 为了更好地理解知识图谱，我们先来看一下其在搜索中的展现形式，即知识卡片(又称Knowledge Card)。知识卡片旨在为用户提供更多与搜索内容相关的信息。更具体地说，知识卡片为用户查询中所包含的实体或返回的答案提供详细的结构化摘要。从某种意义来说，它是特定于查询(query specific)的知识图谱。例如，当在搜索引擎中输入“姚明”作为关键词时，我们发现搜索结果页面的右侧原先用于置放广告的地方被知识卡片所取代。广告被移至左上角，而广告下面则显示的是传统的搜索结果，即匹配关键词的文档列表。这个布局上的微调也预示着各大搜索引擎在提高用户体验和直接返回答案方面的决心。 相关名词解释 Knowledge Base：通常翻译为“知识库”。知识库是人工智能的经典概念之一。最早是作为专家系统（Expert System）的组成部分，用于支持推理。知识库中的知识有很多种不同的形式，例如本体知识、关联性知识、规则库、案例知识等。相比于知识库的概念，知识图谱更加侧重关联性知识的构建，如三元组。 The Semantic Web ：通常翻译为“语义网”或“语义互联网”。语义互联网的核心内涵是：Web不仅仅要通过超链接把文本页面链接起来，还应该把事物链接起来，使得搜索引擎可以直接对事物进行搜索，而不仅仅是对网页进行搜索。谷歌知识图谱是语义互联网这一理念的商业化实现。也可以把语义互联网看做是一个基于互联网共同构建的全球知识库。 Linked Data：通常翻译为“链接数据”，是为了强调语义互联网的目的是要建立数据之间的链接，而非仅仅是把结构化的数据发布到网上。他为建立数据之间的链接制定了四个原则。从理念上讲，链接数据最接近于知识图谱的概念。但很多商业知识图谱的具体实现并不一定完全遵循Tim所提出的那四个原则。 Semantic Net / Semantic Network：通常翻译为“语义网络”或“语义网”，这个翻译通常被与Semantic Web的翻译混淆起来，为了以示区别，这里采用“语义网络”的翻译。WordNet是最典型的语义网络。相比起知识图谱，早期的语义网络更加侧重描述概念以及概念之间的关系，而知识图谱更加强调数据或事物之间的链接。 Ontology：通常翻译为“本体”。本体本身是个哲学名词。在上个世纪80年代，人工智能研究人员将这一概念引入了计算机领域。通俗点讲，本体相似于数据库中的Schema，主要用来定义类和关系，以及类层次和关系层次等。OWL是最常用的本体描述语言。本体通常被用来为知识图谱定义Schema。 知识图谱的构建 知识图谱的规模 据不完全统计，Google知识图谱到目前为止包含了5亿个实体和35亿条事实(形如实体-属性-值，和实体-关系-实体)。其知识图谱是面向全球的，因此包含了实体和相关事实的多语言描述。不过相比占主导的英语外，仅包含其他语言(如中文)的知识图谱的规模则小了很多。与此不同的是，百度和搜狗主要针对中文搜索推出知识图谱，其知识库中的知识也主要以中文来描述，其规模略小于Google的。 知识图谱的数据来源 为了提高搜索质量，特别是提供如对话搜索和复杂问答等新的搜索体验，我们不仅要求知识图谱包含大量高质量的常识性知识，还要能及时发现并添加新的知识。在这种背景下，知识图谱通过收集来自百科类站点和各种垂直站点的结构化数据来覆盖大部分常识性知识。这些数据普遍质量较高，更新比较慢。而另一方面，知识图谱通过从各种半结构化数据(形如HTML表格)抽取相关实体的属性-值对来丰富实体的描述。此外，通过搜索日志(query log)发现新的实体或新的实体属性从而不断扩展知识图谱的覆盖率。相比高质量的常识性知识，通过数据挖掘抽取得到的知识数据更大，更能反映当前用户的查询需求并能及时发现最新的实体或事实，但其质量相对较差，存在一定的错误。这些知识利用互联网的冗余性在后续的挖掘中通过投票或其他聚合算法来评估其置信度，并通过人工审核加入到知识图谱中。 百科类数据 维基百科，通过协同编辑，已经成为最大的在线百科全书，其质量与大英百科媲美。可以通过以下方式来从维基百科中获取所需的内容： 通过文章页面(Article Page)抽取各种实体; 通过重定向页面(Redirect Page)获得这些实体的同义词(又称Synonym); 通过去歧义页面(Disambiguation Page)和内链锚文本(Internal Link Anchor Text)获得它们的同音异义词(又称Homonym); 通过概念页面(Category Page)获得各种概念以及其上下位(subclass)关系; 通过文章页面关联的开放分类抽取实体所对应的类别; 通过信息框(Infobox)抽取实体所对应的属性-值对和关系-实体对。 类似地，从百度百科和互动百科抽取各种中文知识来弥补维基百科中文数据不足的缺陷。此外，Freebase是另一个重要的百科类的数据源，其包含超过3900万个实体(其称为Topics)和18亿条事实，规模远大于维基百科。对比之前提及的知识图谱的规模，我们发现仅Freebase一个数据源就构成了Google知识图谱的半壁江山。更为重要的是，维基百科所编辑的是各种词条，这些词条以文章的形式来展现，包含各种半结构化信息，需要通过事先制定的规则来抽取知识;而Freebase则直接编辑知识，包括实体及其包含的属性和关系，以及实体所属的类型等结构化信息。因此，不需要通过任何抽取规则即可获得高质量的知识。虽然开发Freebase的母公司MetaWeb于2010年被Google收购，Freebase还是作为开放的知识管理平台独立运行。所以百度和搜狗也将Freebase加入到其知识图谱中。 结构化数据 除了百科类的数据，各大搜索引擎公司在构建知识图谱时，还考虑其他结构化数据。其中，LOD项目在发布各种语义数据的同时，通过owl:sameAs将新发布的语义数据中涉及的实体和LOD中已有数据源所包含的潜在同一实体进行关联，从而实现了手工的实体对齐(entity alignment)。LOD不仅包括如DBpedia和YAGO等通用语义数据集，还包括如MusicBrainz和DrugBank等特定领域的知识库。因此，Google等通过整合LOD中的(部分)语义数据提高知识的覆盖率，尤其是垂直领域的各种知识。此外，Web上存在大量高质量的垂直领域站点(如电商网站，点评网站等)，这些站点被称为Deep Web。它们通过动态网页技术将保存在数据库中的各种领域相关的结构化数据以HTML表格的形式展现给用户。各大搜索引擎公司通过收购这些站点或购买其数据来进一步扩充其知识图谱在特定领域的知识。 这样做出于三方面原因： 大量爬取这些站点的数据会占据大量带宽，导致这些站点无法被正常访问; 爬取全站点数据可能会涉及知识产权纠纷; 相比静态网页的爬取，Deep Web爬虫需要通过表单填充(Form Filling)技术来获取相关内容，且解析这些页面中包含的结构化信息需要额外的自动化抽取算法。 半结构化数据挖掘AVP 虽然从Deep Web爬取数据并解析其中所包含的结构化信息面临很大的挑战，各大搜索引擎公司仍在这方面投入了大量精力。一方面，Web上存在大量长尾的结构化站点，这些站点提供的数据与最主流的相关领域站点所提供的内容具有很强的互补性，因此对这些长尾站点进行大规模的信息抽取(尤其是实体相关的属性-值对的抽取)对于知识图谱所含内容的扩展是非常有价值的。另一方面，中文百科类的站点(如百度百科等)的结构化程度远不如维基百科，能通过信息框获得AVP的实体非常稀少，大量属性-值对隐含在一些列表或表格中。一个切实可行的做法是构建面向站点的包装器(Site-specific Wrapper)。其背后的基本思想是：**一个Deep Web站点中的各种页面由统一的程序动态生成，具有类似的布局和结构。**利用这一点，我们仅需从当前待抽取站点采样并标注几个典型详细页面(Detailed Pages)，利用这些页面通过模式学习算法(Pattern Learning)自动构建出一个或多个以类Xpath表示的模式，然后将其应用在该站点的其他详细页面中从而实现自动化的AVP抽取。对于百科类站点，我们可以将具有相同类别的页面作为某个“虚拟”站点，并使用类似的方法进行实体AVP的抽取。自动学习获得的模式并非完美，可能会遗漏部分重要的属性，也可能产生错误的抽取结果。为了应对这个问题，搜索引擎公司往往通过构建工具来可视化这些模式，并人工调整或新增合适的模式用于抽取。此外，通过人工评估抽取的结果，将那些抽取结果不令人满意的典型页面进行再标注来更新训练样本，从而达到主动学习(Active Learning)的目的。 通过搜索日志进行实体和实体属性等挖掘 搜索日志是搜索引擎公司积累的宝贵财富。一条搜索日志形如**&lt;查询，点击的页面链接，时间戳&gt;**。通过挖掘搜索日志，我们往往可以发现最新出现的各种实体及其属性，从而保证知识图谱的实时性。这里侧重于从查询的关键词短语和点击的页面所对应的标题中抽取实体及其属性。选择查询作为抽取目标的意义在于其反映了用户最新最广泛的需求，从中能挖掘出用户感兴趣的实体以及实体对应的属性。而选择页面的标题作为抽取目标的意义在于标题往往是对整个页面的摘要，包含最重要的信息。据百度研究者的统计，90%以上的实体可以在网页标题中被找到。为了完成上述抽取任务，一个常用的做法是：针对每个类别，挑选出若干属于该类的实体(及相关属性)作为种子(Seeds)，找到包含这些种子的查询和页面标题，形成正则表达式或文法模式。这些模式将被用于抽取查询和页面标题中出现的其他实体及其属性。如果当前抽取所得的实体未被包含在知识图谱中，则该实体成为一个新的候选实体。类似地，如果当前被抽取的属性未出现在知识图谱中，则此属性成为一个新的候选属性。这里，我们仅保留置信度高的实体及其属性，新增的实体和属性将被作为新的种子发现新的模式。此过程不断迭代直到没有新的种子可以加入或所有的模式都已经找到且无法泛化。在决定模式的好坏时，常用的基本原则是尽量多地发现属于当前类别的实体和对应属性，尽量少地抽取出属于其他类别的实体及属性。上述方法被称为基于Bootstrapping的多类别协同模式学习。 从抽取图谱到知识图谱 上述所介绍的方法仅仅是从各种类型的数据源抽取构建知识图谱所需的各种候选实体(概念)及其属性关联，形成了一个个孤立的抽取图谱(Extraction Graphs)。为了形成一个真正的知识图谱，我们需要将这些信息孤岛集成在一起。下面我对知识图谱挖掘所涉及的重要技术点逐一进行介绍。 实体对齐 实体对齐(Object Alignment)旨在发现具有不同ID但却代表真实世界中同一对象的那些实体，并将这些实体归并为一个具有全局唯一标识的实体对象添加到知识图谱中。虽然实体对齐在数据库领域被广泛研究，但面对如此多异构数据源上的Web规模的实体对齐，这还是第一次尝试。各大搜索引擎公司普遍采用的方法是聚类。聚类的关键在于定义合适的相似度度量。这些相似度度量遵循如下观察：具有相同描述的实体可能代表同一实体(字符相似);具有相同属性-值的实体可能代表相同对象(属性相似);具有相同邻居的实体可能指向同一个对象(结构相似)。在此基础上，为了解决大规模实体对齐存在的效率问题，各种基于数据划分或分割的算法被提出将实体分成一个个子集，在这些子集上使用基于更复杂的相似度计算的聚类并行地发现潜在相同的对象。另外，利用来自如LOD中已有的对齐标注数据(使用owl:sameAs关联两个实体)作为训练数据，然后结合相似度计算使用如标签传递(Label Propagation)等基于图的半监督学习算法发现更多相同的实体对。无论何种自动化方法都无法保证100%的准确率，所以这些方法的产出结果将作为候选供人工进一步审核和过滤。 知识图谱schema构建 在之前的技术点介绍中，大部分篇幅均在介绍知识图谱中数据层(Data Level)的构建，而没有过多涉及模式层(Schema Level)。事实上，模式是对知识的提炼，而且遵循预先给定的schema有助于知识的标准化，更利于查询等后续处理。为知识图谱构建schema相当于为其建立本体(Ontology)。最基本的本体包括概念、概念层次、属性、属性值类型、关系、关系定义域(Domain)概念集以及关系值域(Range)概念集。在此基础上，我们可以额外添加规则(Rules)或公理(Axioms)来表示模式层更复杂的约束关系。面对如此庞大且领域无关的知识库，即使是构建最基本的本体，也是非常有挑战的。Google等公司普遍采用的方法是自顶向下(Top-Down)和自底向上(Bottom-Up)相结合的方式。这里，自顶向下的方式是指通过本体编辑器(Ontology Editor)预先构建本体。当然这里的本体构建不是从无到有的过程，而是依赖于从百科类和结构化数据得到的高质量知识中所提取的模式信息。更值得一提的是，Google知识图谱的Schema是在其收购的Freebase的schema基础上修改而得。Freebase的模式定义了Domain(领域)，Type(类别)和Topic(主题，即实体)。每个Domain有若干Types，每个Type包含多个Topics且和多个Properties关联，这些Properties规定了属于当前Type的那些Topics需要包含的属性和关系。定义好的模式可被用于抽取属于某个Type或满足某个Property的新实体(或实体对)。另一方面，自底向上的方式则通过上面介绍的各种抽取技术，特别是通过搜索日志和Web Table抽取发现的类别、属性和关系，并将这些置信度高的模式合并到知识图谱中。合并过程将使用类似实体对齐的对齐算法。对于未能匹配原有知识图谱中模式的类别、属性和关系作为新的模式加入知识图谱供人工过滤。自顶向下的方法有利于抽取新的实例，保证抽取质量，而自底向上的方法则能发现新的模式。两者是互补的。 不一致性问题的解决 当融合来自不同数据源的信息构成知识图谱时，有一些实体会同时属于两个互斥的类别(如男女)或某个实体所对应的一个Property对应多个值。这样就会出现不一致性。这些互斥的类别对以及Functional Properties可以看作是模式层的知识，通常规模不是很大，可以通过手工指定规则来定义。而由于不一致性的检测要面对大规模的实体及相关事实，纯手工的方法将不再可行。一个简单有效的方法充分考虑数据源的可靠性以及不同信息在各个数据源中出现的频度等因素来决定最终选用哪个类别或哪个属性值。也就是说，我们优先采用那些可靠性高的数据源(如百科类或结构化数据)抽取得到的事实。另外，如果一个实体在多个数据源中都被识别为某个类别的实例，或实体某个functional property在多个数据源中都对应相同的值，那么我们倾向于最终选择该类别和该值。注：在统计某个类别在数据源中出现的频率前需要完成类别对齐计算。类似地，对于数值型的属性值我们还需要额外统一它们所使用的单位。 知识图谱上的挖掘 通过各种信息抽取和数据集成技术已经可以构建Web规模的知识图谱。为了进一步增加图谱的知识覆盖率，需要进一步在知识图谱上进行挖掘。下面将介绍几项重要的基于知识图谱的挖掘技术。 推理 推理(Reasoning或Inference)被广泛用于发现隐含知识。推理功能一般通过可扩展的规则引擎来完成。知识图谱上的规则一般涉及两大类。一类是针对属性的，即通过数值计算来获取其属性值。例如：知识图谱中包含某人的出生年月，我们可以通过当前日期减去其出生年月获取其年龄。这类规则对于那些属性值随时间或其他因素发生改变的情况特别有用。另一类是针对关系的，即通过(链式)规则发现实体间的隐含关系。例如，我们可以定义规定：岳父是妻子的父亲。利用这条规则，当已知姚明的妻子(叶莉)和叶莉的父亲(叶发)时，可以推出姚明的岳父是叶发。 实体重要性排序 搜索引擎识别用户查询中提到的实体，并通过知识卡片展现该实体的结构化摘要。当查询涉及多个实体时，搜索引擎将选择与查询更相关且更重要的实体来展示。实体的相关性度量需在查询时在线计算，而实体重要性与查询无关可离线计算。搜索引擎公司将PageRank算法应用在知识图谱上来计算实体的重要性。和传统的Web Graph相比，知识图谱中的节点从单一的网页变成了各种类型的实体，而图中的边也由连接网页的超链接(Hyperlink)变成丰富的各种语义关系。由于不同的实体和语义关系的流行程度以及抽取的置信度均不同，而这些因素将影响实体重要性的最终计算结果，因此，各大搜索引擎公司嵌入这些因素来刻画实体和语义关系的初始重要性，从而使用带偏的PageRank算法(Biased PageRank)。 相关实体挖掘 在相同查询中共现的实体，或在同一个查询会话(Session)中被提到的其他实体称为相关实体。一个常用的做法是将这些查询或会话看作是虚拟文档，将其中出现的实体看作是文档中的词条，使用主题模型(如LDA)发现虚拟文档集中的主题分布。其中每个主题包含1个或多个实体，这些在同一个主题中的实体互为相关实体。当用户输入查询时，搜索引擎分析查询的主题分布并选出最相关的主题。同时，搜索引擎将给出该主题中与知识卡片所展现的实体最相关的那些实体作为“其他人还搜了”的推荐结果。 知识图谱的更新和维护 Type和Collection的关系 知识图谱的schema为了保证其质量，由专业团队审核和维护。以Google知识图谱为例，目前定义的Type数在100+数量级。为了提高知识图谱的覆盖率，搜索引擎公司还通过自动化算法从各种数据源抽取新的类型信息(也包含关联的Property信息)，这些类型信息通过一个称为Collection的数据结构保存。它们不是马上被加入到知识图谱schema中。有些今天生成后第二天就被删除了，有些则能长期的保留在Collection中，如果Collection中的某一种类型能够长期的保留，发展到一定程度后，由专业的人员进行决策和命名并最终成为一种新的Type。 结构化站点包装器的维护 站点的更新常常会导致原有模式失效。搜索引擎会定期检查站点是否存在更新。当检测到现有页面(原先已爬取)发生了变化，搜索引擎会检查这些页面的变化量，同时使用最新的站点包装器进行AVP抽取。如果变化量超过事先设定的阈值且抽取结果与原先标注的答案差别较大，则表明现有的站点包装器失效了。在这种情况下，需要对最新的页面进行重新标注并学习新的模式，从而构建更新的包装器。 知识图谱的更新频率 加入到知识图谱中的数据不是一成不变的。Type对应的实例往往是动态变化的。例如，美国总统，随着时间的推移，可能对应不同的人。由于数据层的规模和更新频度都远超schema层，搜索引擎公司利用其强大的计算保证图谱每天的更新都能在3个小时内完成，而实时的热点也能保证在事件发生6个小时内在搜索结果中反映出来。 众包(Crowdsourcing)反馈机制 除了搜索引擎公司内部的专业团队对构建的知识图谱进行审核和维护，它们还依赖用户来帮助改善图谱。具体来说，用户可以对搜索结果中展现的知识卡片所列出的实体相关的事实进行纠错。当很多用户都指出某个错误时，搜索引擎将采纳并修正。这种利用群体智慧的协同式知识编辑是对专业团队集中式管理的互补。 知识图谱在搜索中的应用 查询理解 搜索引擎借助知识图谱来识别查询中涉及到的实体(概念)及其属性等，并根据实体的重要性展现相应的知识卡片。搜索引擎并非展现实体的全部属性，而是根据当前输入的查询自动选择最相关的属性及属性值来显示。此外，搜索引擎仅当知识卡片所涉及的知识的正确性很高(通常超过95%，甚至达到99%)时，才会展现。当要展现的实体被选中之后，利用相关实体挖掘来推荐其他用户可能感兴趣的实体供进一步浏览。 问题回答 除了展现与查询相关的知识卡片，知识图谱对于搜索所带来的另一个革新是：直接返回答案，而不仅仅是排序的文档列表。要实现自动问答系统，搜索引擎不仅要理解查询中涉及到的实体及其属性，更需要理解查询所对应的语义信息。搜索引擎通过高效的图搜索，在知识图谱中查找连接这些实体及属性的子图并转换为相应的图查询(如SPARQL)。这些翻译过的图查询被进一步提交给图数据库进行回答返回相应的答案。 总结 这篇文章比较系统地介绍了知识图谱的表示、构建、挖掘以及在搜索中的应用。通过上述介绍，大家可以看出： 目前知识图谱还处于初期阶段; 人工干预很重要; 结构化数据在知识图谱的构建中起到决定性作用; 各大搜索引擎公司为了保证知识图谱的质量多半采用成熟的算法; 知识卡片的给出相对比较谨慎; 更复杂的自然语言查询将崭露头角(如Google的蜂鸟算法)。 此外，知识图谱的构建是多学科的结合，需要知识库、自然语言理解，机器学习和数据挖掘等多方面知识的融合。有很多开放性问题需要学术界和业界一起解决。我们有理由相信学术界在上述方面的突破将会极大地促进知识图谱的发展。 The link of this page is http://home.meng.uno/articles/349dc05d/ . Welcome to reproduce it!","categories":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://home.meng.uno/categories/Natural-Language-Processing/"}],"tags":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://home.meng.uno/tags/知识图谱/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"http://home.meng.uno/tags/Knowledge-Graph/"}]},{"title":"Java合并List","slug":"combine-list","date":"2018-07-15T02:15:54.000Z","updated":"2020-12-02T01:42:11.000Z","comments":true,"path":"articles/66999e7d/","link":"","permalink":"http://home.meng.uno/articles/66999e7d/","excerpt":"问题 在写我的毕业设计时，遇到了这样两个问题： 1. 给定一个分词结果（List）与一个知道偏置的专有名词（特定领域命名实体）的结果（List），怎么将两者融合成一个统一的分词结果（List）。 2. 给定一个分词结果（List）与一条规则（人为规定的分词结果（List）），怎么将规则整合到分词结果中，得到一个统一的分词结果（List）。 虽然在算法上没有多少难度，但是在实现上还是挺费时间思考，所以我将我的实现保存起来，以后没准我还会再用到。 方案 合并专有名词 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 2","text":"问题 在写我的毕业设计时，遇到了这样两个问题： 给定一个分词结果（List）与一个知道偏置的专有名词（特定领域命名实体）的结果（List），怎么将两者融合成一个统一的分词结果（List）。 给定一个分词结果（List）与一条规则（人为规定的分词结果（List）），怎么将规则整合到分词结果中，得到一个统一的分词结果（List）。 虽然在算法上没有多少难度，但是在实现上还是挺费时间思考，所以我将我的实现保存起来，以后没准我还会再用到。 方案 合并专有名词 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 public List&lt;String&gt; segTemp(List&lt;String&gt; tmp, List&lt;Term&gt; area, int len) &#123; List&lt;String&gt; ret = new ArrayList&lt;String&gt;(); int area_len = area.size(); if (area_len == 0) &#123; return tmp; &#125; int index = 0; int tmp_index = 0; int tmp_i = 0; int i = 0; for (i = 0; i &lt; tmp.size(); i++) &#123; if (index == area.get(tmp_i).getOffe()) &#123; ret.add(area.get(tmp_i).getRealName()); index += area.get(tmp_i).getRealName().length(); i--; if (tmp_i &lt; area_len - 1) &#123; tmp_i++; continue; &#125; else &#123; break; &#125; &#125; else if (index &gt; tmp_index) &#123; if (tmp_index + tmp.get(i).length() &lt;= index)&#123; tmp_index += tmp.get(i).length(); &#125; else &#123; ret.add(tmp.get(i).substring(index - tmp_index)); tmp_index += tmp.get(i).length(); index = tmp_index; &#125; &#125; else if (index + tmp.get(i).length() &lt;= area.get(tmp_i).getOffe()) &#123; ret.add(tmp.get(i)); index += tmp.get(i).length(); tmp_index += tmp.get(i).length(); &#125; else if (index + tmp.get(i).length() &gt; area.get(tmp_i).getOffe() &amp;&amp; index &lt; area.get(tmp_i).getOffe()) &#123; ret.add(tmp.get(i).substring(0, area.get(tmp_i).getOffe() - index)); index += area.get(tmp_i).getOffe() - index; tmp_index += tmp.get(i).length(); &#125; &#125; // 从上一个位置break for (int j = i + 1; j &lt; tmp.size(); j++) &#123; if (index &gt; tmp_index) &#123; if (tmp_index + tmp.get(j).length() &lt;= index) &#123; tmp_index += tmp.get(j).length(); &#125; else &#123; ret.add(tmp.get(j).substring(index - tmp_index)); tmp_index += tmp.get(j).length(); index = tmp_index; &#125; &#125; else &#123; ret.add(tmp.get(j)); &#125; &#125; return ret;&#125; 合并规则 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 public static List&lt;String&gt; match(String text, List&lt;String&gt; ori, List&lt;Rule&gt; rule)&#123; if(rule.isEmpty())&#123; return ori; &#125; List&lt;String&gt; ret = new ArrayList&lt;String&gt;(); boolean flag = false; for(Rule ru : rule)&#123; List&lt;Integer&gt; loc = getLocation(text,ru.toString()); if(loc.isEmpty())&#123; continue; &#125;else&#123; flag = true; int num = loc.size(); int j = 0; int idx = 0; for(int i = 0; i&lt; num;i++)&#123; int tmp = loc.get(i); while(idx+ori.get(j).length() &lt; tmp)&#123; ret.add(ori.get(j)); idx += ori.get(j).length(); j++; &#125; if(ori.get(j).substring(idx+ori.get(j).length() - tmp) != null || !ori.get(j).substring(idx+ori.get(j).length() - tmp).equals(\"\"))&#123; ret.add(ori.get(j).substring(idx+ori.get(j).length() - tmp)); j++; &#125; ret.addAll(ru.getRule()); while(j&lt;ori.size())&#123; if(idx + ori.get(j).length() &lt;= tmp+ru.toString().length())&#123; idx += ori.get(j).length(); j++; &#125;else if(idx + ori.get(j).length() &gt; tmp+ru.toString().length() &amp;&amp; idx &lt; tmp + ru.toString().length())&#123; idx += ori.get(j).length(); ret.add(ori.get(j).substring(idx - (tmp + ru.toString().length() ))); j++; &#125;else&#123; break; &#125; &#125; &#125; if(j&lt;ori.size())&#123; for(int t = j;t &lt; ori.size();t++)&#123; ret.add(ori.get(t)); &#125; &#125; ori = ret; &#125; &#125; if(flag == false)&#123; return ori; &#125; return ret; &#125; The link of this page is http://home.meng.uno/articles/66999e7d/ . Welcome to reproduce it!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://home.meng.uno/categories/Software-Engineering/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://home.meng.uno/tags/Java/"},{"name":"List","slug":"List","permalink":"http://home.meng.uno/tags/List/"}]},{"title":"Coverlutional Neural Networks (CNNs)","slug":"cnn","date":"2018-07-04T04:09:21.000Z","updated":"2021-01-04T06:53:35.000Z","comments":true,"path":"articles/7203e497/","link":"","permalink":"http://home.meng.uno/articles/7203e497/","excerpt":"常见的卷积结构 基本卷积 No padding, no strides Arbitrary padding, no strides Half padding, no strides Full padding, no strides No padding, strides Padding, strides Padding, strides (odd) 转置卷积 * 转置卷积（Transposed Convolution），又称反卷积（Deconvolution）、Fractionally Strided Convolution * 转置卷积是卷积的逆过程，如果把基本的卷积（+池化）看做“缩小","text":"常见的卷积结构 基本卷积 No padding, no strides Arbitrary padding, no strides Half padding, no strides Full padding, no strides No padding, strides Padding, strides Padding, strides (odd) 转置卷积 转置卷积（Transposed Convolution），又称反卷积（Deconvolution）、Fractionally Strided Convolution 转置卷积是卷积的逆过程，如果把基本的卷积（+池化）看做“缩小分辨率”的过程，那么转置卷积就是“扩充分辨率”的过程。 为了实现扩充的目的，需要对输入以某种方式进行填充。 转置卷积与数学上定义的反卷积不同——在数值上，它不能实现卷积操作的逆过程。其内部实际上执行的是常规的卷积操作。 转置卷积只是为了重建先前的空间分辨率，执行了卷积操作。 虽然转置卷积并不能还原数值，但是用于编码器-解码器结构中，效果仍然很好。——这样，转置卷积可以同时实现图像的粗粒化和卷积操作，而不是通过两个单独过程来完成。 No padding, no strides, transposed Arbitrary padding, no strides, transposed Half padding, no strides, transposed Full padding, no strides, transposed No padding, strides, transposed Padding, strides, transposed Padding, strides, transposed (odd) 空洞卷积 空洞卷积（Atrous Convolutions）也称扩张卷积（Dilated Convolutions）、膨胀卷积。 No padding, no strides. 空洞卷积的作用 空洞卷积使 CNN 能够捕捉更远的信息，获得更大的感受野； 同时不增加参数的数量，也不影响训练的速度。 示例：Conv1D + 空洞卷积 可分离卷积 逐通道卷积 Depthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积。 一张5×5像素、三通道彩色输入图片（shape为5×5×3），Depthwise Convolution首先经过第一次卷积运算，DW完全是在二维平面内进行。卷积核的数量与上一层的通道数相同（通道和卷积核一一对应）。所以一个三通道的图像经过运算后生成了3个Feature map(如果有same padding则尺寸与输入层相同为5×5)，如下图所示。 Depthwise Convolution完成后的Feature map数量与输入层的通道数相同，无法扩展Feature map。而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的feature信息。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map。 逐点卷积 Pointwise Convolution的运算与常规卷积运算非常相似，它的卷积核的尺寸为 1×1×M，M为上一层的通道数。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。有几个卷积核就有几个输出Feature map。 经过Pointwise Convolution之后，同样输出了4张Feature map，与常规卷积的输出维度相同。 门卷积 类似 LSTM 的过滤机制，实际上是卷积网络与门控线性单元（Gated Linear Unit）的组合。 核心公式 中间的运算符表示逐位相乘—— Tensorflow 中由 tf.multiply(a, b) 实现，其中 a 和 b 的 shape 要相同；后一个卷积使用sigmoid激活函数。 一个门卷积 Block W 和 V 表明参数不共享。 实践中，为了防止梯度消失，还会在每个 Block 中加入残差 门卷积的作用 减缓梯度消失 解决语言顺序依存问题 门卷积是如何防止梯度消失的 因为公式中有一个卷积没有经过激活函数，所以对这部分求导是个常数，所以梯度消失的概率很小。 如果还是担心梯度消失，还可以加入残差——要求输入输出的 shape 一致。 更直观的理解： 即信息以 1-σ 的概率直接通过，以 σ 的概率经过变换后通过——类似 GRU 因为Conv1D(X)没有经过激活函数，所以实际上它只是一个线性变化；因此与 Conv1D(X) - X 是等价的 CNNs与DNNs CNNs相对于DNNs的优点 CNNs的特征就是共享卷积核，对高维数据处理无压力。图像通过卷积操作后仍然保留原先的位置关系。 DNNs的输入是向量形式，并未考虑到平面的结构信息，在图像和NLP领域这一结构信息尤为重要，例如识别图像中的数字，同一数字与所在位置无关（换句话说任一位置的权重都应相同），CNNs的输入可以是tensor，例如二维矩阵，通过filter获得局部特征，较好的保留了平面结构信息。 CNNs结构总结 卷积层：对图像元素的矩阵变换，是提取图像特征的方法，多种卷积核可以提取多种特征。一个卷积核覆盖的原始图像的范围叫做感受野（权值共享）。一次卷积运算(哪怕是多个卷积核)提取的特征往往是局部的，难以提取出比较全局的特征，因此需要在一层卷积基础上继续做卷积计算 ，这也就是多层卷积。 池化层：降维的方法，按照卷积计算得出的特征向量维度大的惊人，不但会带来非常大的计算量，而且容易出现过拟合，解决过拟合的办法就是让模型尽量“泛化”，也就是再“模糊”一点，那么一种方法就是把图像中局部区域的特征做一个平滑压缩处理，这源于局部图像一些特征的相似性(即局部相关性原理)。 全连接层：softmax分类 训练过程：卷积核中的因子(×1或×0)其实就是需要学习的参数，也就是卷积核矩阵元素的值就是参数值。一个特征如果有M个值，N个特征就有M*N个值，再加上多个层，需要学习的参数还是比较多的。 CNNs filter尺寸计算：Feature Map的尺寸等于(input_size + 2 * padding_size − filter_size)/stride + 1 The link of this page is http://home.meng.uno/articles/7203e497/ . Welcome to reproduce it!","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://home.meng.uno/categories/Deep-Learning/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://home.meng.uno/tags/CNN/"},{"name":"深度学习","slug":"深度学习","permalink":"http://home.meng.uno/tags/深度学习/"}]},{"title":"神经网络、梯度下降和反向传播","slug":"ann_sgd_bf","date":"2018-04-04T04:29:41.000Z","updated":"2021-01-01T18:38:28.000Z","comments":true,"path":"articles/8193f764/","link":"","permalink":"http://home.meng.uno/articles/8193f764/","excerpt":"神经网络的结构 示例：一个用于手写数字识别的神经网络 这个示例相当于深度学习领域中的 “Hello World”. 神经元（隐藏单元）与隐藏层 神经元（隐藏单元） * 简单来说，神经元可以理解为一个用来装数字的容器，而这个数称为激活值 需要强调的是，激活值的值域取决于使用的激活函数，大多数激活函数的值域都是正值 如果使用 sigmoid 激活函数，那么这个数字就在 0 到 1 之间；但通常来说，无论你使用哪种激活函数，这个数字都比较小 * 输入层也可以看做是一组神经元，它的激活值就是输入本身 基","text":"神经网络的结构 示例：一个用于手写数字识别的神经网络 这个示例相当于深度学习领域中的 “Hello World”. 神经元（隐藏单元）与隐藏层 神经元（隐藏单元） 简单来说，神经元可以理解为一个用来装数字的容器，而这个数称为激活值 需要强调的是，激活值的值域取决于使用的激活函数，大多数激活函数的值域都是正值 如果使用 sigmoid 激活函数，那么这个数字就在 0 到 1 之间；但通常来说，无论你使用哪种激活函数，这个数字都比较小 输入层也可以看做是一组神经元，它的激活值就是输入本身 基本的神经网络只能处理向量型的输入，所以需要将这个 28*28 的像素图（矩阵），重排成长为 784 的向量 如果使用卷积神经网络，则可以直接处理矩阵型的输入 对于分类问题，输出层中的激活值代表这个类别正确的可能性 如果使用了 softmax 函数，那么整个输出层可以看作每个类别的概率分布 所谓的“神经元被激活”实际上就是它获得了一个较大的激活值 隐藏层 包含于输入层与输出层之间的网络层统称为“隐藏层” 在这个简单的网络中，有两个隐藏层，每层有 16 个神经元 为什么是两层和 16 个？——层数的大小与问题的复杂度有关，而神经元的数量目前来看是随机的——网络的结构在实验时有很大的调整余地 神经网络的运作机制 神经网络在运作的时候，隐藏层可以视为一个“黑箱” 每一层的激活值将通过某种方式计算出下一层的激活值——神经网络处理信息的核心机制 每一层被激活的神经元不同，（可能）会导致下一层被激活的神经元也不同 为什么神经网络的分层结构能起作用？ 人在初识数字时是如何区分的？——组合数字的各个部分 在理想情况下，我们希望神经网络倒数第二层中的各隐藏单元能对应上每个基本笔画（pattern） 当输入是 9 或 8 这种顶部带有圆圈的数字时，某个神经元将被激活（激活值接近 1） 不光是 9 和 8，所有顶部带有圆圈的图案都能激活这个隐藏单元 这样从倒数第二层到输出层，我们的问题就简化成了“学习哪些部件能组合哪些数字” 类似的，基本笔画也可以由更基础的部件构成 理想情况下，神经网络的处理过程 从输入层到输出层，网络的抽象程度越来越高 深度学习的本质：通过组合简单的概念来表达复杂的事物 神经网络是不是这么做的，我们不得而知（所以是一个“黑箱”），但大量实验表明：神经网络确实在做类似的工作——通过组合简单的概念来表达复杂的事物 语音识别：原始音频 → 音素 → 音节 → 单词 隐藏单元是如何被激活的？ 我们需要设计一个机制，这个机制能够把像素拼成边，把边拼成基本图像，把基本图像拼成数字 这个机制的基本处理方式是：通过上一层的单元激活下一层的单元 示例：如何使第二层的单个神经元识别出图像中的某块区域是否存在一条边 根据激活的含义，当激活值接近 1 时，表示该区域存在一条边，反之不存在 怎样的数学公式能够表达出这个含义？ 考虑对所有输入单元加权求和 图中每条连线关联一个权值：绿色表示正值，红色表示负值，颜色越暗表示越接近 0 此时，只需将需要关注的像素区域对应的权值设为正，其余为 0 这样对所有像素的加权求和就只会累计我们关注区域的像素值 为了使隐藏单元真正被“激活”，加权和还需要经过某个非线性函数，也就是“激活函数” 早期最常用的激活函数是 sigmoid 函数（又称 logistic/逻辑斯蒂曲线） 从 sigmoid 的角度看，它实际上在对加权和到底有多“正”进行打分 但有时，可能加权和大于 10 时激活才有意义； 此时，需要加上“偏置”，保证不能随便激发，比如 -10。然后再传入激活函数 权重和偏置 每个隐藏单元都会和上一层的所有单元相连，每条连线上都关联着一个权重； 每个隐藏单元又会各自带有一个偏置 偏置和权重统称为网络参数 每一层都带有自己的权重与偏置，这样一个小小的网络，就有 13002 个参数 权重与偏置的实际意义 宏观来看，权重在告诉你当前神经元应该更关注来自上一层的哪些单元；或者说权重指示了连接的强弱 偏置则告诉你加权和应该多大才能使神经元的激发变得有意义；或者说当前神经元是否更容易被激活 矢量化编程 把一层中所有的激活值作为一列向量 a 层与层之间的权重放在一个矩阵 W 中：第 n 行就是上层所有神经元与下层第 n 个神经元的权重 类似的，所有偏置也作为一列向量 b 最后，将 Wa + b 一起传入激活函数 sigmoid会对结果向量中的每个值都取一次sigmoid 所谓“矢量化编程”，实际上就是将向量作为基本处理单元，避免使用 for 循环处理标量 通过定制处理单元（GPU运算），可以大幅加快计算速度 机器“学习”的实质 当我们在讨论机器如何“学习”时，实际上指的是机器如何正确设置这些参数 非线性激活函数 神经网络本质上是一个函数 每个神经元可以看作是一个函数，其输入是上一层所有单元的输出，然后输出一个激活值 宏观来看，神经网络也是一个函数 一个输入 784 个值，输出 10 个值的函数；其中有 13000 个参数 早期最常用的激活函数是 sigmoid 函数，它是一个非线性函数 暂不考虑它其他优秀的性质（使其长期作为激活函数的首选）以及缺点（使其逐渐被弃用）； 而只考虑其非线性 为什么要使用非线性激活函数？——神经网络的万能近似定理 视频中没有提到为什么使用非线性激活函数，但这确实是神经网络能够具有如此强大表示能力的关键 使用非线性激活函数的目的是为了向网络中加入非线性因素，从而加强网络的表示能力 为什么加入非线性因素能够加强网络的表示能力？ 首先要有这样一个认识，非线性函数具有比线性函数更强的表示能力。 如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合； 容易验证，此时无论有多少层，神经网络都只是一个线性函数。 万能近似定理 神经网络如果具有至少一个非线性输出层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。 这极大的扩展了神经网络的表示空间 新时代的激活函数——线性整流单元 ReLU 这里简单说下 sigmoid 的问题： sigmoid 函数在输入取绝对值非常大的正值或负值时会出现饱和现象，此时函数会对输入的微小改变会变得不敏感 饱和现象：在图像上表现为函数值随自变量的变化区域平缓（斜率接近 0） 饱和现象会导致基于梯度的学习变得困难，并在传播过程中丢失信息（梯度消失） 线性整流单元 ReLU ReLU 取代 sigmoid 的主要原因就是：使神经网络更容易训练（减缓梯度消失） 此外，一种玄学的说法是，早期引入 sigmoid 的原因之一就是为了模仿生物学上神经元的激发 而 ReLU 比 sigmoid 更接近这一过程。 梯度下降 依然是那个手写识别的例子： 神经网络是怎样学习的？ 我们需要一种算法：通过喂给这个网络大量的训练数据——不同的手写数字图像以及对应的数字标签 算法会调整所有网络参数（权重和偏置）来提高网络对训练数据的表现 此外，我们还希望这种分层结构能够举一反三，识别训练数据之外的图像——泛化能力 虽然使用了“学习”的说法，但实际上训练的过程更像在解一道微积分问题 训练的过程实际上在寻找某个函数的（局部）最小值 在训练开始前，这些参数是随机初始化的 确实存在一些随机初始化的策略，但目前来看，都只是“锦上添花” 损失函数（Loss Function） 显然，随机初始化不会有多好的表现 此时需要定义一个“损失函数”来告诉计算机：正确的输出应该只有标签对应的那个神经元是被激活的 比如这样定义单个样本的损失： 当网络分类正确时，这个值就越小 这里使用的损失函数为“均方误差”（mean-square error, MSE） 现在，我们就可以用所有训练样本的平均损失来评价整个网络在这个任务上的“糟糕程度” 在实践中，并不会每次都使用所有训练样本的平均损失来调整梯度，这样计算量太大了 随机梯度下降 实际上，神经网络学习的过程，就是最小化损失函数的过程 神经网络与损失函数的关系 神经网络本身相当于一个函数 输入是一个向量，输出是一个向量，参数是所有权重和偏置 损失函数在神经网络的基础上，还要再抽象一层： 所有权重和偏置作为它的输入，输出是单个数值，表示当前网络的性能；参数是所有训练样例（？） 从这个角度看，损失函数并不是神经网络的一部分，而是训练神经网络时需要用到的工具 梯度下降法（Gradient Descent） 如何优化这些网络参数？ 能够判断网络的“糟糕程度”并不重要，关键是如何利用它来优化网络参数 示例 1：考虑只有一个参数的情况 如果函数只有一个极值点，那么直接利用微积分即可 如果函数很复杂的话，问题就不那么直接了，更遑论上万个参数的情况 一个启发式的思路是：先随机选择一个值，然后考虑向左还是向右，函数值会减小； 准确的说，如果你找到了函数在该点的斜率，斜率为正就向左移动一小步，反之向右； 然后每新到一个点就重复上述过程——计算新的斜率，根据斜率更新位置； 最后，就可能逼近函数的某个局部极小值点 这个想法最明显的问题是：由于无法预知最开始的值在什么位置，导致最终会到达不同的局部极小值； 关键是无法保证落入的局部极小值就是损失函数可能达到的全局最小值； 这也是神经网络最大的问题： 示例 2：考虑两个参数的情况 输入空间是一个 XY 平面，代价函数是平面上方的曲面 此时的问题是：在输入空间沿哪个方向移动，能使输出结果下降得最快 如果你熟悉多元微积分，那么应该已经有了答案： 函数的梯度指出了函数的“最陡”增长方向，即沿着梯度的方向走，函数增长最快； 换言之，沿梯度的负方向走，函数值也就下降得最快； 此外，梯度向量的长度还代表了斜坡的“陡”的程度。 处理更多的参数也是同样的办法， 这种按照负梯度的倍数，不断调整函数输入值的过程，就叫作梯度下降法 直观来看，梯度下降法能够让函数值收敛到损失函数图像中的某一个“坑”中 理解梯度下降法的另一种思路 梯度下降法的一般处理方式： 将所有网络参数放在一个列向量中，那么损失函数的负梯度也是一个向量 负梯度中的每一项实际传达了两个信息 正负号在告诉输入向量应该调大还是调小——因为是负梯度，所以正好对应调大，负号调小 每一项的相对大小表明每个输入值对函数值的影响程度；换言之，也就是调整各权重对于网络的影响 宏观来看，可以把梯度向量中的每个值理解为各参数（权重和偏置）的相对重要度， 同时指出了改变其中哪些参数的性价比最高 这个理解思路同样可以反馈到图像中 一种思路是在点 (1,1) 处沿着 [3,1] 方向移动，函数值增长最快 另一种解读就是变量 x 的重要性是 y 的三倍； 也就是说，至少在这个点附近，改变 x 会造成更大的影响 梯度下降法描述： 计算损失函数对所有参数的（负）梯度 按梯度的负方向下降一定步长（直接加上负梯度） 重复以上步骤，直到满足精度要求 其中计算梯度的算法是整个神经网络的核心 梯度下降法与反向传播算法 梯度下降法是寻找局部最小值的一种策略 其中最核心的部分利用损失函数 L(θ) 的梯度来更新所有参数 θ 反向传播算法是求解函数梯度的一种方法； 其本质上是利用链式法则对每个参数求偏导 损失函数的平滑性 为了能达到函数的局部最小值，损失函数有必要是平滑的 只有如此，损失函数才能基于梯度下降的方法，找到一个局部最小值 这也解释了为什么要求神经元的激活值是连续的 生物学中的神经元是二元式的 随机梯度下降（Stochastic Gradient Descent） 基本的梯度下降法要求每次使用所有训练样本的平均损失来更新参数，也称为“批量梯度下降” 原则上是这样，但是为了计算效率，实践中并不会这么做 一种常用的方法是每次只随机选取单个样本的损失来计算梯度，该方法称为“随机梯度下降”（Stochastic Gradient Descent, SGD），它比批量梯度下降法要快得多 但更常用的方法是小批量梯度下降，它每次随机选取一批样本，然后基于它们的平均损失来更新参数 SGD 与 小批量梯度下降的优势在于：它们的计算复杂度与训练样本的数量无关 很多情况下，并不区分 SGD 和小批量梯度下降；有时默认 SGD 就是小批量梯度下降，比如本视频 批大小的影响： 较大的批能得到更精确的梯度估计，但回报是小于线性的。 较小的批能带来更好的泛化误差，泛化误差通常在批大小为 1 时最好。但是，因为梯度估计的高方差，小批量训练需要较小的学习率以保持稳定性，这意味着更长的训练时间。 原因可能是由于小批量在学习过程中加入了噪声，它们会有一些正则化效果 (Wilson and Martinez, 2003) 内存消耗和批的大小成正比，当批量处理中的所有样本可以并行处理时。 在某些硬件上使用特定大小可以减少运行时间。尤其是在使用 GPU 时，通常使用 2 的幂数作为批量大小可以获得更少的运行时间。一般，2 的幂数的取值范围是 32 到 256，16 有时在尝试大模型时使用。 神经网络的优化难题 有一个策略可以保证最终解至少能到达一个局部极小值点：使每次移动的步幅和斜率成正比； 因为在最小值附近的斜率会趋于平缓，这将导致每次移动步幅越来越小，防止跳出极值点 但是，这对于现代各种巨大的神经网络而言，是一个负优化策略——它反而会限制网络的学习，导致其陷入某个局部极小值点 当参数数量非常庞大时，可能存在无数个极值点，而其中某些极值点的结果可能非常差。 优化问题是深度学习最核心的两个问题之一，另一个是正则化 也不要太过担心因为局部最小值点太多而无法优化； 事实上，只要你使用的数据不是完全随机，或者说有一定结构，那么最终网络倾向收敛到的各个局部最小值点，实际上都差不多 你可以认为如果数据集已经结构化了，那么你可以更轻松的找到局部最小值点 [1412.0233] The Loss Surfaces of Multilayer Networks https://arxiv.org/abs/1412.0233 再谈神经网络的运作机制 我们对神经网络的期望是： 第一个隐藏层能够识别短边，第二个隐藏层能够将短边拼成圆圈等基本笔画，最后将这些部件拼成数字 利用权重与所有输入像素对应相乘的方法，可以还原每个神经元对应的图案 期望（左）与现实（右） 事实上，与其说它们能识别各种散落的短边，它们不过都是一些松散的图案 就像“在如深海般的 13000 维参数空间中，找到了一个不错的局部最小值位置住了下来” 尽管它们能成功识别大部分手写数字，但它不像我们期望的能识别各种图案的基本部件 一个明显的例子：传入一张随机的图案 如果神经网络真的很“智能”，它应该对这个输入感到困惑——10个输出单元都没有明确的激活值 但实际上，它总会会给出一个确切的答案 它能将一张真正的 “5” 识别成 5，也能把一张随机的噪声识别成 5 或者说，这个神经网络知道怎么识别数字，但它不知道怎么写数字 原因可能是，很大程度上它的训练被限制在了一个很窄的框架内； “从神经网络的视角来看，整个世界都是由网络内清晰定义的静止数字组成的，它的损失函数只会促使它对最后的判断有绝对的自信。” 有一些观点认为，神经网络/深度学习并没有那么智能，它只是记住了所有正确分类的数据，然后尽量把那些跟训练数据类似的数据分类正确 但是，有些观点认为神经网络确实学到了某些更智能的东西： 如果训练时使用的是随机的数据，那么损失函数的下降会很慢，而且是接近线性的过程；也就是说网络很难找到可能存在的局部最小值点； 但如果你使用了结构化的数据，虽然一开始的损失会上下浮动，但接下来会快速下降；也就是很容易就找到了某个局部最小值——这表明神经网络能更快学习结构化的数据。 反向传播（Backpropagation, BP） 反向传播的直观理解 梯度下降法中需要用到损失函数的梯度来决定下降的方向， 其中 BP 算法正是用于求解这个复杂梯度的一种方法 在阐述 BP 算法之前，记住—— 梯度向量中的每一项不光告诉我们每个参数应该增大还是减小，还指示了调整每个参数的“性价比” 示例：单个训练样本对参数调整的影响 假设现在的网络还没有训练好，那么输出层的激活值看起来会很随机 而我们希望的输出应该是正确类别对应的输出值最大，其他尽可能接近 0 因此，我们希望正确类别对应的激活值应该增大，而其他都减小 需要注意的是，激活值是由输入值和权重及偏置决定的；网络不会调整激活值本身，只能改变权重和偏置 一个简单的直觉是：激活值变动的幅度应该与当前值和目标值之间的误差成正比 我们希望第三个输出值增大，其他减小 显然，这里增加 “2” 的激活值比减少 “8” 的激活值更重要；因为后者已经接近它的目标了 激活值的计算公式指明了调整的方向 以 “2” 对应的神经元为例，如果需要增大当前激活值，可以： 增大偏置 增大权重 调整上一层的激活值 如果使用的是 sigmoid 或 ReLU 作为激活函数，那么激活值都 ≥ 0；但无论激活值正负，都是类似的调整方法 如何调整权重和偏置？ 偏置只关注当前神经元，因此，可以正比于当前值和目标值之间的差来调整偏置 权重指示了连接的强弱；换言之，与之相连的上层激活值越大，那么该权重对当前神经元的影响也越大。显然，着重调整这个参数的性价比更高 因此，可以正比于与之关联的上层激活值来调整权重 这种学习的偏好将导致这样一个结果——一同激活的神经元被关联在一起；生物学中，称之为“赫布理论”（Hebbian theory） 这在深度学习中，并不是一个广泛的结论，只是一个粗略的对照；事实上，早期的神经网络确实在模仿生物学上大脑的工作；但深度学习兴起之后，其指导思想已经发生了重要转变——组合表示 如何改变上层激活值？ 因为权重带有正负，所以如果希望增大当前激活值，应该使所有通过正权连接的上层激活值增大，所有通过负权连接的上层激活值减小 类似的，与修改权重时类似，如果想造成更大的影响，应该对应权重的大小来对激活值做出成比例的改变； 但需要注意的是，上层激活值的大小也是由上层的权重和偏置决定的， 所以更准确的说法是，每层的权重和偏置会根据下一层的权重和偏置来做出成比例的改变，而最后一层的权重个偏置会根据当前值和目标值之间的误差来成比例调整 反向传播 除了需要增大正确的激活值，同时还要减小错误的。而这些神经元对于如何改变上一层的激活值都有各自的想法； 因此，需要将这些神经元的期待全部相加，作为改变上层神经元的指示； 这些期待变化，不仅对应权重的倍数，也是每个神经元激活值改变量的倍数。 这其实就是在实现“反向传播”的理念了—— 将所有期待的改变相加，就得到了希望对上层改动的变化量；然后就可以重复这个过程，直到第一层 上面只是讨论了单个样本对所有参数的影响，实践时，需要同时考虑每个样本对权重与偏置的修改，然后将它们期望的平均作为每个参数的变化量； 不严格的来说，这就是梯度下降中的需要的“负梯度” η 表示倍数 BP 算法小结 反向传播算法计算的是单个训练样本对所有权重和偏置的调整——包括每个参数的正负变化以及变化的比例——使其能最快的降低损失。 真正的梯度下降需要对训练集中的每个样本都做一次反向传播，然后计算平均变化值，继而更新参数。 但这么操作会导致算法的复杂度与训练样本的数量相关。 实践时，会采用“随机梯度下降”的策略： 首先打乱所有样本； 然后将所有样本分发到各个 mini-batch 中； 计算每个 mini-batch 的梯度，调整参数 循环至 Loss 值基本不再变化 最终神经网络将会收敛到某个局部最小值上。 反向传播的微积分原理 从数学的角度看，反向传播本质上就是利用链式法则求导的过程； 本节的目标是展示机器学习领域是如何理解链式法则的。 示例：每层只有一个神经元的网络 从最后两个神经元开始： 记最后一个神经元的激活值为 a^(L) 表示在第 L 层，上层的激活值为 a^(L-1)； 给定一个训练样本，其目标值记为 y； 则该网络对于单个训练样本的损失，可以表示为： 其中 为了方便，记加权和为 z，于是有 整个流程可以概括为： 先使用前一个激活值和权重 w 以及偏置 b 计算出 z 再将 z 传入激活函数计算出 a 最后利用 a 和目标值 y 计算出损失 我们的目标是理解代价函数对于参数的变化有多敏感； 从微积分的角度来看，这实际上就是在求损失函数对参数的导数。 以 w^(L) 为例，求 C 对 w^(L)的（偏）导数，记作： 把 ∂w^(L) 看作对 w 的微小扰动，比如 0.001；把 ∂C 看作“改变 w 对 C 造成的变化” 这实际上就是在计算 C 对 w^(L) 的微小变化有多敏感； 根据链式法则，有 进一步分解到每个比值，有 类似的，对偏置的偏导： 对上层激活值的偏导： 更上层的权重与偏置也是类似的方法，只是链的长度会更长而已 一个直观的理解，考虑将它们分别对应到一个数轴上； w^(L) 的微小变化会导致 z^(L) 的微小变化 z^(L) 的微小变化会导致 a^(L) 的微小变化 a^(L) 的微小变化最终会影响到损失值 C 反向传播的过程就是将 C 的微小变化传回去 以上只是单个训练的损失对 w^(L) 的导数，实践中需要求出一个 mini-batch 中所有样本的平均损失 进一步的，∂C/∂w^(L) 只是梯度向量 ▽C 的一个分量； 而梯度向量本身有损失函数对每个参数的偏导构成： 本系列视频称损失函数为“代价函数”，所以使用 C 表示代价（Cost）；更多会用 L 表示损失（Loss）；二者没有区别，但这里已经使用 L 表示网络的层数，所以使用 C 表示损失函数 更复杂的示例 更复杂的神经网络在公式上并没有复杂很多，只有少量符号的变化 首先，利用下标来区分同一层不同的神经元和权重 以下的推导过程几乎只是符号的改变： 然后求偏导： 唯一改变的是，对 (L-1) 层的偏导： 此时激活值可以通过不同的途径影响损失函数 只要计算出倒数第二层损失函数对激活值的敏感度，就可以重复以上过程，计算喂给倒数第二层的权重和偏置。 事实上，如果都使用矢量表示，那么整个推导公式跟单神经元的网络几乎是完全一样的 链式法则给出了决定梯度每个分量的偏导，使我们能不断下探，最小化神经网络的损失。 反向传播的 4 个基本公式 问题描述： 反向传播算法的目标是求出损失函数对所有参数的梯度，具体可分解为对每个权重和偏置的偏导 可以用 4 个公式总结反向传播的过程 标量形式： 其中，上标 (l) 表示网络的层次，(L) 表示输出层（最后一层）；下标 j 和 k 指示神经元的位置； w_jk 表示 l 层的第 j 个神经元与(l-1)层第 k 个神经元连线上的权重 以 均方误差（MSE） 损失函数为例，有 根据以上公式，就可以反向求出所有损失函数对所有参数的偏导了 矢量形式： 在标量形式的基础上，修改下标即可 注意：其中向量相乘都是以按元素相乘的形式，一般用 ⊙ 符号表示。 以上是纯微积分形式的表达，一些机器学习相关书籍上总结了更简洁的形式，比如： 注意：前两个公式为矢量形式，后两个具体到单个参数的是标量形式。 其中，符号 ⊙ 表示按元素相乘，例如： The link of this page is http://home.meng.uno/articles/8193f764/ . Welcome to reproduce it!","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://home.meng.uno/categories/Deep-Learning/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://home.meng.uno/tags/神经网络/"},{"name":"梯度下降","slug":"梯度下降","permalink":"http://home.meng.uno/tags/梯度下降/"},{"name":"反向传播","slug":"反向传播","permalink":"http://home.meng.uno/tags/反向传播/"}]},{"title":"几个常见的社区推荐算法","slug":"ranking","date":"2018-03-18T11:03:48.000Z","updated":"2020-12-02T02:10:31.000Z","comments":true,"path":"articles/82a6b55c/","link":"","permalink":"http://home.meng.uno/articles/82a6b55c/","excerpt":"PageRank算法 PageRank算法预先给每个网页一个PR值（PR值指代PageRank值），PR值在物理意义上为一个网页被访问的概率，所以一般是1/N，其中N为网页总数。 另外，所有网页的PR值的和一般为1。（如果实在不为1也不是不行，最后算出来的不同网页之间PR值的大小关系仍然是正确的，只是这个数值不能直接地反映概率罢了。） 接着，运用下面的算法不断迭代计算，直至达到平稳分布为止。 普通情况 互联网中的众多网页可以看成一个有向图，箭头的指向即为链接的链入，根据上图，我们得到A的PR值为：PR(A)=PR(B)/2+PR©/1。 没有出链 网络中不乏一些没有出链的网页，","text":"PageRank算法 PageRank算法预先给每个网页一个PR值（PR值指代PageRank值），PR值在物理意义上为一个网页被访问的概率，所以一般是1/N，其中N为网页总数。 另外，所有网页的PR值的和一般为1。（如果实在不为1也不是不行，最后算出来的不同网页之间PR值的大小关系仍然是正确的，只是这个数值不能直接地反映概率罢了。） 接着，运用下面的算法不断迭代计算，直至达到平稳分布为止。 普通情况 互联网中的众多网页可以看成一个有向图，箭头的指向即为链接的链入，根据上图，我们得到A的PR值为：PR(A)=PR(B)/2+PR©/1。 没有出链 网络中不乏一些没有出链的网页，如上图，其中，网页C没有出链，也就是说网页C对其他网页没有PR值的贡献，我们不喜欢这种“自私”的网页（其实是为了满足 Markov 链的收敛性），于是设定其对所有网页（包括它自己）都有出链，则此图中A的PR值表示为：PR(A)=PR(B)/2+PR©/4。 出链循环圈 网络中还存在这样的网页：只对自己有出链，或者几个网页的出链形成一个循环圈。那么在不断迭代的过程中，这一个或几个网页的PR值将只增不减，这显然是不合理的。 那么如何解决这个问题呢？我们假设某人正在浏览网页C，显然他不会一直停留在网页C，他可能会随机地输入一个网址从而去往另一个网页，并且其跳转到每个网页的概率是一样的。于是此图中A的PR值表示为：PR(A)=∂(PR(B)/2)+(1-∂)/4。 综上，一般情况下，一个网页的PR值计算公式如下： 其中，Mpi是所有对pi网页有出链的网页集合，L(pj)是网页pj的出链数目，N是网页总数，α一般取0.85。 根据上面的公式，我们就可以计算出每个网页的PR值，在不断迭代并趋于平稳的时候，即为最终结果。 HITS算法 算法简介： 首先把那些根据关键相关返回网页作为根集合S，再由S集合网页节点的链入和链出网页节点派生出结合C，结合C包括S，链入和链出节点集合。 C中的每个节点分配一对权重&lt;h(s),a(s)&gt;, 节点h(s)权重由节点链出的节点的a(s)决定，a(s)由节点的链入节点的h(s)决定。 算法过程： 网页的a权重向量： 关于HITS算法收敛性，可以从如下变换形式来得出： 当算法收敛时候，a其实就是对应矩阵A那个最大特征值对应的特征向量的归一化形式，同样，h也是H矩阵那个最大特征值对应的特征向量的归一化形式。 算法实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485 class HITSIterator: __doc__ = '''计算一张图中的hub,authority值''' def __init__(self, dg): self.max_iterations = 100 # 最大迭代次数 self.min_delta = 0.0001 # 确定迭代是否结束的参数 self.graph = dg self.hub = &#123;&#125; self.authority = &#123;&#125; for node in self.graph.nodes(): self.hub[node] = 1 self.authority[node] = 1 def hits(self): \"\"\" 计算每个页面的hub,authority值 :return: \"\"\" if not self.graph: return flag = False for i in range(self.max_iterations): change = 0.0 # 记录每轮的变化值 norm = 0 # 标准化系数 tmp = &#123;&#125; # 计算每个页面的authority值 tmp = self.authority.copy() for node in self.graph.nodes(): self.authority[node] = 0 for incident_page in self.graph.incidents(node): # 遍历所有“入射”的页面 self.authority[node] += self.hub[incident_page] norm += pow(self.authority[node], 2) # 标准化 norm = sqrt(norm) for node in self.graph.nodes(): self.authority[node] /= norm change += abs(tmp[node] - self.authority[node]) # 计算每个页面的hub值 norm = 0 tmp = self.hub.copy() for node in self.graph.nodes(): self.hub[node] = 0 for neighbor_page in self.graph.neighbors(node): # 遍历所有“出射”的页面 self.hub[node] += self.authority[neighbor_page] norm += pow(self.hub[node], 2) # 标准化 norm = sqrt(norm) for node in self.graph.nodes(): self.hub[node] /= norm change += abs(tmp[node] - self.hub[node]) print(\"This is NO.%s iteration\" % (i + 1)) print(\"authority\", self.authority) print(\"hub\", self.hub) if change &lt; self.min_delta: flag = True break if flag: print(\"finished in %s iterations!\" % (i + 1)) else: print(\"finished out of 100 iterations!\") print(\"The best authority page: \", max(self.authority.items(), key=lambda x: x[1])) print(\"The best hub page: \", max(self.hub.items(), key=lambda x: x[1]))if __name__ == '__main__': dg = digraph() dg.add_nodes([\"A\", \"B\", \"C\", \"D\", \"E\"]) dg.add_edge((\"A\", \"C\")) dg.add_edge((\"A\", \"D\")) dg.add_edge((\"B\", \"D\")) dg.add_edge((\"C\", \"E\")) dg.add_edge((\"D\", \"E\")) dg.add_edge((\"B\", \"E\")) dg.add_edge((\"E\", \"A\")) hits = HITSIterator(dg) hits.hits() SALSA算法 SALSA算法和HITS算法初始部分一样，构建相同的集合集C和彼此的链接关系。 SALSA一种随机游走过程，但是不同经典的随机游走。它涉及到把一个网页节点看成2种不同类型节点：hub和authority，随机游走对应着这样两种不用类型的Markov链：hub链和authority链，状态转移为网页前向和后向。 首先是把构建一个无向图，原图节点分为2类，然后构建边。 这样从某个节点出发，进行两个方向的随机游走。h和a方向的状态转移矩阵： 对于以上的形式可以通过如下的矩阵相乘的方式展现： 有了H和A矩阵，就可以知道节点集合最终的h和a向量：和HITS一样，h和a对应H和A的最大特征值对应的归一化特征向量。其实，计算h和a可以参照HITS，进行迭代求解。 The link of this page is http://home.meng.uno/articles/82a6b55c/ . Welcome to reproduce it!","categories":[{"name":"Data Mining","slug":"Data-Mining","permalink":"http://home.meng.uno/categories/Data-Mining/"}],"tags":[{"name":"推荐","slug":"推荐","permalink":"http://home.meng.uno/tags/推荐/"},{"name":"社交网络","slug":"社交网络","permalink":"http://home.meng.uno/tags/社交网络/"}]},{"title":"商品推荐：协同过滤","slug":"recommendation","date":"2018-03-17T06:05:52.000Z","updated":"2020-12-02T02:03:04.000Z","comments":true,"path":"articles/6f93935a/","link":"","permalink":"http://home.meng.uno/articles/6f93935a/","excerpt":"过去很长时间以及现今，电商都在蓬勃发展，支持电商越做越大的一个很重要的因素就是“商品推荐”。当我们打开天猫，我们发现不同的用户，一般而言，首页是不一样的，原因就是，它为不同的用户推荐了不同的商品。我们为什么要用某一个购物网站或者APP，我觉得很大程度上取决于其推荐的准确与否。本篇博客我将向大家介绍协同过滤在商品推荐上的应用。 关于推荐系统 根据推荐引擎的数据源不同，一般而言，推荐系统可以分为如下三类： * 基于人口统计学的推荐机制（Demographic-based Recommendation）：根据系统用户的基本信息发现用户的相关程度。 * 基于内容的推荐机制（Content","text":"过去很长时间以及现今，电商都在蓬勃发展，支持电商越做越大的一个很重要的因素就是“商品推荐”。当我们打开天猫，我们发现不同的用户，一般而言，首页是不一样的，原因就是，它为不同的用户推荐了不同的商品。我们为什么要用某一个购物网站或者APP，我觉得很大程度上取决于其推荐的准确与否。本篇博客我将向大家介绍协同过滤在商品推荐上的应用。 关于推荐系统 根据推荐引擎的数据源不同，一般而言，推荐系统可以分为如下三类： 基于人口统计学的推荐机制（Demographic-based Recommendation）：根据系统用户的基本信息发现用户的相关程度。 基于内容的推荐机制（Content-based Recommendation）：根据推荐物品或内容的元数据，发现物品或者内容的相关性。 协同过滤的推荐机制（Collaborative Filtering-based Recommendation）：根据用户对物品或者信息的偏好，发现物品或者内容本身的相关性，或者是发现用户的相关性。 根据推荐模型的建立方式不同，推荐系统可以分为这样三类： 基于物品和用户本身。这种推荐引擎将每个用户和每个物品都当作独立的实体，预测每个用户对于每个物品的喜好程度，这些信息往往是用一个二维矩阵描述的。由于用户感兴趣的物品远远小于总物品的数目，这样的模型导致大量的数据空置，即我们得到的二维矩阵往往是一个很大的稀疏矩阵。同时为了减小计算量，我们可以对物品和用户进行聚类， 然后记录和计算一类用户对一类物品的喜好程度，但这样的模型又会在推荐的准确性上有损失。 基于关联规则的推荐（Rule-based Recommendation）。关联规则的挖掘已经是数据挖掘中的一个经典的问题，主要是挖掘一些数据的依赖关系，典型的场景就是“购物篮问题”，通过关联规则的挖掘，我们可以找到哪些物品经常被同时购买，或者用户购买了一些物品后通常会购买哪些其他的物品，当我们挖掘出这些关联规则之后，我们可以基于这些规则给用户进行推荐。 基于模型的推荐（Model-based Recommendation）。这是一个典型的机器学习的问题，可以将已有的用户喜好信息作为训练样本，训练出一个预测用户喜好的模型，这样以后用户在进入系统，可以基于此模型计算推荐。这种方法的问题在于如何将用户实时或者近期的喜好信息反馈给训练好的模型，从而提高推荐的准确度。 协同过滤 关于协同过滤的一个最经典的例子就是看电影，有时候不知道哪一部电影是我们喜欢的或者评分比较高的，那么通常的做法就是问问周围的朋友，看看最近有什么好的电影推荐。在问的时候，都习惯于问跟自己口味差不多的朋友，这就是协同过滤的核心思想。 步骤 要实现协同过滤，一般需要这样三步： 收集用户偏好 找到相似物或人 计算并推荐 收集用户偏好 从用户的行为和偏好中发现规律，并基于此进行推荐，所以如何收集用户的偏好信息成为系统推荐效果最基础的决定因素。用户有很多种方式向系统提供自己的偏好信息，比如：评分，投票，转发，保存书签，购买，点击流，页面停留时间等等。 当然，得到原始数据之后，我们总是需要进行降噪、归一化等，在此不再赘述。 找到相似物或人 既然是找相似，我们就需要设定一个计算相似度的指标，一般而言，余弦相似度与皮尔逊相关系数是很好的选择。 余弦相似度 皮尔逊相关系数 皮尔逊相关也称为积差相关（或积矩相关）是英国统计学家皮尔逊于20世纪提出的一种计算直线相关的方法。 假设有两个变量X、Y，那么两变量间的皮尔逊相关系数可通过以下公式计算： 计算并推荐 基于用户的协同过滤推荐 基于用户的协同过滤推荐的基本原理是，根据所有用户对物品或者信息的偏好，发现与当前用户口味和偏好相似的“邻居”用户群，在一般的应用中是采用计算“K- 邻居”的算法；然后，基于这 K 个邻居的历史偏好信息，为当前用户进行推荐。 上图示意出基于用户的协同过滤推荐机制的基本原理，假设用户 A 喜欢物品 A，物品 C，用户 B 喜欢物品 B，用户 C 喜欢物品 A ，物品 C 和物品 D；从这些用户的历史喜好信息中，我们可以发现用户 A 和用户 C 的口味和偏好是比较类似的，同时用户 C 还喜欢物品 D，那么我们可以推断用户 A 可能也喜欢物品 D，因此可以将物品 D 推荐给用户 A。 基于用户的协同过滤推荐机制和基于人口统计学的推荐机制都是计算用户的相似度，并基于“邻居”用户群计算推荐，但它们所不同的是如何计算用户的相似度，基于人口统计学的机制只考虑用户本身的特征，而基于用户的协同过滤机制可是在用户的历史偏好的数据上计算用户的相似度，它的基本假设是，喜欢类似物品的用户可能有相同或者相似的口味和偏好。 基于项目的协同过滤推荐 基于项目的协同过滤推荐的基本原理也是类似的，只是说它使用所有用户对物品或者信息的偏好，发现物品和物品之间的相似度，然后根据用户的历史偏好信息，将类似的物品推荐给用户。 假设用户 A 喜欢物品 A 和物品 C，用户 B 喜欢物品 A，物品 B 和物品 C，用户 C 喜欢物品 A，从这些用户的历史喜好可以分析出物品 A 和物品 C 时比较类似的，喜欢物品 A 的人都喜欢物品 C，基于这个数据可以推断用户 C 很有可能也喜欢物品 C，所以系统会将物品 C 推荐给用户 C。 与上面讲的类似，基于项目的协同过滤推荐和基于内容的推荐其实都是基于物品相似度预测推荐，只是相似度计算的方法不一样，前者是从用户历史的偏好推断，而后者是基于物品本身的属性特征信息。 基于模型的协同过滤推荐 基于模型的协同过滤推荐就是基于样本的用户喜好信息，训练一个推荐模型，然后根据实时的用户喜好的信息进行预测，计算推荐。 基于协同过滤的推荐机制是现今应用最为广泛的推荐机制，它有以下几个显著的优点： 它不需要对物品或者用户进行严格的建模，而且不要求物品的描述是机器可理解的，所以这种方法也是领域无关的。 这种方法计算出来的推荐是开放的，可以共用他人的经验，很好的支持用户发现潜在的兴趣偏好 而它也存在以下几个问题： 方法的核心是基于历史数据，所以对新物品和新用户都有“冷启动”的问题。 推荐的效果依赖于用户历史偏好数据的多少和准确性。 在大部分的实现中，用户历史偏好是用稀疏矩阵进行存储的，而稀疏矩阵上的计算有些明显的问题，包括可能少部分人的错误偏好会对推荐的准确度有很大的影响等等。 对于一些特殊品味的用户不能给予很好的推荐。 由于以历史数据为基础，抓取和建模用户的偏好后，很难修改或者根据用户的使用演变，从而导致这个方法不够灵活。 代码实现 基于用户的CF： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112 import java.util.HashMap;import java.util.HashSet;import java.util.Iterator;import java.util.Map;import java.util.Map.Entry;import java.util.Scanner;import java.util.Set;/** * 基于用户的协同过滤推荐算法实现 A a b d B a c C b e D c d e * @author Administrator * */public class UserCF &#123; public static void main(String[] args) &#123; /** * 输入用户--&gt;物品条目 一个用户对应多个物品 * 用户ID 物品ID集合 * A a b d * B a c * C b e * D c d e */ Scanner scanner = new Scanner(System.in); System.out.println(\"Input the total users number:\"); //输入用户总量 int N = scanner.nextint(); int[][] sparseMatrix = new int[N][N]; //建立用户稀疏矩阵，用于用户相似度计算【相似度矩阵】 Map&lt;String, Integer&gt; userItemLength = new HashMap&lt;&gt;(); //存储每一个用户对应的不同物品总数 eg: A 3 Map&lt;String, Set&lt;String&gt;&gt; itemUserCollection = new HashMap&lt;&gt;(); //建立物品到用户的倒排表 eg: a A B Set&lt;String&gt; items = new HashSet&lt;&gt;(); //辅助存储物品集合 Map&lt;String, Integer&gt; userID = new HashMap&lt;&gt;(); //辅助存储每一个用户的用户ID映射 Map&lt;Integer, String&gt; idUser = new HashMap&lt;&gt;(); //辅助存储每一个ID对应的用户映射 System.out.println(\"Input user--items maping infermation:&lt;eg:A a b d&gt;\"); scanner.nextLine(); for (int i = 0; i &lt; N ; i++)&#123; //依次处理N个用户 输入数据 以空格间隔 String[] user_item = scanner.nextLine().split(\" \"); int length = user_item.length; userItemLength.put(user_item[0], length-1); //eg: A 3 userID.put(user_item[0], i); //用户ID与稀疏矩阵建立对应关系 idUser.put(i, user_item[0]); //建立物品--用户倒排表 for (int j = 1; j &lt; length; j ++)&#123; if(items.contains(user_item[j]))&#123; //如果已经包含对应的物品--用户映射，直接添加对应的用户 itemUserCollection.get(user_item[j]).add(user_item[0]); &#125; else&#123; //否则创建对应物品--用户集合映射 items.add(user_item[j]); itemUserCollection.put(user_item[j], new HashSet&lt;String&gt;()); //创建物品--用户倒排关系 itemUserCollection.get(user_item[j]).add(user_item[0]); &#125; &#125; &#125; System.out.println(itemUserCollection.toString()); //计算相似度矩阵【稀疏】 Set&lt;Entry&lt;String, Set&lt;String&gt;&gt;&gt; entrySet = itemUserCollection.entrySet(); Iterator&lt;Entry&lt;String, Set&lt;String&gt;&gt;&gt; iterator = entrySet.iterator(); while(iterator.hasNext())&#123; Set&lt;String&gt; commonUsers = iterator.next().getValue(); for (String user_u : commonUsers) &#123; for (String user_v : commonUsers) &#123; if(user_u.equals(user_v))&#123; continue; &#125; sparseMatrix[userID.get(user_u)][userID.get(user_v)] += 1; //计算用户u与用户v都有正反馈的物品总数 &#125; &#125; &#125; System.out.println(userItemLength.toString()); System.out.println(\"Input the user for recommendation:&lt;eg:A&gt;\"); String recommendUser = scanner.nextLine(); System.out.println(userID.get(recommendUser)); //计算用户之间的相似度【余弦相似性】 int recommendUserId = userID.get(recommendUser); for (int j = 0;j &lt; sparseMatrix.length; j++) &#123; if(j != recommendUserId)&#123; System.out.println(idUser.get(recommendUserId)+\"--\"+idUser.get(j)+\"相似度:\"+sparseMatrix[recommendUserId][j]/Math.sqrt(userItemLength.get(idUser.get(recommendUserId))*userItemLength.get(idUser.get(j)))); &#125; &#125; //计算指定用户recommendUser的物品推荐度 for (String item: items)&#123; //遍历每一件物品 Set&lt;String&gt; users = itemUserCollection.get(item); //得到购买当前物品的所有用户集合 if(!users.contains(recommendUser))&#123; //如果被推荐用户没有购买当前物品，则进行推荐度计算 double itemRecommendDegree = 0.0; for (String user: users)&#123; itemRecommendDegree += sparseMatrix[userID.get(recommendUser)][userID.get(user)]/Math.sqrt(userItemLength.get(recommendUser)*userItemLength.get(user)); //推荐度计算 &#125; System.out.println(\"The item \"+item+\" for \"+recommendUser +\"'s recommended degree:\"+itemRecommendDegree); &#125; &#125; scanner.close(); &#125;&#125; 基于项目的CF： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140 import randomimport sysimport mathimport osfrom operator import itemgetterrandom.seed(0)class ItemBasedCF(object): def __init__(self): self.trainset = &#123;&#125; self.testset = &#123;&#125; self.n_sim_movie = 20 self.n_rec_movie = 10 self.movie_sim_mat = &#123;&#125; self.movie_popular = &#123;&#125; self.movie_count = 0 print('Similar movie number = %d' % self.n_sim_movie, file = sys.stderr) print('Recommendend movie number = %d' % self.n_rec_movie,file = sys.stderr) @staticmethod def loadfile(filename): fp = open(filename, 'r') for i, line in enumerate(fp): yield line.strip('\\r\\n') if i % 100000 == 0: print ('load %s(%s)' %(filename,i), file = sys.stderr) fp.close() print('load %s succ' %filename, file = sys.stderr) def generate_dataset(self, filename, pivot = 0.7): trainset_len = 0 testset_len = 0 for line in self.loadfile(filename): user, movie, rating , _= line.split('::') if random.random() &lt; pivot: self.trainset.setdefault(user,&#123;&#125;) self.trainset[user][movie] = int(rating) trainset_len += 1 else: self.testset.setdefault(user,&#123;&#125;) self.testset[user][movie] = int(rating) testset_len += 1 print('split succ , trainset is %d , testset is %d' %(trainset_len,testset_len) , file = sys.stderr) def calc_movie_sim(self): for user, movies in self.trainset.items(): for movie in movies: if movie not in self.movie_popular: self.movie_popular[movie] = 0 self.movie_popular[movie] += 1 print('count movies number and pipularity succ',file = sys.stderr) self.movie_count = len(self.movie_popular) print('total movie number = %d' %self.movie_count, file = sys.stderr) itemsim_mat = self.movie_sim_mat print('building co-rated users matrix', file = sys.stderr) for user, movies in self.trainset.items(): for m1 in movies: for m2 in movies: if m1 == m2: continue itemsim_mat.setdefault(m1,&#123;&#125;) itemsim_mat[m1].setdefault(m2,0) itemsim_mat[m1][m2] += 1 print('build co-rated users matrix succ', file = sys.stderr) print('calculating movie similarity matrix', file = sys.stderr) simfactor_count = 0 PRINT_STEP = 2000000 for m1, related_movies in itemsim_mat.items(): for m2, count in related_movies.items(): itemsim_mat[m1][m2] = count / math.sqrt(self.movie_popular[m1] * self.movie_popular[m2]) simfactor_count += 1 if simfactor_count % PRINT_STEP == 0: print('calcu movie similarity factor(%d)' %simfactor_count, file = sys.stderr) print('calcu similiarity succ', file = sys.stderr) def recommend(self,user): K = self.n_sim_movie N = self.n_rec_movie rank = &#123;&#125; watched_movies = self.trainset[user] for movie, rating in watched_movies.items(): for related_movie, similarity_factor in sorted(self.movie_sim_mat[movie].items(), key=itemgetter(1), reverse=True)[0:K]: if related_movie in watched_movies: continue rank.setdefault(related_movie, 0) rank[related_movie] += similarity_factor * rating return sorted(rank.items(), key=itemgetter(1), reverse=True)[0:N] def evaluate(self): print('evaluation start', file = sys.stderr) N = self.n_rec_movie hit = 0 rec_count = 0 test_count = 0 all_rec_movies = set() popular_sum = 0 for i, user in enumerate(self.trainset): if i % 500 == 0: print('recommend for %d users ' %i , file = sys.stderr) test_movies = self.testset.get(user,&#123;&#125;) rec_movies = self.recommend(user) for movie, _ in rec_movies: if movie in test_movies: hit += 1 all_rec_movies.add(movie) popular_sum += math.log(1 + self.movie_popular[movie]) rec_count += N test_count += len(test_movies) precision = hit / (1.0 * rec_count) recall = hit / (1.0 * test_count) coverage = len(all_rec_movies) / (1.0 * self.movie_count) popularity = popular_sum / (1.0 * rec_count) print('precision is %.4f\\t recall is %.4f \\t coverage is %.4f \\t popularity is %.4f' %(precision,recall,coverage,popularity), file = sys.stderr)if __name__ == '__main__': ratingfile = os.path.join('ml-1m', 'ratings.dat') itemcf = ItemBasedCF() itemcf.generate_dataset(ratingfile) itemcf.calc_movie_sim() itemcf.evaluate() The link of this page is http://home.meng.uno/articles/6f93935a/ . Welcome to reproduce it!","categories":[{"name":"Data Mining","slug":"Data-Mining","permalink":"http://home.meng.uno/categories/Data-Mining/"}],"tags":[{"name":"社会网络","slug":"社会网络","permalink":"http://home.meng.uno/tags/社会网络/"},{"name":"商品推荐","slug":"商品推荐","permalink":"http://home.meng.uno/tags/商品推荐/"},{"name":"协同过滤","slug":"协同过滤","permalink":"http://home.meng.uno/tags/协同过滤/"}]},{"title":"JVM的垃圾回收机制","slug":"java-gc","date":"2018-03-09T07:14:32.000Z","updated":"2020-12-02T01:50:00.000Z","comments":true,"path":"articles/dde60b3a/","link":"","permalink":"http://home.meng.uno/articles/dde60b3a/","excerpt":"简介 Java GC（Garbage Collection，垃圾收集，垃圾回收）机制，是Java与C++/C的主要区别之一，作为Java开发者，一般不需要专门编写内存回收和垃圾清理代码，对内存泄露和溢出的问题，也不需要像C程序员那样战战兢兢。这是因为在Java虚拟机中，存在自动内存管理和垃圾清扫机制。概括地说，该机制对JVM（Java Virtual Machine）中的内存进行标记，并确定哪些内存需要回收，根据一定的回收策略，自动的回收内存，永不停息（Nerver Stop）的保证JVM中的内存空间，防止出现内存泄露和溢出问题。 关于JVM，需要说明一下的是，目前使用最多的Sun公司的JD","text":"简介 Java GC（Garbage Collection，垃圾收集，垃圾回收）机制，是Java与C++/C的主要区别之一，作为Java开发者，一般不需要专门编写内存回收和垃圾清理代码，对内存泄露和溢出的问题，也不需要像C程序员那样战战兢兢。这是因为在Java虚拟机中，存在自动内存管理和垃圾清扫机制。概括地说，该机制对JVM（Java Virtual Machine）中的内存进行标记，并确定哪些内存需要回收，根据一定的回收策略，自动的回收内存，永不停息（Nerver Stop）的保证JVM中的内存空间，防止出现内存泄露和溢出问题。 关于JVM，需要说明一下的是，目前使用最多的Sun公司的JDK中，自从1999年的JDK1.2开始直至现在仍在广泛使用的JDK6，其中默认的虚拟机都是HotSpot。2009年，Oracle收购Sun，加上之前收购的EBA公司，Oracle拥有3大虚拟机中的两个：JRockit和HotSpot，Oracle也表明了想要整合两大虚拟机的意图，但是目前在新发布的JDK7中，默认的虚拟机仍然是HotSpot，因此本文中默认介绍的虚拟机都是HotSpot，相关机制也主要是指HotSpot的GC机制。 学习Java GC机制，可以帮助我们在日常工作中排查各种内存溢出或泄露问题，解决性能瓶颈，达到更高的并发量，写出更高效的程序。 解决哪些问题 既然是要进行自动GC，那必然会有相应的策略，而这些策略解决了哪些问题呢，粗略的来说，主要有以下几点。 哪些对象可以被回收。 何时回收这些对象。 采用什么样的方式回收。 说到垃圾回收（Garbage Collection，GC），很多人就会自然而然地把它和Java联系起来。在Java中，程序员不需要去关心内存动态分配和垃圾回收的问题，这一切都交给了JVM来处理。 顾名思义，垃圾回收就是释放垃圾占用的空间，那么在Java中，什么样的对象会被认定为“垃圾”？那么当一些对象被确定为垃圾之后，采用什么样的策略来进行回收（释放空间）？在目前的商业虚拟机中，有哪些典型的垃圾收集器？ 如何确定某个对象是“垃圾”？ 既然垃圾收集器的任务是回收垃圾对象所占的空间供新的对象使用，那么垃圾收集器如何确定某个对象是“垃圾”？即通过什么方法判断一个对象可以被回收了。 在java中是通过引用来和对象进行关联的，也就是说如果要操作对象，必须通过引用来进行。那么很显然一个简单的办法就是通过引用计数来判断一个对象是否可以被回收。不失一般性，如果一个对象没有任何引用与之关联，则说明该对象基本不太可能在其他地方被使用到，那么这个对象就成为可被回收的对象了。这种方式成为引用计数法。 这种方式的特点是实现简单，而且效率较高，但是它无法解决循环引用的问题，因此在Java中并没有采用这种方式（Python采用的是引用计数法）。看下面这段代码： 12345678910111213141516 public class Main &#123; public static void main(String[] args) &#123; MyObject object1 = new MyObject(); MyObject object2 = new MyObject(); object1.object = object2; object2.object = object1; object1 = null; object2 = null; &#125;&#125;class MyObject&#123; public Object object = null;&#125; 最后面两句将object1和object2赋值为null，也就是说object1和object2指向的对象已经不可能再被访问，但是由于它们互相引用对方，导致它们的引用计数都不为0，那么垃圾收集器就永远不会回收它们。 为了解决这个问题，在Java中采取了 可达性分析法。该方法的基本思想是通过一系列的“GC Roots”对象作为起点进行搜索，如果在“GC Roots”和一个对象之间没有可达路径，则称该对象是不可达的，不过要注意的是被判定为不可达的对象不一定就会成为可回收对象。被判定为不可达的对象要成为可回收对象必须至少经历两次标记过程，如果在这两次标记过程中仍然没有逃脱成为可回收对象的可能性，则基本上就真的成为可回收对象了。 至于可达性分析法具体是如何操作的我暂时也没有看得很明白，如果有哪位朋友比较清楚的话请不吝指教。 下面来看个例子： 1234567 Object aobj = new Object ( ) ;Object bobj = new Object ( ) ;Object cobj = new Object ( ) ;aobj = bobj;aobj = cobj;cobj = null;aobj = null; 第几行有可能会使得某个对象成为可回收对象？第7行的代码会导致有对象会成为可回收对象。至于为什么留给读者自己思考。 再看一个例子： 12345 String str = new String(\"hello\");SoftReference&lt;String&gt; sr = new SoftReference&lt;String&gt;(new String(\"java\"));WeakReference&lt;String&gt; wr = new WeakReference&lt;String&gt;(new String(\"world\")); 这三句哪句会使得String对象成为可回收对象？第2句和第3句，第2句在内存不足的情况下会将String对象判定为可回收对象，第3句无论什么情况下String对象都会被判定为可回收对象。 最后总结一下平常遇到的比较常见的将对象判定为可回收对象的情况： 显示地将某个引用赋值为null或者将已经指向某个对象的引用指向新的对象，比如下面的代码： 12345 Object obj = new Object();obj = null;Object obj1 = new Object();Object obj2 = new Object();obj1 = obj2; 局部引用所指向的对象，比如下面这段代码： 12345678 void fun() &#123;..... for(int i=0;i&lt;10;i++) &#123; Object obj = new Object(); System.out.println(obj.getClass()); &#125; &#125; 循环每执行完一次，生成的Object对象都会成为可回收的对象。 只有弱引用与其关联的对象，比如： 1 WeakReference&lt;String&gt; wr = new WeakReference&lt;String&gt;(new String(\"world\")); 典型的垃圾收集算法 在确定了哪些垃圾可以被回收后，垃圾收集器要做的事情就是开始进行垃圾回收，但是这里面涉及到一个问题是：如何高效地进行垃圾回收。由于Java虚拟机规范并没有对如何实现垃圾收集器做出明确的规定，因此各个厂商的虚拟机可以采用不同的方式来实现垃圾收集器，所以在此只讨论几种常见的垃圾收集算法的核心思想。 需要明确的一点是，这里谈到的垃圾回收算法针对的是JVM的堆内存，栈基本上不存在垃圾回收方面的困扰。 Mark-Sweep（标记-清除）算法 这是最基础的垃圾回收算法，之所以说它是最基础的是因为它最容易实现，思想也是最简单的。标记-清除算法分为两个阶段：标记阶段和清除阶段。标记阶段的任务是标记出所有需要被回收的对象，清除阶段就是回收被标记的对象所占用的空间。 标记—清除算法是最基础的收集算法，它分为“标记”和“清除”两个阶段：首先标记出所需回收的对象，在标记完成后统一回收掉所有被标记的对象，它的标记过程其实就是前面的根搜索算法中判定垃圾对象的标记过程。 最基础的收集算法是“标记-清除”（Mark-Sweep）算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段。 标记-清除算法实现起来比较容易，但是有一个比较严重的问题就是容易产生内存碎片，碎片太多可能会导致后续过程中需要为大对象分配空间时无法找到足够的空间而提前触发新的一次垃圾收集动作。 该算法有如下缺点： 标记和清除过程的效率都不高。 标记清除后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不触发另一次垃圾收集动作。 我们在程序（程序也就是指我们运行在JVM上的JAVA程序）运行期间如果想进行垃圾回收，就必须让GC线程与程序当中的线程互相配合，才能在不影响程序运行的前提下，顺利的将垃圾进行回收。 为了达到这个目的，标记/清除算法就应运而生了。它的做法是当堆中的有效内存空间（available memory）被耗尽的时候，就会停止整个程序（也被成为stop the world），然后进行两项工作，第一项则是标记，第二项则是清除。 标记：标记的过程其实就是，遍历所有的GC Roots，然后将所有GC Roots可达的对象标记为存活的对象。 清除：清除的过程将遍历堆中所有的对象，将没有标记的对象全部清除掉。 就是当程序运行期间，若可以使用的内存被耗尽的时候，GC线程就会被触发并将程序暂停，随后将依旧存活的对象标记一遍，最终再将堆中所有没被标记的对象全部清除掉，接下来便让程序恢复运行。 Copying（复制）算法 为了解决效率问题，一种称为“复制”（Copying）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。 当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。 只是这种算法的代价是将内存缩小为了原来的一半，未免太高了一点。 现在的商业虚拟机都采用这种收集算法来回收新生代，IBM公司的专门研究表明，新生代中的对象98%是“朝生夕死”的，所以并不需要按照1:1的比例来划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor[1]。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。 HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，也就是每次新生代中可用内存空间为整个新生代容量的90%（80%+10%），只有10%的内存会被“浪费”。 当然，98%的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 内存的分配担保就好比我们去银行借款，如果我们信誉很好，在98%的情况下都能按时偿还，于是银行可能会默认我们下一次也能按时按量地偿还贷款，只需要有一个担保人能保证如果我不能还款时，可以从他的账户扣钱，那银行就认为没有风险了。 内存的分配担保也一样，如果另外一块Survivor空间没有足够空间存放上一次新生代收集下来的存活对象时，这些对象将直接通过分配担保机制进入老年代。 复制算法是针对标记—清除算法的缺点，在其基础上进行改进而得到的，它讲课用内存按容量分为大小相等的两块，每次只使用其中的一块，当这一块的内存用完了，就将还存活着的对象复制到另外一块内存上面，然后再把已使用过的内存空间一次清理掉。复制算法有如下优点： 每次只对一块内存进行回收，运行高效。 只需移动栈顶指针，按顺序分配内存即可，实现简单。 内存回收时不用考虑内存碎片的出现。 它的缺点是：可一次性分配的最大内存缩小了一半。 这种算法虽然实现简单，运行高效且不容易产生内存碎片，但是却对内存空间的使用做出了高昂的代价，因为能够使用的内存缩减到原来的一半。 很显然，Copying算法的效率跟存活对象的数目多少有很大的关系，如果存活对象很多，那么Copying算法的效率将会大大降低。 我们首先一起来看一下复制算法的做法，复制算法将内存划分为两个区间，在任意时间点，所有动态分配的对象都只能分配在其中一个区间（称为活动区间），而另外一个区间（称为空闲区间）则是空闲的。 当有效内存空间耗尽时，JVM将暂停程序运行，开启复制算法GC线程。接下来GC线程会将活动区间内的存活对象，全部复制到空闲区间，且严格按照内存地址依次排列，与此同时，GC线程将更新存活对象的内存引用地址指向新的内存地址。 此时，空闲区间已经与活动区间交换，而垃圾对象现在已经全部留在了原来的活动区间，也就是现在的空闲区间。事实上，在活动区间转换为空间区间的同时，垃圾对象已经被一次性全部回收。 很明显，复制算法弥补了标记/清除算法中，内存布局混乱的缺点。不过与此同时，它的缺点也是相当明显的。 它浪费了一半的内存，这太要命了。 如果对象的存活率很高，我们可以极端一点，假设是100%存活，那么我们需要将所有对象都复制一遍，并将所有引用地址重置一遍。复制这一工作所花费的时间，在对象存活率达到一定程度时，将会变的不可忽视。 所以从以上描述不难看出，复制算法要想使用，最起码对象的存活率要非常低才行，而且最重要的是，我们必须要克服50%内存的浪费。 Mark-Compact（标记-整理）算法 为了解决Copying算法的缺陷，充分利用内存空间，提出了Mark-Compact算法。该算法标记阶段和Mark-Sweep一样，但是在完成标记之后，它不是直接清理可回收对象，而是将存活对象都向一端移动，然后清理掉端边界以外的内存。 复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。 更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 标记/整理算法与标记/清除算法非常相似，它也是分为两个阶段：标记和整理。下面LZ给各位介绍一下这两个阶段都做了什么。 标记：它的第一个阶段与标记/清除算法是一模一样的，均是遍历GC Roots，然后将存活的对象标记。 整理：移动所有存活的对象，且按照内存地址次序依次排列，然后将末端内存地址以后的内存全部回收。因此，第二阶段才称为整理阶段。 复制算法比较适合于新生代，在老年代中，对象存活率比较高，如果执行较多的复制操作，效率将会变低，所以老年代一般会选用其他算法，如标记—整理算法。该算法标记的过程与标记—清除算法中的标记过程一样，但对标记后出的垃圾对象的处理情况有所不同，它不是直接对可回收对象进行清理，而是让所有的对象都向一端移动，然后直接清理掉端边界以外的内存。 不难看出，标记/整理算法不仅可以弥补标记/清除算法当中，内存区域分散的缺点，也消除了复制算法当中，内存减半的高额代价，可谓是一举两得，一箭双雕，一石两鸟。 不过任何算法都会有其缺点，标记/整理算法唯一的缺点就是效率也不高，不仅要标记所有存活对象，还要整理所有存活对象的引用地址。从效率上来说，标记/整理算法要低于复制算法。 Generational Collection（分代收集）算法 分代收集算法是目前大部分JVM的垃圾收集器采用的算法。它的核心思想是根据对象存活的生命周期将内存划分为若干个不同的区域。一般情况下将堆区划分为老年代（Tenured Generation）和新生代（Young Generation），老年代的特点是每次垃圾收集时只有少量对象需要被回收，而新生代的特点是每次垃圾回收时都有大量的对象需要被回收，那么就可以根据不同代的特点采取最适合的收集算法。 目前大部分垃圾收集器对于新生代都采取Copying算法，因为新生代中每次垃圾回收都要回收大部分对象，也就是说需要复制的操作次数较少，但是实际中并不是按照1：1的比例来划分新生代的空间的。一般来说是将新生代划分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden空间和其中的一块Survivor空间，当进行回收时，将Eden和Survivor中还存活的对象复制到另一块Survivor空间中，然后清理掉Eden和刚才使用过的Survivor空间。 而由于老年代的特点是每次回收都只回收少量对象，一般使用的是Mark-Compact算法。 注意，在堆区之外还有一个代就是永久代（Permanet Generation），它用来存储class类、常量、方法描述等。对永久代的回收主要回收两部分内容：废弃常量和无用的类。 当前商业虚拟机的垃圾收集 都采用分代收集，它根据对象的存活周期的不同将内存划分为几块，一般是把Java堆分为新生代和老年代。在新生代中，每次垃圾收集时都会发现有大量对象死去，只有少量存活，因此可选用复制算法来完成收集，而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用标记—清除算法或标记—整理算法来进行回收。 典型的垃圾收集器 垃圾收集算法是 内存回收的理论基础，而垃圾收集器就是内存回收的具体实现。下面介绍一下HotSpot（JDK 7)虚拟机提供的几种垃圾收集器，用户可以根据自己的需求组合出各个年代使用的收集器。 Serial/Serial Old Serial/Serial Old收集器是最基本最古老的收集器，它是一个单线程收集器，并且在它进行垃圾收集时，必须暂停所有用户线程。Serial收集器是针对新生代的收集器，采用的是Copying算法，Serial Old收集器是针对老年代的收集器，采用的是Mark-Compact算法。它的优点是实现简单高效，但是缺点是会给用户带来停顿。 ParNew ParNew收集器是Serial收集器的多线程版本，使用多个线程进行垃圾收集。 Parallel Scavenge Parallel Scavenge收集器是一个新生代的多线程收集器（并行收集器），它在回收期间不需要暂停其他用户线程，其采用的是Copying算法，该收集器与前两个收集器有所不同，它主要是为了达到一个可控的吞吐量。 Parallel Old Parallel Old是Parallel Scavenge收集器的老年代版本（并行收集器），使用多线程和Mark-Compact算法。 CMS CMS（Current Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器，它是一种并发收集器，采用的是Mark-Sweep算法。 G1 G1收集器是当今收集器技术发展最前沿的成果，它是一款面向服务端应用的收集器，它能充分利用多CPU、多核环境。因此它是一款并行与并发收集器，并且它能建立可预测的停顿时间模型。 The link of this page is http://home.meng.uno/articles/dde60b3a/ . Welcome to reproduce it!","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://home.meng.uno/categories/Programming-Language/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://home.meng.uno/tags/JVM/"},{"name":"GC","slug":"GC","permalink":"http://home.meng.uno/tags/GC/"},{"name":"垃圾回收","slug":"垃圾回收","permalink":"http://home.meng.uno/tags/垃圾回收/"}]},{"title":"Unix进程的那些事","slug":"unix-process","date":"2018-03-04T12:06:22.000Z","updated":"2020-12-02T02:08:45.000Z","comments":true,"path":"articles/aeaab565/","link":"","permalink":"http://home.meng.uno/articles/aeaab565/","excerpt":"不知道你们有没有这样的疑惑，每次在看资料时遇到fork(2)，我都不理解，为什么fork()函数需要2做参数？还只能是2。 本篇博客在阅读了《Working with Unix Processes》之后总结而成。 回答疑问 首先，我就来解释一下之前留下的那个疑问，用过Mac或者Linux的同学都知道电脑中有很多man文件夹，我也不知道怎么回事，莫非是因为我是个man？后来我知道了，man是manpages的意思，中文译作“Unix手册页”，和我们现实中使用的手册一样，这个手册也是分节的，其中比较重要的几节： * 节1：一般命令 * 节2：系统调用 * 节3：C库函数 * 节4：特","text":"不知道你们有没有这样的疑惑，每次在看资料时遇到fork(2)，我都不理解，为什么fork()函数需要2做参数？还只能是2。 本篇博客在阅读了《Working with Unix Processes》之后总结而成。 回答疑问 首先，我就来解释一下之前留下的那个疑问，用过Mac或者Linux的同学都知道电脑中有很多man文件夹，我也不知道怎么回事，莫非是因为我是个man？后来我知道了，man是manpages的意思，中文译作“Unix手册页”，和我们现实中使用的手册一样，这个手册也是分节的，其中比较重要的几节： 节1：一般命令 节2：系统调用 节3：C库函数 节4：特殊文件 那么我们该如何使用这个手册呢？ 很简单 我们只需要：man [节号] 命令名就可以了。 例如：man 2 fork 1234567891011121314151617181920212223242526272829303132333435 fork() will fail and no child process will be created if: [EAGAIN] The system-imposed limit on the total number of processes under execution would be exceeded. This limit is configuration-depen- dent. [EAGAIN] The system-imposed limit MAXUPRC (&lt;sys/param.h&gt;) on the total num- ber of processes under execution by a single user would be exceeded. [ENOMEM] There is insufficient swap space for the new process.LEGACY SYNOPSIS #include &lt;sys/types.h&gt; #include &lt;unistd.h&gt; The include file &lt;sys/types.h&gt; is necessary.SEE ALSO execve(2), sigaction(2), wait(2), compat(5)HISTORY A fork() function call appeared in Version 6 AT&amp;T UNIX.CAVEATS There are limits to what you can do in the child process. To be totally safe you should restrict yourself to only executing async-signal safe operations until such time as one of the exec functions is called. All APIs, including global data sym- bols, in any framework or library should be assumed to be unsafe after a fork() unless explicitly documented to be safe or async-signal safe. If you need to use these frameworks in the child process, you must exec. In this situation it is rea- sonable to exec yourself.4th Berkeley Distribution June 4, 1993 4th Berkeley Distribution(END) 这就是完整的对fork(2)的解释。 Unix进程 提示 所有Ruby代码皆需要在irb环境下运行，如何安装Ruby，可以百度。 进程标识 每个人都有一个唯一的身份证号，进程也是如此，这个唯一的标识符叫做pid。我们输入：puts Process.pid就可以得到当前进程的pid了。 pid并不传达关于进程本身的任何信息，它仅仅是一个顺序标识符。在内核眼中进程只是一个数字而已。 pid是对进程的一种简单通用的描述，至于用途之一，比如我们常常会在日志文件中发现pid，当有多个进程向一个日志文件写入日志的时候，在每一行加入pid就可以知道哪一行日志是由哪个进程写入的。 父进程 系统中运行的每一个进程都有对应的父进程，每一个进程都知道其父进程的标识符(ppid)。多数情况下特定进程的父进程就是调用它的那个进程。比如启动终端并进入bash提示符，此时新创建的bash进程的父进程就是终端进程。如果在bash中调用ls等命令，那么bash进程便是ls进程的父进程。 父进程对于检测守护进程有比较重要的作用。 我们输入：puts Process.ppid就可以得到当前进程的父进程pid了。 文件描述符 我们讨论进程，怎么突然说道“文件”？其实，在Unix眼中，一切皆为“文件”！设备是文件，套接字是文件，文件也是文件。当然为了避免误解，一般将文件称为文件，其他称为资源。 我们使用这样的语句打印某“文件”的描述符：puts 文件名.fileno。 例如，STDIN的描述符：puts STDIN.fileno，结果是不是很吃惊？因为居然是0！！！ 同理，排在其后的分别是STDOUT与STDERR，他们三者也被称为标准流。 资源限制 文件描述符代表已经打开的资源，当资源没有被关闭时，该资源的文件描述符编号会一直被占用，文件描述符编号一直处于递增状态，而内核为每个进程设置了最大文件描述符号，即施加了一些资源限制。对于文件描述符编号的限制有软限制和硬限制。软限制一般可以比较小而硬限制一般数值比较大而且可以修改。如果超出限制则会报错。 资源限制除了允许打开的最大资源数以外，还包括可创建的最大文件长度和进程最大段的大小等。对于用户内核会限制其最大并发进程数。 我们使用p Process.getrlimit(:NOFILE)来获取当前进程的资源限制，在我的电脑上结果是：[256, 9223372036854775807]。 可能我们觉得软限制256有点少，那好，我们尝试给他设置一个大的。使用如下命令： Process.setrlimit(:NOFILE,4096) 环境变量 我们每个人都设置过环境变量，环境变量是包含进程数据的键值对。所有进程都从其父进程继承环境变量，它们由父进程设置并被子进程所继承。每一个进程都有环境变量，环境变量对于特定进程而言是全局性的。比如环境变量PWD对应的值为当前的工作目录等等。环境变量经常作为一种将输入传递到命令行程序中的方法。 参数 所有进程都可以访问名为ARGV的特殊数组（p ARGV），它是一个参数向量或数组。保存了在命令行中传递给当前进程的参数。有些像C语言中main函数中第二个参数：char** argv。 进程名 系统中每一个进程都有名称，进程名可以在运行期间被修改并作为一种通信手段。一般都会有一个全局变量来存储当前进程的名称。可以通过给这个全局变量赋值来修改当前进程的名称。 我们可以用puts $PROGRAM_NAME来打印当前进程的进程名。 退出码 我们写C程序的时候，总是默认加上return 0，可能大家也遇到过其他的返回值，例如exit(1)等，这里的0、1就是退出码。 所有进程在退出时都带有数字退出码(0-255)用于指明进程是否顺利结束。一般退出码为0的进程被认为是顺利结束，其他的退出码则表明出现了错误，不同的退出码代表不同的错误。 尽管退出码通常用来表明不同的错误，它们其实是一种通信手段。作为程序员的你可以以适合自己程序的方式来处理各种进程退出码。 exit 默认进程退出码为0，可以传递指定的退出码。exit 22代表定制进程退出码为22，不指定数字则默认为0，而且指定退出码在0-255之间的数值才是有效的。 exit! 默认进程退出码为1，可以传递指定的退出码。exit!33代表定制进程退出码为33，不指定数字时默认为1，而且指定退出码在0-255之间的数值才是有效的。 abort 会将当前进程的退出码设置为1，而且可以传递一条消息给STDERR。例如abort “Something went wrong!”，则进程退出码为1且会在STDERR中打印“Something went wrong”。注意该方法不能指定退出码。 raise raise方法不会立即结束进程，它只是抛出一个异常，该异常会沿着调用栈向上传递并可能会得到处理。如果没有代码对其进程处理，那么这个未处理的异常将会终结该进程。类似于abort方法，一个未处理的异常会将退出码设置为1。也可以传递一条消息给STDERR。例如raise “Something went wrong!”，则进程退出码为1且会在STDERR中打印“Something went wrong”。注意该方法也不能指定退出码。 fork()与友好进程 fork()系统调用允许运行中的进程以编程的形式创建新的进程，这个心进程和原始进程一模一样。调用fork()的进程被称为父进程，新创建的进程被称为子进程。因子进程是一个全新的进程，所以它拥有自己唯一的进程id。 子进程从父进程处继承了其所占用内存中的所有内容，以及所有属于父进程的已打开的文件描述符的编号。这样，两个进程就可以共享打开的文件、套接字等。因子进程会复制父进程在内存中的所有内容，所以子进程可以随意更改其内存内容的副本，而不会对父进程造成任何影响(后面会介绍COW写时复制技术)。 对于fork()方法的一次调用实际上会返回两次。fork方法创造了一个新进程，在调用进程(父进程)中返回一次，且会返回子进程的pid；在新创建的进程(子进程)中又返回一次，返回0。 fork创建了一个和旧进程一模一样的新进程。所以试想一个使用了500MB内存的进程进行了衍生，那么就有1GB的内存被占用了。重复同样的操作十次，很快就会耗尽内存，这通常被称为“fork炸弹”。 所以现代的Unix/Linux操作系统采用写时复制(copy-on-write, COW)的方法来克服这个问题。COW将实际的内存复制操作推迟到了真正需要写入的时候。所以说父进程和子进程实际上是在共享内存中的数据，直到它们其中一个需要对数据进行修改，届时才会进行内存复制，使得两个进程保持适当的隔离。 这里多补充点COW的知识，自己在面试中也被问到这个问题，当时并不了解这个知识点，所以对这个知识点印象比较深刻。当采用COW技术时，子进程并不完全复制父进程的数据，只是以只读的方式共享父进程的页表，并将符进程的页表项也标记为只读。当父子进程中任何一个进程试图修改这些地址空间时，就会引发系统的页错误异常。异常错误处理程序将会生成该页的一份复制，并修改进程的页表项，指向新生成的页面，并将该页标记为已修改。 除了修改的数据和页面之外，其余的部分依然可以共享。 在一些语言当中，比如ruby中，会通过block代码块来使用fork。将一个block代码块传递给fork方法，那么这个block代码块将在新的子进程中执行，而父进程会跳过block中的内容。而且子进程执行完block之后就会退出，并不会像父进程那样指向随后的代码。 孤儿进程 当父进程结束后而子进程没有结束时，子进程会照常继续运行，此时子进程被称为孤儿进程。孤儿进程会被系统当中的守护进程所收养，该进程是一种长期运行的进程，而且是有意作为孤儿进程存在。 进程等待与僵尸进程 wait是一个阻塞调用，该调用使得父进程一直等到它的某个子进程退出以后才继续执行。wait会返回其等待子进程的pid。wait2会返回两个值(pid, status)。除了pid之外还包括status，该变量存储有大量关于子进程的有用的信息，可让我们获知某个进程是怎样退出的。 wait/wait2是等待任意子进程的退出，而waitpid/waitpid2则是等待特定的由pid指定的子进程退出。 内核将退出的进程信息加入到队列，这样以来父进程就总是能够依照子进程退出的顺序接收到信息。就是说，即使子进程退出而父进程还没有准备妥当的时候，父进程也总能够通过队列获取到每个子进程的退出信息。注意，如果不存在子进程，调用wait的任一变体都会抛出ERRNO::ECHILD异常。所以最好让调用wait的数量和创建的子进程的数量相等才不会抛出异常。 一些服务器会使用看护进程这一模式：有一个衍生出多个并发子进程的进程，这个进程看管这些子进程，确保它们能够保持响应，并对子进程的退出做出响应，这个进程就是看护进程。 内核会将已退出的子进程的状态信息加入队列，所以即便父进程在子进程退出很久之后才调用wait，依然可以获取它的状态信息。内核会一直保留已退出的子进程的状态信息直到父进程调用wait请求这些消息。如果父进程一直不发出请求，那么状态信息就会被内核一直保留着，因此创建一个即发即弃的子进程却不去请求状态信息，便是在浪费内核资源，比如pid，要知道内核可创建的pid和进程控制块PCB是有限的，如果一直创建进程其父进程却不去请求它的退出信息，那么pid和PCB有可能会被耗尽而使得系统无法继续产生新进程。此时的子进程就被称为僵尸进程，所以说僵尸进程是有害的。 任何应结束的进程，如果它的状态信息一直未能读取，那么它就是一个僵尸进程，任何子进程在结束之时其父进程仍在运行，那么这个子进程很快就会称为僵尸进程。一旦父进程读取了僵尸进程的状态信息，那么它就不复存在，也就不再消耗内核资源。 有一种避免僵尸进程出现的方法就是分离父子进程，当父进程新创建一个子进程以后，如果不打算调用wait去等待和读取子进程的退出信息，可以使用detach方法。detach方法核心就是生成一个新线程，这个线程唯一的工作就是等待有pid所指定的那个进程退出并获取进程退出信息，从而确保内核不会一直保留进程的状态信息造成僵尸进程的出现和内核资源的浪费。 那么怎么识别僵尸进程呢？ 很简答，我们使用如下指令：pid = fork{ sleep 1} ; puts pid; sleep的方式，发现结果为：z。 信号量 wait为父进程提供了一种很好方式来监管子进程。但它是一个阻塞调用：直到子进程结束，调用才会返回，任何一行代码都可能被信号中断。信号投递时不可靠的。如果你的代码正在处理CHLD信号，这时候另一个子进程结束了，那么你未必能收到第二个CHLD信号(CHLD信号：提醒父进程子进程退出的信号)。如果同一个信号在极短间隔内被多次收到，就会出现这种情况。这时可以考虑使用wait的非阻塞方法，形如wait(-1, Process::WNOHANG)。当获得一个信号并返回值以后就继续等待信号的产生。 信号是一种异步通信，当进程从内核接收到一个信号时，它可以执行下列某一个操作： 忽略该信号； 执行特定操作； 执行默认操作。 信号有内核发出，信号是由一个进程发送给另一个进程，不过内核作为中介而已。下表为常用信号介绍，大部分信号的默认行为都是终止进程，其中dump动作表示进程会立即结束并进行核心转储(栈跟踪)，而且比较特殊信号有SIGKILL和SIGSTOP信号不能被捕获、阻塞或忽略，SIGSR1和SIGSR2两个信号对应的操作由你的进程来定义。 信号是一个了不起的工具，不过捕获一个信号有点像使用全局变量，有可能把其他程序锁依赖的东西给修改了，不过和全局变量不同的是信号处理程序并没有命名空间。从最佳事件角度来说，个人代码不应该定义任何信号处理程序，除非它是服务器。正如一个从命令行启动的长期运行的进程，库代码极少会捕获信号。 进程可以在任何时候接收到信号，这就是信号的美所在！而且信号是异步的。有了信号，一旦知道了对方的pid，系统中的进程便可以彼此通信，使得信号成为一种极其强大的通信工具，常见的用法是使用kill方法来发送信号。实践当中，信号多是由长期运行的进程响应和使用，例如服务器和守护进程。而多数情况下，发送信号的都是人类用户而非自动化程序。 进程通讯 进程间通信(IPC)两个常见的实用方法是管道和套接字对(socket pairs)。 管道是一个单向数据流。打开一个管道，一个进程拥有管道的一段，另一个进程拥有另一端。然后数据就沿着管道单向传递。因此如果某个进程将自己作为一个管道的reader，而非writer，那么它就无法向管道中写入数据，反之亦然。例如在ruby脚本程序中： 1234 reader，writer = IO.pipewriter.write(&quot;I am writing something..&quot;)writer.closeputs reader.read 结果为： 1 I am writing something.. pipe返回一个包含两个元素的数组，第一个元素为reader的信息，第二个元素为writer的信息。 向管道写完信息就关闭writer，是因为reader调用read方法时，会不停地试图从管道中读取数据，直到读到一个EOF(文件结束标志)。这个标志告诉reader已经读完管道中所有的数据了。只要writer保持打开，那么reader就可能读到更多的数据，因此它就会一直等待。在读取之前关闭writer，将一个EOF放入管道中，这样一来，reader获得原始数据之后就会停止读取。要是忘记或者省去关闭writer这一步，那么reader就会被阻塞并不停地试图读取数据。 因为管道是单向的，所以再上诉程序中，reader只能读取，writer只能写入。 当某个进程衍生出一个子进程的时候，会与子进程共享打开的资源，管道也被认为是一种资源，它有自己的文件描述符等，因此可以与子进程共享。 当使用诸如管道或TCP套接字这样的IO流时，将数据写入流中，之后跟着一些特定协议的分隔符，随后从IO流中读取数据时，一次读取一块(chuck)，遇到分隔符就停止读取。 Unix套接字是一种只能用于在同一台物理主机中进行通信的套接字，它比TCP套接字快很多，非常适合用于IPC。 管道和套接字都是对进程间通信的有益抽象。它们即快速有简单，多被用作通信通道，来代替更为原始的方法，如共享数据库或日志文件。使用哪种方法取决于自己的需要，不过记得管道提供的是单向通信，套接字提供的是双向通信。 终端进程 我们在终端执行每一条命令，其实都是创建了一个终端进程。 exec()系统调用非常简单，它允许使用另一个进程来替换当前进程，exec()这种转变是有去无回的，一旦你将当前进程转变为另外一个别的进程，那就再也变不回来了。 在要生成新进程的时候，fork()+exec()的组合是常见的一种用法，使用fork()创建一个新进程，然后用exec()把这个进程变成自己想要的进程，你的当前进程仍像从前一样运行，也仍可以根据需要生成其他进程。如果程序依赖于exec()调用的输出结果，可用wait方法来确保你的程序一直等到子进程完成它的工作，这样就可取回结果。exec()在默认情况下不会关闭任何打开的文件描述符或进行内存清理。 把字符串传递给exec实际上会启动一个shell进程，然后shell进程对这个字符串进行解释，传递一个数组的话，它会跳过shell，直接将此数组作为新进程的ARGV-参数数组，除非真的需要，一般尽可能地传递数组。 fork()是有成本的，记住这点有益无害，有时候它会成为性能瓶颈，主要是因为fork()的新子进程的两个独特属性： 获得了一份父进程在内存中所有内容的副本； 获得了父进程已打开的所有文件描述符的副本。 有一个系统调用posix_spawn，子保留了第2条，没有保留第1条。posix_spawn所生成的子进程可以访问父进程打开的所有文件描述符，却无法与父进程共享内存。这也是为什么posix_spawn比fork快、更有效率的原因。但事务都有两面性，也会因此而缺乏灵活性。 守护进程 守护进程是在后台运行的进程，不受终端用户控制。Web服务器或数据库服务器都属于常见的守护进程，它们一直在后台运行响应请求。守护进程也是操作系统的核心功能，有很多进程一直在后台运行以保证系统的正常运行，任何进程都可变成守护进程。 当内核被引导时会产生一个叫做init的进程。该进程的pid是1，而ppid是0，作为所有进程的祖父。它是首个进程，没有祖先。一个孤儿进程会被init进程收养，孤儿进程的ppid始终是1，这是内核能够确保一直运行的唯一进程。 每一个进程都属于某个组，每一个组都有唯一的整数id，称为进程组id。进程组是一个相关进程的集合，通常是父进程与子进程。但是也可以按照需要将进程分组，可以通过setpgrp(new_group_ip)方法来设置进程组id。通常情况下，进程组id和进程组组长的id是相同的。进程组组长是终端命令的发起进程。也就是说，如果在终端启动一个进程，那么它就会成为一个新进程组的组长，它所创建的子进程就成为同一个进程组的组员。 这里进一步说明一下，之前讲过孤儿进程，子进程在父进程退出后会被init进程收养而继续运行，这是父进程退出的行为，但是如果父进程由终端控制并被信号终止的话，孤儿进程也会被终止的。这是因为父子进程属于同一个进程组，而父进程由终端控制，当父进程收到来自终端的终止信号时，与父进程属于同一个进程组的子进程也会收到终止信号而被终止。 会话组是更高一级的抽象，它是进程组的集合。一个会话组可以依附于一个终端，也可以不依附与任何终端，比如守护进程。终端用一种特殊的方法来处理会话组：发送给会话领导的信号会被转发到该会话中的所有进程组内，然后再转发到这些进程组中的所有进程。系统调用getsid()可用来检索当前的会话组id。 以下是创建一个守护进程的过程： 首先在终端创建一个进程，并在进程中衍生出一个子进程，然后作为父进程的自己退出。启动该进程的终端察觉到进程退出后，将控制返回给用户，但是衍生出的子进程仍然拥有从父进程中继承而来的组id和会话组id，此时这个衍生进程既非会话领导也非进程组组长。因终端与衍生进程之间仍有牵连，如果终端发送信号到衍生进程的会话组，衍生进程会接收到这个信号，但我们想要的是完全脱离终端。 setsid方法可使得衍生进程成为一个新进程组的组长和新会话组的领导，而且此时新的会话组并没有控制终端。注意，如果在某个已经是进程组组长的进程中调用setsid方法，则会失败，它只能从子进程中调用。 已经成为进程组和会话组组长的衍生进程再次进行衍生，然后自己退出。新衍生出的进程不再是进程组和会话组组长，由于之前会话领导并没有相应的控制终端，且此进程也不是会话领导，因此该进程绝对不会有相应的控制终端存在，如此就可以确保进程现在是完全脱离了控制终端并且可以独立运行。 将进程的工作目录更改为系统的根目录，可避免进程的启动进程出于个各种问题被删除或者卸载。 将所有标准流重定向到“/dev/null”，也就是将其忽略，主要是因为守护进程已不再依附于某个终端会话，所以标准流也就无用了，但是不能简单的关闭，因为一些进程可能还指望它们随时可用。 以下是ruby语言创建一个守护进程的完整程序： 12345678 exit if fork Process.setsidexit if forkDir.chdir &quot;/&quot;STDIN.reopen &quot;/dev/null&quot;STDOUT.reopen &quot;/dev/null&quot;, &quot;a&quot;STDERR.reopen &quot;/dev/null&quot;, &quot;a&quot; 对于是否需要创建一个守护进程，就应该问自己一个基本问题：这个进程是否需要一直保持响应？如果答案为否，那么你也许可以考虑定时任务或后台作业系统，如果答案是肯定的，那就去创建，不用犹豫。 The link of this page is http://home.meng.uno/articles/aeaab565/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"http://home.meng.uno/tags/Unix/"},{"name":"进程","slug":"进程","permalink":"http://home.meng.uno/tags/进程/"}]},{"title":"自制简单搜索引擎及Wiser的使用","slug":"wiser","date":"2018-03-03T06:10:07.000Z","updated":"2021-01-01T18:56:13.000Z","comments":true,"path":"articles/c49b2caf/","link":"","permalink":"http://home.meng.uno/articles/c49b2caf/","excerpt":"自己开发一个搜索引擎，可能是每一个计算机爱好者的梦想，但是当我们看到网上开源搜索引擎那么庞大时，未免有点害怕。那么开发一个搜索引擎真的很难么？ 本博文在阅读了《How to Develop a Search Engineer》之后，总结而成。 代码下载：Wiser 搜索引擎简介 搜索引擎（Search Engine）是指根据一定的策略、运用计算机技术从互联网上搜集信息，在对信息进行组织和处理后，为用户提供检索服务。在日常生活中，可以看到 Google 等 Web 检索网站，还有邮件检索和专利检索等各种应用程序。 背景知识 在自己写一个搜索引擎之前，需要先了解基本的原理和概念。比如分词，","text":"自己开发一个搜索引擎，可能是每一个计算机爱好者的梦想，但是当我们看到网上开源搜索引擎那么庞大时，未免有点害怕。那么开发一个搜索引擎真的很难么？ 本博文在阅读了《How to Develop a Search Engineer》之后，总结而成。 代码下载：Wiser 搜索引擎简介 搜索引擎（Search Engine）是指根据一定的策略、运用计算机技术从互联网上搜集信息，在对信息进行组织和处理后，为用户提供检索服务。在日常生活中，可以看到 Google 等 Web 检索网站，还有邮件检索和专利检索等各种应用程序。 背景知识 在自己写一个搜索引擎之前，需要先了解基本的原理和概念。比如分词，倒排索引，排序算法（BM25、PageRank）等。 搜索引擎工作步骤分为这几步： 爬虫模块 Crawler 在网页上抓取感兴趣的网页数据存储为 Cached pages 索引构造器 Indexer 对 Cached pages 处理生成倒排索引(Inverted Index) 对查询词 Query 在倒排索引中查找对应的文档 Document 计算 Query 和 Document 的关联度，返回给用户 TopK 个结果 根据用户点击 TopK 的行为去修正用户查询的 Query，形成反馈闭环。 搜索引擎的四大组件： 文档管理器(Document Manager) 索引构建器(Indexer) 索引管理器(Index Manager) 索引检索器(Index Searcher) 组件关系图： Wiser使用 编译运行 下载好wiser.zip文件，并解压缩到相应位置，进入文件夹，运行make wiser，稍待片刻，即可完成编译。 收集数据 在本次使用wiser的实验中，直接从https://dumps.wikimedia.org/zhwiki/latest/下载相应的xml文件即可（省去了实际的爬虫过程）。 使用wiser存入sqlite使用命令：wiser -x XXX.xml -m 100 wiki.db 此时，我们已经将10条数据存入.db文件中了。 构建倒排索引 *在上一步已经完成。 检索文档 使用wiser搜索一个关键词使用命令：wiser -q &quot;XXX&quot; wiki.db 排序并呈现 从截图中可见，score代表匹配指数，已经计算好，并返回给我们。 Wiser代码剖析 在此先简单的介绍各个主要的.c文件实现的功能： wiser.c: 主程序，接收命令行输入，并相应的调用其他函数； database.c: 操作sqlite，包括增，查等功能； search.c: 全文检索，TF-IDF求相关度； postings.c: 倒排索引压缩与解压缩； token.c: 创建倒排索引，N-gram分词； wikiload.c: 加载wikipedia上下载的xml文件； util.c: 编码相关的杂项。 其他详情，还请实际使用啊！ The link of this page is http://home.meng.uno/articles/c49b2caf/ . Welcome to reproduce it!","categories":[{"name":"Data Mining","slug":"Data-Mining","permalink":"http://home.meng.uno/categories/Data-Mining/"}],"tags":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://home.meng.uno/tags/搜索引擎/"},{"name":"Wiser","slug":"Wiser","permalink":"http://home.meng.uno/tags/Wiser/"},{"name":"倒排文件","slug":"倒排文件","permalink":"http://home.meng.uno/tags/倒排文件/"}]},{"title":"Eclipse的代码自动规范","slug":"code-format","date":"2018-03-01T12:59:33.000Z","updated":"2021-01-02T13:12:12.000Z","comments":true,"path":"articles/548d5dfd/","link":"","permalink":"http://home.meng.uno/articles/548d5dfd/","excerpt":"不知道大家在用Eclipse的时候有没有感觉到，当代码量一大就看不懂自己代码了呢？有人可能要说了，Eclipse不是自己带有格式化代码工具吗？确实如此，但是我们可能想使用更高级的自动化工具，例如Google-Java-Style和Ali-CodeAnalysis。这篇博文，我将展示如何使用eclipse自带的自动化代码规范工具，以及怎么配置Google-Java-Style和Ali-CodeAnalysis。 使用Eclipse自带代码规范 * 快捷键： Ctrl/Command + Shift + F * 鼠标： * 单个文件：进入文件/对着文件名点右键 > 找到Source > 点","text":"不知道大家在用Eclipse的时候有没有感觉到，当代码量一大就看不懂自己代码了呢？有人可能要说了，Eclipse不是自己带有格式化代码工具吗？确实如此，但是我们可能想使用更高级的自动化工具，例如Google-Java-Style和Ali-CodeAnalysis。这篇博文，我将展示如何使用eclipse自带的自动化代码规范工具，以及怎么配置Google-Java-Style和Ali-CodeAnalysis。 使用Eclipse自带代码规范 快捷键： Ctrl/Command + Shift + F 鼠标： 单个文件：进入文件/对着文件名点右键 &gt; 找到Source &gt; 点击Format (其实就是快捷键的作用！) 项目：对着项目名/包名点右键 &gt; 找到Source &gt; 点击Format 如下截图： 更换成Google Style 文件下载 Eclipse: 进入官网 Google Java Format File: 点击下载 配置 当我们下载了本博客提供的eclipse-java-google-style.xml，就可以开始为formatter改风格了。 打开eclipse的Preferences找到Java，再展开Code Style，找到Formatter。 点击Import，在弹出窗口里选择我们下载的文件，确定即可。 再次进入项目，对着想要格式化的对象进行格式化操作，在进度条走完，我们就得到一份Google Java Style的代码了。 总结 按照相似的步骤，我们也可以Import其他风格的代码规范； Google不仅提供了eclipse上Java的代码规范，还有其他很多规范，详见Goole Style 如果任何代码规范都不和心意，也可以打开某个代码规范，自己做相应的改动。 阿里巴巴代码规范 环境：JDK1.8，Eclipse4+。有同学遇到过这样的情况，安装插件重启后，发现没有对应的菜单项，从日志上也看不到相关的异常信息，最后把JDK从1.6升级到1.8解决问题。 Help -&gt; Install New Software… 输入Update Site地址：https://p3c.alibaba.com/plugin/eclipse/update 回车，然后勾选Ali-CodeAnalysis，再一直点Next Next…按提示走下去就好。 然后就是提示重启了，安装完毕。 注意：有同学反映插件扫描会触发很多 “JPA Java Change Event Handler (Waiting)” 的任务，这个是Eclipse的一个bug，因为插件在扫描的时候会对文件进行标记，所以触发了JPA的任务。卸载JPA插件，或者尝试升级到最新版的Eclipse。附：JPA project Change Event Handler问题解决 插件使用 目前插件实现了开发手册中的53条规则，大部分基于PMD实现，其中有4条规则基于Eclipse实现，支持4条规则的QuickFix功能。 * 所有的覆写方法，必须加@Override注解， * if/for/while/switch/do等保留字与左右括号之间都必须加空格, * long或者Long初始赋值时，必须使用大写的L，不能是小写的l） * Object的equals方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals。 目前不支持代码实时检测，需要手动触发，希望更多的人加入进来一起把咱们的插件做得越来越好，尽量提升研发的使用体验。 代码扫描 可以通过右键菜单、Toolbar按钮两种方式手动触发代码检测。同时结果面板中可以对部分实现了QuickFix功能的规则进行快速修复。 触发扫描 在当前编辑的文件中点击右键，可以在弹出的菜单中触发对该文件的检测。 在左侧的Project目录树种点击右键，可以触发对整个工程或者选择的某个目录、文件进行检测。 也可以通过Toolbar中的按钮来触发检测，目前Toolbar的按钮触发的检测范围与您IDE当时的焦点有关，如当前编辑的文件或者是Project目录树选中的项，是不是感觉与右键菜单的检测范围类似呢。 扫描结果 简洁的结果面板，按规则等级分类，等级-&gt;规则-&gt;文件-&gt;违规项。同时还提供一个查看规则详情的界面。 清除结果标记更方便，支持上面提到的4条规则QuickFix。 查看所有规则 国际化 The link of this page is http://home.meng.uno/articles/548d5dfd/ . Welcome to reproduce it!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://home.meng.uno/categories/Software-Engineering/"}],"tags":[{"name":"代码规范","slug":"代码规范","permalink":"http://home.meng.uno/tags/代码规范/"},{"name":"Google—Java-Style","slug":"Google—Java-Style","permalink":"http://home.meng.uno/tags/Google—Java-Style/"},{"name":"Ali-CodeAnalysis","slug":"Ali-CodeAnalysis","permalink":"http://home.meng.uno/tags/Ali-CodeAnalysis/"},{"name":"Eclipse","slug":"Eclipse","permalink":"http://home.meng.uno/tags/Eclipse/"}]},{"title":"What are Human Genome Project and ENCODE Project?","slug":"genome","date":"2018-02-18T11:54:31.000Z","updated":"2020-12-02T01:46:56.000Z","comments":true,"path":"articles/32469d52/","link":"","permalink":"http://home.meng.uno/articles/32469d52/","excerpt":"Human Genome Project The Profile of the Project 人类基因组计划(Human Genome Project,简称HGP)是由美国科学家于1985年率先提出，又于1990年正式启动的。来自美国、英国、法国、德国、日本和中国科学家共同参与了这一预算达30亿美元的人类基因组计划。按照这个计划的设想，在2005年，要把人体内约10万个基因的密码全部解开，同时绘制出人类基因的谱图。换句话说，就是要揭开组成人体4万个基因的30亿个碱基对的秘密。 这一计划的最终目的是测定人类基因组30亿个基本化学组成（称为碱基对或核苷酸），进而揭开与人类的生老病死有关的数万个","text":"Human Genome Project The Profile of the Project 人类基因组计划(Human Genome Project,简称HGP)是由美国科学家于1985年率先提出，又于1990年正式启动的。来自美国、英国、法国、德国、日本和中国科学家共同参与了这一预算达30亿美元的人类基因组计划。按照这个计划的设想，在2005年，要把人体内约10万个基因的密码全部解开，同时绘制出人类基因的谱图。换句话说，就是要揭开组成人体4万个基因的30亿个碱基对的秘密。 这一计划的最终目的是测定人类基因组30亿个基本化学组成（称为碱基对或核苷酸），进而揭开与人类的生老病死有关的数万个基因的相互关系。1984年，美国能源部开会，开始酝酿“人类基因组计划”。1989年，美国能源部和美国国家卫生研究所提出了人类基因图谱工程。美国在1990年10月1日率先启动人类基因组计划。美国人类基因组顾问委员会委员梅纳德•奥尔森是人类基因组计划最早的推动者之一，另外美国一个测序中心的主任罗伯特•沃特斯顿以及英国的人类基因组总负责人均表示支持。美国完成人类基因组计划近54%的工作量，为人类基因组计划最大的贡献国。英国是人类基因组计划的第二大贡献国，共34%的贡献都是由Wellcome基金会资助的Sanger中心完成的。日本、法国、德国对人类基因组计划的贡献分别为6.8%、2.8%与2.2%。中国承担了3号染色体区域短臂端粒侧约30 cM，约占人类整个基因组1% 的测序工作。中国的华大基因、国家自然科学基金会、中科院遗传所南方基因中心、北方人类基因组中心等单位及于军、杨焕明、汪建、刘斯奇、吴旻、强伯勤、陈竺等也给予人类基因组计划大力的推动。 The Importance of the Project 目的 人类是在“进化”历程上最高级的生物，对人类基因的研究有助于认识自身、掌握生老病死规律、疾病的诊断和治疗、了解生命的起源。 测出人类基因组DNA的30亿个碱基对的序列，发现所有人类基因，找出它们在染色体上的位置，破译人类全部遗传信息。 在人类基因组计划中，还包括对五种生物基因组的研究：大肠杆菌、酵母、线虫、果蝇和小鼠，称之为人类的五种“模式生物”。 HGP的目的是解码生命、了解生命的起源、了解生命体生长发育的规律、认识种属之间和个体之间存在差异的起因、认识疾病产生的机制以及长寿与衰老等生命现象、为疾病的诊治提供科学依据。 意义 人类基因组计划是一项规模宏大，跨国跨学科的科学探索工程。其宗旨在于测定组成人类染色体(指单倍体)中所包含的30亿个碱基对组成的核苷酸序列，从而绘制人类基因组图谱，并且辨识其载有的基因及其序列，达到破译人类遗传信息的最终目的。基因组计划是人类为了探索自身的奥秘所迈出的重要一步。 “人类基因组计划”与”曼哈顿原子弹计划”和”阿波罗计划”并称为二十世纪三大科学计划。 The Achievement of the Project 2000年6月26日，美国总统克林顿与英国首相布莱尔共同宣布人类基因组计划工作草图完成；次年2月，工作草图的具体序列信息、测序所采用的方法以及序列的分析结果被国际人类基因组测序联盟和塞雷拉基因组的科学家分别公开发表于《自然》与《科学》杂志。这一工作草图覆盖了基因组序列的83％，包括常染色质区域的90％（带有150,000个空缺，且许多片断的顺序和方位并没有得到确定）。 2001年2月12日，美国Celera公司与人类基因组计划分别在《科学》和《自然》杂志上公布了人类基因组精细图谱及其初步分析结果。 2003年，发现了新的方法通过检测另外的库来关闭Gaps。使用FISH技术或其他方法来分析没有闭合的Gaps大小。22，21条染色体就是用这种方式。 1999年至2006年，完成了全部23条染色体的测序工作，具体如下： 1999年12月，22号染色体测序完成； 2000年5月，21号染色体测序完成； 2001年12月，20号染色体测序完成； 2003年2月，14号染色体测序完成； 2003年6月，男性特有的Y染色体测序完成； 2003年5月和7月，7号染色体测序完成； 2003年10月，6号染色体测序完成； 2004年4月，13号和19号染色体测序完成； 2004年5月，9号和10号染色体测序完成； 2004年9月，5号染色体测序完成； 2004年12月，16号染色体测序完成； 2005年3月，X染色体测序完成； 2005年4月，2号和4号染色体测序完成； 2005年9月，18号染色体测序完成； 2006年1月，8号染色体测序完成； 2006年3月，11号,12号和15号染色体测序完成； 2006年4月，17号和3号染色体测序完成；Human Genome Project Information 2006年5月，1号染色体测序完成；Human Genome Project Information 2004年，国际人类基因组测序联盟的研究者宣布，人类基因组中所含基因的预计数目从先前的30,000至40,000（在计划初期的预计数目则高达2,000,000）调整为20,000至25,000。预期还需要多年的时间来确定人类基因组中所含基因的精确数目。 截止到2005年，人类基因组计划的测序工作已经完成。 The Research Contents of the Project 遗传图谱 遗传图谱又称连锁图谱（linkage map），它是以具有遗传多态性（在一个遗传位点上具有一个以上的等位基因，在群体中的出现频率皆高于1%）的遗传标记为“路标”，以遗传学距离（在减数分裂事件中两个位点之间进行交换、重组的百分率，1%的重组率称为1cM）为图距的基因组图。遗传图谱的建立为基因识别和完成基因定位创造了条件。意义：6000多个遗传标记已经能够把人的基因组分成6000多个区域，使得连锁分析法可以找到某一致病的或表现型的基因与某一标记邻近（紧密连锁）的证据，这样可把这一基因定位于这一已知区域，再对基因进行分离和研究。对于疾病而言，找基因和分析基因是个关键。 物理图谱 物理图谱是指有关构成基因组的全部基因的排列和间距的信息，它是通过对构成基因组的DNA分子进行测定而绘制的。绘制物理图谱的目的是把有关基因的遗传信息及其在每条染色体上的相对位置线性而系统地排列出来。DNA物理图谱是指DNA链的限制性酶切片段的排列顺序，即酶切片段在DNA链上的定位。因限制性内切酶在DNA链上的切口是以特异序列为基础的，核苷酸序列不同的DNA，经酶切后就会产生不同长度的DNA片段，由此而构成独特的酶切图谱。因此，DNA物理图谱是DNA分子结构的特征之一。DNA是很大的分子，由限制酶产生的用于测序反应的DNA片段只是其中的极小部分，这些片段在DNA链中所处的位置关系是应该首先解决的问题，故DNA物理图谱是顺序测定的基础，也可理解为指导DNA测序的蓝图。广义地说，DNA测序从物理图谱制作开始，它是测序工作的第一步。制作DNA物理图谱的方法有多种，这里选择一种常用的简便方法──标记片段的部分酶解法，来说明图谱制作原理。 序列图谱 随着遗传图谱和物理图谱的完成，测序就成为重中之重的工作。DNA序列分析技术是一个包括制备DNA片段化及碱基分析、DNA信息翻译的多阶段的过程。通过测序得到基因组的序列图谱。 基因图谱 简介 基因图谱是在识别基因组所包含的蛋白质编码序列的基础上绘制的结合有关基因序列、位置及表达模式等信息的图谱。在人类基因组中鉴别出占具2%~5%长度的全部基因的位置、结构与功能，最主要的方法是通过基因的表达产物mRNA反追到染色体的位置。 意义 它能有效地反应在正常或受控条件中表达的全基因的时空图。通过这张图可以了解某一基因在不同时间不同组织、不同水平的表达；也可以了解一种组织中不同时间、不同基因中不同水平的表达，还可以了解某一特定时间、不同组织中的不同基因不同水平的表达。人类基因组是一个国际合作项目：表征人类基因组，选择的模式生物的DNA测序和作图，发展基因组研究的新技术，完善人类基因组研究涉及的伦理、法律和社会问题，培训能利用HGP发展起来的这些技术和资源进行生物学研究的科学家，促进人类健康。 The Contributions of the Project 对人类疾病的贡献 人类疾病相关的基因是人类基因组中结构和功能完整性至关重要的信息。对于单基因病，采用“定位克隆”和“定位候选克隆”的全新思路，导致了亨廷顿氏舞蹈症、遗传性结肠癌和乳腺癌等一大批单基因遗传病致病基因的发现，为这些疾病的基因诊断和基因治疗奠定了基础。对于心血管疾病、肿瘤、糖尿病、神经精神类疾病（老年性痴呆、精神分裂症）、自身免疫性疾病等多基因疾病是疾病基因研究的重点。健康相关研究是HGP的重要组成部分，1997年相继提出：“肿瘤基因组解剖计划”“环境基因组学计划”。 对医学的贡献 基因诊断、基因治疗和基于基因组知识的治疗、基于基因组信息的疾病预防、疾病易感基因的识别、风险人群生活方式、环境因子的干预。 对生物技术的贡献 基因工程药物 分泌蛋白（多肽激素，生长因子，趋化因子，凝血和抗凝血因子等）及其受体。 诊断和研究试剂 基因和抗体试剂盒、诊断和研究用生物芯片、疾病和筛药模型。 细胞工程 胚胎和成年期干细胞、克隆技术、器官再造技术。 The Project with China 作为继美、英、法、德、日6个成员国之后中唯一的发展中国家，中国对人类基因组的的贡献不只是工作量，在这个划时代的里程碑上，已经刻上了中国人的名字，中国在生物组学的发展上占有一席之地，通过参与这一计划，我们可以分享数据、资源、技术与发言权，最终来开发我国自己的基因资源。中国的加入改变了国际人类基因组计划原有的组织格局，提高其国际合作的形象，带来了国际社会对“国际人类基因组计划精神”的支持，联合国教科文组织关于人类基因组基本信息免费共享的声明，就是在中国代表的直接努力下促成的。可以说，中国需要人类基因组计划，而基因组计划也使我国的基因测序能力进人世界前列，在中国本土成长起来的作为我国基因组学的典型代表、创新型机构——华大基因已经成为全球最大的基因组学中心。 因此，人类基因组计划对华大基因的影响力也是举足轻重的，华大基因也因此而“生”的伟大。华大基因随着“国际人类基因组计划1%项目”的正式启动而诞生。华大基因自成立之日起就站在世界同步的轨迹上，使得中国的基因组学研究位于跟踪——参与——同步的国际地位。为后期的华大基因在基因组上的引领及跨越式发展奠定了基础。 在人类基因组计划之后，人类基因研究开始朝着与人类生育健康、肿瘤个体化治疗、病原微生物、遗传性疾病、血液病等的相关疾病的基因检测方向发展，未来，医疗技术将从末端的疾病治疗，逐步走向前端的基因诊断和预防，个性化医疗及精准医疗。人类将通过基因检测技术、通过个性化医疗以更精确的诊断，预测潜在疾病的风险，提供更有效、更有针对性的治疗，预防某种疾病的发生，比“治有病”更节约治疗成本。 华大基因希望凭借全球领先的基因组学技术，华大基因将千万家庭远离遗传性出生缺陷，肿瘤能早期检测和诊断并能全景式、定期监控个人健康动态，人人做到“我的基因我知道，我的健康我做主”。其研究方向主要涉及遗传性出生缺陷、肿瘤、心脑血管疾病、精准医疗 # The ENCODE Project The Profile of the Project The ENCODE Project（即Encyclopedia Of DNA Elements，中文译作DNA元件百科全书计划），是美国国立人类基因组研究院（US National Human Genome Research Institute，NHGRI）在2003年9月启动的跨国研究项目。该项目旨在解析人类基因组中的所有功能性元件，它是人类基因组计划完成之后，又一重要的跨国基因组学研究项目。该项目联合了来自美国，英国，西班牙，新加坡和日本的32个实验室的422名科学家的努力，获得了迄今最详细的人类基因组分析数据（他们获得并分析了超过15兆兆字节的原始数据）。研究花费了约300年的计算机时间，对147个组织类型进行了分析，以确定哪些能打开和关闭特定的基因，以及不同类型细胞之间的“开关”存在什么差异。 The Achievement of the Project 近年来基因研究已经取得巨大进展。不过，迄今为止，这些研究主要还集中在编码蛋白的特定基因上，而它们所佔的比例不到整个人类基因组的2%。ENCODE计划首次系统地研究了所有类型的功能元件的位点和组织方式。 迄今为止，ENCODE计划主要集中研究了44个靶标共3000万个DNA硷基对。负责该计划数据整合和分析工作的欧洲分子生物学实验室主任Ewan Birney说：“我们的结论揭示了有关DNA功能元件构成的重要原理，为从DNA转录到哺乳动物进化的一切过程提供了新的认识。” 研究发现，人类基因组中的大多数DNA都会转录成RNA，这些副本会普遍交叠。因此，人类基因组实际上是一个非常复杂的网络，所谓的无用基因实际上非常少。基因只不过是众多具有特定功能的DNA序列类型之一。科学家们在基因之外的调控区域新发现了4491个转录启动位点，这一数字超过了已知基因的10倍。这些都挑战了长期以来的一个观点，即基因组中的基因是孤立的，同时，新的发现也支持了人类基因数量应该超过3万个的看法。 ENCODE计划的另一个巨大成就就是对哺乳动物基因组进化的认识。传统理论认为，与生理功能相关的重要DNA序列往往位于基因组中的“进化限制”区域，它们在物种进化过程中更容易保存下来。但是，最新的研究表明，大约一半人类基因组中的功能元件在进化过程中不会受到很大限制。科学家认为，哺乳动物缺乏“进化限制”这一点，很可能意味著许多物种的基因组都囊括了大量包括RNA转录副本在内的功能元件，在进化过程中，这些功能元件成了基因“仓库”。 此次ENCODE计划的成果亮点还包括：确定了许多之前不为人知的DNA转录启动位点；推翻了传统观点的认识，调控区域也有可能位于DNA转录启动位点的下游；确定了组蛋白变化的特定标记；加深了人们对组蛋白改变协调DNA复制的理解。 2012年9月5日，ENCODE项目的阶段性研究结果被整理成30篇论文发表于《自然》（6篇），《基因组研究》（6篇）和《基因组生物学》（18篇）上。 研究结果显示，人类基因组内的非编码DNA至少80%是有生物活性的，而并非之前认为的“垃圾” DNA （junk DNA）。这些新的发现有望帮助研究人员理解基因受到控制的途径，以及澄清某些疾病的遗传学风险因子。 ENCODE是人类基因组计划之后国际科学界在基因组学研究领域取得的又一重大进展。 2012年12月21日，ENCODE项目被《科学》杂志评为本年度十大科学突破之一。 The Research Contents of the Project 试点研究的内容 对编码的功能DNA进行鉴定和分类；对已存在的几种方法进行测试和比较，严格分析了人类基因组序列中已被定义的序列。 阐明人类生物学和疾病之间的关系。 对大量鉴定基因特征的方法、技术和手段进行检测和评估。 研究对象 编码蛋白基因 非编码蛋白基因 调控区域 染色体结构维持和调节染色体复制能力的DNA元件 研究特点 采用综合性研究策略 重视新技术的研发 将计划向学术界和公司开放 The Contributions of the Project 人细胞转录全景图 通过ENCODE项目，人们知道RNA是基因组编码的遗传信息的直接输出。细胞的大部分调节功能都集中在RNA的合成、加工和运输、修饰和翻译之中。研究人员证实，75%的人基因组能够发生转录，并且观察到几乎所有当前已标注的RNA和上千个之前未标注的RNA的表达范围与水平、定位、加工命运、调节区和修饰。总之，这些观察结果表明人们需要重新定义基因的概念。 人基因组中可访问的染色质全景图 DNase I超敏感位点(DNase I hypersensitive sites, DHSs)是调节性DNA序列的标记物。研究人员通过对125个不同的细胞和组织类型进行全基因组谱分析而鉴定出大约290万个人DHSs，并且首次大范围地绘制出人DHSs图谱。 基因启动子的远距离相互作用全景图 在ENCODE项目中，研究人员选择1%的基因组作为项目试点区域，并且利用染色体构象捕获碳拷贝(chromosome conformation capture carbon copy, 简称为5C)技术来综合性地分析了这个区域中转录起始位点和远端序列元件之间的相互作用。他们获得GM12878、K562和HeLa-S3细胞的5C图谱。在每个细胞系，他们发现启动子和远端序列元件之间存在1000多个远距离相互作用。 GENCODE：ENCODE项目的人基因组参照标注 GENCODE项目旨在利用计算分析、人工标注和实验验证来鉴定出人基因组中所有的基因特征。GENCODE第七版(GENCODE v7)公开发布了基因组标注数据集，包含了20687个蛋白编码的RNA基因座位、9640个长链非编码RNA基因座位，并且拥有33977个在UCSC基因数据库和RefSeq数据库中不存在的编码性转录本。它还对公开获得的长链非编码RNA(long noncoding RNA, lncRNA)进行最全面的标注。 我的认识 在上这门课之前，我从没认真想过这个问题，到底研究基因有什么用？通过这几天的学习，以及对文章所提的两个项目的检索、认识，我对基因测序这一工作，有了更深层次的认识。 虽然外界关于基因测序有不同的看法，例如有人支持，因为它可以为医学做贡献；有人反对，因为这样做相当于为基因做了一次曝光，这样一来，就有优劣基因之分。在我看来，这一任务还是利大于弊的，毕竟现在看来是这样。科学家可以通过对已有的基因测序结果的分析，总结出基因的“中心法则”，使我们对自身有了更进一步的了解。再者，基因分析有很多好的应用，通过对胎儿基因分析可以达到优生的目的，以及对有基因缺陷、先天性遗传病患者可以提供治标治本的治疗方案。 当然，要了解所有基因的功能还有很长的一段路要走。例如以前人们所认为的垃圾DNA实际上并不“垃圾”，它们在基因组的进化、每个个体的差异性以及许多其他方面扮演着重要角色，是世界上许多实验室着力研究的目标。 即使已经过了将近30年，人类基因组也没有完成“完全”测序，不过我们了解到了基因并不是静态的，而是处在复杂的变化之中，所以对人类基因的研究也是对人类自身的研究，这一研究将会一直进行下去，永无终点。 虽然人类基因组目前也只是一张初步的蓝图，需要经过更多的研究和分析。但是人类已经通过对基因组的学习，进入了医学的新纪元，为预防、诊断和治疗疾病带来了新的方法。所以对基因组的研究势必将成为人类新的曙光。 总之，我对基因组计划以及ENCODE计划充满期待与支持。 参考资料 HGP计划百度百科：http://dwz.cn/3ITVf3 人类基因组计划- 维基百科http://dwz.cn/3JHOap 科学松鼠会之人类基因组计划 http://dwz.cn/3JHOXZ ENCODE项目百度百科：http://dwz.cn/3ITSPr Genome网 https://www.genome.gov/10005107/encode-project ENCODE项目官网：https://www.encodeproject.org “DNA元件百科全书”首批成果出炉，链接：http://big5.cas.cn/xw/kjsm/gjdt/200706/t20070619_1011212.shtml The link of this page is http://home.meng.uno/articles/32469d52/ . Welcome to reproduce it!","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"http://home.meng.uno/categories/Bioinformatics/"}],"tags":[{"name":"生物信息","slug":"生物信息","permalink":"http://home.meng.uno/tags/生物信息/"},{"name":"Genome","slug":"Genome","permalink":"http://home.meng.uno/tags/Genome/"},{"name":"ENCODE","slug":"ENCODE","permalink":"http://home.meng.uno/tags/ENCODE/"}]},{"title":"关于比特币（Bitcoin）","slug":"bitcoins","date":"2018-02-14T11:47:44.000Z","updated":"2020-12-02T01:41:40.000Z","comments":true,"path":"articles/7bfe1542/","link":"","permalink":"http://home.meng.uno/articles/7bfe1542/","excerpt":"比特币术语 比特币 首字母大写的Bitcoin用来表示比特币的概念或整个比特币网络本身。例如：“今天我学了些有关Bitcoin协议的内容。” 而没有大写的bitcoin则表示一个记账单位。例如：“我今天转出了10个bitcoin。”该单位通常也简写为BTC或XBT。 比特币地址 比特币地址就像一个物理地址或者电子邮件地址。这是别人付给你比特币时你唯一需要提供的信息。然而一个重要的区别是，每个地址应该只用于单笔交易。 对等式网络 对等式网络是指，通过允许单个节点与其他节点直接交互，从而实现整个系统像有组织的集体一样运作的系统 。对于比特币来说，比特币网络以这样一种方式构建——每个用户都在传","text":"比特币术语 比特币 首字母大写的Bitcoin用来表示比特币的概念或整个比特币网络本身。例如：“今天我学了些有关Bitcoin协议的内容。” 而没有大写的bitcoin则表示一个记账单位。例如：“我今天转出了10个bitcoin。”该单位通常也简写为BTC或XBT。 比特币地址 比特币地址就像一个物理地址或者电子邮件地址。这是别人付给你比特币时你唯一需要提供的信息。然而一个重要的区别是，每个地址应该只用于单笔交易。 对等式网络 对等式网络是指，通过允许单个节点与其他节点直接交互，从而实现整个系统像有组织的集体一样运作的系统 。对于比特币来说，比特币网络以这样一种方式构建——每个用户都在传播其他用户的交易。而且重要的是，不需要银行作为第三方。 哈希率 哈希率是衡量比特币网络处理能力的测量单位。为保证安全，比特币网络必须进行大量的数学运算。当网络达到10Th/秒的哈希率时，就意味着它能够进行每秒10万亿次的计算。 交易确认 交易确认意味着一笔交易已经被网络处理且不太可能被撤销。当交易被包含进一个块时会收到一个确认，后续的每一个块都对应一个确认。对于小金额交易单个确认便可视为安全，然而对于比如1000美元的大金额交易，等待6个以上的确认比较合理。每一个确认都成指数级地降低交易撤销的风险。 块链 块链是一个按时间顺序排列的比特币交易公共记录。块链由所有比特币用户共享。它被用来验证比特币交易的永久性并防止双重消费。 密码学 密码学是数学的一个分支，它让我们创造出可以提供很高安全性的数学证明。电子商务和网上银行也用到了密码学。对于比特币来说，密码学用来保证任何人都不可能使用他人钱包里的资金，或者破坏块链。密码学也用来给钱包加密，这样没有密码就用不了钱包。 签名 密码学签名是一个让人可以证明所有权的数学机制。对于比特币来说，一个比特币钱包和它的私钥通过一些数学魔法关联到一起。当你的比特币软件用对应的私钥为一笔交易签名，整个网络都能知道这个签名和已花费的比特币相匹配。但是，世界上没有人可以猜到你的私钥来窃取你辛苦赚来的比特币。 钱包 比特币钱包大致实体钱包在比特币网络中的等同物。钱包中实际上包含了你的私钥，可以让你消费块链中分配给钱包的比特币。和真正的钱包一样，每个比特币钱包都可以显示它所控制的所有比特币的总余额，并允许你将一定金额的比特币付给某人。这与商家进行扣款的信用卡不同。 区块 一个块是块链中的一条记录，包含并确认待处理的交易。平均约每10分钟就有一个包含交易的新块通过挖矿的方式添加到块链中。 双重消费 如果一个不怀好意的用户试图将比特币同时支付给两个不同的收款人，就被称为双重消费。比特币挖矿和块链将就两比交易中那笔获得确认并被视为有效在网络上达成一致。 私钥 私钥是一个证明你有权从一个特定的钱包消费比特币的保密数据块，是通过一个密码学签名来实现的 。如果你使用的是钱包软件，你的私钥就存储在你的计算机内；如果使用的是在线钱包，你的私钥就存储在远程服务器上。千万不能泄露私钥，因为它们可以让你消费对应比特币钱包里的比特币。 挖矿 比特币挖矿是利用计算机硬件为比特币网络做数学计算进行交易确认和提高安全性的过程。作为对他们服务的奖励，矿工可以得到他们所确认的交易中包含的手续费，以及新创建的比特币。挖矿是一个专业的、竞争激烈的市场，奖金按照完成的计算量分割。并非所有的比特币用户都挖矿，挖矿赚钱也并不容易。 Bit Bit是标明一个比特币的次级单位的常用单位 1,000,000 bit 等于1 比特币 (BTC 或 B⃦).，这个单位对于标示小费、商品和服务价格更方便。 BTC BTC 是用于标示一个比特币 (B⃦). 的常用单位。 比特币账户 我们可以在bitcoin.org上选择自己的钱包。我在这里向大家展示使用一个浏览器插件GreenAddress，下载链接是：https://chrome.google.com/webstore/detail/greenaddress/dgbimgjoijjemhdamicmljbncacfndmp/related 注册 打开安装好的GreenAddress，没有账户点击右上角，开始注册。 打码的位置请保存下来，需要用它来在其他地方登录或者恢复钱包 接着是验证你保存没保存（想的还很周到）。 再就是添加两步验证，这个比较常见了，我只选了“邮件”验证，推荐是选两个，要不然总是有warning。 使用 接着就进入主界面了，有很多配置需要大家自己去查看，主界面显示了你的“Bitcoin URI”，分享这个，别人就可以向你转钱了，当然别人也可以扫描你的二维码向你转钱。 最后强调一下，我的比特币地址是：15HVex2sMR1XRZXS14VGGqpQwvoMAeTAgc The link of this page is http://home.meng.uno/articles/7bfe1542/ . Welcome to reproduce it!","categories":[{"name":"FinTech","slug":"FinTech","permalink":"http://home.meng.uno/categories/FinTech/"}],"tags":[{"name":"比特币","slug":"比特币","permalink":"http://home.meng.uno/tags/比特币/"},{"name":"Bitcoin","slug":"Bitcoin","permalink":"http://home.meng.uno/tags/Bitcoin/"}]},{"title":"简单的Python3爬虫","slug":"crawl-py","date":"2018-02-12T12:18:15.000Z","updated":"2020-12-02T01:42:21.000Z","comments":true,"path":"articles/51d32f19/","link":"","permalink":"http://home.meng.uno/articles/51d32f19/","excerpt":"我们先从分析原理入手，然后再使用Python提供的基本的库urllib。 注意，我全程使用的是Python3，如果你必须使用不同版本，请自行百度某些库及函数的转换，需要使用的库不一定你的电脑上预装了，所以请自行百度安装。 原理 网络爬虫，也叫网络蜘蛛(Web Spider)，如果把互联网比喻成一个蜘蛛网，Spider就是一只在网上爬来爬去的蜘蛛。网络爬虫就是根据网页的地址来寻找网页的，也就是URL。 URL URL就是统一资源定位符(Uniform Resource Locator)，它的一般格式如下(带方括号[]的为可选项)： protocol ://hostname[:port]/","text":"我们先从分析原理入手，然后再使用Python提供的基本的库urllib。 注意，我全程使用的是Python3，如果你必须使用不同版本，请自行百度某些库及函数的转换，需要使用的库不一定你的电脑上预装了，所以请自行百度安装。 原理 网络爬虫，也叫网络蜘蛛(Web Spider)，如果把互联网比喻成一个蜘蛛网，Spider就是一只在网上爬来爬去的蜘蛛。网络爬虫就是根据网页的地址来寻找网页的，也就是URL。 URL URL就是统一资源定位符(Uniform Resource Locator)，它的一般格式如下(带方括号[]的为可选项)： protocol ://hostname[:port]/path/[;parameters][?query]#fragment 可见，一个URL包含三个部分： protocol：协议，例如https，http等； hostname[:port]：主机名(端口号为可选参数)，一般网站默认的端口号为80，例如我的博客域名www.meng.uno，可以作为主机名使用; path：第三部分就是主机资源的具体地址，如目录和文件名等。 爬虫就是向URL发送请求，然后得到响应，基本就实现了爬取网页的功能。 URI可以分为URL,URN或同时具备locators 和names特性的一个东西。URN作用就好像一个人的名字，URL就像一个人的地址。换句话说：URN确定了东西的身份，URL提供了找到它的方式。 从浏览器发送和接收数据看起 进入我的首页www.meng.uno，打开浏览器的“检查”功能，选项卡选到“Network”，然后点击所有文章，随便选择一条，我们可以发现如下截图的&quot;Headers&quot; 我们可以发现最明显的有两个区域（我已经圈出来了）：“request”和“response”。从字面意思上来看，我们就知道分别是（发送的）请求和（收到的）回复。 接收的信息是我们请求的网页给的，不用我们管，但是“请求的网页”是我们需要提前设定的，当然最简单的方式就是什么都不设置。爬虫会增加网站的负荷，所以很多网站希望大家通过API的方式使用其开放的资源而禁止爬虫，其中的一个做法就是判断你的请求内容（不全的基本都是爬虫）。于是，为了做到一个完整的可用的爬虫，我们需要模拟真实用户的请求，这就要求我们伪造“User Agent”。 常见的“User Agent”列举如下： Android Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19 Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30 Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1 Firefox Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0 Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0 Google Chrome Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36 Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19 iOS Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3 Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3 User Agent已经设置好了，但是还应该考虑一个问题，程序的运行速度是很快的，如果我们利用一个爬虫程序在网站爬取东西，一个固定IP的访问频率就会很高，这不符合人为操作的标准，因为人操作不可能在几ms内，进行如此频繁的访问。所以一些网站会设置一个IP访问频率的阈值，如果一个IP访问频率超过这个阈值，说明这个不是人在访问，而是一个爬虫程序。 一个很简单的解决办法就是设置延时，但是这显然不符合爬虫快速爬取信息的目的，所以另一种更好的方法就是使用IP代理。使用代理的步骤： 调用urlib.request.ProxyHandler()，proxies参数为一个字典； 创建Opener(类似于urlopen，这个代开方式是我们自己定制的)； 安装Opener； 这个网站提供了很多代理主机：http://www.xicidaili.com/ 正则表达式 我直接以表格的形式呈现好了： 元字符 说明 . 代表任意字符 [ ] 匹配内部的任一字符或子表达式 [^] 对字符集和取非 - 定义一个区间 \\ 对下一字符取非（通常是普通变特殊，特殊变普通） * 匹配前面的字符或者子表达式0次或多次 *? 惰性匹配上一个 + 匹配前一个字符或子表达式一次或多次 +? 惰性匹配上一个 ? 匹配前一个字符或子表达式0次或1次重复 {n} 匹配前一个字符或子表达式 {m,n} 匹配前一个字符或子表达式至少m次至多n次 {n,} 匹配前一个字符或者子表达式至少n次 {n,}? 前一个的惰性匹配 ^ 匹配字符串的开头 \\A 匹配字符串开头 $ 匹配字符串结束 [\\b] 退格字符 \\c 匹配一个控制字符 \\d 匹配任意数字 \\D 匹配数字以外的字符 \\t 匹配制表符 \\w 匹配任意数字字母下划线 \\W 不匹配数字字母下划线 代码 简单带错误信息的获取网页内所有URL的爬虫 1234567891011121314151617181920212223242526272829303132333435363738 #获取URL的包import urllib#获取字符集编码方式import chardet#正则表达式import re#Request 对象req = urllib.request.Request(\"http://meng.uno/\")data = Nonetry: #得到Response response = urllib.request.urlopen(req,data) #读出response == 请求文件的全部字符 html = response.read() #获取这个response的编码方式 charset = chardet.detect(html) print(\"编码方式：\",charset) #以这种编码方式解码打印 html = html.decode(charset.get(\"encoding\")) print(html) urls = re.findall('href=\\\"https*://w*\\.*meng\\.uno/.*?\\\"', html,re.S) uris = re.findall('href=\\\"/[^/].*?[^\\.]\\\"',html, re.S) for item in urls: print(item[6:-1]) for item in uris: if \".html\" in item: print(\"http://www.meng.uno\"+item[6:-1]) elif '.' in item: continue else: print(\"http://www.meng.uno\"+item[6:-1])except urllib.error.HTTPError as e: if hasattr(e, 'code'): print(\"HTTPError\") print(e.code) elif hasattr(e, 'reason'): print(\"URLError\") print(e.reason) 模拟真实环境的爬虫 12345678910111213141516171819 import urllib #访问网址url = 'http://www.whatismyip.com.tw/'#这是代理IPproxy = &#123;'https':'110.73.48.189:8123'&#125;#创建ProxyHandlerproxy_support = urllib.request.ProxyHandler(proxy)#创建Openeropener = urllib.request.build_opener(proxy_support)#添加User Angentopener.addheaders = [('User-Agent','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36')]#安装OPenerurllib.request.install_opener(opener)#使用自己安装好的Openerresponse = urllib.request.urlopen(url)#读取相应信息并解码html = response.read().decode(\"utf-8\")#打印信息print(html) 通过队列获取网站所有URL的爬虫 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 #python系统关于队列的包import queue#获取URL的包import urllib#获取字符集编码方式import chardet#正则表达式import reinitial_page = \"http://www.meng.uno\"url_queue = queue.Queue()seen = set()seen.add(initial_page)url_queue.put(initial_page)def extract_urls(url): req = urllib.request.Request(url) #得到Response response = urllib.request.urlopen(req) #读出response == 请求文件的全部字符 html = response.read() #获取这个response的编码方式 charset = chardet.detect(html) #以这种编码方式解码打印 html = html.decode(charset.get(\"encoding\")) urls = re.findall('href=\\\"https*://w*\\.*meng\\.uno/.*?\\\"', html,re.S) uris = re.findall('href=\\\"/[^/].*?[^\\.]\\\"',html, re.S) tempseen = set() for item in urls: tempseen.add(item[6:-1]) for item in uris: if \".html\" in item: tempseen.add(\"http://www.meng.uno\"+item[6:-1]) elif '.' in item: continue else: tempseen.add(\"http://www.meng.uno\"+item[6:-1]) return tempseen while(True): #一直进行直到海枯石烂 if url_queue.qsize()&gt;0: current_url = url_queue.get() #拿出队例中第一个的url print(current_url) #把这个url代表的网页存储好 for next_url in extract_urls(current_url): #提取把这个url里链向的url if next_url not in seen: seen.add(next_url) url_queue.put(next_url) else: break 这里先简单解释，以后有实际项目会再补充！ The link of this page is http://home.meng.uno/articles/51d32f19/ . Welcome to reproduce it!","categories":[{"name":"Application","slug":"Application","permalink":"http://home.meng.uno/categories/Application/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"http://home.meng.uno/tags/Python3/"},{"name":"爬虫","slug":"爬虫","permalink":"http://home.meng.uno/tags/爬虫/"}]},{"title":"Java开发中的小细节","slug":"2length","date":"2018-02-10T13:58:04.000Z","updated":"2021-01-06T18:11:18.000Z","comments":true,"path":"articles/61c2f1f1/","link":"","permalink":"http://home.meng.uno/articles/61c2f1f1/","excerpt":".length与length()的区别 当我们需要使用数组或者字符串长度时，习惯了使用IDE自动补全的我们是否知道.length与length()的区别喻原因呢？ 上面问题的答案是： * 数组使用.length属性 * 字符串使用length()方法 为什么数组有.length属性？ 在Java中，数组是容器对象，其中包含了固定数量的同一类型的值，一旦数组创建，其长度就是固定的了，于是，其长度可以作为一个属性。 为什么字符串需要length()方法？ Java中的String，实际上是一个char类型数组，而char[]已经有了.length属性，所以在实现String时就没必要再","text":".length与length()的区别 当我们需要使用数组或者字符串长度时，习惯了使用IDE自动补全的我们是否知道.length与length()的区别喻原因呢？ 上面问题的答案是： 数组使用.length属性 字符串使用length()方法 为什么数组有.length属性？ 在Java中，数组是容器对象，其中包含了固定数量的同一类型的值，一旦数组创建，其长度就是固定的了，于是，其长度可以作为一个属性。 为什么字符串需要length()方法？ Java中的String，实际上是一个char类型数组，而char[]已经有了.length属性，所以在实现String时就没必要再定义重复的属性了，于是需要定义一个方法来返回其长度。 Substring()的实现 写过Java的人应该都用过substring(int bedinIndex, int endIndex)方法。我发现这个简单的方法在实现上居然经过了一次大的变革。 substring()的用途 代码: 123 String origin = \"asdfg\"; origin = origin.substring(1,3);System.out.println(origin); 输出: 1 sd 我们发现它能将原始字符串中从下标为beginIndex到endIndex-1之间的子串取出。那它是怎么实现的呢？ substring()的实现 Java中的字符串有三个域：char value[], int offset以及int count，它们分别存储字符串的值，起始下标与长度。 JDK6版本 在这个版本中，每次执行substring()方法时并不会新建新的string，仅仅只是将上述三个域中的offset，count做必要的修改。返回对象仍指向原来的数据。 这样一来，缺点就比较明显：当原始字符串比较长，而截取的子串比较短时，在后续的使用中就会浪费大量的空间。 JDK7+版本 在上一个版本基础上，这个方法进行了改进，每次使用这个方法都会新建一个string对象，并将其返回。 String+操作分析 背景 昨天，我和队友讨论字符串拼接问题时，他提到了这个问题：直接“+”操作好像是生成了临时的一个新String，然后拼接，再复制给原来的String。带着这个问题，我查了下，得出以下的结论。 结论 我查到的信息之一这样说：因为“+”拼接字符串，每拼接一次都是再内存重新开辟一个新的内存区域（堆里边）,然后把得到的新的字符串存在这块内存，字符串如果很大，循环次多又多，那么浪费了很多时间和空间的开销。 我查到的信息之二这么说：当拼接次数较少时，其实编译器会将其优化为StringBuilder类型，只是当拼接次数特别多时，编译器优化时将会产生过多的StringBuilder类型，从而导致空间浪费。 策略 当拼接次数较少时，我们可以直接使用“+”操作，而当拼接数量较大时，我们最好使用StringBuilder类型。 操作 123 StringBuilder SB = new StringBuilder();SB.append(……);String Result = SB.toString(); Java中的Lambda表达式 Java8已经更新了好久了。变化很大，其中最广为人知的就是Lambda表达式。 Lambda表示式 Lambda的基本格式为()-&gt;内容,以箭头为分隔，左边为参数区，右边为代码区。 可以把它看成是一个临时定义的方法，没有方法名，默认为public,根据代码区里的有没有return来决定返回值．如果没有return则是void。 单行代码可以省略return 多行代码使用{}来包裹. 代码内可以引用外部局部变量，实际上是隐式的把变量变成final 网上用得最多的例子就是(x,y)-&gt;x+y;,很蛋疼的例子，当初看Junit单元测试时也是这样的例子。 来个实际点的例子吧： 12345 public boolean isEmpty(String text)&#123; return text==null||\"\".eqauls(text)&#125;//用Lambda是(text)-&gt;return text==null||\"\".eqauls(text); 看到这里估计很多人会觉得更蛋疼了。明明Lambda使用是的匿名内部类的简写，为什么要拿方法来做比较，另外明明有方法可以调用，何必要用Lambda。 Lambda的起源 曾经无数次的羡慕js有闭包，可以把方法当做参数来传递。在项目中有很多类都存在着很类似的方法： 1234567891011 public String getIds(List&lt;T&gt; list)&#123; if(list==null||list.isEmpty)&#123; return null; &#125; StringBuilder builder = new StringBuilder(); for(T t:list)&#123; String id=doSomething(t) builder.append(\",\").append(id); &#125; return builder.deleteCharAt(0).toString();&#125; 想抽取一下,奈何每个类里T不同，doSomething的实现也不同．T可以用泛型，doSomething（T t）这个方法怎么传呢？ 面向接口吧。我们可以定义一个接口来干这个事： 123 public interface GetId&lt;T&gt;&#123; String getId(T t);&#125; 上面的方法就可以抽到工具类中了： 1234567891011 public static &lt;T&gt; String getIds(List&lt;T&gt; list,GetId&lt;T&gt; interf)&#123; if(list==null||list.isEmpty)&#123; return null; &#125; StringBuilder builder = new StringBuilder(); for(T t:list)&#123; String id=interf.getId(t); builder.append(\",\").append(id); &#125; return builder.deleteCharAt(0).toString();&#125; 说来说去好像不关Lambda什么的事呀．实际上上面的例子就是Lambda的起源。 随着函数式编程广为程序员喜爱，Java也在考虑加入函数式编程．但有两个问题，一 做为一个强类型语言，类型转换有点蛋疼，二 不能把方法当成参数(有返回值的还好，void方法当参数是相当的无语的)。 Java考虑了半天，使用了上面的例子的逻辑来处理，用接口来包装方法，把接口当参数来代替。 上面的例子中。在类中调用如下： 12345 String ids=Utils.getIds(List&lt;XXX&gt; list,new GetId&lt;XXX&gt;()&#123; getId(XXX x)&#123; return ooxx(x); &#125;&#125;); 这是一个参数，如果是多个参数那不要吓死人： 123456789101112131415 String ids=Utils.getIds(List&lt;XXX&gt; list, new GetId&lt;XXX&gt;()&#123; getId(XXX x)&#123; return ooxx(x); &#125; &#125;, new GetId&lt;XXX&gt;()&#123;getId(XXX x)&#123; return xxoo(x); &#125; &#125;, new GetId&lt;XXX&gt;()&#123;getId(XXX x)&#123; return oxox(x); &#125; &#125;); 所以Lambda第一个作用就体现出来了简化书写，好写好看。如： 1 String ids=Utils.getIds(List&lt;XXX&gt; list,(x)-&gt;ooxx(x)); 这里有点奇怪x是什么呢？其实完整的应该是： 1 String ids=Utils.getIds(List&lt;XXX&gt; list,(XXX x)-&gt;ooxx(x)); 这里就体现了lambda的牛逼之处类型推导(准确来说是Java8的特性),编译时推导参数类型和返回类型。 最上面那个坑爹的例子估计很多人都会像我当初一样蒙Ｂ。(x,y)-&gt;x+y;,这ｘ和ｙ是什么的东西？ 实际上这个例子应该是(int x,int y)-&gt;return x+y;，这样看就清楚多了，传两个int,返回其和。 再给个实际点的例子吧： 1234567891011121314151617 String[] strings=&#123;\"xx\",\"oo\",\"xxoo\"&#125;;//传统的排序是这样的Arrays.sort(strings,new Comparetor&lt;String&gt;()&#123; public int compareTo(String s1,String s2)&#123; return s1.length-s2.length; &#125;&#125;)//使用lambda是这样的Arrays.sort(strings,(String s1,String s2)-&gt;return s1.length-s2.length);//或者Arrays.sort(strings,(s1,s2)-&gt;s1.length-s2.length);//或者Arrays.sort(strings,(s1,s2)-&gt;&#123; int x= s1.length; int y=s2.length; return x-y;&#125;); Lambda的实现 lambda的原理就是以把接口当参数传递的方式来形成闭包．所以这个接口只能定义一个方法，这种接口叫函数接口.这种接口可以隐式转为lambda。 为了防止该接口的单方法性被破坏，Java8定义了一个注解FunctionInterface，Java库中所有这种接口都已经添加了这个注解。如： 1234 @FunctionInterfacepublic Interface Runable&#123; void run();&#125; 当然，自己也可以定义函数接口，很简单，只要是单方法都可以。最好是加上注解，防止自己或别人去添加新的方法。 另外Java8中提供了很多常用的接口，免得自己去创建。这些接口放在java.utils.function包里。 默认的接口可接收和返回的基本数据只有int,long,double三种，String视为对象。 Custmer一家，提供了void类的接口，可以接收单个或两个的基本数据类型或对象的参数(无参的有现成的Runable)。如： 1234567 interface Custmer&lt;T&gt;&#123; void apply(T t)&#125;interface BiConsumer&lt;T,U&gt;&#123; void apply(T t,U u);&#125; 另外还支持对象+基本数据类型的参数。 Predicate一家，提供了可接收1-2个基本数据类型或对象的参数，返回boolean的接口(无参的是booleanSupplier接口)。 Function,Oprator,Supplier一家，提供了接收0-2个参数，返回基本数据类型或对象的接口。 Function一家可接收1-2个参数，返回的是对象 Supplier一家不接收参数，返回基本数据类型和对象 Operator一家接收1-2个参数，返回同类型的数据。unary前缀的是接收一个参数，binary前缀的是接收两个参数。那个坑爹的(x,y)-&gt;x+y的例子就是由IntBinaryOperator接口来实现的。 123 interface IntBinaryOperator&#123; int applyAsInt(int left,int right);&#125; 默认提供的接口，如果是两个基本数据类型的参数，则参数的类型都必须是相同的.也就是说两个以内的参数基本上不用自己定义函数接口了。上面的例子就可以这样写： 12345678910111213 public static &lt;T&gt; String getIds(List&lt;T&gt; list,Function&lt;T,String&gt; interf)&#123; if(list==null||list.isEmpty)&#123; return null; &#125; StringBuilder builder = new StringBuilder(); for(T t:list)&#123; String id=interf.apply(t); builder.append(\",\").append(id); &#125; return builder.deleteCharAt(0).toString();&#125;//使用时如下 Utils.getIds(list,t-&gt;ooxx(t);) 函数式编程 函数式编程需要可以把函数当成参数，在Java中所有的东西都是需要有类型的，只有函数自身是没有的。Lambda以间接的方式提供了函数的类型，及类型的推导，为函数式编程清扫了最大的障碍。再配合新提供的Stream，在Java中终与可以愉快的的使用函数式编程了。 然而，没有Lambda,没有Java8， Rxjava一样可以愉快的函数式编程。 Lambda表达式与匿名类的区别 使用匿名类与 Lambda 表达式的一大区别在于关键词的使用。对于匿名类，关键词this解读为匿名类，而对于 Lambda 表达式，关键词this解读为写就 Lambda 的外部类。 Lambda 表达式与匿名类的另一不同在于两者的编译方法。Java 编译器编译 Lambda 表达式并将他们转化为类里面的私有函数，它使用 Java 7 中新加的invokedynamic指令动态绑定该方法。 The link of this page is http://home.meng.uno/articles/61c2f1f1/ . Welcome to reproduce it!","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://home.meng.uno/categories/Programming-Language/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://home.meng.uno/tags/Java/"},{"name":"Length","slug":"Length","permalink":"http://home.meng.uno/tags/Length/"},{"name":"Substring","slug":"Substring","permalink":"http://home.meng.uno/tags/Substring/"},{"name":"StringBuilder","slug":"StringBuilder","permalink":"http://home.meng.uno/tags/StringBuilder/"},{"name":"string-plus","slug":"string-plus","permalink":"http://home.meng.uno/tags/string-plus/"},{"name":"Lambda","slug":"Lambda","permalink":"http://home.meng.uno/tags/Lambda/"}]},{"title":"Java异常与处理原则","slug":"java-exceptions","date":"2018-02-09T14:11:15.000Z","updated":"2021-01-01T17:41:22.000Z","comments":true,"path":"articles/1164dab2/","link":"","permalink":"http://home.meng.uno/articles/1164dab2/","excerpt":"异常结构层次 在Java中，异常分为checked与unchecked，他们都在一个分类层次中，如下图。 其中，红色的异常是checked异常，意味着在一个方法中，他们throw后必须catch或者declare。 另一种颜色的为unchecked异常，他们的异常不需要被recover。 异常处理 在Java中，调用某方法，就必须处理被调用方法抛出的异常，同时超类也可以用来捕获或者处理子类异常。 调用方法必须处理被调用方法抛出的异常 下面是一个处理异常的程序。我们可以测试一下，如果在一个方法中抛出一个异常，不仅是该方法，而且所有调用该方法的方法都必须声明或抛出异常。 1 2 3","text":"异常结构层次 在Java中，异常分为checked与unchecked，他们都在一个分类层次中，如下图。 其中，红色的异常是checked异常，意味着在一个方法中，他们throw后必须catch或者declare。 另一种颜色的为unchecked异常，他们的异常不需要被recover。 异常处理 在Java中，调用某方法，就必须处理被调用方法抛出的异常，同时超类也可以用来捕获或者处理子类异常。 调用方法必须处理被调用方法抛出的异常 下面是一个处理异常的程序。我们可以测试一下，如果在一个方法中抛出一个异常，不仅是该方法，而且所有调用该方法的方法都必须声明或抛出异常。 123456789101112131415 public class exceptionTest &#123; private static Exception exception; public static void main(String[] args) throws Exception &#123; callDoOne(); &#125; public static void doOne() throws Exception &#123; throw exception; &#125; public static void callDoOne() throws Exception &#123; doOne(); &#125;&#125; 超类可以用来捕获或处理子类异常 可以使用如下代码验证。 123456789101112131415161718192021 class myException extends Exception&#123; &#125; public class exceptionTest &#123; private static Exception exception; private static myException myexception; public static void main(String[] args) throws Exception &#123; callDoOne(); &#125; public static void doOne() throws myException &#123; throw myexception; &#125; public static void callDoOne() throws Exception &#123; doOne(); throw exception; &#125;&#125; 这也就是为什么catch子句只有一个父类在语法上安全的原因。 The link of this page is http://home.meng.uno/articles/1164dab2/ . Welcome to reproduce it!","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://home.meng.uno/categories/Programming-Language/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://home.meng.uno/tags/Java/"},{"name":"Exception","slug":"Exception","permalink":"http://home.meng.uno/tags/Exception/"}]},{"title":"二分查找的效率","slug":"binsearch","date":"2018-02-08T09:20:00.000Z","updated":"2020-12-02T01:41:34.000Z","comments":true,"path":"articles/fff444e8/","link":"","permalink":"http://home.meng.uno/articles/fff444e8/","excerpt":"查找是比较常见的工作，今天我通过对比几种在数组中查找一个确定的值的例子来向大家展示二分查找的魅力。 数组查找元素的几种方法 使用List 1 2 3 public static boolean useList(String[] arr, String targetValue) { return Arrays.asList(arr).contains(targetValue); } 使用Set 1 2 3 4 public static boolean useSet(String[] arr, String targetValue) { Set set = ne","text":"查找是比较常见的工作，今天我通过对比几种在数组中查找一个确定的值的例子来向大家展示二分查找的魅力。 数组查找元素的几种方法 使用List 123 public static boolean useList(String[] arr, String targetValue) &#123; return Arrays.asList(arr).contains(targetValue);&#125; 使用Set 1234 public static boolean useSet(String[] arr, String targetValue) &#123; Set&lt;String&gt; set = new HashSet&lt;String&gt;(Arrays.asList(arr)); return set.contains(targetValue);&#125; 使用for-loop 1234567 public static boolean useLoop(String[] arr, String targetValue) &#123; for(String s: arr)&#123; if(s.equals(targetValue)) return true; &#125; return false;&#125; 使用二分 1234567 public static boolean useArraysBinarySearch(String[] arr, String targetValue) &#123; int a = Arrays.binarySearch(arr, targetValue); if(a &gt; 0) return true; else return false;&#125; 时间复杂性 代码 使用如下代码来验证不同数据规模（5，1k，10k）的查找任务下四种方法的时间复杂性。（二分查找需要对数据排序，排序时间未计算在内。） 123456789101112131415161718192021222324252627282930 public static void main(String[] args) &#123; String[] arr = new String[] &#123; \"CD\", \"BC\", \"EF\", \"DE\", \"AB\"&#125;; //use list long startTime = System.nanoTime(); for (int i = 0; i &lt; 100000; i++) &#123; useList(arr, \"A\"); &#125; long endTime = System.nanoTime(); long duration = endTime - startTime; System.out.println(\"useList: \" + duration / 1000000); //use set startTime = System.nanoTime(); for (int i = 0; i &lt; 100000; i++) &#123; useSet(arr, \"A\"); &#125; endTime = System.nanoTime(); duration = endTime - startTime; System.out.println(\"useSet: \" + duration / 1000000); //use loop startTime = System.nanoTime(); for (int i = 0; i &lt; 100000; i++) &#123; useLoop(arr, \"A\"); &#125; endTime = System.nanoTime(); duration = endTime - startTime; System.out.println(\"useLoop: \" + duration / 1000000);&#125; &quot;5&quot;结果 123 useList: 13useSet: 72useLoop: 5 &quot;1k&quot;结果 随机生成数据 123456 String[] arr = new String[1000]; Random s = new Random();for(int i=0; i&lt; 1000; i++)&#123; arr[i] = String.valueOf(s.nextInt());&#125; 结果 1234 useList: 112useSet: 2055useLoop: 99useArrayBinary: 12 &quot;10k&quot;结果 1234 useList: 1590useSet: 23819useLoop: 1526useArrayBinary: 12 结论 通过以上结果，我们可以发现二分搜索确实很高效，而且当数据量变大时，其时间增长幅度还比较小。 以后，我们就可以使用Arrays.binarySearch()来高效查找某元素了。 The link of this page is http://home.meng.uno/articles/fff444e8/ . Welcome to reproduce it!","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://home.meng.uno/categories/Algorithm/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://home.meng.uno/tags/Java/"},{"name":"算法复杂性","slug":"算法复杂性","permalink":"http://home.meng.uno/tags/算法复杂性/"},{"name":"二分查找","slug":"二分查找","permalink":"http://home.meng.uno/tags/二分查找/"}]},{"title":"微积分(Calculus)","slug":"calculus","date":"2018-02-04T02:29:41.000Z","updated":"2021-01-01T15:30:35.000Z","comments":true,"path":"articles/93227351/","link":"","permalink":"http://home.meng.uno/articles/93227351/","excerpt":"微积分（Calculus） 微积分的主要内容 * 求导公式 * 乘积法则 * 链式法则 * 隐函数求导 * 积分、微分的互逆关系 * 泰勒级数 微积分的三个中心思想： 1. 积分 2. 微分 3. 积分与微分的互逆 积分的直观理解 - 推导圆的面积 回顾圆的面积公式： 如何从积分的角度推导出圆的面积公式？ 不同的划分方法会带来不同的积分公式，下面考虑将圆划分为大量的同心圆环，这种方法保留了圆的对称性。 考虑其中一个环的面积，可以将其看做一个“类矩形” 虽然这不是标准的矩形，但只要dr越小，它就越接近。它的面积可表示为： 于是，圆的面积可以看作","text":"微积分（Calculus） 微积分的主要内容 求导公式 乘积法则 链式法则 隐函数求导 积分、微分的互逆关系 泰勒级数 微积分的三个中心思想： 积分 微分 积分与微分的互逆 积分的直观理解 - 推导圆的面积 回顾圆的面积公式： 如何从积分的角度推导出圆的面积公式？ 不同的划分方法会带来不同的积分公式，下面考虑将圆划分为大量的同心圆环，这种方法保留了圆的对称性。 考虑其中一个环的面积，可以将其看做一个“类矩形” 虽然这不是标准的矩形，但只要dr越小，它就越接近。它的面积可表示为： 于是，圆的面积可以看作是这一系列矩形面积的叠加。 这部分面积的求和可以等价于求“函数y = 2πr图像在区间[0, R]下的面积”。 这个推导的过程其实可以看作是对函数y = 2πr在[0, R]下的积分。 积分与导数 直观来说，对函数f(x)在[a, b]上积分就是求函数f(x)在区间[a,b]下的图像与坐标轴包围的面积。记作： 这实际上是定积分的概念，此外还有不定积分。 如果是其他图像，比如抛物线，该怎么求这部分的面积呢？ 能不能找到一个函数 A(x) 表示 0 到 x 之间函数图像下的面积？——这个函数 A(x) 就是该函数的积分（函数）。 这里强调 0 到 x 之间，是为了使问题具有实际意义 以抛物线 f(x)=x^2 为例。类似的，我们可以将这块区域划分成一系列细长的矩形。 将 x 增加 dx，增加的面积可以看做是一个长f(x)、宽dx的矩形，只要dx越小，这条窄带就越接近矩形。 把这部分面积记作 dA，表示面积的微小变化（difference in Area） 通过这个矩形，可以得到 A、f(x) 与 dx 之间的关系： 这里引出了微积分中另一个重要的概念——导数。dA/dx 就是 “A 的导数” 更严格的说法是：&quot;A 的导数&quot;是“当 dx → 0 时，dA/dx 所趋向的值”。（下一节会讨论导数的定义） 一般不会刻意区分导数和导数函数的区别，都统称为导数，具体含义视语境而定；同样，积分也是如此。 导数是解决积分问题的关键——积分需要还原出某个导数原本的函数——如果你能熟练的计算导数，那么你也能解决这个问题。 积分与导数之间的这种互相转化的关系，也就是“某个图像下方面积函数的导数能够还原出定义这个图像的函数”，就叫做微积分基本定理。该定理表明，“在某种意义上”，两者互为逆运算。 导数（Derivative） “瞬时变化率”引起的歧义 — 导数的悖论 “瞬时变化率”的歧义——只有在不同的时间点之间，变化才能发生；而将时间限制在某个瞬间点的时候，变化也就不存在了。 考虑这个示例：一辆车从 A 点起，先加速，再减速，至 100 米外的 B 点停下，整个过程花费 10 秒。 把车速的图像加入其中，可以发现：两者存在着某种联系：其中之一改变的话，也会引起另一个发生变化。 直觉上，速度越大，距离-时间函数的图像也越陡峭，意味着车在单位时间内移动的距离更长 速度的大小是如何随着距离-时间函数的变化而变化的？——“瞬时”速度的矛盾： 在绘制速度图像的时候，需要给每个单独的时间点关联一个速度值，但是计算速度却需要两个时间点上的距离。 记时间差为 dt，距离差为 ds，那么这段时间内的速度就能用 ds/dt 表示 当年的微积分创始人们也经历了同样的思维冲突。 换言之，速度只有在一段会时间内的才有意义；“瞬时”的说法会带来矛盾 实际的做法是：会选取一个很小的 dt 值，然后把 ds/dt 看做是这个瞬间的速度。 导数的定义与计算 导数的定义 在纯数学领域，导数不是 dt 为某个具体值时 ds/dt 的值 ，而是当 dt 的值无限逼近 0 时这个比值的极限。 从图像的角度，（某一点的）导数有一个直观的含义：就是经过图像上该点切线的斜率。 注意：这里的 dt 不是“无穷小”，也不是 0；它永远是一个有限小的量，接近 0 而不是 0。 这种说法在试图规避“瞬时”带来的矛盾，使“某个时间点的变化率”有意义。 导数的计算 抛开求导公式，先来看一下面对一个实际的问题，该如何求解（在某一点处的）导数。 对于 s(t)=t^3 在 t=2 处的导数，根据导数的定义，有 当 dt 趋向 0 时，后两项也会趋于 0，进而消去。代入 t=2 可以得到在该点处的导数为 12 更一般的，称 s'(t) = 3*t^2 为 s(t) = t^3 的导函数。 对于常见的函数，有一系列总结出的求导公式可以快速计算（下一节会演示如何从几何的角度来推导这些公式） 导数的含义是“变化率的最佳近似” 回顾之前关于距离-速度的示例，思考这个问题：当 t=0 时，车在不在移动？ 一方面，利用导函数公式可以得到 t=0 时的速度为 0——这似乎在说“车没有移动”；另一方面，如果车在 0 时刻没有移动，那么它是何时开始移动的？——关键在于这个问题本身就是没有意义的。 因为导数并不是用来测量“瞬时变化”的。 t=0点的导数为 0 的真正含义是指“在第 0 秒附近，车速的最佳近似为 0 米/秒”——换句话说，就是当时间间隔 dt 越来越小时，表示速度的比值 ds/dt 就越趋向于 0——这并不表示车在 0 时刻就是静止的，只能说它此时的速度近似于 0. 用几何来求导 为什么导数很重要？——当需要使用微积分来解决现实中的实际问题时，需要将其抽象成各种代表性的函数来描述；而如果能掌握这些抽象函数的变化率，那你就学会了这门可以精准描述事物变化率的语言。 从几何的角度看“微小变化量” 以 f(x) = x^2 为例： 从坐标轴上看，x^2 的图像是一条抛物线，我们已经知道，导数可以描述为切线的斜率 此外，x^2 还有另一个更直接的含义：长为 x 的正方形的面积。 假如给边长 x 一个微小的增量 dx，那么正方形的增量（变化量）是多少？ 应该时刻记住的 dx 是一个微小的量——这意味着你可以忽略所有次数高于 1 的 dx 项——换言之，一个微小量的平方（或更高次方）是一个可以忽略的变化量 由此，可以得到： 上一节给出了 f(x) = x^3 导数的代数推导过程，这里也可以作为立方体体积来用几何的方式推导。 负的变化量 上述两个例子的变化量都是增量（正值），但实际上变化量也可能是负值，比如 f(x) = 1/x——考虑这样一个特殊的矩形，长 x，宽 1/x，它的面积恒为 1. 注意：这里的变化量不再是矩形的面积了，而是矩形的高 通过简单的几何知识，可知矩形高的变化量为： 从而得到 1/x 的导数为： 幂函数的导数 以上 x^2、x^3 和 x^-1 都遵循了幂函数的求导公式： 思考一下为什么这个公式也适用于 2 和 3 以外的指数——求导的一个关键点在于很大一部分项因为包含 dx 的高次幂，可以被忽略——因此有 当指数小于 0 时，会更复杂一些——比如 x^-2 可以考虑一个边长为 √x 的正方形。 大多数时候，我们都有合适的求导公式来使用，但是以上利用几何求导的过程能锻炼我们借助微小变化量来考虑的导数的能力。 三角函数的导数 正弦函数的定义 θ=0.8 表示单位圆弧长为 0.8 时对应的角度为 θ（单位圆的周长为 2π） sin(θ) 表示此时该点距离 x 轴的高度（可能为负） 当 θ 增加时，sin(θ) 的值会在 -1 到 1 之间上下摆动 sin(θ) 当 θ 增加时的微小变化量 在近似的观点下，应该有这样的直觉——可以把那一段微小的圆弧dθ看作是直线 根据三角函数的定义，有 直观理解链式法则 求导公式概览： 函数的和/差——加法法则 函数的积/商——乘法法则 复合函数的求导法则 加法法则 加法法则比较简单，可以在坐标轴中展示 乘法法则 “左乘右导 + 右乘左导”： 在数学中，如果你要处理两项的乘积，用面积来理解会更方便 复合函数的求导法则 — 链式法则 链式法则： 链式法则的表达式在说：看看输出值 g 的微小变化除以 h 的微小变化是多少（h 是要带入函数 g 中的值）；然后乘以 h 微小变化与 x 微小变化的比值 这些 dh 最终会被消去，结果就是输出值 g 的微小变化与输入值 x 的微小变化的比值 dh 的消去并不只是符号上的技巧，而是真实反映出在求导时，各微小变化量发生了什么。 指数函数的导数 — 自然常数 e 的定义 从2^t开始： 根据导数的定义，有 当 dt → 0 时，h 趋向于一个常数 0.6931... 也就是说，2^t 图像上各点处切线的斜率 = 该点的函数值 * 一个常数 类似的，3^t 也存在这样一个 h=1.0986...；8^t 则是 h=2.0794... 注意到，8^t 的 h 大概是 2^t 的三倍，而 8^t = 2^{3t} 是否存在某个数，使 h 恰为 1？——这个数就是自然常数 e e^t 时，h=1的原因是因为e就是如此定义的。 和“为什么π正好等于圆的周长比直径”一样，π就是这么定义的。 自然常数不是完全这样发现的，但过程类似 指数函数的导数 有了自然常数 e 以及链式法则，就可以求出其他指数函数的导数了 根据指数函数的性质与链式法则，有 事实上，在微积分中，指数函数基本都是以 e^ct 的形式出现的，很少会直接使用 c^t 的形式 为什么称 e 为自然常数？ 现实世界中，很多自然现象里的变化率与变化量是成正比的 比如：在室温环境下，热水变凉的速率与水和房间的温差成正比（或者说，温差的变化率与温差本身成正比） 尽管指数函数有多种写法，但是将其表达为以 e 为底的幂函数是非常自然的。 因为 e 有着非常自然的含义，它就是变化率与数量本身的比例系数。 隐函数求导 什么是隐函数？ 如果方程 F(x,y)=0 能确定 y 是 x 的函数，那么称这种方式表示的函数是隐函数； 直观来说，就是满足某种关于变量 x 和 y 的关系的、所有 (x, y) 点的集合，相应的曲线就是“隐函数曲线”。 示例 1：圆上某一点切线的斜率 圆的方程就是一个隐函数，比如 x^2 + y^2 = 5^2，下面需要求圆上一点的斜率（不考虑几何方法） 因为这里的曲线并不是一个函数图像，所以不能单纯对其求导——它不存在变量 x 的微小变化对函数值 y 的微小变化 当然，如果我们的目标只是求 dy/dx，那么对等式两边同时求导可以解决： 示例 2：相关变化率 这是一个更实际的例子：开始时，梯子上端距地面 4m(y=4)，下端距墙 3m(x=3)。如果梯子的上端以 1 m/s 的速度下滑，那么下端左滑的速度是多少？ 等式的左边可以看做是一个关于时间 t 的函数，于是对表达式左边求导的含义相当于在问“经过短暂的时间 dt，y 会减少一点，x 会增加一点，那么整体的变化量是多少？”，而表达式右边告诉我们这个变化量为 0。 带入 x(t)=3, y(t)=4, dy/dt=1，就能求出 dx/dt 了。 隐函数求导的含义 从纯数学的角度看，示例 1 与示例 2 的做法没有区别。但是示例 2 求导时带有明确的物理意义——表达式随时间的变化率。但是求圆切线时似乎难以解释为什么这么做。 其实隐函数的导数也是一个隐函数，它对 dx 和 dy 的关系做了限制——在圆的例子上，它要求整体变化量 dS = 2xdx + 2ydy 为 0；也就是说为了让 S = x^2 + y^2 始终保持在 25，那么 dS 就需要为 0. 严格来讲，这个条件实际上是保证每一步落在过该点的切线上，而不是落在圆本身。当 dx 和 dy 足够小时，这两者才没有区别。 在隐函数的导数中，原来的变量看做常数，比如这里 2xdx + 2ydy = 0 中的 x 和 y 显然，（左）的正体变化量就不满足使 S 保持不变 示例 3：sin(x)y^2 = x 该函数的图像是一系列 U 型曲线 有了这个等式，就能方便求出 dy/dx 了。 对表达式的两边求导，就可以得到导数的隐函数，它对 x 和 y 的整体微小变化量作出了限制。 示例 4：ln(x)的导数——从已有的导函数推算出其他函数的导函数 根据指数函数与对数函数的关系，有 多元微积分 隐函数求导是多元微积分的入门。两者的要点是一样的，需要理解这多个变量是如何联系在一起变化的。 极限 简单来说，“极限”就是逼近/趋近更“洋气”的说法。 导数的正式定义 导数的计算公式： 导数的正式定义： 或 为什么不用 dx？——dx 本身已经表达了求极限的含义——使用 dx 的表示容易将其理解成“无穷小的变化量”；更正确的解读应该是把它看做一个具体的、有限小的变化量，并时刻考虑 dx → 0 时的情况。 为什么用一个新的变量 h？——明确变化量 h 只是一个普通的数，跟“无穷小”没有任何关系。 极限的 (ϵ, δ) 定义 所谓极限，指的是变量逼近 0 时的影响，而非无穷小变量的影响 导数由极限定义，而极限本身由 (ϵ, δ) 定义，下面是比较书面的说法： 设函数f(x)在点x0的某一去心邻域内有定义，如果存在常数a，对于任意给定的正数ϵ，都∃δ&gt;0，使不等式|f(x)-a| &lt; ϵ在|x-x0| ∈ (0,δ)恒成立，那么a就叫做函数f(x)当x→x0时的极限，记作 极限存在与不存在的两个例子 极限存在的前提是，你总能在距离极限点x0距离为δ(&gt;0)的取值范围内，找到一系列取值点，使得范围内的任一取值点，它的函数值都在距离f(x0)为ϵ(&gt;0)的范围内——关键在于这种情况对于任意ϵ都成立——无论ϵ多小，你总能找到与之对应的δ 利用导数来求极限——洛必达法则 如果函数在点x0处是有定义的，那么该点的极限值 == 函数值本身。 如果函数在该点没有定义呢？比如 sin(πx)/(x^2-1)，该函数在x0=±1处就没有定义。 洛必达法则 洛必达法则专门用于求解 0/0 型和 ∞/∞ 的极限值。 导数是由极限定义的，但反过来，也能利用导数来求极限 洛必达法则利用了这样一个性质——当f(a)=0时，在a的附近，有f(a+dx)=f'(a+dx)。如图所示： 当 dx 越小，这个比值就越精确 洛必达法则： 条件：1）f(x)和g(x)在点a处的极限都为 0；2）f(x)和g(x)在点a的某去心领域内可导，且g'(x)!=0；3）A可以为实数，也可以为±∞ 注意，只要满足以上条件，洛必达法则是可以继续进行的 这么看，实际上，求解导数的过程也是在使用洛必达法则 分子分母在 h→0 处的极限都为 0 积分与微积分基本定理 积分其实就是求导的逆运算 示例：距离是速度对时间的积分 已知速度-时间的关系 v(t)=t(8-t)，求这段时间驶过的距离。 现在“已知每个时间点的速度 v(t)，来求距离s(t)” 从数学的角度，前者的问题等价于求“s(t)的导数”，于是现在的问题也就是求“哪个函数的导数是v(t)”——这通常被称为“求函数的‘原函数’（或‘反导数’）” 这个问题可以转化成求解曲线下方的面积 这个过程可以表示为 v(t) 的“积分”： 为什么不适用∑？——∫指的并不是具体的dt的加和，而是当dt→0时，加和逼近的值，这个逼近的值就是曲线下方的面积；当dt越趋近 0，加和就越趋近精确解。 积分的含义就是将所有小量“累积”在一起 “求函数图像与横轴围成的面积”是许多不相干问题的共通之处——可以被拆成小量，然后近似为这些小量的和 积分与导数是一组互逆的运算 根据以上的分析，距离实际上是速度对时间的积分，而速度是距离对时间的导数——这表明积分与导数是一组互逆的运算 面积函数的导数等于函数本身 如果固定左端点，将右端点当做一个变量 T，那么这个积分可以看做以上限为自变量的函数 s(T)——这是距离关于时间的导数，同时也是图形下方的面积函数 这表明任一函数图像下方面积函数的导函数等于该函数本身 从几何的角度看，矩形的面积为 ds，宽为 dT，它的高为 v(T)，于是有 ds/dT = v(T) 即图像所表示的函数就是面积函数的导数 因此，求解积分的过程就是求导的逆过程 每个函数都有无数个原函数 因为常数的导数为 0，所以在原函数的基础上加上任意常数，其导数不变——这意味着每个函数的原函数有无数个 所以v(t) = 8t - t^2正确的原函数应该是 从图像上来看，曲线上下移动并不会影响其在每一点的斜率 在不同的问题中，一般会有额外的条件帮你决定该使用哪个原函数 比如，在本问题中，t=0 时，s(t)=0，于是 C=0 8.2. 微积分基本定理 在对任意函数求（定）积分时，你是在对 x 在一定范围内的所有 f(x)*dx 求和，而积分值就是 dx → 0 时，这个和趋近的值。 求积分的第一步是找出原函数 F，使其导数为积分内的函数。 积分值就等于原函数在上限时的值减去其在下限时的值。 这个过程就是“微积分基本定理” 积分异于常识的一点在于：它连续地遍历了从下限到上限中的每一个自变量的值，而我们在利用原函数求值时，只需要关注上限与下限两个自变量 负面积 如果图像有部分出现在横轴的下方，那么这部分就是负面积 有符号的面积 积分计算的不是真正的面积，而是图像与横轴围城的带有正负的面积。 连续变量的平均值 结论：区间上的平均斜率等于起点和终点连线的斜率 如何求连续变量的平均值？ 问题等价于求图像下方面积的平均高度 如果把面积作为一个整体来看，这是一个很简单的问题——平均高度 = 积分面积 / 上下限宽度 值得注意的是，(F(b)-F(a))/(b-a) 实际上也是原函数F(x)在 x=a 和 x=b 两点连线的斜率 这表明在某一区间上所有点处切线的平均斜率等于起点和终点连线的斜率 根据定义，f(x)是其原函数F(x)的导函数，也就是说它给出了F(x)在每个点上切线的斜率，所以f(x)在(a,b)上的平均值也就是原函数从x=a到x=b上所有切线斜率的平均值。 换言之，求解连续函数的平均值，可以转化为求解其原函数在各点切线的平均斜率；而两点间的平均斜率等于这两点的斜率。 适用“积分”的场景 可以通过细分然后相加的方式进行估算时 求解连续变量的均值时（特别在概率中） 泰勒级数 高阶导数 这里介绍高阶导数的目的是帮助得到函数的近似——泰勒级数 二阶导数描述的是曲线的弯曲程度 根据导数的定义，二阶导数也就是导数的微小变化率，即斜率的变化率——直观来看，也就是曲线的弯曲程度 考虑函数的一个取值 x，然后向右连续的增加两个小量 dx 第一个增量是函数产生了第一个变化量 df1，第二个同理，记 df2 这两个变化量的差也就是函数值变化量的变化量，记 d(df). 它和 (dx)^2 成正比 所谓二阶导数，就是 d(df) 和 (dx)^2 当 dx → 0 时比值的极限 用符号表示为 其实中间的写法才是最正确的，但为了书写方便，通常写成最右侧的形式 一个理解二阶导数的现实示例就是加速度 二阶导数为正，说明车在加速；反之，为负，说明在减速 泰勒多项式与泰勒级数 泰勒级数的作用——函数近似工具。比如，在 x=0 附近，有： 泰勒级数利用多项式函数去近似复杂抽象的函数，从而化简问题 多项式函数的优势：易计算、易求导、易积分 深度学习中的梯度下降（一阶）、牛顿法（二阶）也是利用的泰勒级数 示例：cos(x) 在 x=0 附近如何用二次多项式近似？ 问题等价于在所有可能的 c0 + c1*x + c2*x^2 中确定系数使其在 x=0 附近最近似 cos(x) 不考虑任何先验知识，看看只凭直觉应该怎么确定这三个系数 首先，cos(x) 在 x=0 处等于 1，那么至少多项式这一点要满足，即 c0=1 如果多项式在 x=0 处切线的斜率也与 cos(x) 相同，那么近似程度应该会更高，于是 c1=0 二阶导数描述的是曲线的弯曲程度，那么让两个图像在 x=0 处的弯曲程度相同应该会更近似，cos(x) 的二阶导是 -cos(x)，于是 2c2=-cos(0)=-1 -&gt; c2=-1/2 还可以用更高次的多项式去近似，比如 在非 0 点，有类似的结果，如 x=π 带入 x=π 就能消去所有无关项 “泰勒多项式”小结 在对高阶多项式求导时，自然而然的出现了阶乘的形式 当然，真正的系数需要除以这个阶乘 在向近似多项式中添加更高次的项时，不会影响低次项 因此，多项式任意 n 阶的导数在 x=0 时的值都由唯一的系数控制 这样的多项式就称为“泰勒多项式” 泰勒多项式的本质 泰勒多项式实际上是利用函数在某点处的高阶导数，来近似该点附近的函数值 宏观来讲：泰勒级数把某一点处高阶导数的信息转化成了在那一点附近的函数值信息 x=0 时，它的常数项能让它与 f(0) 的值相等 它的一次项让两者的斜率相等 二次项让两者斜率的变化率相同 以此类推，项数越多，近似就越精确 从几何角度看二阶泰勒多项式 考虑如何近似曲线下的面积函数 f_area(x) 当 dx → 0 时，我们可以将增长的部分近似为矩形 但如果想将面积的变化近似得更准确，就需要考虑那块近似三角形的部分 假设已知面积函数 f 在 a 点的导数信息，想要近似在 x 时的面积 三角形的高 = 斜率 * 底，斜率 = 曲线在 a 点的导数 = f''(a) 任意函数的泰勒多项式 函数f(x)在x=0处的泰勒多项式 函数f(x)在x=a处的泰勒多项式 级数的概念 级数（series）的定义——无限项的和 当泰勒多项式无限累加下去，就成了泰勒级数 级数的一些重要概念 级数的收敛（Converges）与发散（diverges）、收敛半径 无限累加下去的和就“等于”级数收敛到的值；这里 x=1 一些级数无论你带入什么值，它的级数都会收敛，比如 e^x 在 x=0 处的泰勒级数 无论 x 取何值，e^x 都等于该泰勒级数 但有些级数就只会在一定取值范围内才会收敛，比如 ln(x) 在 x=1 处的泰勒级数 只有当 x∈(0,2]时，该级数才会收敛 换言之，在 x=1 处取得的导数信息无法拓展到更广的取值范围 这个泰勒级数的收敛半径为 1，即x∈(0, 2] 常见的泰勒级数 泰勒级数_百度百科 https://baike.baidu.com/item/泰勒级数/7289427?fr=aladdin#6 The link of this page is http://home.meng.uno/articles/93227351/ . Welcome to reproduce it!","categories":[{"name":"Mathematical Modeling","slug":"Mathematical-Modeling","permalink":"http://home.meng.uno/categories/Mathematical-Modeling/"}],"tags":[{"name":"微积分","slug":"微积分","permalink":"http://home.meng.uno/tags/微积分/"},{"name":"数学","slug":"数学","permalink":"http://home.meng.uno/tags/数学/"}]},{"title":"Bash, Zsh and commonly used Shell commands","slug":"shell-normal","date":"2018-01-11T02:22:44.000Z","updated":"2021-01-06T18:20:50.000Z","comments":true,"path":"articles/61f3f0dd/","link":"","permalink":"http://home.meng.uno/articles/61f3f0dd/","excerpt":"Bash的使用 显示 “Hello world!” echo Hello world! 每一句指令以换行或分号隔开 echo ‘This is the first line’; echo ‘This is the second line’ 声明一个变量 Variable=“Some string” 这是错误的做法： Variable = “Some string” 原因： Bash 会把 Variable 当做一个指令，由于找不到该指令，因此这里会报错。 也不可以这样： Variable= ‘Some string’ 原因： Bash 会认为 ‘Some string’ 是一条指令","text":"Bash的使用 显示 “Hello world!” echo Hello world! 每一句指令以换行或分号隔开 echo ‘This is the first line’; echo ‘This is the second line’ 声明一个变量 Variable=“Some string” 这是错误的做法： Variable = “Some string” 原因： Bash 会把 Variable 当做一个指令，由于找不到该指令，因此这里会报错。 也不可以这样： Variable= ‘Some string’ 原因： Bash 会认为 ‘Some string’ 是一条指令，由于找不到该指令，这里再次报错。这个例子中 ‘Variable=’ 这部分会被当作仅对 ‘Some string’ 起作用的赋值。） 使用变量 123 echo $Variableecho \"$Variable\"echo '$Variable' 当你赋值 (assign) 、导出 (export)，或者以其他方式使用变量时，变量名前不加 $。如果要使用变量的值， 则要加$。 注意: ' (单引号) 不会展开变量（即会屏蔽掉变量）。 在变量内部进行字符串代换 echo ${Variable/Some/A} 会把 Variable 中首次出现的 “some” 替换成 “A”。 变量的截取 Length=7 echo ${Variable:0:Length} 这样会仅返回变量值的前7个字符 变量的默认值 echo ${Foo:-&quot;DefaultValueIfFooIsMissingOrEmpty&quot;} 对 null (Foo=) 和空串 (Foo=&quot;&quot;) 起作用； 零（Foo=0）时返回0 注意这仅返回默认值而不是改变变量的值 内置变量 下面的内置变量很有用 12345 echo \"Last program return value: $?\"echo \"Script's PID: $$\"echo \"Number of arguments: $#\"echo \"Scripts arguments: $@\"echo \"Scripts arguments separated in different variables: $1 $2...\" 读取输入 123 echo \"What's your name?\"read Name # 这里不需要声明新变量echo Hello, $Name! if 结构 ’man test’ 可查看更多的信息 123456 if [ $Name -ne $USER ]then echo \"Your name isn't your username\"else echo \"Your name is your username\"fi 根据上一个指令执行结果决定是否执行下一个指令 12 echo \"Always executed\" || echo \"Only executed if first command fails\"echo \"Always executed\" &amp;&amp; echo \"Only executed if first command does NOT fail\" 在 if 语句中使用 &amp;&amp; 和 || 需要多对方括号 123456789 if [ $Name == \"Steve\" ] &amp;&amp; [ $Age -eq 15 ]then echo \"This will run if $Name is Steve AND $Age is 15.\"fiif [ $Name == \"Daniya\" ] || [ $Name == \"Zach\" ]then echo \"This will run if $Name is Daniya OR Zach.\"fi 表达式的格式 echo $(( 10 + 5 )) 指令可以带有选项 与其他编程语言不同的是，bash 运行时依赖上下文。比如，使用 ls 时，列出当前目录。 ls -l 列出文件和目录的详细信息 前一个指令的输出可以当作后一个指令的输入 grep 用来匹配字符串。 列出当前目录下所有的 txt 文件 ls -l | grep &quot;\\.txt&quot; 以 ^EOF$ 作为结束标记从标准输入读取数据并覆盖 hello.py 123456789 cat &gt; hello.py &lt;&lt; EOF#!/usr/bin/env pythonfrom __future__ import print_functionimport sysprint(\"#stdout\", file=sys.stdout)print(\"#stderr\", file=sys.stderr)for line in sys.stdin: print(line, file=sys.stdout)EOF 重定向可以到输出，输入和错误输出 1234567 python hello.py &lt; \"input.in\"python hello.py &gt; \"output.out\"python hello.py 2&gt; \"error.err\"python hello.py &gt; \"output-and-error.log\" 2&gt;&amp;1python hello.py &gt; /dev/null 2&gt;&amp;1# &gt; 会覆盖已存在的文件， &gt;&gt; 会以累加的方式输出文件中。python hello.py &gt;&gt; \"output.out\" 2&gt;&gt; \"error.err\" 覆盖 output.out, 追加 error.err 并统计行数 12 info bash 'Basic Shell Features' 'Redirections' &gt; output.out 2&gt;&gt; error.errwc -l output.out error.err 运行指令并打印文件描述符 （比如 /dev/fd/123） 12 # 具体可查看： man fdecho &lt;(echo \"#helloworld\") 以 “#helloworld” 覆盖 output.out 1234 cat &gt; output.out &lt;(echo \"#helloworld\")echo \"#helloworld\" &gt; output.outecho \"#helloworld\" | cat &gt; output.outecho \"#helloworld\" | tee output.out &gt;/dev/null 清理临时文件并显示详情（增加 ‘-i’ 选项启用交互模式） 1 rm -v output.out error.err output-and-error.log 一个指令可用$( )嵌套在另一个指令内部 以下的指令会打印当前目录下的目录和文件总数 1 echo &quot;There are $(ls | wc -l) items here.&quot; 反引号起相同作用，但不允许嵌套 优先使用 $() 1 echo &quot;There are `ls | wc -l` items here.&quot; Bash 的 case 语句与 Java 和 C++ 中的 switch 语句类似 123456 case \"$Variable\" in # 列出需要匹配的字符串 0) echo \"There is a zero.\";; 1) echo \"There is a one.\";; *) echo \"It is not null.\";;esac 循环遍历给定的参数序列 变量$Variable的值会被打印 3 次 1234 for Variable in &#123;1..3&#125;do echo \"$Variable\"done 传统的 “for循环” 1234 for ((a=1; a &lt;= 3; a++))do echo $adone 也可以用于文件 用 cat 输出 file1 和 file2 内容 1234 for Variable in file1 file2do cat \"$Variable\"done 或作用于其他命令的输出 对 ls 输出的文件执行 cat 指令 1234 for Output in $(ls)do cat \"$Output\"done while 循环 12345 while [ true ]do echo \"loop body here...\" breakdone 使用函数 定义函数 1234567 function foo ()&#123; echo \"Arguments work just like script arguments: $@\" echo \"And: $1 $2...\" echo \"This is a function\" return 0&#125; 更简单的方法 12345 bar ()&#123; echo \"Another way to declare functions!\" return 0&#125; 调用函数 1 foo &quot;My name is&quot; $Name 有很多有用的指令需要学习 打印 file.txt 的最后 10 行 1 tail -n 10 file.txt 打印 file.txt 的前 10 行 1 head -n 10 file.txt 将 file.txt 按行排序 1 sort file.txt 报告或忽略重复的行，用选项 -d 打印重复的行 1 uniq -d file.txt 打印每行中 ‘,’ 之前内容 1 cut -d &apos;,&apos; -f 1 file.txt 将 file.txt 文件所有 ‘okay’ 替换为 ‘great’（兼容正则表达式） 1 sed -i &apos;s/okay/great/g&apos; file.txt 将 file.txt 中匹配正则的行打印到标准输出 这里打印以 “foo” 开头, “bar” 结尾的行 \"^foo.*bar$\" file.txt``` 1234 使用选项 &quot;-c&quot; 统计行数```grep -c &quot;^foo.*bar$&quot; file.txt 如果只是要按字面形式搜索字符串而不是按正则表达式，使用 fgrep (或 grep -F) 1 fgrep &quot;^foo.*bar$&quot; file.txt 以 bash 内建的 ‘help’ 指令阅读 Bash 自带文档 123456 helphelp helphelp forhelp returnhelp sourcehelp . 用 man 指令阅读相关的 Bash 手册 123 apropos bashman 1 bashman bash 用 info 指令查阅命令的 info 文档 （info 中按 ? 显示帮助信息） 1234 apropos info | grep '^info.*('man infoinfo infoinfo 5 info 阅读 Bash 的 info 文档 1234 info bashinfo bash 'Bash Features'info bash 6info --apropos bash Zsh的配置与使用 不少程序员都觉得Mac的一大优势就是其Shell，也有很多人觉得Mac与Linux在Shell上很相似。不错，但是Mac还是略胜一筹或者说高一个量级。今天，我将向大家介绍一个Mac特有的Shell（Linux也可以安装，但是不是系统自带。）—— Zsh。 切换到Zsh 使用cat /etc/shells指令，我们可以看看自己的系统有哪些Shells，下面是我的Mac的结果： 1234567 /bin/bash/bin/csh/bin/ksh/bin/sh/bin/tcsh/bin/zsh/usr/local/bin/fish 使用这个指令切换到Zsh：chsh -s /bin/zsh。（想使用其他Shell也是同样的指令哦。） 这是，我们的Shell配置文件就为.zshrc了。 我觉得从这里我们应该可以知道，为什么之前的Shell配置文件要以.bash_profile命名了吧。因为Mac默认Shell是Bash。 迁移Bash配置 我使用Bash有好几年了，那些配置都是一些环境变量啊什么的，如果在Zsh的配置里再写一遍，无疑是一件很费时又低效的事。那有没有什么快捷的方式呢？当然有！ 通过如下指令：source ~/.bash_profile就可以将.bash_profile里的配置全部引入到.zshrc中了。同理，如果你想自己写配置，也可以通过这种方式引入。（后文你将看到一个第三方工具就是这么做的。） 安装oh my zsh 通过wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh指令安装。 这时我们发现在.zshrc文件中，多了两行： 12 export ZSH=/Users/NAME/.oh-my-zshsource $ZSH/oh-my-zsh.sh 自定义Shell主题 使用oh my zsh主要的原因是使用其提供的漂亮的主题，主题目录在.oh-my-zsh/themes/下，选择主题ZSH_THEME=&quot;robbyrussell&quot;。这时我的Shell主题就是robbyrussell了。 打开robbyrussell.zsh-theme文件，我们可以看见几条配置。 我将其中的PROMPT修改为： PROMPT='${ret_status} %{$fg[cyan]%}%d %{$reset_color%} $(git_prompt_info)%{$fg_bold[red]%}&gt;%{$fg_bold[yellow]%}&gt;%{$fg_bold[green]%}&gt; ' 这时我的Shell就变成了这样： 可以发现我的定制有：显示绝对路径，&gt;&gt;&gt;等。 还有很多主题与配置，大家可以自己尝试。 定制Shell Zsh还有个功能就是“别名”。不知道大家有没有这样的经历，需要打开.plist这样的文件，如果用普通编辑器打开会非常界面不友好，而用Xcode打开则完美可观。那怎么在控制台直接用Xcode打开文件呢？（其他软件同理） 我在.zshrc中添加：alias xcode=&quot;/Applications/Xcode.app/Contents/MacOS/Xcode&quot;，之后我就可以使用xcode X来用Xcode打开X文件了。 我们也可以为某种类型文件设置默认打开方式：alias -s html=atom（当我们键入.html文件时，会自动用Atom打开）。 安装插件 oh my zsh为Zsh提供了100+插件，如果我们需要安装某插件，只需要在.zshrc文件中的plugins=()中添加，用空格隔开，只需要填插件名字，默认添加了git。 在这里我向大家介绍几种网上很常见的插件： git当你处于一个 git 受控的目录下时，Shell 会明确显示 「git」和 branch，如上图所示，另外对 git 很多命令进行了简化，例如 gco=’git checkout’、gd=’git diff’、gst=’git status’、g=’git’等等，熟练使用可以大大减少 git 的命令长度，命令内容可以参考~/.oh-my-zsh/plugins/git/git.plugin.zsh。 osxtab 增强，quick-look filename 可以直接预览文件，man-preview grep 可以生成 grep手册 的pdf 版本等。 autojump像他的名字一样，提供自动补全等很多功能，大家自己去尝试吧。 注意：安装autojump建议使用Homebrew brew install autojump 然后按照提示将一句类似这个 [ -f /usr/local/etc/profile.d/autojump.sh ] &amp;&amp; . /usr/local/etc/profile.d/autojump.sh 的句子插入到.zshrc文件中即可。 其他Shell常用指令 统计文件夹中文件数量 当前文件夹下文件个数[不包括文件夹] $ ls -l |grep &quot;^-&quot;|wc -l 当前目录下文件夹个数 $ ls -lR | grep &quot;^d&quot; | wc -l 当前文件夹下文件数目[包括子目录] $ ls -lR| grep &quot;^-&quot; | wc -l 登录服务器 $ ssh [-p 端口] username@host_address #Enter后输入密码 conda虚拟环境 新建环境 $ conda create -n 环境名 python=X.X #X.X = 2.7、3.6等 激活环境 $ source activate 环境名 环境中安装额外的包 $ conda install -n 环境名 packages 删除环境中的某个包 $ conda remove --name 环境名 package 关闭虚拟环境 $ source deactivate 删除环境 $ conda remove -n 环境名 --all 文件链接 $ ln -s 源 目的 查寻文件或命令的路径 文件中字符串的查寻 $ grep string file 显示命令的路径 $ whereis command 显示命令的路径，及使用者所定义的别名 $ which command 显示命令功能的摘要 $ whatis command 搜寻指定路径下某文件的路径 $ find path -name filename -print 查看进程并关闭 1234 $ ps [-aux] # -x自己的，# -au所有用户的，# -aux系统内部和所有用户的 $ kill [-9] PID(进程号) 查看后台运行的进程 $ jobs $ kill %n #n为jobs查询出的后台作业号 命令记录表 设定命令记录表长度 $ set history = 长度 查看命令记录表的内容 $ history 重复执行前一个命令 $ !! 重复执行命令记录表命令编号n的命令 $ !n 重复前面执行过的以’xxx’为起始字符串的命令 $ !xxx The link of this page is http://home.meng.uno/articles/61f3f0dd/ . Welcome to reproduce it!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://home.meng.uno/categories/Software-Engineering/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://home.meng.uno/tags/Shell/"},{"name":"Bash","slug":"Bash","permalink":"http://home.meng.uno/tags/Bash/"},{"name":"Zsh","slug":"Zsh","permalink":"http://home.meng.uno/tags/Zsh/"}]},{"title":"跨领域分词国内外研究现状","slug":"word-seg-history","date":"2017-12-22T12:15:00.000Z","updated":"2020-12-02T02:10:53.000Z","comments":true,"path":"articles/e38d3f1c/","link":"","permalink":"http://home.meng.uno/articles/e38d3f1c/","excerpt":"国内研究 国内研究中文分词的科研单位主要有：中科院、清华、北大、北京语言学院、东北大学、MSRA、IBM研究院以及哈工大等。 国内主要的成熟的分词系统：ICTCLAS（汉语词法分析系统）、海量信息、盘古分词、结巴分词、BosonNLP以及**哈工大语言云（LTP-Cloud）**等。 国内在中文分词算法的研究上进展颇丰，参与的科研机构也比较多，使用的方法也比较杂乱，从[1]—[19]可以看出。国内分词算法上的进展主要有：2005年，哈工大[13]在分词阶段以基于词的n-gram方法为核心。先将词按照词典初步切分，并从训练语料统计得到3-gram信息，动态规划计算哪条切分路径最优。但在命名实","text":"国内研究 国内研究中文分词的科研单位主要有：中科院、清华、北大、北京语言学院、东北大学、MSRA、IBM研究院以及哈工大等。 国内主要的成熟的分词系统：ICTCLAS（汉语词法分析系统）、海量信息、盘古分词、结巴分词、BosonNLP以及**哈工大语言云（LTP-Cloud）**等。 国内在中文分词算法的研究上进展颇丰，参与的科研机构也比较多，使用的方法也比较杂乱，从[1]—[19]可以看出。国内分词算法上的进展主要有：2005年，哈工大[13]在分词阶段以基于词的n-gram方法为核心。先将词按照词典初步切分，并从训练语料统计得到3-gram信息，动态规划计算哪条切分路径最优。但在命名实体识别、新词识别、消除分词歧义部分使用ME模型。2007年，赵海等人[19]研究了基于子串标注的分词算法，在Bakeoff-2005测试集上准确度较高。2009年，[3]利用一种基于N元语法的汉语自动分词系统, 将分词与标注结合起来, 用词性标注来参与评价分词结果。[34]提出了一种字词联合解码的分词方法，算法中使用了字、词信息，充分发挥由字构词识别未登录词的能力。2010年，[35]提出基于词边界分类的分词方法，该方法对字符之间的边界进行分类，判断是否为词的边界，从而达到分词目的。[36]将基于字的生成模型与基于字的判别模型进行联合。2014年，[29]对[28]的模型做了重要改进，引入了标签向量来更精细地刻画标签之间的转移关系，其改进程度类似于引入Markov特征到最大熵模型之中。2015年，为了更完整精细地对分词上下文建模，[30]提出了一种带有自适应门结构的递归神经网络(GRNN)抽取n-gram特征，其中的两种定制的门结构（重置门、更新门）被用来控制n-gram信息的融合和抽取。2016年，[31]将GRNN和LSTM联合起来使用。该模型中，先用双向LSTM提取上下文敏感的局部信息，然后在滑动窗口内将这些局部信息用带门结构的递归神经网络融合起来，最后用作标签分类的依据。[32]提出了一种基于转移的模型用于分词，并将传统的特征模版和神经网络自动提取的特征结合起来，在神经网络自动提取的特征和传统的离散特征的融合方法做了尝试。2017年，[33]通过简化网络结构，混合字词输入以及使用早期更新（early update）等收敛性更好的训练策略，设计了一个基于贪心搜索(greedy search)的快速分词系统。该算法与之前的深度学习算法相比不仅在速度上有了巨大提升，分词精度也得到了进一定提高。 在领域自适应方面相关研究比较少，2008年，[45]利用并发展针对单个汉字的构词能力和构词模式公式, 计算词的构词能力和词的构词模式, 并以此作为新词发现的规则, 对科技领域做了新词发现和新技术发现的实验。2012年，[41]通过将外部词典信息融入统计分词模型 (使用CRF 统计模型)来实现领域自适应性。在确定一个领域并给出这个领域的文献数据集合的前提下，[44]主要从这两个步骤进行新词发现：首先对特定领域的文献集合进行分词处理，在进行分词处理方面使用了基于统计的N-Gram方法，较为有效地找出了词典中所不存在地新词汇；第二个步骤为新的专业词汇的抽取，这是一个根据已有专业词汇来发现未知专业词汇的过程，目的从第一步中所产生的新的词汇中抽取出新的属于目标领域的专业词汇，在这个步骤中，使用了Apriori方法。2013年，[40]实现了基于生语料的领域自适应分词模型和双语引导的汉语分词，并提出融合多种分词结果的方法，通过构建格状(Lattice)结构并使用动态规划算法得到较佳汉语分词结果。2015年，[39]提出Active Learning与n-gram统计特征相结合，通过对目标领域文本与已有标注语料差异统计分析，选择含有最多未标记过得语言现象的小规模语料优先进行人工标注的方法，此法验证在科技文献上有所提高。[43]提出使用卡方统计 量以及边界熵提升未登录词的处理能力，并结合自学习和协同学习策略进一步改善字标注分词方法在领域适应性方面的性能。2016年，[42]提出一种条件随机场与领域词典相结合的方法提高领域自适应性，并根据构词规则提出了固定词串消解，动词消解，词概率消解三种方法消除歧义。 国外研究 国外研究中文分词的主要科研机构有：斯坦福、SUTD、UC Berkeley、CMU、CityU等。 国外成熟的分词系统有：Core NLP（斯坦福 NLP Group）、Zpar（SUTD）、Basis Technology、Open NLP (Apache 基金会)等。 国外分词算法上的进展：2003年之前，主要集中在词典与人工规则相结合，词典与概率统计规则相结合。2005年，开始使用基于字序列标注的分词方法，该方法始于[20]，第一次将严格的串标注学习应用于分词在[21]和[22]之后。[23]与[24]的出现，基于CRF模型崭露头角，在此之后，CRF多个变种构成了深度学习时代之前的标准分词模型。基于词的随机过程建模导致一个CRF变种，即semi-CRF(半条件随机场)模型的直接应用。2006年，基于字序列标注的方法已经开始盛行，核心模型仍然是ME与CRF，同年，[25]发表semi-CRF的第一个分词实现。[26]提出了一种基于子词（subword）的标注学习，基本思路是从训练集中抽取高频已知词构造子词词典。2007年，ME的方法已经开始退出舞台，CRF越来越成为主流。2010年，核心方法还是基于CRF模型，后处理是SVM-HMM模型。2011年，当子串的抽取和统计度量得分计算扩展到训练集之外，[27]实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。2013年，[28]提出神经网络中文分词方法，首次验证了深度学习方法应用到中文分词任务上的可行性。 在领域自适应上，由耶鲁大学教授提出的Active Learning得到了较为广泛的使用。 待补充 参考文献 [1] 马晏. 基于评价的汉语自动分词系统的研究与实现[D]. 清华大学, 1991. [2] 张国兵, 李淼. 一种基于局部歧义词网格的快速分词算法[J]. 计算机工程与应用, 2008, 44(12):175-177. [3] 石佳, 蔡皖东. 基于N元语法的汉语自动分词系统研究[J]. 微电子学与计算机, 2009, 26(7):98-101. [4] 韩莹, 王茂发, 陈新房,等. 汉语自动分词词典新机制—词值哈希机制[J]. 计算机系统应用, 2013, 22(2):233-235. [5] 蒋才智, 王浩. 基于memcached的动态四字双向词典机制[J]. 计算机应用研究, 2011, 28(1):152-154. [6] 刘超, 王卫东. 基于双哈希词典机制中文分词的研究[J]. 信息技术, 2016, 40(11). [7] 刘挺, 吴岩, 王开铸. 串频统计和词形匹配相结合的汉语自动分词系统[J]. 中文信息学报, 1998, 12(1):17-25. [8] 唐涛. 面向特定领域的中文分词技术的研究[D]. 沈阳航空航天大学, 2012. [9] 卢志茂, 刘挺, 郎君,等. 神经网络和贝叶斯网络在汉语词义消歧上的对比研究[J]. 高技术通讯, 2004, 14(8):15-19. [10] 廖先桃, 于海滨, 秦兵,等. HMM与自动规则提取相结合的中文命名实体识别[C]// 全国学生计算语言学研讨会. 2004. [11] 程志刚. 基于规则和条件随机场的中文命名实体识别方法研究[D]. 华中师范大学, 2015. [12] 祝继锋. 基于SVM和HMM算法的中文机构名称识别[D]. 吉林大学, 2017. [13] ZHUORAN WANG, TING LIU. Chinese Unknown Word Identification Based on Local Bigram Model[J]. International Journal of Computer Processing of Oriental Languages, 2012, 1(3):185-196. [14] 原媛, 彭建华, 张汝云. 基于统计的汉语词义消歧研究[J]. 信息工程大学学报, 2007, 8(4):501-504. [15] 肖建涛. 基于最大熵原理的汉语词义消歧与标注语言模型研究[D]. 北京机械工业学院 北京信息科技大学, 2007. [16] 张旭. 一个基于词典与统计的中文分词算法[D]. 电子科技大学, 2007. [17] 佟德琴. 基于字词联合解码的中文分词研究[D]. 大连理工大学, 2011. [18] 赵海, 揭春雨, 宋彦. 基于字依存树的中文词法-句法一体化分析[C]// 中国计算机语言学研究前沿进展. 2009. [19] 赵海, 揭春雨. 基于有效子串标注的中文分词[J]. 中文信息学报, 2007, 21(5):8-13. [20] Nianwen Xue. Chinese Word Segmentation as Character Tagging. Computational Linguistics and Chinese Language Processing, 8(1), 2003, pp. 29–48. [21] Hwee Tou Ng and Jin Kiat Low. Chinese part-of-speech tagging: One-at-a-time or all-at-once? word-based or character-based? In Conference on Empirical Methods in Natural Language Processing, 2004, pp. 277–284. [22] Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. A maximum entropy approach to Chinese word segmentation. In Proceedings of the SIGHAN Workshop on Chinese Language Processing, 2005, pp. 448–455. [23] Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. A conditional random field word segmenter for SIGHAN bakeoff 2005. In Proceedings of the SIGHAN workshop on Chinese language Processing, vol. 171, 2005. [24] Fuchun Peng, Fangfang Feng, and Andrew McCallum. Chinese segmentation and new word detection using conditional random fields. In Proceedings of the international conference on Computational Linguistics, 2004, pp. 562–569. [25] Galen Andrew. A hybrid Markov/semi-Markov conditional random field for sequence segmentation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2006, pp. 465– 472. [26] Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. Subword-based tagging for confidence-dependent Chinese word segmentation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and the international conference on Computational Linguistics, 2006, pp. 961–968. [27] Hai Zhao and Chunyu Kit. Integrating Unsupervised and Supervised Word Segmentation: the Role of Goodness Measures. Information Sciences, 181(1), 2011, pp. 163–183. [28] Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. Deep learning for Chinese word segmentation and POS tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2013, pp.647–657. [29] Wenzhe Pei, Tao Ge, and Baobao Chang. Max-margin tensor neural network for Chinese word segmentation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2014, pp. 293–303. [30] Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing Huang. Gated recursive neural network for Chinese word segmentation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, 2015a, pp. 1744–1753. [31] Jingjing Xu and Xu Sun. Dependency-based gated recursive neural network for Chinese word segmentation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 2016, pp. 567–572. [32] Meishan Zhang, Yue Zhang, and Guohong Fu. Transition-based neural word segmentation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2016, pp. 421–431. [33] Deng Cai, Hai Zhao, Zhisong Zhang, Yuan Xin, Yongjian Wu, and Feiyue Huang. Fast and accurate neural word segmentation for Chinese. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2017. [34] 宋彦, 蔡东风, 张桂平,等. 一种基于字词联合解码的中文分词方法[J]. 软件学报, 2009, 20(9):2366-2375. [35] 李寿山, 黄居仁. 基于词边界分类的中文分词方法[J]. 中文信息学报, 2010, 24(1):3-7. [36] Wang K, Su K Y, Su K Y. A character-based joint model for Chinese word segmentation[C]// International Conference on Computational Linguistics. Association for Computational Linguistics, 2010:1173-1181. [37] 王娟, 曹庆花, 黄精籼,等. 基于受限领域的中文分词系统[J]. 信息系统工程, 2011(11):106-106. [38] 张少阳. 领域自适应中文分词系统的研究与实现[D]. 沈阳航空航天大学, 2017. [39] 许华婷, 张玉洁, 杨晓晖,等. 基于Active Learning的中文分词领域自适应[J]. 中文信息学报, 2015, 29(5):55-62. [40] 苏晨, 张玉洁, 郭振,等. 适用于特定领域机器翻译的汉语分词方法[J]. 中文信息学报, 2013, 27(5):184-190. [41] 张梅山, 邓知龙, 车万翔,等. 统计与词典相结合的领域自适应中文分词[J]. 中文信息学报, 2012, 26(2):8-12. [42] 朱艳辉, 刘璟, 徐叶强,等. 基于条件随机场的中文领域分词研究[J]. 计算机工程与应用, 2016, 52(15):97-100. [43] 韩冬煦, 常宝宝. 中文分词模型的领域适应性方法[J]. 计算机学报, 2015, 38(2):272-281. [44] 李明. 针对特定领域的中文新词发现技术研究[D]. 南京航空航天大学, 2012. [45] 王文荣, 乔晓东, 朱礼军. 针对特定领域的新词发现和新技术发现[J]. 现代图书情报技术, 2008, 24(2):35-40. The link of this page is http://home.meng.uno/articles/e38d3f1c/ . Welcome to reproduce it!","categories":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://home.meng.uno/categories/Natural-Language-Processing/"}],"tags":[{"name":"分词","slug":"分词","permalink":"http://home.meng.uno/tags/分词/"},{"name":"跨领域","slug":"跨领域","permalink":"http://home.meng.uno/tags/跨领域/"}]},{"title":"Software Verification Approaches","slug":"software-verification","date":"2017-12-11T13:19:48.000Z","updated":"2021-01-01T17:42:00.000Z","comments":true,"path":"articles/55e262ef/","link":"","permalink":"http://home.meng.uno/articles/55e262ef/","excerpt":"Network Function Virtualization (NFV) In the beginning, we need to know NFV which through the establishment of VNF (Virtualized Network Function) to achieve some network functions on a common server, switches, memory and other hardware devices, making these network functions on a common hardware dev","text":"Network Function Virtualization (NFV) In the beginning, we need to know NFV which through the establishment of VNF (Virtualized Network Function) to achieve some network functions on a common server, switches, memory and other hardware devices, making these network functions on a common hardware device run, do not need to configure a new dedicated network elements, can greatly enhance the flexibility of the network deployment, and lower investment costs. In the process of realization of network functionality through NFV technology, VNF in the form of software running on the hardware, by way of example and to achieve termination VNF allocation and deallocation of resources. In order to avoid VNF packet forgery in transit and in storage and tampering, increasing the signature files in the software package VNF, the receiving end after receiving the VNF software package by verifying signature files for VNF package for secure authentication to ensure VNF packet during transmission security; in addition, the receiving end before VNF instantiated need for storage VNF package for secure authentication to ensure VNF package in the store security, which increased VNF instantiation delay, reduce the VNF instantiated performance. Systems-Theoretic Process Analysis (STPA) STPA is for identifying harmful circumstances which could lead to accidents and generating detailed safety requirements which must be implemented in the design to prevent the occurrence of these unsafe scenarios in the system. STPA is a top-down process and it addresses many types of hazards of components and their interactions like design errors, software flaws and component interaction failures. One of the advantages of STPA is that it can be applied at any stage of the system development process. STPA is performed by four main steps: Before conducting an STPA analysis, the safety analysts should establish fundamentals of the analysis (e.g. accidents, the associated hazards) and construct the control structure diagram. For each control action in the control diagram, the safety analysts must identify the potentially unsafe control actions of the system that could lead to a hazardous state. A unsafe control action is a control action that violates system safety constraints. Use the identified hazardous control actions to create safety requirements and constraints. Determine how each potentially hazardous control action, identified in step 2., could occur by augmenting the control structure diagram with a process model. Software Model Checking (SMC) SMC is an automatic technique based on a verification model which explore all possible software states in a brute-force manner to prove properties of their execution. The model checking process involves the target software to be formally modeled in the input language of a model checker and specifications (properties) to be formalized in a temporal logic. Many safety-critical software systems are being written in ANSI-C. Therefore, there exist a number of software model checker tools which are used to verify code conducted a comparison and evaluation of existing model checking tools for C code. This comparison showed that the SPIN model checker, a general-purpose model checker, uses an efficient algorithm to reduce the state explosion problem. Safety Analysis Combining STPA and SMC This method can derive software safety requirements at the system level and to verify them at the code level. This approach is divided into three kinds of activities: Deriving software safety requirements using STPA; Formalizing of safety requirements and Verifying software against its safety requirements at the code level. The structure is like this: Fault Tree Analysis (FTA) FTA is a top-down, deductive failure analysis in which an undesired state of a system is analyzed using Boolean logic to combine a series of lower-level events. The propose is to understand how systems can fail, to identify the best ways to reduce risk or to determine event rates of a safety accident or a particular system level failure. This method can divide into 5 steps: Define the undesired event to study; Obtain an understanding of the system; Construct the fault tree; Evaluate the fault tree; Control the hazards identified. The link of this page is http://home.meng.uno/articles/55e262ef/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Software Verification","slug":"Software-Verification","permalink":"http://home.meng.uno/tags/Software-Verification/"}]},{"title":"VR/AR+：医疗","slug":"vr-ar-medicine","date":"2017-11-18T13:07:15.000Z","updated":"2021-01-04T07:28:26.000Z","comments":true,"path":"articles/c1dd0093/","link":"","permalink":"http://home.meng.uno/articles/c1dd0093/","excerpt":"VR/AR介绍 虚拟现实技术（VR）是一种可以创建和体验虚拟世界的计算机仿真系统，它利用计算机生成一种模拟环境，是一种多源信息融合的、交互式的三维动态视景和实体行为的系统仿真使用户沉浸到该环境中。 VR是仿真技术的一个重要方向，是仿真技术与计算机图形学人机接口技术多媒体技术传感技术网络技术等多种技术的集合，是一门富有挑战性的交叉技术前沿学科和研究领域。VR主要包括模拟环境、感知、自然技能和传感设备等方面。模拟环境是由计算机生成的、实时动态的三维立体逼真图像。感知是指理想的VR应该具有一切人所具有的感知。除计算机图形技术所生成的视觉感知外，还有听觉、触觉、力觉、运动等感知，甚至还包括嗅觉和味觉","text":"VR/AR介绍 虚拟现实技术（VR）是一种可以创建和体验虚拟世界的计算机仿真系统，它利用计算机生成一种模拟环境，是一种多源信息融合的、交互式的三维动态视景和实体行为的系统仿真使用户沉浸到该环境中。 VR是仿真技术的一个重要方向，是仿真技术与计算机图形学人机接口技术多媒体技术传感技术网络技术等多种技术的集合，是一门富有挑战性的交叉技术前沿学科和研究领域。VR主要包括模拟环境、感知、自然技能和传感设备等方面。模拟环境是由计算机生成的、实时动态的三维立体逼真图像。感知是指理想的VR应该具有一切人所具有的感知。除计算机图形技术所生成的视觉感知外，还有听觉、触觉、力觉、运动等感知，甚至还包括嗅觉和味觉等，也称为多感知。自然技能是指人的头部转动，眼睛、手势、或其他人体行为动作，由计算机来处理与参与者的动作相适应的数据，并对用户的输入作出实时响应，并分别反馈到用户的五官。传感设备是指三维交互设备。 而增强现实技术（AR）是一种实时地计算摄影机影像的位置及角度并加上相应图像、视频、3D模型的技术，这种技术的目标是在屏幕上把虚拟世界套在现实世界并进行互动。AR最早是于1990年提出的，之后随着电子产品运算能力的提升，其应用也是用途愈广。尤其是近两年来AR技术可谓是备受关注！现在的市场中由于可穿戴设备的出现，以及手机性能的进一步提升，AR技术也是持续升温。 AR是一种将真实世界信息和虚拟世界信息“无缝”集成的新技术，是把原本在现实世界的一定时间空间范围内很难体验到的实体信息（视觉信息,声音,味道,触觉等），通过电脑等科学技术，模拟仿真后再叠加，将虚拟的信息应用到真实世界，被人类感官所感知，从而达到超越现实的感官体验。真实的环境和虚拟的物体实时地叠加到了同一个画面或空间同时存在。不仅展现了真实世界的信息,而且将虚拟的信息同时显示出来，两种信息相互补充、叠加。在视觉化的AR实现中，用户利用头盔显示器，把真实世界与电脑图形多重合成在一起，便可以看到真实的世界围绕着它。 在接下来的叙述中，我将在第二部分分析VR/AR对于医生角度的应用，第三部分分析在病人角度的应用，第四部分是在医疗教育方面的应用，其他相关的研究我将在第五部分陈述，现阶段研究的不足将在第六部分分析，最后两部分是一点我的个人感想和本文结论。 对于医生的应用 模拟手术 虚拟手术作为一个虚拟现实领域的重要研究方向，正成为科学家们的关注焦点。它是集现代医学、计算机图形学、计算机视觉、生物力学、材料学、机器人等诸多学科为一体的新型交叉研究领域。医生可以通过虚拟现实技术来模拟、指导医学手术所涉及的各种过程，包括手术计划制定、手术排练演习、手术教学、手术技能训练、术中引导手术、术后康复等。 80%的手术失误是人为因素引起的，因此手术训练极其重要。以前医生手术训练都只能在病人身上做，所以经常说有名的医生都是踏过多少人的血液才走过来的，这样付出的代价实在太大。在VR/AR技术飞速发展并广泛应用的今天，模拟手术在医生训练过程中非常重要的应用就是，医生可以在不接触实际病人的前提下模拟各种手术场景，模拟的场景可能比真实遇到的情况还要多，这样模拟训练的效果实际上就超过了真实的练刀。一方面，模拟训练可以在任何地方、任何时间反复模拟，深化印象，这个优点在实际病人身上是不可能也不允许做的，因为病人的资源是有限的。模拟手术就是把每个可能的手术场景都呈现在你的面前，每个人得到的机会都是无穷次的，医生可以反复看、反复练，并且对病人没有伤害。如果从这点来讲，一个经过模拟训练的医生再给病人做手术时，他的学习周期就会很短很短，这个实际上是对病人利益的极大保护。当前模拟手术在中国也已经有了，像强生公司、科汇公司的外科培训中心已经有腹腔镜的模拟，也有介入手术的模拟，当然还有达芬奇手术的模拟，但这种模拟还不是真实场景的，也就是说未来在VR技术里面，可以完全进入到手术室，然后在真实的场景里面进行模拟手术，那就更加接近于现实。 大家应该知道在体育运动里，比如足球、体操，都有慢动作回放，有动作捕捉去分析运动员动作是否做到位了，失误的原因是什么。这个技术同样可以用到外科手术里。通过运动捕捉或者手势识别和VR技术，在外科大夫进行学习或尝试以后，可以复原手术，看到手术的过程，如果有失误，原因是什么。这种回放能极大的帮助医生改进他们的技术。 远程干预/指导 世界上首例实验性远程手术已经在1999年成功地进行。虚拟手术与远程干预将能够使在手术室中的外科医生能实时地获得远程专家的交互式会诊。交互工具可以使顾问医生把靶点投影于患者身上来帮助指导主刀外科医生的操作，甚或通过遥控帮助操纵仪器。这能使专家们技能的发挥不受空间距离的限制。如果VR技术在这一方面继续发展的话，可能会出现以后医生不用到医院上班，无论在任何地方都可以实施手术。缓解了当今医院基础设施不足的现状。同时，有些相同的手术，某专家可以通过VR技术远程指导，这样就打破了一个时间段只能进行一项手术的限制，大大提高医疗的效率。 精确操作 如上文已经提到的达芬奇机器人，还有很多像这种医生能够远程控制其操作的手术设备，通过使用这些设备，能够在实际的手术中，避免因为医生长时间工作造成的身体情况变化而带来的手术质量参差不起的问题（例如某医生一天手术过多，太过劳累，这样后来的手术必然会质量下降）。如果使用了VR技术，远程控制诸如达芬奇机器人一样的手术设备，不管医生自己身体状况如何，只要控制到位，那些手术设备是不会因为工作时间长就产生不适的，这样也能保证手术质量优秀。同时，一个医生只有两只手，所以有些手术需要助手，而使用那些设备之后，一个医生可以控制很多只手，这样就能够协调统一，对手术质量有百利而无一害。 协助执行日常任务 医生需要经常查看病人的病情，而在现在的医疗设施情况下，对于一个医生，可能得通过“查房”一间一间地去病房里与病人交流，而且交流记录难免会记忆不清或者甚至搞混了。有了VR技术虚拟医生协助真实医生实现这些复杂的工作，效率必然会大大提高。医生甚至都不用出办公室，只需要观察相应的VR反馈就可以基本实现之前的“查房”任务。这样一来就可以大大减轻医生的负担，必然会对医生的日常工作效率产生积极的影响。这些虚拟医生与病人交流的数据可以保存用来对之后的反馈对比，当然可以用其他机器学习的方法来进行一些预测之类的计算。并且基于大数据，可能这些虚拟医生对单个病人情况的分析会比真实的医生更专业，因为医生也不是能对每个症状都了如指掌的。 有些病人（例如老人和小孩等）可能在认识自己病情上有欠缺，例如忘记吃药，每次吃多少药等，在传统的方案中，要么有专人提醒，要么在显眼的地方做上标记提醒，总之效率不高或者费时费人力。当我们在某种小的设备上加上VR/AR提醒，以一个三维立体的形象的形式来提醒病人，既省时又省人力，效率相对还比较高。 获得医药信息 医生可能不能记住每种药物的作用，以及每种药物的使用方式，假如能通过VR技术将每种药的使用信息都记录下来，通过相关的设备，让医生在给病人开药的时候头脑里能对这种药的所有情况都回顾一遍，同时还可以与几种相似的药做对比，找出最好的组合，避免偶尔的误用药。 获取用户机体信息 在现在正常的医学治疗步骤中，医生想要检查病人某部位的病变情况，只能使用诸如CT、CTA等基于切片的二维人体部位图像，这种图像相对来讲专业要求度较高，对人体伤害较大，同时准确性也不能得到保障。将VR应用于此种工作，我们可以得到人体某部位的三维立体真实大小的模型，这样一来医生的判断会更加准确而且这种图识别起来相对不需要那么复杂的专业知识。 血管照明（辅助手术） 这算是一种比较专业地说法，和辅助手术概念比较类似，简而言之就是通过PC应用软件帮助医务人员在手术中能够查看隐藏的血管。此前，心脏病专家借助谷歌眼镜疏通了一位49岁男患者阻塞的右冠状动脉。冠状动脉成像（CTA）和三维数据呈现在智能眼镜的显示器上，根据这些实时放大图像，医生可以方便地将血液导流到动脉。不同于传统手术，AR的介入就像一个”AR放大镜”，直接放大手术创口，患者的彩超、MRI、CT图像等将直接映在手术部位，让医生能够看到肉眼难以分辨的细微情况，获得“透视”功能，大大提高手术操作的效率和舒适度。 损伤评估 在传统的医疗实践中，如果病人就诊，医生为了确定病人病情只能通过明显的外表特征或者患者的口头表述来确定，而这样的手法往往会带来误判或者病情程度判断不清等问题，特别是那种很难表述清楚的内科疾病。运用虚拟现实技术，我们可以针对不同的疾病设定不同的诊断场景，让用户在特定场景中将应该表现出来的病症全部表现出来，这样一来有利于增大诊断的准确性。 对于病人的应用 智能康复系统 基于Kinnect等运动捕捉设备所设计出的很多结合VR/AR技术的智能康复系统，这些系统能够很好的帮助病人进行相应的康复训练。在之前我所看到的一个例子中， 病人只需要带上相应的可穿戴传感器就可以将自己的动作传到相应的计算机进而在计算机显示屏上显示用户的动作以及在显示屏中出现的影像之间的交互。在我所看到的实例中，他们只是能做到在场景中增加一些物体（例如长方体等），让病人在虚拟的环境中模拟各种体力锻炼。这样一来，我们就不需要实际花费很多的金钱来布置训练场景，而且VR所形成的场景还可以很轻易得更换，不用为维护实际的场景而花费很多精力。 调查显示，以色列的研究人员开发出一套“电脑辅助康复环境系统”，通过模拟划船、打球、慢跑等各种情景，来帮助残障人士改善平衡能力，恢复身体的运动机能。足部神经受损的博罗夫斯基就在接受“电脑辅助康复环境系统”的治疗。现在系统模拟的是“海上冲浪”，大屏幕上显示的是冲浪的场景，博罗夫斯基脚下的踏板会根据设置好的程序相应的摇动，他必须通过调节身体的姿势来保持平衡。同时，连接在他身上的传感器还能把各种身体体征数据传回电脑，以便医生调整训练强度和难度。丰富的影像和新颖的训练方式，让患者像在做游戏一样，更能提高患者本身的主动性和积极性，加强训练效果，缩短住院时间，加快康复过程。无论从时间、人力还是金钱上讲，VR/AR技术的使用必然会在康复系统上带来一场新的革命。 帮助治疗某些疾病 有些疾病（例如某种危险情况恐惧症），需要病人在再临其境时才可能克服，但如果让患者在现实中的危险环境再次体验，可能会有安全性问题，而且相同的场景很难真实地完美还原。如果使用VR技术，就可以在基本不耗费后期资源的情况下反复体验相同的场景，在现实中，用户也不会有任何的伤害，而且可调节范围比较广，毕竟是电脑虚拟程序，恐怖程度、危险指数等都可以随心所欲地变化，与现实相比真的是有巨大的优势。 截肢者中最常见的烦恼就是幻肢痛——患者感到被切断的肢体仍在，且在该处发生疼痛。疼痛多在断肢的远端出现，疼痛性质有多种，如电击、切割、撕裂或烧伤等。对幻肢痛的发生原理，目前尚无统一意见，西医亦乏有效疗法。很有可能大脑对肢体仍然存有意识，即使它已经不存在了。尽管这样的问题发生存在一定的频率，但至今没有一种有效地方法适用于所有的截肢者。在使用VR技术治疗过程中研究人员使用头戴式耳机和一个传感器将患者带入虚拟的世界，患者可以感受到自己的肢体还在，并可以控制虚拟肢体从事某项工作或游戏。这样就能很好的解决这一疾病，有研究表明对这种疾病VR治疗作用十分显著。 VR对创伤后应激障碍也同样有很好的治疗作用。在对从伊拉克和阿富汗返回患有创伤后应激障碍的士兵所进行的VR治疗过程中，VR设备会将会把士兵带回中东的一个小镇，让他们再次“经历”战争和死亡，使其在适当的压力下逐渐学会处理，控制自己的情绪。虽然很多人对于这种治疗方式存在争议，支持者说使用虚拟现实技术与其他的治疗方式相结合会达到非常很理想的治疗效果。 VR可以帮助治疗的心理障碍并不仅仅局限于创伤后应激障碍。还有些心理问题（例如自闭症、害羞等），是一个杀人于无形的凶手，目前呈现向低龄化蔓延的趋势。虚拟现实可能也会成为这个问题的解决方案之一。如果有人能既为患者保守秘密有能很好的与用户交流，对治疗这些疾病肯定是有很大的好处的，但是现实中不可能存在这样的一个完美的“倾听者”，即使有，价钱也是不菲的。如果VR/AR能够起到这样的作用那必然会是又一大医学进展。 很多心理治疗师和精神专家常用的方式是通过在治疗过程中去引导患者回忆或者想象场景，以此来达到治疗的目的。VR的好处在于它能够让这种环境场景变得可视化和标准化。因此在心理治疗领域，比如说创伤应急、障碍症、恐惧症、自闭症、恐高症、幽闭症、公开演讲恐惧症、密集恐惧症等都可以通过VR技术的环境再现以达到治疗的目的。再比如说，焦虑症、注意力缺陷以及精神分裂症也可以通过VR来虚拟特定的人或是特效来改善相关的一些症状。 虚拟问诊 在国内外，好的医生都是十分欠缺的，然而相同的疾病缺屡见不鲜，可以说很多小病完全可以在还是在早期的时候就通过及时就诊可以避免出现的，在之前的这么多年，很多搜索引擎也都在做相关的疾病问答系统，可悲的是大多受金钱诱惑，为金钱奴役，不干正事反而虚假宣传。VR技术的出现，必然在医疗问答方面带来很大的革新，患者可以通过VR设备与虚拟的医生直接进行交谈，不仅避免了文字表述不清的问题，而且也避免受网页上那么多的虚假广告的诱惑，更是给人一种如见真实医生的舒适的感觉，而且VR医生会比现实中医生更有耐心，更专职为你一人服务。 缓解疼痛 读到这样一个故事，“2017年年初，美国的一位脂肪瘤患者开刀时，因为平时血压过高，医生只为她注射了少量镇静剂。医生为她戴上了VR设备，在手术过程中患者一直在玩一个埃及探险的游戏。手术过程中，监测仪器显示患者的一切生命体征参数，都非常平稳。在整个手术过程中，患者的血压不仅没有提升，反而下降了。手术完成后，患者表示她几乎没有感到疼痛。”我觉得有了这个实例，我不需要过多的解释，已经能够很好的说明VR在缓解病人疼痛上的作用了吧。无独有偶，接受重度烧伤治疗是一段痛苦的经历。伤口清理和绷带变化都会引发疼痛，即便使用吗啡等麻醉药物，仍有86%的病人会感到或多或少的疼痛，并且大量使用还会对身体造成一定伤害。1996年，华盛顿大学人机界面技术实验室（HITLab）研究人员发现孩子们在玩游戏时是越来越全神贯注后，想出了为治疗烧伤提供VR游戏，假设沉浸在游戏中会为病人带来积极疗效，他们会更专注于游戏，而减轻对疼痛的注意力。据调查，社会行为医学2011年发布的调查展示了浸入式游戏作为止痛剂的强大作用。并且现在这项技术已经被美国军方使用，帮助受伤的士兵接受治疗。 当然在平时的生活中，VR也是能够起到很好的。当我们沉浸在一个虚拟的美好的游戏环境中时，我们可以忘记暂时的伤痛，这是一个常识，也是VR能在这一方面得到巨大应用的一个原因。 戒瘾 当今社会的另一大“疾病”，不是身体上的，更大多数是心理上的，例如网瘾、烟瘾甚至毒瘾。在以前的戒瘾所，采取的唯一方式就是物理上的隔离，在VR技术快速发展的今天，我们完全可以通过VR将患者的精力集中到正确的角度上，不仅可以将效果提高，而且省时省力。查阅资料发现，我国相关法律已经开始涉足使用VR进行戒毒活动了。 当然，其实在这里VR可能成为一把双刃剑，可能能将用户的注意力从其他那些及其不正常的生活习惯中解救出来，但是可能又会让用户陷入“VR瘾”中，现在我们还无法证明这个“VR瘾”和其他的“网瘾”、“烟瘾”等孰轻孰重，所以此法需要慎重使用。 VR/AR与视觉结合 VR本身就是可以直接作用于人的视觉，视觉治疗方案与VR技术很容易匹配。现在升学、工作压力，导致大部分人都有眼部疾病或视觉障碍。VR在治疗眼部的疾病，比如儿童的斜视、近视以及立体视力的缺陷上有很好的效果。虽然在不久之前就有对近视等疾病的物理治疗方案，但是大多费时费力，而且没有听说有什么可观的进展。但是VR/AR的到来就完全不同了，大大提高了效率而且效果明显提高。同时可以制作一种VR设备用来缓解疲劳的眼球，当我们工作或者学习很长时间之后，可以用它来对眼球“做做操”。虽然我们都明白久看之后需要远眺一会，或者看看绿色的动西，但是迫于现实，我们可能很难做到（例如哈尔滨的冬天没有绿树。），但是这一切都可以用VR设备来实现。 缓解术前压力 接受手术的患者到了陌生环境，难免会有应激反应。他们可能觉得在手术室外等候，麻药还没有生效的时候，他们有强烈的紧张感和恐惧感，甚至有了濒临死亡的体验。现在想一想，这种体验对每个人来说都是非常不好的。虽然术前医生和患者有充足时间做沟通，但是单纯靠用嘴讲、用图、甚至手画示意图，都是一件非常累的事情，因为手术的复杂性和专业性，包括一些专有名词，对患者来说真的是选择性记忆，他们有可能听了上半句，就忘了下半句，甚至把最主要的东西漏掉，支离破碎地理会医生的意思，即使再出色的现场描述也比不过真实环境的真实还原。如果运用VR技术，在手术开始前给患者放一段容易让他放松的内容，让他在进手术室之前，大致了解手术室和手术过程是什么样子的，就能消除他对未知环境的恐惧。我相信这对患者来说是人文的关怀，这在提高医学服务水平里面可以得到有效的应用。 隐私保护 现在我国的搜索引擎在医疗上很不受人看好的一个主要原因就是其泄露用户隐私的现象太严重，所以很少有人真正愿意相信搜出来的东西。加入VR/AR元素进入我们的生活，我们可以与虚拟形象进行完全不用担心泄露隐私的情况下的交流。 医疗教育的应用 手术教学 伴随互联网的继续发展，如果医学手术教育能够通过VR/AR技术实现，至少在解剖学课程上可以实现意想不到的效果。在相关的设计中，例如心脏的VR模型可以做的特别逼真，让学生可以把心脏托在手里面，随时都可以转圈、打开肌肉看，血流都可以看，这就可以大大缩短医学生的学习周期（而且可以打破时间、地点的限制来学习）。尤其对于外科医生来讲，外科医生很重要的工作就是解剖。局部解剖以前医学生只能通过书本来学习，看到的都是平面的东西，脑子里很少有立体的概念，无法产生互动，进而需要很长的时间来消化知识，甚至很多时候大多数医学生并不能形成很好的立体的模型在脑子中，所以这就是很多庸医出现的根源。VR/AR技术的出现使得医学生在真正操刀做手术之前，就能获得局部解剖的经验，能够有效提高手术的安全性和成功率。而如果医学生通过VR/AR技术实现解剖模拟，学习速度会大大加快，同时降低学习的成本。当然AR在医学教育的应用并不仅仅是在解剖学上面，在生理生化方面都是可以通过VR技术模拟场景的。如果我们可以获得很形象的信息的话，对医生的知识拓展和学习速度都会急剧加快。 据早在1993年的统计里，全球市场上出现的805个虚拟现实应用系统中就有49个应用于医学，主要应用在虚拟人体、医学图像学、药物分子研究等方面。大家都知道，在传统的医学教育中，如人体标本解剖和各种手术实训，大都受到标本、场地等限制，实训费用高昂。而且医学生不能通过反复在病人身上进行操作来提高临床实践能力、临床实践具有较大风险。而VR的直观和体验特性却可以很好地解决以上问题。目前在医学教育上应用较多的有虚拟人体解剖学、手术训练教学、虚拟实验室、虚拟医院等。类别也从内容、软件到硬件，甚至还有从事交叉研发的，比如，Oculus涉足了内容、硬件和软件等方面，微软HoloLens则是软硬件结合等。 利用VR技术来做外科手术培训另一个重要的特点是，大大节约了成本。在外科领域，医疗知识每隔6~8年就要翻一番，所以外科大夫在专业教育上尤其是在继续教育上需要不断追求对新技术的学习，这种新技术的学习成本是高昂的，方法是复杂的。而VR技术可以在某种程度上帮助大家学习或者熟悉这种新技术。 教学用具改革 传统解剖学挂图和大部分多媒体课件上应用的教学图片都是二维模式，缺少直观的、立体的体验，造成了解剖学习的困难。模型、标本虽具有立体结构，但形式单一、僵硬，不能满足多角度、多层次的教学和实训需求。而虚拟人体解剖图，可在显示人体组织器官解剖结构的同时，显示其断面解剖结构，并可以任意旋转，提供器官或结构在人体空间中的准确定位、三维测量数据和立体图像。此前，美国加州健康科学西部大学（波莫纳）开设了一个虚拟现实学习中心，该中心拥有四种VR技术、zSpace显示屏、Anatomage虚拟解剖台、Oculus Rift和iPad上的斯坦福大学解剖模型，旨在帮助学生利用VR学习牙科、骨科、兽医、物理治疗和护理等知识。 相关研究进展 尽管业内不少人将2016/2017年称之为“VR发展元年”，若追溯VR发展的历史，早在1932年，Aldous Huxley在其推出的科幻小说《美丽新世界》中即对虚拟现实概念进行了描述。而直到1968年计算机科学家Ivan Sutherland开发了“达摩克利斯之剑”，使得VR设备具备了基本的雏形。随后，VR设备开始应用在一些专精领域，如宇航员的训练活动中。直到1987年，任天堂推出了Famicom3D System眼镜之后，Virtual Boy等设备将VR概念正式带入民用领域。而随着近些年来，视频技术以及移动硬件领域的不断发展，民用VR平台也根据使用者的不同呈现出了分化的状态，包括以游戏平台作为计算平台的专属VR平台、以PC作为计算平台的综合体感VR平台、以及以移动设备作为计算与显示窗口的VR眼镜。 硬件厂商方面，在对各类VR设备的研发加大投入力度，VR头盔，眼镜，以及附属的传感器设备在过去的一年中纷纷涌现。 视频平台方面，除了传统视频上传方式外，各大视频平台均开放了“全景视频”的上传接口，用于鼓励视频制作团队为平台增加全景类视频的内容量。但由于目前全景视频的制作与存储成本非常高，能够完成全景视频录制与制作的团队并不多，所以目前多数存留在VR平台端的视频实际为3D的“沉浸式”的视频内容。除此之外，部分平台采取聚合方式，将目前市面上鲜见的VR视频内容加以收集整理，集中呈现在用户面前。而技术方案提供方虽然距离用户较远，却是目前推进整个VR行业发展的最为重要的一方，技术方案将成型的算法，图像引擎等输出给视频平台或硬件厂商，以增强用户在两方的使用体感。 存在的问题 VR在医疗应用方面，相比用作练习、模拟来讲还是很欠缺的。AR in China 最近期的统计，如今国内从事AR应用开发的企业有200多家，其中80%倾向和已开发游戏类应用，剩余的也多偏向影视、购物等生活类应用。而专注在医健领域的应用，根据公开信息推测目前不超过10家。而在海外，据 CBInsights、CrunchBase、AngelList 网站的综合数据查询，目前有30家左右初创公司正专注在AR医疗应用领域。其中9家初创公司获得融资，总融资额达5.52亿美元，获投率达到了30%。AR在医健领域的应用还处于蓝海探索期。 纯粹的VR在医疗领域还是有很大欠缺的。在现在的VR辅助手术中，医生只能利用AR技术的一些优势，并不能完全交给VR来做，还需要加上传统方法，一边做手术一边对着电脑屏幕比对着看。 VR用于治疗方式的缺点是患者的想象和回忆难于把控，所以效果很难评估。 而且现在的VR在显示和精确度方面还是有很大的提升空间的。特别实在医疗领域，准确度至关重要。 对于医生而言，还有适应问题，这些新技术对于有经验的外科医生及其他专业医护人员来讲，“适应”是最大的挑战。 虽然VR技术在医学上应用后能够减少现实中的直接的隐私泄露，但是如果VR数据泄露将导致比现在信息泄露更严重的后果，毕竟VR可以记录整个人的信息而不仅仅是文字信息。 VR的一个很大的问题就是基础硬件设备的体验问题。如果要让医生或者被治疗者长时间呆在虚拟环境中，很容易产生一些生理不适的症状。 虽然说VR在医学上的应用很丰富，尤其是在心理治疗方面颇有成效。但是考虑到治疗的针对性和VR内容制作难度等各种问题，这种辅助治疗方法在现阶段很难进行大规模的应用。 医疗行业需要的是严谨的专业知识和态度，所以对于内容的要求也就相应的提高。如果要开发出一套模拟的人体用来交互培训，需要的是具备医学加上合理内容开发的复合型人才。而且考虑到医学诊断的高精度要求，许多器官或者组织的建模都要非常精细，不能有一丝的马虎，而现在的VR医疗应用更多的还是停留在头戴式VR视频方面。 其他要面临的困难还有：治疗和评估标准没有相关的评估标准、应用系统的交互性和易用性还不够完美。虚拟现实系统设备及其外设性价比例失衡，设备相对比较昂贵,致使大规模应用的时机还不够成熟。 个人感想 虽然最近VR/AR技术越来越火，相关研究也相当多，涉及的行业也是相当全面，但是在现在的情况下，绝大多数成就集中在仿真、游戏上，在医疗领域还是存在很多问题，还有很长的路要走。但希望不要又只能火两年而已。 在医疗工作的各个领域推广VR技术的应用，可以节省大量的时间与资源，从而更快捷、更安全的挽救生命。国际上由于虚拟现实技术的发展而发展起来的医疗电子设备正以每年10%的速度增长。随着计算机、多媒体技术、传感技术、通讯技术的发展以及各国对虚拟现实技术的日益重视，相信这一技术在医学上的应用在将来会取得更大的发展，它的发展前景非常诱人。可以预言，虚拟现实技术在医学中更广泛、更深入的应用将会给传统医疗带来革命性的变化。 在我们国家，新一轮的医疗体制改革如火如荼，传统的医疗体系已经岌岌可危，惟有引入新的信息技术才能适应时代要求。而且，新的医疗体制改革凸现了“社区医疗”的概念。“社区医疗”在全球医疗信息化中对应其第三个阶段，区域医疗信息网络（GMIS），是继医院管理系统（HMIS）、临床医疗信息系统（CIS）之后的阶段。美国所有医疗机构均实现了信息化管理，而中国尚处在初级阶段。将来中国的趋势必然是走向医疗信息化（e-Health）。为了实现这一战略目标，虚拟现实技术就是最佳的操作工具。因此，虚拟现实技术将在中国的“医改”过程中以及今后医疗事业的发展中扮演更加积极、重要的角色。 医疗VR是一个给人无限遐想的领域，它不再只存在于科幻小说爱好者的想象中，而是已经走进了临床研究者和现实生活中的医疗工作者的视野。虽然这是一个全新的领域，还不为大众所知，但是医疗VR技术是对患者的生活和医生的工作都可以产生积极影响的应用。我相信VR必将为医学领域带来一场大变革。 结论 VR/AR作为目前比较新潮的技术，在医学领域作用空前。其在医疗上无论从医生角度、患者角度还是医学教育角度都有着十分巨大的作用。现在相关的科学研究也发展迅速，当然也是存在很多的不足，尤其是我们国家在这方面的研究还很欠缺。在今后的一段时间内，我国的VR开发者还是应该多学习和借鉴国外的先进经验，同时保持在这方面的热情，相信在不久的将来，VR/AR定会在医学领域大大地大放异彩。 参考文献 刘建武, 叶志前, 陆金芳. 虚拟现实在医学中的应用进展[J]. 国际生物医学工程杂志, 2000(6):321-324. 王海舜, 潘利庆. 虚拟现实技术在医学中的应用[J]. 计算机应用, 1998, 22(6):49-54. 刘聚卑, 庄天戈. 虚拟现实在医学上的应用[J]. 北京生物医学工程, 2000, 19(1):47-54. 谭珂, 郭光友, 王勇军,等. 虚拟现实技术在医学手术仿真训练中的应用[J]. 解放军医学院学报, 2002, 23(1):77-79. 范立冬, 李曙光, 张治刚. 虚拟现实技术在医学训练中的应用[J]. 创伤外科杂志, 2008, 10(6):568-570. 谭海珠, 杨棉华, 陈丹芸,等. 虚拟现实技术在医学中的发展与应用[J]. 中华医学教育探索杂志, 2005, 4(6):410-412. 张晗 虚拟现实技术在医学教育中的应用探讨[J]. 西北医学教育, 2010, 18(1):48-51. 孙秀伟, 阎丽, 李彦锋. 虚拟现实技术(VR)在医疗中的应用展望[J]. 临床医学工程, 2007(5):17-20. 吴奇, 程薇曦. 虚拟现实技术在医学手术中的实现与应用[J]. 重庆医学, 2008, 37(21):2489-2491. 李舫, 宋志坚. HMD式光学穿透技术在医学增强现实中的研究进展[J]. 中国数字医学, 2012, 07(1):14-20. 孙国臣, 余新光, 陈晓雷,等. 基于多模态功能神经导航的虚拟现实及增强现实技术在神经外科教学中的应用[J]. 中国医学教育技术, 2015(1):66-69. 李潜. 增强现实技术为医学教育开拓无限未来[J]. 电脑知识与技术, 2012, 08(2):481-482. 张军毅. 医学增强现实建模及可视化研究[D]. 首都医科大学, 2008. 赵娜, 杨谊平. 增强现实技术与手术模拟[J]. 中华医学丛刊, 2004(4):58-59. Wang S, Parsons M, Stonemclean J, et al. Augmented Reality as a Telemedicine Platform for Remote Procedural Training.[J]. Sensors, 2017, 17(10):2294. Noll C, Jan U V, Raap U, et al. Mobile Augmented Reality as a Feature for Self-Oriented, Blended Learning in Medicine: Randomized Controlled Trial[J]. Jmir Mhealth &amp; Uhealth, 2017, 5(9):e139. Mero M, Susin A, Aplicada D M. Deformable 3D Objects for a VR medical application[J]. 2007. Crossan A, Brewster S, Reid S, et al. Multi-session VR Medical Training: The HOPS Simulator[J]. People and Computers XVI - Memorable Yet Invisible, 2002:213–226. Bezerra A. Evaluation of VR medical training applications under the focus of professionals of the health area[C]// ACM Symposium on Applied Computing. ACM, 2009:821-825. JLM Vazquez, BK Wiederhold, I Miller, et al. Virtual Reality Assisted Anesthesia (VRAA) during Upper Gastrointestinal Endoscopy: Report of 115 Cases— Analysis of Physiological Responses, 2017 The link of this page is http://home.meng.uno/articles/c1dd0093/ . Welcome to reproduce it!","categories":[{"name":"Computer Graphic","slug":"Computer-Graphic","permalink":"http://home.meng.uno/categories/Computer-Graphic/"}],"tags":[{"name":"VR","slug":"VR","permalink":"http://home.meng.uno/tags/VR/"},{"name":"AR","slug":"AR","permalink":"http://home.meng.uno/tags/AR/"},{"name":"医疗","slug":"医疗","permalink":"http://home.meng.uno/tags/医疗/"}]},{"title":"中文分词小赛数据","slug":"nlpc","date":"2017-10-30T08:02:41.000Z","updated":"2020-12-02T01:56:17.000Z","comments":true,"path":"articles/649482ba/","link":"","permalink":"http://home.meng.uno/articles/649482ba/","excerpt":"纪念一下大四组织的一次中文分词小比赛。 分项数据 * 训练数据： 链接: https://pan.baidu.com/s/1sl9JLqX 密码: 8am6 * 测试数据： 链接: https://pan.baidu.com/s/1eSeYhfO 密码: cnw2 * 相关参考答案： 链接: https://pan.baidu.com/s/1c2tVto0 密码: 3rpt * 有切分歧义的100个句子：链接: https://pan.baidu.com/s/1gfJ7Duz 密码: 8mmx 所有数据 所有文件下载：链接: https://pan.baidu.com/s/1smU","text":"纪念一下大四组织的一次中文分词小比赛。 分项数据 训练数据： 链接: https://pan.baidu.com/s/1sl9JLqX 密码: 8am6 测试数据： 链接: https://pan.baidu.com/s/1eSeYhfO 密码: cnw2 相关参考答案： 链接: https://pan.baidu.com/s/1c2tVto0 密码: 3rpt 有切分歧义的100个句子：链接: https://pan.baidu.com/s/1gfJ7Duz 密码: 8mmx 所有数据 所有文件下载：链接: https://pan.baidu.com/s/1smU54gl 密码: x5ne 测试结果 相关PPT：PPT下载 The link of this page is http://home.meng.uno/articles/649482ba/ . Welcome to reproduce it!","categories":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://home.meng.uno/categories/Natural-Language-Processing/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://home.meng.uno/tags/NLP/"},{"name":"中文分词","slug":"中文分词","permalink":"http://home.meng.uno/tags/中文分词/"}]},{"title":"中断","slug":"interrupt","date":"2017-08-06T14:38:33.000Z","updated":"2020-12-08T10:10:34.000Z","comments":true,"path":"articles/81b224e1/","link":"","permalink":"http://home.meng.uno/articles/81b224e1/","excerpt":"什么是中断？ 中断是能够打断CPU指令序列的事件，它是在CPU内外，由硬件产生的电信号。CPU接收到中断后，就会向OS反映这个信号，从而由OS就会对新到来的数据进行处理。不同的事件，其对应的中断不同，而OS则是通过中断号(也即IRQ线)来找到对应的处理方法。不同体系中，中断可能是固定好的，也可能是动态分配的。 中断产生后，首先会告诉中断控制器。中断控制器负责收集所有中断源的中断，它能够控制中断源的优先级、中断的类型，指定中断发给哪一个CPU处理。 中断控制器通知CPU后，对于一个中断，会有一个CPU来响应这个中断请求。CPU会暂停正在执行的程序，转而去执行相应的处理程序，也即OS当中的中断","text":"什么是中断？ 中断是能够打断CPU指令序列的事件，它是在CPU内外，由硬件产生的电信号。CPU接收到中断后，就会向OS反映这个信号，从而由OS就会对新到来的数据进行处理。不同的事件，其对应的中断不同，而OS则是通过中断号(也即IRQ线)来找到对应的处理方法。不同体系中，中断可能是固定好的，也可能是动态分配的。 中断产生后，首先会告诉中断控制器。中断控制器负责收集所有中断源的中断，它能够控制中断源的优先级、中断的类型，指定中断发给哪一个CPU处理。 中断控制器通知CPU后，对于一个中断，会有一个CPU来响应这个中断请求。CPU会暂停正在执行的程序，转而去执行相应的处理程序，也即OS当中的中断处理程序。这里，中断处理程序是和特定的中断相关联的。 中断描述符表 那么CPU是如何找到中断服务程序的呢？为了让CPU由中断号去查找到对应的中断程序入口，就需要在内存中建立一张查询表，也即中断描述符(IDT)。在CPU当中，有专门的寄存器IDTR来保存IDT在内存中的位置。这里需要注意的是，常说的中断向量表，是在实模式下的，中断向量是直接指出处理过程的入口，而中断描述符表除了入口地址还有别的信息。 IDTR有48位，前32位保存了IDT在内存中的线性地址，后16位则是保存IDT的大小。而IDT自身，则是一个最大为256项的表（对应了8位的中断码），表中的每个向量，是一个入口。这里IDT表项的异常类型可以分为三种，其表项的格式也不同： 任务门：利用新的任务方式去处理，需要切换TSS。它包含有一个进程的TSS段选择符，其偏移量部分没有用，linux没有采用它来进行任务切换。 中断门：适宜处理中断，在进入中断处理时，处理器会清IF标志，避免嵌套中断发生。中断门中的DPL(Descriptor privilege Level)为0，因此用户态不能访问中断门，中断处理程序都是用中断门来激活的，并且限制在内核态。 陷阱门：适宜处理异常，和中断门类似，但它不会屏蔽中断。 以下是32bit中的IDT表项。 值得注意的是，CPU还提供一种门，调用门，它是linux内核特别设置的，通常通过CALL和JMP指令来使用，能够转移特权级。 实模式和保护模式 在了解CPU是如何通过中断向量表调用具体的服务程序之前，首先需要了解CPU的工作方式。 对于IA-32架构，它支持实模式、保护模式和系统管理模式。 实模式以拓展对方式实现了8086CPU的程序运行环境，处理器在刚刚上电和重启后时，处于实模式，其寻址空间最大为1M(2^20)。实模式的主要意义，在于提供更好的兼容性，开发者能够直接使用BIOS中断，从而在boot阶段不必关注硬件的具体实现。实模式主要还是为进入保护模式进行准备。 8086处理器有16-bit寄存器和16-bit的外部数据总线，但能够访问20-bit的地址，因为它引入了“分段机制”，一个16bit的段寄存器包含了一个64KB的段的基址。而段寄存器＋16bit的指针，就能够提供20bits的地址空间。其计算方式为：16位基地址左移4位＋16位偏移量＝20位。 保护模式是处理器的根本模式。保护模式可以直接为实模式程序提供保护的，多任务的环境，这种特性被称为虚拟8086模式，它实际上是保护模式的一种属性。保护模式能够为任何任务提供这种属性。在保护模式中，地址依然通过“段＋偏移量”的形式来实现，但此时段寄存器中保存的不再是一个段的基址，而是一个索引。通过这个索引可以找到一个表项，里面存放了段基址等许多属性，这个表项也就是段描述符，而这个表也就是GDT表。 保护模式的最大寻址是2^32次方，也即4G，并且可以通过PAE模式访问超过4G的部分。它有4个安全级别，内存操作时，有安全检查。其分页功能带来了虚拟地址和物理地址的区别。 系统管理模式为操作系统或者执行程序提供透明的机制去实现平台相关的特性，例如电源管理、系统安全。 对于Intel 64架构，它增加了两种子模式。 兼容模式允许绝大部分16bit-32bit应用无需编译就能在64bit下运行，它类似于保护模式，有4G的地址空间限制。 64bit模式在64bit线性地址空间上运行应用程序，通用寄存器被增加到64bits。它取消了分段机制，其默认地址长度为64bits。 x64寻址 在保护模式下(32bit)，物理地址的翻译分为两步：逻辑地址翻译(段)和线性地址翻译(页)。逻辑地址利用16bit segment selector和32bit offset来表示。处理器首先要将逻辑地址翻译为线性地址(32bit)。这个翻译过程如下： 通过segment selector，在对应的GDT或LDT中去找到段描述符； 检查段描述符，访问是否合法，段是否能够访问，偏移量是否在范围之内； 将段基地址和偏移量相加来获取线性地址的值。 在IA-32e模式下(64bit)，逻辑地址的翻译步骤和上述过程类似，唯一不同的是，其段基地址和偏移量，都是64bit，而不是32bit的。线性地址同理也是32bit的。 段寻址，也即将内存分成不同的段，利用段寄存器能够找到其对应的段描述符，从而获得相关的段基址、大小、权限等信息。 段选择子Segment selector的示意图如下： 段选择子会被存在段寄存器当中，其中最低两位为RPL(cs寄存器不同，最低位位CPL)。而第三位Table Indicator则是表示该从GDT还是LDT寻找对应的段描述符，后面的bits就是对应的index了。 为了减少地址翻译的开销，处理器提供了6个段寄存器，CS，SS，DS，ES，FS，GS。通常来说一个程序至少有CS、DS、SS三个selector。假设程序要使用段来访问地址，那么必须将segment selector载入段寄存器当中。对此，Intel是提供了特殊的指令的，直接载入的指令包括MOV，POP，LDS，LES等。而隐含的载入则包括CALL，JMP，RET，SYSENTER等等。它们会改变CS寄存器(有时也会改变其它段寄存器)的内容。 而在IA-32e模式下(64bit mode)，ES，DS，SS段寄存器都不会使用了，因此它们的域会被忽视掉，而且某些load指令也被视为违法的，例如LDS。与ES，DS，SS段有关的地址计算，会被视为segment base为0。为了保证兼容性，在64bit mode当中，段load指令会正常执行，从GDT、LDT中读取时，也会读取寄存器的隐藏部分，并且值都会正常的载入。但是data、stack的segment selector和描述符都会被忽略掉。 而FS和GS段在64bit mode能够手动使用，它们的计算方式为(FS/GS).base+index+displacement。用这种方式去进行内存访问时，是不会进行检查的。载入的时候不会载入Visible Part，也即Segment Selector，也就是把段机制给忽略了。 IDT，LDT和GDT 中断向量表提供了一个入口，但这个入口还需要进一步的计算。这个入口的计算，是通过段寻址来实现的。而段的信息，则是保存在LDT和GDT当中。 段描述符的结构如下图： 段描述符最重要的部分是DPL位，它会在权限检查的时候使用。在进程需要装载一个新的段选择子时，会判断当前的CPL和RPL是否都比相应的DPL权限高，如果是则允许加载新的段选择子，否则产生GP。 在操作系统中，全局描述符只有一张，也即一个CPU对应一个GDT。GDT可以存放在内存中的任何地址，但CPU必须知道GDT的入口，因此有一个寄存器GDTR用来存放GDT的入口地址，它存放了GDT在内存中的基址和表长。 但是在64位系统当中，段机制就被取代了，而页表项也能够达到数据访问的保护目的。但是对于不同特权级之间的控制流转移，还是和原来的机制一样。在64-bit模式中，GDT依然存在，但不会改变，而其寄存器被拓展到了80bit。 而GDT中会包含一个LDT段的段描述符，LDT是通过它的段描述符来访问的。 在IA-32e模式下，段描述符表可以包含2^13个8-byte描述符。这里，描述符分为两种，段描述符会占据一个entry(8bit)，而系统描述符会占据两个entry(16bit)。而GDTR和LDTR被拓展为能够保存64bit的基地址。其中，IDT描述符、LDT、TSS描述符和调用门描述符都被拓展称为了16bytes。 64bit IDT描述符的格式如下 64bit LDT描述符的格式如下 中断的处理过程 在intel 手册上看到的大图，很详细的解释了IA-32模式和IA-32e模式下的系统架构，它也就包含了中断处理和线性地址的翻译过程。 在中断产生之后，处理器会将中断向量号作为索引，在IDT表中找到对应的处理程序。IDT表将每个中断/异常向量和一个门描述符关联起来。在保护模式下，它是一个8-byte的描述符（与GDT，LDT类似），IDT最大有256项。IDT能够保存在内存中的任何位置，处理器用IDTR寄存器来保存它的值。 在中断/陷阱门描述符中，segment selector指向了GDT或当前LDT中的代码段描述符，而offser域指向了exception/interrupt的处理过程。 在执行call这一步的时候，倘若handler过程会在一个更低的权限执行，那么就会涉及到stack switch。当stack switch发生时，segment selector和新的栈指针都需要通过TSS来获取，在这个栈上，处理器会把之前的segment selector和栈指针压入栈中。处理器还将保存当前的状态寄存器在新的栈上。 如果handler过程会在相同的权限执行，处理器会把状态寄存器的值保存在当前的栈上。 从中断处理程序返回时，handler必须使用IRET指令。它与RET类似，但它会将保存的标志位恢复到EFLAGS寄存器中。如果stack switch在调用过程中发生了那么IRET会切换到中断前的stack上。在中断过程中，权限级的保护与CALL调用过程类似，会对CPL进行检查。 64-bit模式下的中断处理 在64bit模式下，中断和异常的处理与非64bit模式下几本一致，但也存在一些不同的地方。包括有： IDT所指向的代码是64bit代码 中断栈push的大小是64bit 栈指针(SS:RSP)在中断时，无条件的被push（保护模式下是由CPL来决定的） 当CPL有变化时，新的SS会被设置为NULL IRET的过程不同 stack-switch的机制不同 中断stack的对齐不同 其中，64bit的IDT门描述符在前面已经介绍了。IST（interrupt Stack Table）用于stack-switch。通过中断门来调用目标代码段时，它必须为一个64bit的代码段(CS.L=1,CS.D=0)。如果不是也会触发#GP。在IA-32e模式下，只有64bit的中断和陷阱门能够被调用，遗留的32bit中断/陷阱都被重新定义为64bit的。 在遗留模式中，IDT entry的大小是16/32bit，它决定了interrupt-stack-frame push时的大小。并且SS:ESP只在CPL发生改变时被压入stack中。在64bit模式下，interrupt-stack-frame push的大小被固定为8bytes(因为只有64bit模式的门能够被调用)，而且SS:RSP是无条件压入栈中的。遗留模式下，Stack pointer能够在任何地址进行push，但是IA-32e模式之下，RSP必须是16-byte边界对齐的，而stack frame在中断处理程序被调用时也会对齐。而在中断服务结束时，IRET也会无条件的POP出SS:RSP，即使CPL=0。 IA-32e模式下，stack-switching机制被替代了，它被称为interrupt stack table(IST)。 遗留模式下，在64bit中，中断如果造成了权限级的改变，那么stack就会switch，但是这时不会载入新的SS描述符，而只会从TSS中载入一个inner-level的RSP。新的SS selector被强制设置为NULL，这样就能够处理内嵌的far transfers。而旧的SS和RSP会被保存在新的栈上。也就是说stack-switch机制除了SS selector不会从TSS加载之外，其余都一样。 而新的IST模式，则是无条件的进行stack switch。它是基于IDT表项中的一块区域实现的，它的设计目的，是为特殊的中断(NMI、double-fault、machine-check)等提供方法。在IA-32e模式下，一部分中断向量能够使用IST，另一部分能够使用遗留的方法。 IST在TSS中，提供7个IST指针，在中断门的描述符当中，由一个3bit的IST索引位，它们用来找到TSS中IST的偏移量。通过这个机制，处理器将IST所指向的值加载到RSP当中。而当中断发生时，新的SS selector被设置为NULL，并且SS selector的RPL区域被设置为新的CPL。旧的SS、RSP、RFLAGS、CS和RIP被push入新的栈中。如果IST的索引为0，那么就会使用修改后的、旧的stack-switch机制。 保护机制 Intel 64/IA-32架构提供了段/页级别的保护机制，它们利用权限级，来限制对于的段/页的访问，例如重要的OS代码和数据能够被放在更高权限级的段中，操作系统会保护它们不被应用程序访问。当保护机制启用时，每次内存访问都会被检查，这些检查包括： Limit Tyep Privilege level Restriction of Procedure entry-points Restriction of instruction set 通过CR0寄存器当中的PE flag能够开启保护模式，打开段保护机制；而页保护机制则是在分页机制启用时，自动开启的。虽然64bit中，不再使用分段机制了，但代码段依然存在。对于地址计算来说，其段地址被视为0，CS描述符当中的内容被忽略，但其余部分保持一致。代码段描述符、selector依然存在，它们在处理器的操作模式、执行权限级上依然发挥作用。其工作方式如下： CS描述符中会使用一个保留位，Bit 53被定义为64 bit flag位(L)，并且被用来在64bit/兼容模式之间切换。当CS.L ＝ 0时，CPU处于兼容模式，CS.D则决定了数据和地址的位数为16/32bit。如果CS.L为1，那么只有CS.D = 1是合法的，并且地址和数据的位数是64bit。在IA-32e模式下，CS描述符当中的DPL位被用来做执行权限的检查(与32bit模式一样)。 Limit Checking 在段描述符当中，有一个limit field，它防止程序访问某个段之外的的内存位置，其有效值由G flag来决定，对于数据段来说，其limit还由E flag和B flag决定。在64bit模式下，处理器不会对代码段活着数据段进行limit check，但是会对描述符表的limit进行检查。 Type checking 段描述符包含两个type 信息，S flag和type field。处理器会使用这个信息，来检查对段和门的不正确使用。S flag表示descriptor的类型，它包括系统/代码/数据三种类型。在处理一个段选择子时，处理器会在： 将segment selector载入段寄存器：寄存器只能包含对应的描述符类型 指令访问段时：段只能被相应的指令访问 指令包含segment selector时：指令只能对某些特定类型的段/门进行访问 进行某些具体操作时：far call、far jump，对调用门、任务门的call/jump等，会判断描述符中的类型是否符合要求。 Privilege levels 处理器的段保护机制包含有4个privilege levels，从0到3，0最高，3最低。处理器利用这种机制，来防止一个低权限的进程，访问更高权限的部分。为了实现这个目的，处理器使用3种类型的权限级： CPL：当前执行任务的权限级。它保存在CS和SS段寄存器的bit 0-1中。通常，CPL和当前代码段的权限一致，当跳转到一个有不同权限的代码段时，CPL会发生变化。如果目标是一致代码段，则会延续当前的CPL。 DPL：segment或者gate的权限级。它保存在段或者门的描述符当中，当当前的代码段执行，需要访问一个段或者gate的时候，这个段/门的DPL就会被拿来与CPL和RPL进行比较。在不同的环境下，DPL的意义也是不同的。 RPL：与segment selector有关的，能够对权限进行覆盖的权限级。它保存在segment selector的bit 0-1中。处理器会通过CPL和RPL来判断对segment的访问是否合法。即使请求访问某个段的程序，拥有比段更高的权限，如果RPL不是有效的，访问还是会被拒绝。也就是说如果RPL把CPL高，那么RPL会覆盖CPL。RPL能够保证提权的代码，不能随意访问一个segment段，除非它自身有这个权限。直观的说，必须CPL和RPL都比DPL要高，只有这种情况下，才会允许这个段的访问。其主要目的，是允许高权限为低权限提供服务的时候，能够通过较低的权限来加载段。 门调用符与权限检查： TSS 处理器执行工作的单位，被称为task。一个task分为两个部分，task的执行空间和task-state segment(TSS)。前者指的是code/stack/data segment，而后者则定义了组成前者的各个段。task是由TSS的segment selector来识别的。当一个任务被加载到处理器中执行时，segment selector、基址、limit、TSS的段描述符等都会被加载到**task register(TR)**当中去。分页启动时，页目录的基址还会载入到控制寄存器CR3当中去。 一个任务的状态，由一系列的寄存器和TSS来定义。这里，处理器定义了5个数据结构，来处理任务相关的活动。 TSS Task-gate描述符 TSS描述符 Task寄存器 EFLAGS寄存器中的NT flag 为了恢复一个task，处理器所需要的信息，保存在一个系统段中，它被称为TSS。在64bit模式下，它的格式如下： 而TSS描述符，则和其他的段一样，是由一个段描述符来定义的，它的结构在上文中已经给出了（与LDT是一致的），它只能放在GDT当中，不能放在LDT或者IDT当中。 Task寄存器保存了当前TSS的段选择子和整个段描述符。它包含可见和不可见两个部分（能否被软件修改）。段选择子位于可见部分，指向GDT当中的TSS描述符。不可见部分则是用来保存TSS的段描述符（能够提高执行效率）。 中断的发生 中断指的是CPU在运行时，系统内发生了需要“急需处理”的事件，于是乎CPU暂停了当前正在执行对程序，转而去执行相应的时间处理程序，在处理完之后返回原来的地方执行。那么这些事件包含：（1）外部中断指的是那些CPU外的周边原件引发的中断，例如I/O中断，I/O设备异常（接下来我们把它称为“中断”）；（2）内部中断指的是在CPU内部执行时，由程序自身、异常、陷阱（例如程序中的断点）产生的中断（包括硬件中断和软件中断，其中软件中断是指一系列的指令）（接下来我们把它称作“异常”）。 这两种中断类型不同，产生方式也不一样： （1）中断因为是由外设硬件发出的，所以需要由中断控制器（APIC）参与其中。在中断发出后，首先由APIC来进行处理。这种方式解决两个问题：（a）有大量的外设，而CPU的引脚资源有限，所以不能满足所有的直连需要；（b）如果设备中断和CPU直连，那么在MP系统中，中断负载等需求就无法实现了。 可以看到，在x86_64系统下，local APIC通过I/O APIC接受链接，I/O APIC把中断处理成中断消息，并按照规则转发给local APIC。APIC决定了由哪个CPU来处理中断，为某个引脚产生特定的中断向量（中断投递协议），并把中断请求发送给对应的CPU处理。CPU之间的中断通信，也是通过APIC来完成的。 I/O设备通过IRQ线与APIC相连，APIC将信号转化为对应的向量，并把这个向量放在它的I/O端口上，允许CPU通过数据总线来读这个向量；随后它发送一个信息给CPU的INTR引脚，从而触发中断，当CPU通过把中断信号写进APIC的I/O端口时，把INTR线清除。目前，外部中断的编号是从32开始，这是由于0-31号中断是留给异常和内部中断使用的，INTEL手册上也给出了这样一个表，详细说明了中断号的对应关系（新的CPU确实用的编号也变多了，就比如#VE）： （2）异常则是由CPU自身产生的中断。那么这种中断是否需要APIC介入呢？除了I/O APIC的中断信号外，local APIC还会接收其他来源的中断，例如CPU LINT0/LINT1中断（本地连接的I/O），性能计数器中断、APIC内部错误中断等。但这不意味着所有异常都需要中断控制器的参与；软件中断的中断号是可以由指令直接给出的，因此是不需要中断控制器的参与的。 中断的屏蔽 注意，这里的中断屏蔽指的是对外部中断的屏蔽（mask）。CPU内部的中断（异常）是不能屏蔽的。在内核同步中，通常采用这种方式来屏蔽外部的中断；并结合自旋锁来保证中断不被打断。 IRQ和NMI分别是可屏蔽和不可屏蔽中断（例子：打印机中断和电源掉电）。CPU在处理NMI中断时，不从外部硬件接收中断向量号，其对应中断向量号固定为2。NMI中断通常用于故障处理（协处理器运算出错、存储器校验出错等危急事件）。NMI处理程序通常应以IRET指令来结束。IRQ则是可以屏蔽的一类中断，通过设置CPU中的IF位，可以对IRQ进行屏蔽，这个标志位可以通过软件来设置。例如在处理某个高优先级中断时，CPU收到了低优先级的中断，那么就会对它进行屏蔽。 而对于内核的同步，则是由操作系统内部来实现的。可以说，目前我们讨论的只是中断是如何被送到CPU的，而CPU把中断和异常送给操作系统，并操作系统做出反应的过程，则属于另一个范畴了（在另一片博文http://sec-lbx.tk/2017/02/15/中断相关）。 异常和中断的处理 我们可以把内核，理解成一个服务器，它不断处理着用户的各种请求。因此，它需要保证每项服务在处理时，不会互相造成影响；其并发的来源包括内核的抢占和中断处理等。在内核态，中断处理程序也可以嵌套，这种情况下中断处理程序必须永不阻塞，在它运行的期间不能发生进程切换（不过缺页异常是一个例外，它不会引起进一步的异常，所以缺页异常可以切换进程，提高效率）。中断处理程序既可以抢占其他的中断处理程序，也可以抢占异常处理程序。相反的，异常处理程序从不会抢占中断处理程序。如果已经在内核态了，就只可能发生缺页异常（当然，也包含有现在的EPT缺页），但它们不会进一步的进行导致缺页的操作。 异常处理程序通常包含三个部分：（1）在内核堆栈保存大多数寄存器的内容；（2）用高级的C函数对异常进行处理；（3）通过ret_from_exception()函数，从异常处理程序退出。 中断处理程序与异常处理程序不同，因为当下运行的进程可能和中断完全无关。中断可以分为：I/O中断、时钟中断、处理器间中断。这里，以I/O中断为例。I/O中断必须能够为多个设备同时提供服务，而多个设备却可能会共享一个IRQ线。它往往包含四个部分：（1）在内核态堆栈中保存IRQ的值和寄存器的内容；（2）给为IRQ服务的PIC发一个应答，允许PIC进一步发出中断；（3）执行共享这个IRQ的所有设备的中断服务历程（do_IRQ()会执行与一个中断相关的所有中断服务历程，并且验证它的设备是否需要关注，这也与驱动注册相关）；（4）跳转到ret_from_inrt()后终止。 IRQ的动态分配：IRQ线可能在最后时刻才和一个设备驱动相关联，这样即使几个硬件设备不共享IRQ，也能够让几个设备在不同时刻使用同一个IRQ向量。 处理器间的中断（IPI） 由某个CPU向系统中的其他CPU发送中断信号，它不由IRQ总线，而是由本地APIC的总线传递。Linux定义了这样几种处理器间中断。 CALL_FUNCTION：强制所有剩余CPU执行发送者传递过来的函数 RESCHEDULE_VECTOR：让被中断的CPU重新调度 INVALIDATE_TLB_VECTOR：强制CPU清洗TLB 软中断和tasklet 软中断是一种提高运行效率的手段，其核心思想是把不紧迫懂、可以延时处理的中断部分，在中断上下文外，由操作系统自行安排运行时机来运行。tasklet则是建立在软中断之上来实现的，它是I/O驱动中实现可延迟函数的主要方法。对于挂起的软中断，内核会用ksoftirqd进行检查和运行。 工作队列 工作队列是内核中，另外一种将工作推后的形式。其特点在于，它允许重新调度和睡眠。其本质就是将工作交给内核线程处理。如果推后执行的任务需要睡眠、或者延时指定的时间再触发，则使用这种形式比较好；倘若推后的任务需要在一个tick内处理，那么还是选择软中断或者tasklet的形式比较好。 The link of this page is http://home.meng.uno/articles/81b224e1/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://home.meng.uno/tags/Linux/"},{"name":"操作系统","slug":"操作系统","permalink":"http://home.meng.uno/tags/操作系统/"},{"name":"系统安全","slug":"系统安全","permalink":"http://home.meng.uno/tags/系统安全/"}]},{"title":"Word2Vec and GloVe","slug":"word_embedding","date":"2017-07-05T12:09:21.000Z","updated":"2020-12-08T03:01:13.000Z","comments":true,"path":"articles/5a3aed16/","link":"","permalink":"http://home.meng.uno/articles/5a3aed16/","excerpt":"背景知识 什么是词向量/词嵌入 * 词向量（word embedding）是一个固定长度的实值向量 * 词向量是神经语言模型的副产品。 * 词向量是针对“词”提出的。事实上，也可以针对更细或更粗的粒度来进行推广——比如字向量、句向量、文档向量等 词向量的理解 word2vec 中的数学原理详解（三）背景知识 - CSDN博客 * 在 NLP 任务中，因为机器无法直接理解自然语言，所以首先要做的事情就是将语言数学化——词向量就是一种数学化的方式。 分布式表示 (distributed representation) * 分布式假设 * 常见的分布式表示方法 * 潜在语义分析","text":"背景知识 什么是词向量/词嵌入 词向量（word embedding）是一个固定长度的实值向量 词向量是神经语言模型的副产品。 词向量是针对“词”提出的。事实上，也可以针对更细或更粗的粒度来进行推广——比如字向量、句向量、文档向量等 词向量的理解 word2vec 中的数学原理详解（三）背景知识 - CSDN博客 在 NLP 任务中，因为机器无法直接理解自然语言，所以首先要做的事情就是将语言数学化——词向量就是一种数学化的方式。 分布式表示 (distributed representation) 分布式假设 常见的分布式表示方法 潜在语义分析 (Latent Semantic Analysis, LSA) SVD 分解 隐含狄利克雷分布 (Latent Dirichlet Allocation, LDA)，主题模型 神经网络、深度学习 Word2Vec Word2Vec 本质上也是一个神经语言模型，但是它的目标并不是语言模型本身，而是词向量；因此，其所作的一系列优化，都是为了更快更好的得到词向量 Word2Vec 提供了两套模型：CBOW 和 Skip-Gram(SG) CBOW 在已知 context(w) 的情况下，预测 w SG 在已知 w 的情况下预测 context(w) 从训练集的构建方式可以更好的理解和区别 CBOW 和 SG 模型 每个训练样本为一个二元组 (x, y)，其中 x为特征，y为标签 假设上下文窗口的大小 context_window =5，即 或者说 skip_window = 2，有 context_window = skip_window*2 + 1 CBOW 的训练样本为： SG 的训练样本为： 一般来说，skip_window &lt;= 10 除了两套模型，Word2Vec 还提供了两套优化方案，分别基于 Hierarchical Softmax (层次SoftMax) 和 Negative Sampling (负采样) 基于层次 SoftMax 的 CBOW 模型 【输入层】将 context(w) 中的词映射为 m 维词向量，共 2c 个 【投影层】将输入层的 2c 个词向量累加求和，得到新的 m 维词向量 【输出层】输出层对应一棵哈夫曼树，以词表中词作为叶子节点，各词的出现频率作为权重——共 N 个叶子节点，N-1 个非叶子节点 【输入层】前者使用的是 w 的前 n-1 个词，后者使用 w 两边的词 这是后者词向量的性能优于前者的主要原因 【投影层】前者通过拼接，后者通过累加求和 【隐藏层】后者无隐藏层 【输出层】前者为线性结构，后者为树形结构 模型改进 从对比中可以看出，CBOW 模型的主要改进都是为了减少计算量——取消隐藏层、使用层Softmax代替基本Softmax 层次 SoftMax 的正向传播 层 Softmax 实际上是把一个超大的多分类问题转化成一系列二分类问题 示例：求 P(&quot;足球&quot;|context(&quot;足球&quot;)) 从根节点到“足球”所在的叶子节点，需要经过 4 个分支，每次分支相当于一次二分类（逻辑斯蒂回归，二元Softmax） 这里遵从原文，将 0 作为正类，1 作为负类 而 P(&quot;足球&quot;|context(&quot;足球&quot;)) 就是每次分类正确的概率之积，即 这里每个非叶子都对应一个参数 θ_i 为什么层次 SoftMax 能加速 Softmax 大部分的计算量在于分母部分，它需要求出所有分量的和 而层次 SoftMax 每次只需要计算两个分量，因此极大的提升了速度 层次 Softmax 的反向传播 TODO word2vec 中的数学原理详解（四）基于 Hierarchical Softmax 的模型 - CSDN博客 基于层次 Softmax 的 Skip-gram 模型 这里保留了【投影层】，但实际上只是一个恒等变换 从模型的角度看：CBOW 与 SG 模型的区别仅在于 x_w 的构造方式不同，前者是 context(w) 的词向量累加；后者就是 w 的词向量 虽然 SG 模型用中心词做特征，上下文词做类标，但实际上两者的地位是等价的 基于负采样的 CBOW 和 Skip-gram 层次 Softmax 还不够简单，于是提出了基于负采样的方法进一步提升性能 负采样（Negative Sampling）是 NCE(Noise Contrastive Estimation) 的简化版本 &gt; 噪音对比估计（NCE） - CSDN博客 CBOW 的训练样本是一个 (context(w), w) 二元对；对于给定的 context(w)，w 就是它的正样本，而其他所有词都是负样本。 如果不使用负采样，即 N-gram 神经语言模型中的做法，就是对整个词表 Softmax 和交叉熵 负采样相当于选取所有负例中的一部分作为负样本，从而减少计算量 Skip-gram 模型同理 负采样算法 负采样算法，即对给定的 w ，生成相应负样本的方法 最简单的方法是随机采样，但这会产生一点问题，词表中的词出现频率并不相同 如果不是从词表中采样，而是从语料中采样；显然，那些高频词被选为负样本的概率要大于低频词 在词表中采样时也应该遵循这个 因此，负采样算法实际上就是一个带权采样过程 Word2Vec 中的做法 记 以这 N+1 个点对区间 [0,1] 做非等距切分 引入的一个在区间 [0,1] 上的 M 等距切分，其中 M &gt;&gt; N 源码中取 M = 10^8 然后对两个切分做投影，得到映射关系 采样时，每次生成一个 [1, M-1] 之间的整数 i，则 Table(i) 就对应一个样本； 当采样到正例时，跳过 特别的，Word2Vec 在计算 len(w) 时做了一些改动——为 count(·) 加了一个指数 一些源码细节 σ(x) 的近似计算 类似带权采样的策略，用查表来代替计算 具体计算公式如下 因为 σ(x) 函数的饱和性，当 x &lt; -6 || x &gt; 6 时，函数值基本不变了 低频词的处理 对于低频词，会设置阈值（默认 5），对于出现频次低于该阈值的词会直接舍弃，同时训练集中也会被删除 高频词的处理 高频词提供的信息相对较少，为了提高低频词的词向量质量，有必要对高频词进行限制 高频词对应的词向量在训练时，不会发生明显的变化，因此在训练是可以减少对这些词的训练，从而提升速度 Sub-sampling 技巧 源码中使用 Sub-sampling 技巧来解决高频词的问题，能带来 2~10 倍的训练速度提升，同时提高低频词的词向量精度 给定一个词频阈值 t，将 w 以 p(w) 的概率舍弃，p(w) 的计算如下 Word2Vec 中的Sub-sampling 显然，Sub-Sampling 只会针对 出现频次大于 t 的词 特别的，Word2Vec 使用如下公式计算 p(w)，效果是类似的 自适应学习率 预先设置一个初始的学习率 η_0（默认 0.025），每处理完 M（默认 10000）个词，就根据以下公式调整学习率 随着训练的进行，学习率会主键减小，并趋向于 0 为了方式学习率过小，Word2Vec 设置了一个阈值 η_min（默认 0.0001 * η_0）；当学习率小于 η_min，则固定为 η_min。 参数初始化 词向量服从均匀分布 [-0.5/m, 0.5/m]，其中 m 为词向量的维度 所有网络参数初始化为 0 GloVe 共现矩阵 共现矩阵的实现方式 基于文档 - LSA 模型（SVD分解） 基于窗口 - 类似 skip-gram 模型中的方法 skip_window = 1 的共现矩阵 构架共现矩阵的细节 功能词的处理 功能词：如 “the”, “he”, “has”, … 法1）直接忽略 在一些分类问题上可以这么做；如果目标是词向量，则不建议使用这种方法 法2）设置阈值 min(x, t) 其中 x 为功能词语其他词的共现次数，t 为设置的阈值 可以尝试使用一些方法代替单纯的计数，如皮尔逊相关系数，负数记为 0 但是似乎没有人这么做 GloVe 的基本思想 GloVe 模型的是基于共现矩阵构建的 GloVe 认为共现矩阵可以通过一些统计信息得到词之间的关系，这些关系可以一定程度上表达词的含义 solid related to ice but not steam gas related to stream but not ice water related to both fashion relate not to both 说明 TODO GloVe 的基本思想： 假设词向量已知，如果这些词向量通过某个函数（目标函数）可以拟合共现矩阵中的统计信息，那么可以认为这些词向量也拥有了共现矩阵中蕴含的语义 模型的训练过程就是拟合词向量的过程 GloVe 的目标函数 其中 w_i 和 w_j 为词向量 x_ij 为 w_i 和 w_j 的共现次数 f(x) 是一个权重函数，为了限制高频词和防止 x_ij = 0 当 x_ij = 0 时，有 GloVe 目标函数的推导过程 以前整理在 OneNote 上的，有时间在整理 目标函数 w_i 的权重函数 GloVe 与 Word2Vec 的区别 Word2Vec 本质上是一个神经网络； Glove 也利用了反向传播来更新词向量，但是结构要更简单，所以 GloVe 的速度更快 Glove 认为 Word2Vec 对高频词的处理还不够，导致速度慢；GloVe 认为共现矩阵可以解决这个问题 实际 Word2Vec 已结有了一些对高频词的措施 从效果上看，虽然 GloVe 的训练速度更快，但是词向量的性能在通用性上要弱一些： 在一些任务上表现优于 Word2Vec，但是在更多的任务上要比 Word2Vec 差 FastText FastText 是从 Word2Vec 的 CBOW 模型演化而来的； 从网络的角度来看，两者的模型基本一致；区别仅在于两者的分类目标不同； 具体来说，FastText 的分类目标是文档的标签，CBOW 则是中心词的标签 FastText 与 CBOW 的相同点： 包含三层：输入层、隐含层、输出层（Hierarchical Softmax） 输入都是多个单词的词向量 隐藏层（投影层）都是对多个词向量的叠加平均 输出都是一个特定的 target 从网络的角度看，两者基本一致 不同点： CBOW 的输入是中心词两侧skip_window内的上下文词；FastText 除了上下文词外，还包括这些词的字符级 N-gram 特征 CBOW 的输出是中心词的类标，fastText 的输出是句子对应的类标 注意，字符级 N-gram 只限制在单个词内，以英文为例 12 // 源码中计算 n-grams 的声明，只计算单个词的字符级 n-gramcompute_ngrams(word, unsigned int min_n, unsigned int max_n); 1234567 # &gt; https://github.com/vrasneur/pyfasttext#get-the-subwords&gt;&gt;&gt; model.args.get('minn'), model.args.get('maxn')(2, 4)# 调用源码的 Python 接口，源码上也会添加 '&lt;' 和 '&gt;'&gt;&gt;&gt; model.get_all_subwords('hello') # word + subwords from 2 to 4 characters['hello', '&lt;h', '&lt;he', '&lt;hel', 'he', 'hel', 'hell', 'el', 'ell', 'ello', 'll', 'llo', 'llo&gt;', 'lo', 'lo&gt;', 'o&gt;']&gt;&gt;&gt; # model.get_all_subwords('hello world') # warning 值得一提的是，因为 FastText 使用了字符级的 N-gram 向量作为额外的特征，使其能够对未登录词也能输出相应的词向量； 具体来说，未登录词的词向量等于其 N-gram 向量的叠加 gensim.models.FastText 使用示例 构建 FastText 以及获取词向量 12345678910111213141516171819202122232425262728 # gensim 示例import gensimimport numpy as npfrom gensim.test.utils import common_textsfrom gensim.models.keyedvectors import FastTextKeyedVectorsfrom gensim.models._utils_any2vec import compute_ngrams, ft_hashfrom gensim.models import FastText# 构建 FastText 模型sentences = [[\"Hello\", \"World\", \"!\"], [\"I\", \"am\", \"huay\", \".\"]]min_ngrams, max_ngrams = 2, 4 # ngrams 范围model = FastText(sentences, size=5, min_count=1, min_n=min_ngrams, max_n=max_ngrams)# 可以通过相同的方式获取每个单词以及任一个 n-gram 的向量print(model.wv['hello'])print(model.wv['&lt;h'])\"\"\"[-0.03481839 0.00606661 0.02581969 0.00188777 0.0325358 ][ 0.04481247 -0.1784363 -0.03192253 0.07162753 0.16744071]\"\"\"print()# 词向量和 n-gram 向量是分开存储的print(len(model.wv.vectors)) # 7print(len(model.wv.vectors_ngrams)) # 57# gensim 好像没有提供直接获取所有 ngrams tokens 的方法print(model.wv.vocab.keys())\"\"\"['Hello', 'World', '!', 'I', 'am', 'huay', '.']\"\"\"print() 获取单个词的 ngrams 表示 利用源码中 compute_ngrams 方法，gensim 提供了该方法的 Python 接口 1234567891011121314151617181920 sum_ngrams = 0for s in sentences: for w in s: w = w.lower() # from gensim.models._utils_any2vec import compute_ngrams ret = compute_ngrams(w, min_ngrams, max_ngrams) print(ret) sum_ngrams += len(ret)\"\"\"['&lt;h', 'he', 'el', 'll', 'lo', 'o&gt;', '&lt;he', 'hel', 'ell', 'llo', 'lo&gt;', '&lt;hel', 'hell', 'ello', 'llo&gt;']['&lt;w', 'wo', 'or', 'rl', 'ld', 'd&gt;', '&lt;wo', 'wor', 'orl', 'rld', 'ld&gt;', '&lt;wor', 'worl', 'orld', 'rld&gt;']['&lt;!', '!&gt;', '&lt;!&gt;']['&lt;i', 'i&gt;', '&lt;i&gt;']['&lt;a', 'am', 'm&gt;', '&lt;am', 'am&gt;', '&lt;am&gt;']['&lt;h', 'hu', 'ua', 'ay', 'y&gt;', '&lt;hu', 'hua', 'uay', 'ay&gt;', '&lt;hua', 'huay', 'uay&gt;']['&lt;.', '.&gt;', '&lt;.&gt;']\"\"\"assert sum_ngrams == len(model.wv.vectors_ngrams)print(sum_ngrams) # 57print() 计算一个未登录词的词向量 未登录词实际上是已知 n-grams 向量的叠加平均 123456789101112 # 因为 \"a\", \"aa\", \"aaa\" 中都只含有 \"&lt;a\" ，所以它们实际上都是 \"&lt;a\"print(model.wv[\"a\"])print(model.wv[\"aa\"])print(model.wv[\"aaa\"])print(model.wv[\"&lt;a\"]) \"\"\"[ 0.00226487 -0.19139008 0.17918809 0.13084619 -0.1939924 ][ 0.00226487 -0.19139008 0.17918809 0.13084619 -0.1939924 ][ 0.00226487 -0.19139008 0.17918809 0.13084619 -0.1939924 ][ 0.00226487 -0.19139008 0.17918809 0.13084619 -0.1939924 ]\"\"\"print() 只要未登录词能被已知的 n-grams 组合，就能得到该词的词向量 gensim.models.keyedvectors.FastTextKeyedVectors.word_vec(token) 的内部实现 1234567891011121314151617181920212223242526272829303132333435 word_unk = \"aam\"ngrams = compute_ngrams(word_unk, min_ngrams, max_ngrams) # min_ngrams, max_ngrams = 2, 4word_vec = np.zeros(model.vector_size, dtype=np.float32)ngrams_found = 0for ngram in ngrams: ngram_hash = ft_hash(ngram) % model.bucket if ngram_hash in model.wv.hash2index: word_vec += model.wv.vectors_ngrams[model.wv.hash2index[ngram_hash]] ngrams_found += 1if word_vec.any(): # word_vec = word_vec / max(1, ngrams_found)else: # 如果一个 ngram 都没找到，gensim 会报错；个人认为把 0 向量传出来也可以 raise KeyError('all ngrams for word %s absent from model' % word_unk)print(word_vec)print(model.wv[\"aam\"])\"\"\"[ 0.02210762 -0.10488641 0.05512805 0.09150169 0.00725085][ 0.02210762 -0.10488641 0.05512805 0.09150169 0.00725085]\"\"\"# 如果一个 ngram 都没找到，gensim 会报错# 其实可以返回一个 0 向量的，它内部实际上是从一个 0 向量开始累加的；# 但返回时做了一个判断——如果依然是 0 向量，则报错# print(model.wv['z'])r\"\"\"Traceback (most recent call last): File \"D:/gensim/FastText.py\", line 53, in &lt;module&gt; print(model.wv['z']) File \"D:\\program\\work\\Python\\Anaconda3\\envs\\tf\\lib\\site-packages\\gensim\\models\\keyedvectors.py\", line 336, in __getitem__ return self.get_vector(entities) File \"D:\\program\\work\\Python\\Anaconda3\\envs\\tf\\lib\\site-packages\\gensim\\models\\keyedvectors.py\", line 454, in get_vector return self.word_vec(word) File \"D:\\program\\work\\Python\\Anaconda3\\envs\\tf\\lib\\site-packages\\gensim\\models\\keyedvectors.py\", line 1989, in word_vec raise KeyError('all ngrams for word %s absent from model' % word)KeyError: 'all ngrams for word z absent from model'\"\"\" CharCNN 字向量 CharCNN 的思想是通过字符向量得到词向量 [1509] Character-level Convolutional Networks for Text Classification 其他实践 一般 embedding 维度的选择 Feature Columns | TensorFlow 经验公式 embedding_size = n_categories ** 0.25 在大型语料上训练的词向量维度通常会设置的更大一些，比如 100~300 如果根据经验公式，是不需要这么大的，比如 200W 词表的词向量维度只需要 200W ** 0.25 ≈ 37 The link of this page is http://home.meng.uno/articles/5a3aed16/ . Welcome to reproduce it!","categories":[{"name":"Natural Language Processing","slug":"Natural-Language-Processing","permalink":"http://home.meng.uno/categories/Natural-Language-Processing/"}],"tags":[{"name":"Word2Vec","slug":"Word2Vec","permalink":"http://home.meng.uno/tags/Word2Vec/"},{"name":"GloVe","slug":"GloVe","permalink":"http://home.meng.uno/tags/GloVe/"},{"name":"词向量","slug":"词向量","permalink":"http://home.meng.uno/tags/词向量/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://home.meng.uno/tags/自然语言处理/"}]},{"title":"静态链接","slug":"static-lnker","date":"2017-06-11T04:14:23.000Z","updated":"2020-12-02T02:05:11.000Z","comments":true,"path":"articles/41311c08/","link":"","permalink":"http://home.meng.uno/articles/41311c08/","excerpt":"GCC的工作，到生成汇编代码为止。剩下的工作，交给了Binutils来完成：assembler和static linker。linker主要完成的是静态链接，目标文件合并的工作。例如，把多个.o文件合并成一个可执行文件。 两步链接 两步链接指的是： 1. 空间与地址的分配 链接器会首先扫描所有的输入文件，获得各个段的长度、属性和位置，将段合并；并将输入目标文件中的符号表合并为全局符号表。 2. 符号解析与重定位 使用上一步中收集到的信息，进行符号解析和重定位，调整代码中的地址等。这一步也是链接过程的核心，特别是重定位过程。 链接器首先获取","text":"GCC的工作，到生成汇编代码为止。剩下的工作，交给了Binutils来完成：assembler和static linker。linker主要完成的是静态链接，目标文件合并的工作。例如，把多个.o文件合并成一个可执行文件。 两步链接 两步链接指的是： 空间与地址的分配 链接器会首先扫描所有的输入文件，获得各个段的长度、属性和位置，将段合并；并将输入目标文件中的符号表合并为全局符号表。 符号解析与重定位 使用上一步中收集到的信息，进行符号解析和重定位，调整代码中的地址等。这一步也是链接过程的核心，特别是重定位过程。 链接器首先获取各个段的虚拟地址；在确定段的虚拟地址之后，也就能确定各个符号的虚拟地址了。 重定位与符号解析 在完成空间和地址的分配之后，链接器开始进行符号解析和重定位的过程。在链接之前，各个段中的符号地址，都是以0为基地址的，对于未知的地址，也通通用0进行替代。编译器在编译时，对于不知道的符号地址，全部用一个假值替代，把真正的工作留给链接器去做。 而链接器在分配了虚拟地址之后，就可以修正每一个需要重定位的入口。这个工作是借助于重定位表来实现的。重定位表包括：重定位入口（也就是需要重定位的地方），偏移表示入口在被重定位的段中的位置。 在x86_64下，重定位表的结构也很简单（定义在elf.h当中）： typedef struct{ Elf64_Addr r_offset; Elf64_Xword r_info; }Elf64_Rel; typedef struct{ Elf64_Addr r_offset; Elf64_Xword r_info; ELF64_Sxword r_addend; }Elf64_Rela 这里，r_offset指定应用重定位操作的位置；r_info则指定必须对其进行重定位的符号表索引以及要应用的重定位类型。其低位表示重定位入口的类型；高位表示重定位入口的符号，在符号表中的下标。（不同处理器的格式不一样） 符号解析则是为符号的重定位提供帮助，根据多个目标文件中的符号表，生成全局符号表，找到相应的符号并进行重定位。对于未定义的符号，链接器都应该能在全局符号表中找到，否则就报出符号未定义的错误。 PS：x86_64只使用Elf64_Rela。 指令修正 在x86_64中，call、jmp、mov、lea等指令的寻址方式千差万别。对于重定位来说，修正指令的寻址方式定义在binutils/elfcpp/x86_64.h当中。这其中主要包括：R_X86_64_64和R_X86_64_PC32两种。这是因为X86_64上，相对寻址依然只支持32位（实际上这也很科学；因为一个可执行文件通常不会有4G那么大）。 两种寻址方式的修正方法分别为：符号地址 + 保存在被修正位置的值和符号地址 + 保存在被修正位置的值 - 被修正的位置相对于段开始的偏移量 源码分析 初始化，parsing command line &amp; script file linker的入口，在ldmain.c当中(通常在链接的时候，通过编译器内部直接进行调用)。首先，linker会调用bfd库，识别二进制文件的格式，生成各个段的描述符，并且转换为canonical form。（例如linker中的符号表识别工作，就是首先由BFD来进行分析和转化，然后linker直接在canonical form上进行操作，再由BFD来进行输出）因此在ldmain.c的main中，首先进行的也是bfd的初始化bfd_init。随后linker进行了一系列的设置，包括路径，回调函数、初始化，加载插件、读取命令、linker script等。 文件和符号的加载 随后，lang_process中，linker会对每个输入文件进行处理。对于每个输入文件，linker都会分配一个bfd，对输入文件进行扫描，识别出其中的符号。首先open_input_bfds为所有文件建立了bfd，随后载入文件中的所有符号。每个符号对应一个bfd_link_hash_entry，它们保存在bfd_link_hash_table当中。 bfd_link_add_symbols将符号添加到hash_table当中。 输入文件的分析和合并 在链接的第一部分完成后，第二部分开始前，链接器首先调用了ldctor_build_sets函数，它主要为C++中的constructor/dectructor提供支持。随后链接器lang_do_memory_region计算出内存区域（它们保存在lang_memory_region_list当中）。再通过lang_common处理全局符号，将它们添加到对应的section，移除没有被使用的sections等。随后链接器建立输入section和输出section之间的映射关系，并且将文件的section合并，以及设置段的属性等。 重定位 第四步是符号的重定位工作。这里lang_size_section首先获取所有section的大小，然后lang_set_startof会修正section的大小和位置。在确定了sections的信息之后，就可以对符号进行重定位了，这便是lang_do_assignments和ldexp_finalize_syms的工作。它们会按照前面提到的方法，对符号进行重定位。最后链接器还会检查符号和section的正确性。 交给bfd，输出文件 在完成重定位之后，如果没有出现异常，linker就把工作交给bfd了。ldwrite负责把链接好的文件输出。完成一些清理工作后，整个链接过程就结束了。 The link of this page is http://home.meng.uno/articles/41311c08/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"链接器","slug":"链接器","permalink":"http://home.meng.uno/tags/链接器/"},{"name":"静态链接","slug":"静态链接","permalink":"http://home.meng.uno/tags/静态链接/"}]},{"title":"正则表达式","slug":"regular-exp","date":"2017-04-04T06:44:58.000Z","updated":"2020-12-02T02:03:43.000Z","comments":true,"path":"articles/2f57a694/","link":"","permalink":"http://home.meng.uno/articles/2f57a694/","excerpt":"正则表达式（英语：Regular Expression，在代码中常简写为regex、regexp或RE）使用单个字符串来描述、匹配一系列符合某个句法规则的字符串搜索模式。搜索模式可用于文本搜索和文本替换。 什么是正则表达式？ * 正则表达式是由一个字符序列形成的搜索模式。 * 当你在文本中搜索数据时，你可以用搜索模式来描述你要查询的内容。 * 正则表达式可以是一个简单的字符，或一个更复杂的模式。 * 正则表达式可用于所有文本搜索和文本替换的操作。 使用字符串方法 在 JavaScript 中，正则表达式通常用于","text":"正则表达式（英语：Regular Expression，在代码中常简写为regex、regexp或RE）使用单个字符串来描述、匹配一系列符合某个句法规则的字符串搜索模式。搜索模式可用于文本搜索和文本替换。 什么是正则表达式？ 正则表达式是由一个字符序列形成的搜索模式。 当你在文本中搜索数据时，你可以用搜索模式来描述你要查询的内容。 正则表达式可以是一个简单的字符，或一个更复杂的模式。 正则表达式可用于所有文本搜索和文本替换的操作。 使用字符串方法 在 JavaScript 中，正则表达式通常用于两个字符串方法 : search() 和 replace()。 search() 方法用于检索字符串中指定的子字符串，或检索与正则表达式相匹配的子字符串，并返回子字符串的起始位置。 replace() 方法用于在字符串中用一些字符替换另一些字符，或替换一个与正则表达式匹配的子字符串。 search() 方法使用正则表达式 使用正则表达式搜索 “w3cschool” 字符串，且不区分大小写： varstr =&quot;Visit w3cschool&quot;; varn = str.search(/w3cschool/i); 输出结果为： 6 replace() 方法使用正则表达式 使用正则表达式且不区分大小写将字符串中的 Microsoft 替换为 w3cschool : varstr =&quot;Visit Microsoft!&quot;; varres = str.replace(/microsoft/i,&quot;w3cschool&quot;); 结果输出为: Visit w3cschool! 正则表达式修饰符 修饰符可以在全局搜索中不区分大小写: 修饰符描述 i 执行对大小写不敏感的匹配。 g 执行全局匹配（查找所有匹配而非在找到第一个匹配后停止）。 m 执行多行匹配。 正则表达式模式 方括号用于查找某个范围内的字符： 表达式描述 [abc] 查找方括号之间的任何字符。 [0-9] 查找任何从 0 至 9 的数字。 (x|y) 查找任何以 | 分隔的选项。 元字符是拥有特殊含义的字符 元字符描述 \\s：用于匹配单个空格符，包括tab键和换行符； \\S：用于匹配除单个空格符之外的所有字符； \\d：用于匹配从0到9的数字； \\w：用于匹配字母，数字或下划线字符； \\W：用于匹配所有与\\w不匹配的字符； . ：用于匹配除换行符之外的所有字符。 \\uxxxx 查找以十六进制数 xxxx 规定的 Unicode 字符。 量词 量词描述 n+ 匹配任何包含至少一个n的字符串。 n* 匹配任何包含零个或多个n的字符串。 n? 匹配任何包含零个或一个n的字符串。 使用 RegExp 对象 在 JavaScript 中，RegExp 对象是一个预定义了属性和方法的正则表达式对象。 使用 test() test() 方法是一个正则表达式方法。 test() 方法用于检测一个字符串是否匹配某个模式，如果字符串中含有匹配的文本，则返回 true，否则返回 false。 以下实例用于搜索字符串中的字符 “e”： 实例 varpatt = /e/; patt.test(&quot;The best things in life are free!&quot;); 字符串中含有 “e”，所以该实例输出为： true 你可以不用设置正则表达式的变量，以上两行代码可以合并为一行： /e/.test(&quot;The best things in life are free!&quot;) 使用 exec() exec() 方法是一个正则表达式方法。 exec() 方法用于检索字符串中的正则表达式的匹配。 该函数返回一个数组，其中存放匹配的结果。如果未找到匹配，则返回值为 null。 以下实例用于搜索字符串中的字母 “e”: Example 1 /e/.exec(&quot;The best things in life are free!&quot;); 字符串中含有 “e”，所以该实例输出为: e The link of this page is http://home.meng.uno/articles/2f57a694/ . Welcome to reproduce it!","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://home.meng.uno/categories/Programming-Language/"}],"tags":[{"name":"正则表达式","slug":"正则表达式","permalink":"http://home.meng.uno/tags/正则表达式/"}]},{"title":"MongDB的使用","slug":"mongdb","date":"2017-03-20T10:34:11.000Z","updated":"2020-12-02T01:43:21.000Z","comments":true,"path":"articles/83c0afa5/","link":"","permalink":"http://home.meng.uno/articles/83c0afa5/","excerpt":"开启服务 1 mongod --dbpath /data/db/ --port 27017 --fork --logpath --config --nohttpinterface * dbpath 数据库目录 (请先创建好，如果不创建，直接运行mongod会报错) * port 端口 * fork 守护进程 * logpath 日志目录 * config 配置文件 * nohttpinterface 关闭大于1000的管理接口 * auth 开启服务器验证 把配置放置在config文件里，增强复用性 1 2 3 4 5 port = 27017 logpath =","text":"开启服务 1 mongod --dbpath /data/db/ --port 27017 --fork --logpath --config --nohttpinterface dbpath 数据库目录 (请先创建好，如果不创建，直接运行mongod会报错) port 端口 fork 守护进程 logpath 日志目录 config 配置文件 nohttpinterface 关闭大于1000的管理接口 auth 开启服务器验证 把配置放置在config文件里，增强复用性 12345 port = 27017logpath = /Users/jan/mongdb.logauth = truefork = truedbpath = /Users/jan/data/db 关闭服务 在mongo环境中关闭 12 use admindb.shutdownServer() 使用 mongod 命令关闭 1 mongod --shutdown --dbpath /database/mongodb/data/ 使用kill命令关闭 给服务器发送一个SIGINT,SIGTERM信号，即 “kill -2 PID,” 或者 “kill -15 PID“ 1234 ps aux | grep mongojan 12899 0.4 0.5 3046076 41216 ?? S 11:44上午 0:00.22 mongod --config ./mongod.configkill -2 12899 非正常关闭mongodb，导致无法启动 删除 /data/db/mongod.locks文件 使用repair 选项修复mongodb./mongod --repair 重启启动mongodb./mongod 认证 在开启MongoDB 服务时不添加任何参数时，可以对数据库任意操作，而且可以远程访问数据库。如果启动的时候指定—auth参数，可以对数据库进行用户验证。 添加帐号 1234 use testdb.addUser(&quot;username&quot;,&quot;password&quot;) //添加db.addUser(&quot;read_only&quot;,&quot;username&quot;,&quot;password&quot;) //添加只能读取权限帐号 更新密码 本质 账户信息存在admin.system.users集合里，存储格式为： 12345 &#123; &quot;user&quot;:&quot;xxxx&quot;, &quot;readOnly&quot;:true, &quot;pwd&quot;:password_hash&#125; 如果想查询数据库先添加一个授权用户 为数据库添加用户 123456789 use admin db.addUser(&quot;root&quot;,&quot;1234&quot;) &gt;&gt; output &gt;&gt; &#123; &quot;user&quot; : &quot;root&quot;, &quot;readOnly&quot; : false, &quot;pwd&quot; : &quot;fa0450e8c3e5fff6005de2f88559c3d9&quot;, &quot;_id&quot; : ObjectId(&quot;53ca83234b763e5d3dbf15c2&quot;) &#125; 使用授权用户链接数据库 123456789 mongo admin -uroot -p1234 db.system.users.find()&gt;&gt; output &gt;&gt; &#123; &quot;_id&quot; : ObjectId(&quot;53ca83234b763e5d3dbf15c2&quot;), &quot;user&quot; : &quot;root&quot;, &quot;readOnly&quot; : false, &quot;pwd&quot; : &quot;fa0450e8c3e5fff6005de2f88559c3d9&quot; &#125; 链接服务 本地链接 1 mongo -uroot -p1234 u 认证用户名 p 认证密码 远程链接 1 mongo -uroot -p1234 ip:port/dbname 导出 &amp; 导入 导出 1234 mongoexport -d templates -c page -o templates.txt//远程导出mongoexport -d templates -c template -h 10.32.84.119:27017 -o templates.txt 更多help 1 mongoexport --help 导入 1 mongoimport -d templates -c template --file template.txt 更多help 1 mongoimport --help remote db 连接远程终端 1 mongo -u admin -p admin 192.168.0.197:27017/templates 更多help 1 mongo -h The link of this page is http://home.meng.uno/articles/83c0afa5/ . Welcome to reproduce it!","categories":[{"name":"Database","slug":"Database","permalink":"http://home.meng.uno/categories/Database/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://home.meng.uno/tags/数据库/"},{"name":"NoSQL","slug":"NoSQL","permalink":"http://home.meng.uno/tags/NoSQL/"},{"name":"MongDB","slug":"MongDB","permalink":"http://home.meng.uno/tags/MongDB/"}]},{"title":"Docker的基本使用","slug":"docker-using","date":"2017-03-09T03:17:06.000Z","updated":"2020-12-02T01:45:37.000Z","comments":true,"path":"articles/ca4a993b/","link":"","permalink":"http://home.meng.uno/articles/ca4a993b/","excerpt":"本博文在观看网易蜂巢推出的玩转Docker镜像视频教程之后总结而成。 基本使用 谈到使用Docker，首先，我们必须要了解Dockerfile。（命名为Dockerfile，不要后缀） Dockerfile的解读 首先引入一个例子： 1 2 3 4 5 6 FROM hub.c.163.com/bingohuang/debian:163 MAINTAINER bingohuang RUN apt-get update && apt-get install -y nginx COPY docker-mario /usr/share/nginx/w","text":"本博文在观看网易蜂巢推出的玩转Docker镜像视频教程之后总结而成。 基本使用 谈到使用Docker，首先，我们必须要了解Dockerfile。（命名为Dockerfile，不要后缀） Dockerfile的解读 首先引入一个例子： 123456 FROM hub.c.163.com/bingohuang/debian:163MAINTAINER bingohuang &lt;me@bingohuang.com&gt;RUN apt-get update &amp;&amp; apt-get install -y nginxCOPY docker-mario /usr/share/nginx/wwwEXPOSE 80ENTRYPOINT [\"nginx\", \"-g\", \"daemon off;\"] 逐句解释： 关键字 FROM，选用包含163更新源的Debian镜像，版本7.9 关键字 MAINTAINER，添加作者信息和联系方式（有任何问题或反馈欢迎发邮件沟通） 关键字 RUN，运行命令，这里是更新程序列表并安装 Nginx 程序 关键字 COPY，将 docker-mario 源码拷贝到容器的 /usr/share/nginx/www 目录下（这是第三步安装好 Nginx 程序后自动生成的目录 关键字 EXPOSE，暴露端口，这里是 Nginx 的默认端口：80 关键字 ENTRYPOINT，指定容器运行的默认指令（不会被用户指令覆盖） 构建镜像 1 docker build -t XXX:1.0 . 查看镜像 1 docker images 输出样式： 1 REPOSITORY TAG DIGEST IMAGE ID CREATED SIZE 本地运行 1 docker run --name docker-mario -d -p 1989:80 XXX:1.0 查看运行的镜像： 1 docker ps 访问镜像： 1 open http://127/0/0/1:1989/ 上传镜像 登录网易蜂巢： 1 docker login -u &#123;你的网易云邮箱账号或手机号码&#125; -p &#123;你的网易云密码&#125; hub.c.163.com 标记本地镜像： 1 docker tag &#123;镜像名或ID&#125; hub.c.163.com/&#123;你的用户名&#125;/&#123;标签名&#125; 推送至网易云镜像仓库： 1 docker push hub.c.163.com/&#123;你的用户名&#125;/&#123;标签名&#125; 附一张Docker操作流程图 其他未列出的Docker指令 帮助 1 docker -h 获取镜像 12 sudo docker pull NAME[:TAG]sudo docker pull centos:latest 启动Container盒子 12 sudo docker run [OPTIONS] IMAGE [COMMAND] [ARG...]sudo docker run -t -i contos /bin/bash 查看镜像列表，列出本地的所有images 12 sudo docker images [OPTIONS] [NAME]sudo docker images centos 查看容器列表，可看到我们创建过的所有container 12 sudo docker ps [OPTIONS]sudo docker ps -a 删除镜像，从本地删除一个已经下载的镜像 12 sudo docker rmi IMAGE [IMAGE...]sudo docker rmi centos:latest 移除一个或多个容器实例 1 sudo docker rm [OPTIONS] CONTAINER [CONTAINER...] 移除所有微运行的容器 1 sudo docker rm sudo docker ps -aq 停止一个正在运行的容器 12 sudo docker kill [OPTIONS] CONTAINER [CONTAINNER...]sudo docker kill 026e 重启一个正在运行的容器 12 sudo docker restart [OPTIONS] contains[CONTAINER]sudo docker restart 026e 停止一个已经停止的容器 12 sudo docker start [OPTIONS] CONTAINER [CONTAINER..]sudo docker start 026e 制作镜像 ldd: 打印共享依赖库 123456 ldd redis-3.0.0/src/redis-server linux-vdso.so.1 =&gt; (0x00007fffde365000) libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f307d5aa000) libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f307d38c000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f307cfc6000) /lib64/ld-linux-x86-64.so.2 (0x00007f307d8b9000) 打包.so文件 1 tar ztvf rootfs.tar.gz ##制成Dockerfile 12345 FROM scratchADD rootfs.tar.gz /COPY redis.conf /etc/redis/redis.confEXPOSE 6379CMD [&quot;redis-server&quot;] 执行构建 1 docker build -t redis . 测试 12345 docker run -d --name redis redisredis-cli -h $(docker inspect -f &apos;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&apos; redis)redis-benchmark -h $(docker inspect -f &apos;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&apos; redis-05) The link of this page is http://home.meng.uno/articles/ca4a993b/ . Welcome to reproduce it!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://home.meng.uno/categories/Software-Engineering/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://home.meng.uno/tags/Docker/"}]},{"title":"Annotation Compile Tool","slug":"about_APT","date":"2017-03-07T06:32:53.000Z","updated":"2021-01-01T18:29:07.000Z","comments":true,"path":"articles/63b8fab5/","link":"","permalink":"http://home.meng.uno/articles/63b8fab5/","excerpt":"APT: 注解编译工具Annotation Compile Tool,用来在编译时根据注解自动生成Java代码。像ButterKnife,EventBus等库都为了避免因为反射带来的性能损失,都使用了APT注解方式。 使用APT需要的组件 通常一个库有三个以下部分组成： * 核心包 用来对外提供使用,及库的逻辑部分 * Anntation包 用来存放自定义的注解 * Compiler包 这是APT运行所必需的,用来解释注解,告诉APT注解的意思及使用 对与一个使用APT的库来说,Compiler包是必须的.Anntation包有时会包含在核心包里面。 APT Plugin 虽然J","text":"APT: 注解编译工具Annotation Compile Tool,用来在编译时根据注解自动生成Java代码。像ButterKnife,EventBus等库都为了避免因为反射带来的性能损失,都使用了APT注解方式。 使用APT需要的组件 通常一个库有三个以下部分组成： 核心包 用来对外提供使用,及库的逻辑部分 Anntation包 用来存放自定义的注解 Compiler包 这是APT运行所必需的,用来解释注解,告诉APT注解的意思及使用 对与一个使用APT的库来说,Compiler包是必须的.Anntation包有时会包含在核心包里面。 APT Plugin 虽然Java提供了APT,但在使用Gradle2.2之前的版本在编译时并不会调用APT,所以通过插件来调用。 这个插件也只能用Javac的方式进行编译。 从Gradle2.2开始,内置了APT的插件,不需要再进行声明,而且还支持以Jack的方式编译。 APT的使用 2.2之前的版本 在工程的Gradle中声明APT插件的依赖 1234 dependencies &#123; classpath 'com.android.tools.build:gradle:2.1.0' classpath 'com.neenbedankt.gradle.plugins:android-apt:1.8' &#125; 在Module的Gradle中声明使用APT插件 1 apply plugin : `android-apt` 在Module的依赖中添加库的依赖.核心包和Compiler包 12345 dependencies &#123; compile fileTree(include: ['*.jar'], dir: 'libs') compile 'com.jakewharton:butterknife:8.5.1' apt 'com.jakewharton:butterknife-compiler:8.5.1'&#125; 任何Module,只要需要使用APT,就必须在Gradle中声明使用APT插件及添加Compiler包的依赖。 2.2之后的版本 工程的Gradle中不需要声明APT插件,除非是库指定. 1234 dependencies &#123; classpath 'com.android.tools.build:gradle:2.3.0' classpath 'com.jakewharton:butterknife-gradle-plugin:8.5.1' &#125; 在项目的Gradle中不需要声明使用的插件,除非是库指定的.如果是个库,则依赖这个库的其它Module都不需要再声明 1 apply plugin: 'com.jakewharton.butterknife' 在Module的Gradle中添加核心包及Compiler包的依赖.Compiler包不再使用APT命令,而是annnotationProcessor 12345 dependencies &#123; compile fileTree(include: ['*.jar'], dir: 'libs') compile 'com.jakewharton:butterknife:8.5.1' annotationProcessor 'com.jakewharton:butterknife-compiler:8.5.1'&#125; 任何Module,只需要使用APT,就必须在Gradle中添加Compiler包的依赖。 The link of this page is http://home.meng.uno/articles/63b8fab5/ . Welcome to reproduce it!","categories":[{"name":"Application","slug":"Application","permalink":"http://home.meng.uno/categories/Application/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://home.meng.uno/tags/Android/"},{"name":"Java","slug":"Java","permalink":"http://home.meng.uno/tags/Java/"},{"name":"APT","slug":"APT","permalink":"http://home.meng.uno/tags/APT/"}]},{"title":"Java语法小结","slug":"java-lang","date":"2017-03-04T13:11:06.000Z","updated":"2021-01-01T15:52:21.000Z","comments":true,"path":"articles/50525a4c/","link":"","permalink":"http://home.meng.uno/articles/50525a4c/","excerpt":"注释 每种语言，第一个教我们的都是“注释”。Java基本有如下三种注释。 1 // 单行注释 1 2 3 /* 多行注释 */ 1 2 3 /** JavaDoc（Java文档）注释是这样的。可以用来描述类和类的属性。 */ 文件头 包 包就像定义文件路径一样，实际上的存储也是按包的名称存储。 1 package uno.meng; 导入类 导入一个类： 1 import java.util.ArrayList; 导入所有类： 1 import java.security.*; 书写类 每个 .java 文件都包含一个public类，这个","text":"注释 每种语言，第一个教我们的都是“注释”。Java基本有如下三种注释。 1 // 单行注释 123 /*多行注释*/ 123 /**JavaDoc（Java文档）注释是这样的。可以用来描述类和类的属性。*/ 文件头 包 包就像定义文件路径一样，实际上的存储也是按包的名称存储。 1 package uno.meng; 导入类 导入一个类： 1 import java.util.ArrayList; 导入所有类： 1 import java.security.*; 书写类 每个 .java 文件都包含一个public类，这个类的名字必须和这个文件名一致。 1 public class LearnJava &#123; 每个程序都需要有一个main函数作为入口： 1 public static void main (String[] args) &#123; 标准输出： 1 System.out.println(\"Hello World!\"); 使用+来拼接字符串： 1234 System.out.println( \"Integer: \" + 10 + \" Double: \" + 3.14 + \" Boolean: \" + true); 如果要在输出后不想自动换行，可以使用System.out.print方法： 12 System.out.print(\"Hello \");System.out.print(\"World\"); 类型与变量 基本类型定义 用 来声明变量： 12345678910 byte fooByte = 100;short fooShort = 10000;int fooInt = 1;long fooLong = 100000L; // L可以用来表示一个数字是长整型的。float fooFloat = 234.5f; // f用来表示一个数字是浮点型的。double fooDouble = 123.4;boolean fooBoolean = true;char fooChar = 'A';final int HOURS_I_WORK_PER_WEEK = 9001; // final 标识常量。String fooString = \"My String Is Here!\"; 数组 数组在声明时大小必须已经确定 数组的声明格式: &lt;数据类型&gt; [] &lt;变量名&gt; = new &lt;数据类型&gt;[&lt;数组大小&gt;]; 123 int [] intArray = new int[10];String [] stringArray = new String[1];boolean [] booleanArray = new boolean[100]; 声明并初始化数组也可以这样: 1 int [] y = &#123;9000, 1000, 1337&#125;; 随机访问数组中的元素: 1 System.out.println(\"intArray @ 0: \" + intArray[0]); 数组下标从0开始并且可以被更改: 12 intArray[1] = 1;System.out.println(\"intArray @ 1: \" + intArray[1]); // =&gt; 1 其他数据类型 ArrayLists - 类似于数组，但是功能更多，并且大小也可以改变，主要有： LinkedLists Maps HashMaps 操作符 多重申明 1 int i1 = 1, i2 = 2; // 多重声明可以简化 算数运算 1234 System.out.println(\"1+2 = \" + (i1 + i2)); // =&gt; 3System.out.println(\"2-1 = \" + (i2 - i1)); // =&gt; 1System.out.println(\"2*1 = \" + (i2 * i1)); // =&gt; 2System.out.println(\"1/2 = \" + (i1 / i2)); // =&gt; 0 (0.5 truncated down) 取余 1 System.out.println(\"11%3 = \"+(11 % 3)); // =&gt; 2 比较操作符 123456 System.out.println(\"3 == 2? \" + (3 == 2)); // =&gt; falseSystem.out.println(\"3 != 2? \" + (3 != 2)); // =&gt; trueSystem.out.println(\"3 &gt; 2? \" + (3 &gt; 2)); // =&gt; trueSystem.out.println(\"3 &lt; 2? \" + (3 &lt; 2)); // =&gt; falseSystem.out.println(\"2 &lt;= 2? \" + (2 &lt;= 2)); // =&gt; trueSystem.out.println(\"2 &gt;= 2? \" + (2 &gt;= 2)); // =&gt; true 位运算操作符 123456789 /*~ 取反，求反码&lt;&lt; 带符号左移&gt;&gt; 带符号右移&gt;&gt;&gt; 无符号右移&amp; 和^ 异或| 相容或*/ 自增 123 int i = 0;System.out.println(\"\\n-&gt;Inc/Dec-rementation\");// ++ 和 -- 操作符使变量加或减1。放在变量前面或者后面的区别是整个表达式的返回值。操作符在前面时，先加减，后取值。操作符在后面时，先取值后加减。 1234 System.out.println(i++); // 后自增 i = 1, 输出0System.out.println(++i); // 前自增 i = 2, 输出2System.out.println(i--); // 后自减 i = 1, 输出2System.out.println(--i); // 前自减 i = 0, 输出0 控制结构 If语句和C的类似 12345678 int j = 10;if (j == 10)&#123; System.out.println(\"I get printed\");&#125; else if (j &gt; 10) &#123; System.out.println(\"I don't\");&#125; else &#123; System.out.println(\"I also don't\");&#125; While循环 123456789 int fooWhile = 0;while(fooWhile &lt; 100)&#123; //System.out.println(fooWhile); //增加计数器 //遍历99次， fooWhile 0-&gt;99 fooWhile++;&#125;System.out.println(\"fooWhile Value: \" + fooWhile); Do While循环 123456789 int fooDoWhile = 0;do&#123; //System.out.println(fooDoWhile); //增加计数器 //遍历99次, fooDoWhile 0-&gt;99 fooDoWhile++;&#125;while(fooDoWhile &lt; 100);System.out.println(\"fooDoWhile Value: \" + fooDoWhile); For 循环 1234567 int fooFor;//for 循环结构 =&gt; for(&lt;起始语句&gt;; &lt;循环进行的条件&gt;; &lt;步长&gt;)for(fooFor=0; fooFor&lt;10; fooFor++)&#123; //System.out.println(fooFor); //遍历 10 次, fooFor 0-&gt;9&#125;System.out.println(\"fooFor Value: \" + fooFor); Switch Case 语句 switch可以用来处理 byte, short, char, 和 int 数据类型，也可以用来处理枚举类型,字符串类,和原始数据类型的包装类：Character, Byte, Short, 和 Integer 1234567891011121314151617 int month = 3;String monthString;switch (month)&#123; case 1: monthString = \"January\"; break; case 2: monthString = \"February\"; break; case 3: monthString = \"March\"; break; default: monthString = \"Some other month\"; break;&#125;System.out.println(\"Switch Case Result: \" + monthString); 类型转换 数据转换 将字符串转换为整型 1 Integer.parseInt(&quot;123&quot;);//返回整数123 将整型转换为字符串 1 Integer.toString(123);//返回字符串&quot;123&quot; 其他的数据也可以进行互相转换: Double Long String 类型转换 你也可以对java对象进行类型转换, 但其中会牵扯到很多概念在这里可以查看更详细的信息: http://docs.oracle.com/javase/tutorial/java/IandI/subclasses.html 方法 用new来实例化一个类 1 Bicycle trek = new Bicycle(); 调用对象的方法 12 trek.speedUp(3); // 需用getter和setter方法trek.setCadence(100); toString 可以把对象转换为字符串 1 System.out.println(&quot;trek info: &quot; + trek.toString()); 你也可以把其他的非public类放入到.java文件中！！ 类定义的语法 1234 &lt;public/private/protected&gt; class &lt;类名&gt;&#123; //成员变量, 构造函数, 函数 //Java中函数被称作方法&#125; 构造方法 构造方法不写返回值，返回类型，必须是public。 以下是一个默认构造函数 123456 public Bicycle() &#123; gear = 1; cadence = 50; speed = 5; name = \"Bontrager\";&#125; 以下是一个含有参数的构造函数 12345 public Bicycle(int startCadence, int startSpeed, int startGear, String name) &#123; this.gear = startGear; this.cadence = startCadence; this.speed = startSpeed; this.name = name; 方法书写语法 &lt;public/private/protected&gt; &lt;返回值类型&gt; &lt;函数名称&gt;(&lt;参数列表&gt;) Java类中经常会用getter和setter来对成员变量进行操作。 类的继承 PennyFarthing 是 Bicycle 的子类 1 class PennyFarthing extends Bicycle &#123; 通过super调用父类的构造函数 123 public PennyFarthing(int startCadence, int startSpeed)&#123; super(startCadence, startSpeed, 0, \"PennyFarthing\");&#125; 你可以用@注释来表示需要重载的方法 123456 @Override public void setGear(int gear) &#123; gear = 0; &#125;&#125; The link of this page is http://home.meng.uno/articles/50525a4c/ . Welcome to reproduce it!","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://home.meng.uno/categories/Programming-Language/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://home.meng.uno/tags/Java/"},{"name":"Language","slug":"Language","permalink":"http://home.meng.uno/tags/Language/"}]},{"title":"Map Reduce","slug":"mapreduce","date":"2017-03-01T06:36:02.000Z","updated":"2020-12-02T01:43:16.000Z","comments":true,"path":"articles/25d9ef10/","link":"","permalink":"http://home.meng.uno/articles/25d9ef10/","excerpt":"Map Reduce 的意义 集群 在传统的单节点模型中，CPU从内存读取数据，当内存空间不够时，再从磁盘读取数据，当磁盘空间不够了呢？ 即使磁盘空间足够，磁盘带宽是50MB/sec，若从磁盘读取200TB数据，大约需要46+天，完全没法接受呀！ 需要这样一个集群…… 优点（解决困难） 节点故障（node failures） 如果单个服务器能坚持3年（1000天），1000台服务器的集群平均每天大概发生1次故障，1M台服务器的集群平均每天大概发生1000次故障。节点故障时亟须解决的问题： * 如何存储数据，即使节点故障时仍可用？ * 若正在进行大规模计算，如果节点发生故障该如何","text":"Map Reduce 的意义 集群 在传统的单节点模型中，CPU从内存读取数据，当内存空间不够时，再从磁盘读取数据，当磁盘空间不够了呢？ 即使磁盘空间足够，磁盘带宽是50MB/sec，若从磁盘读取200TB数据，大约需要46+天，完全没法接受呀！ 需要这样一个集群…… 优点（解决困难） 节点故障（node failures） 如果单个服务器能坚持3年（1000天），1000台服务器的集群平均每天大概发生1次故障，1M台服务器的集群平均每天大概发生1000次故障。节点故障时亟须解决的问题： 如何存储数据，即使节点故障时仍可用？ 若正在进行大规模计算，如果节点发生故障该如何处理？ Map Reduce在多个节点冗余存储，保证数据持久存储和获取。 网络瓶颈（network bottleneck） Map Reduce的计算靠近数据端，减少数据移动。 分布式程序编写困难 Map Reduce简单的编程模型，隐藏了复杂的细节。 Map Reduce 简介 冗余存储架构 冗余存储架构采用分布式文件系统（distributed file system），例如：Google GFS、Hadoop HDFS。典型的应用是处理大文件，一次存储多次读取追加更新。 数据分块（chuck）存储在多台服务器。如上图所示，一个大文件分割成C0～C5共6块，每块在多台服务器存储备份。每台存储服务器也做计算用，使得计算靠近存储端。 计算模型 123456789 输入：key-value对的集合 - Map(k,v) —&gt; &lt;k’, v’&gt;* - 输入一个key-value对，输出多个key-value对； - 对所有的(k,v)对，只有一个Map函数。- Reduce(k’, &lt;v’&gt;*) —&gt; &lt;k’, v”&gt; - 所有的具有相同k’的v’都被reduce到一起； - 对同一个k’，只有一个Reduce函数。 Map-Reduce的计算模型分为Map和Reduce两步，Map分布式处理任务，Reduce合并任务。 上图展示了用Map-Reduce统计超大规模文件中单词出现次数，红色横线将不同节点的实现分割开。对于Map节点，所有相同单词都输出到同一个节点，比如the都在第二个节点。为了保证效率，Map-Reduce都采用的是顺序读取。 调度与数据流 Map-Reduce的数据流： 输入输出存储在分布式文件系统； 中间结果存储在本地文件系统； 输出通常再输入到另一个Map-Reduce任务。 上图是Map-Reduce分布式系统的并行实现，Partition Function部分采用Hash算法，将相同key的value映射到同一节点。 Map-Reduce环境的主要任务： 分割输入数据； 多机之间程序调度； 执行按key分组操作； 处理节点故障； 处理多机间通信。 Map Reduce的实现分为Master节点、Map节点和Reduce节点，Master节点的任务： 管理每个任务状态：空闲（idle，等待处理）、处理中（in-progress）、completed（结束）； 将空闲任务安排到可用节点； 当Map任务结束，向Master发送其R中间文件（存放在本地文件系统中）的位置和大小，每个reducer一个中间文件； Master推送信息到Reducer； Master周期性ping检测节点是否出故障。 Map Reduce系统有M个Map任务和R个Reduce任务，M比集群中的节点数目大得多，R通常比M小。 Map Reduce 的改进 合并操作 通常在一个Map任务中会产生多个相同key的(k,v)对，在Map节点合并这些相同的key可有效降低网络流量，如上图所示。合并函数通常与Reduce函数相同。 合并时需要注意Reduce函数是否支持在Map节点的合并操作，也就是合并操作会不会改变Reduce的结果。 改写分割函数 例如：系统采用的默认分割函数hash(key) mod R可以改写为hash(hostname(URL)) mod R，使同一个主机的url输出到相同的文件。 The link of this page is http://home.meng.uno/articles/25d9ef10/ . Welcome to reproduce it!","categories":[{"name":"Data Mining","slug":"Data-Mining","permalink":"http://home.meng.uno/categories/Data-Mining/"}],"tags":[{"name":"Map","slug":"Map","permalink":"http://home.meng.uno/tags/Map/"},{"name":"Reduce","slug":"Reduce","permalink":"http://home.meng.uno/tags/Reduce/"},{"name":"集群","slug":"集群","permalink":"http://home.meng.uno/tags/集群/"},{"name":"分布式","slug":"分布式","permalink":"http://home.meng.uno/tags/分布式/"}]},{"title":"数据加密与Java实现","slug":"data_safety","date":"2017-02-28T10:18:39.000Z","updated":"2021-01-01T16:26:13.000Z","comments":true,"path":"articles/7f8aa6cf/","link":"","permalink":"http://home.meng.uno/articles/7f8aa6cf/","excerpt":"几种常见的加密方式： * MD5 * BASE64 * AES * DES * RSA * SSL/TSL MD5加密 MD5英文全称Message-Digest Algorithm 5,全称消息算法摘要,是一种不可逆加密方式 特点 * 压缩性：任意长度的数据，算出的MD5值长度都是固定的。 * 容易计算：从原数据计算出MD5值很容易。 * 抗修改性：对原数据进行任何改动，哪怕只修改1个字节，所得到的MD5值都有很大区别。 * 强抗碰撞：已知原数据和其MD5值，想找到一个具有相同MD5值的数据（即伪造数据）是非常困难的。 因为其特点,MD5加密通常用来验证文件的完整","text":"几种常见的加密方式： MD5 BASE64 AES DES RSA SSL/TSL MD5加密 MD5英文全称Message-Digest Algorithm 5,全称消息算法摘要,是一种不可逆加密方式 特点 压缩性：任意长度的数据，算出的MD5值长度都是固定的。 容易计算：从原数据计算出MD5值很容易。 抗修改性：对原数据进行任何改动，哪怕只修改1个字节，所得到的MD5值都有很大区别。 强抗碰撞：已知原数据和其MD5值，想找到一个具有相同MD5值的数据（即伪造数据）是非常困难的。 因为其特点,MD5加密通常用来验证文件的完整性,防串改. 实现 在Java中提供了MessageDigest类来加密数据. 123456789101112 byte[] resource; //需要加密的数据MessageDigest md5=MessageDigest.getInstance(\"MD5\");byte[] bytes=md5.digest(resource);StringBuilder builder=new StringBuilder(bytes.lenght);for (byte byte :bytes ) &#123; String temp = Integer.toHexString(b &amp; 0xff); if (temp.lenght==1) &#123; builder.append(0); &#125; builder.append(temp);&#125;String MD5=builder.toString(); MD5加密是对字节数组进行操作的.所以无论加密文件或是其它都需要先转换为字节数组.加密后的结果也是字节数组. 为了方便查看,需要按规定转换为字节数组. BASE64 BASE64并不是一种加密方式,而是对字节数组的一种编码方式.实际上是让字节数据转换为文本的方案. 原理 BASE64是把字节数据转换为ASC码中对应的64个字符(英文大小写,+及/). 转换方式为每6位字节对应一个字符.不足位的后面补0.6位字节转换为十进制数值.范围是0-63,对应64个字符.如果有不足位的,在编码后会补上数量对应的= 用处 统一编码方式,不需要考虑字符集对数据的影响 可以把任何文件或数据都以纺一的文本方式保存 实现 在Java中提供了Base64类来实现编码转换 1234 //编码String encode= Base64.encode(byte[] bytes,int flag);//解码byte[] decode=Base64.decode(byte[] bytes,int flag); flag表示编码方式,通常使用Base64.DEFAULT Base64.DEFAULT 默认的编码方式 Base64.NO_PADDING 忽略后缀的=号 Base64.NO_WRAP 省略换行符 Base64.CTRL 使用微软的换行符,而不是Liunux的 Base64.URL_SAFE 使用-和_代替+和/以保证在URL的安全.通常用与网络传输 AES Advanced Encryption Standard,全称为高级加密标准,又称为 Rijndael 加密法,使用的是区块加密方案。 区块加密方案是把一定数量的字节划分为一个区块,把区块内的数据加密成相同位数的数据。 AES是一种对称加密方案,所以需要加密端和解密端都使用相同的密钥. 特点 区块 AES是对字节数组按区块划分,对区块进行加密.AES区块大小固定为128位. 密钥 AES的密钥长度有三种,128位/192位/256位 位数越多安全性越高. 矩阵 AES每次对区块中的16个字节进行操作,这4个字节会组成一个4x4的矩阵. 回合密钥 AES对每个矩阵的操作时都会根据密钥生成一个16位的回合密钥.对应矩阵上的每一个字节 实现 在Java中通过MessageDigest来进行AES加解密 123456789101112 //密钥原数据字符串,两端必须一样,128位的密钥字符串长度必须为16private static final String KEY=\"\"//加密方式,AES表示AES加密,CBC表示区块的处理方式,PKC5Padding表示区块的填充方式.private static final String MODE=\"AES/CBC/PKCS5Padding\"//生成密钥KeyGenerator keyGenerator = KeyGenerator.getInstance(KEY);SecretKey secretKey = keyGenerator.generateKey();//初始化密码处理类Cipher cipher = Cipher.getInstance(MODE);cipher.init(Cipher.ENCRIPT_MODE,secretKey); //第一个参数为模式,ENCRIPT_MODE为加密, DECRIPT为解密//cipher.init(Cipher.DECRIPT_MODE,secretKey,\"BC\"); 使用`PKCS7Padding`方式时需要加载bouncycastle包,BC为提供该填充方式的Provicerbyte[] date=cipher.doFinal(byte[] data); //加密和解密都是同一个方法,由初始化的模式决定.对源字节数组进行处理.生成新的字节数组. 填充方式有几种.Java6没有实现所有的填充方式. NoPadding,PKCS5Padding,ISO10126Padding Java6实现 PKCS7Padding,ZeroBytePadding 需要加载bouncycastle包 生成密钥时,getInstance()有个重载的方法可以添加一个provier.为密钥添加一个随机数。 AES加密安全性高,速度快。 DES Data Encryption Standard,标准加密算法.使用64位密钥的对称加密.使用的也是区块加密方案. 因为是64位密钥,安全性不高,现已经被AES加密替代. 实现方法同AES加密,只需要把MODE中的AES改为DES RSA 目前使用最广泛的非对称加密.生成一对密钥–公钥和私钥.公钥对来对外提供.私钥只有密钥生成者自己拥有。 非对称算法需要指定密钥长度,越长安全性越好,但加解密的速度就越慢.通常指定1024或2048。 一次加密的的密文长度为密钥长度/8-11,所以1024长度的密钥一次只能加密117字节.2048能加密245字节。 所以非对称加密通常只用与短数据加密,如签名或对称加密的密钥。 RSA加密都是一用与一对多的场景。 RSA有两种使用方式： 加密算法 公钥加密,只有私钥才能解密 签名算法 私钥签名,只有公钥才能验证. 如Github使用的SSH登录就是使用的RSA加密算法.用户把公钥保存到服务器,通过SSH登录时服务器发一个随机数给客户端,客户端使用私钥加密发送到服务器,服务器用保存的公钥解密。 RSA的密钥的产生 随机获取两个大质数,得其积N 获取N的欧拉函数值-&gt;整数R 随机获取一个小与R并与之互质的整数E,计算出E的反模元素D 公钥是(N,E),私钥是(N,D) RSA的原理 欧拉定理 两个互质的正整数,A和N,N的欧拉函数为P,则A的P次方除以N余1 费马小定理(RSA算法核心) 因为质数P的欧拉函数为P-1,所以一个整数A和一个质数N互质时,A的N-1次方除以N余1 反模元素 两个互质的正整数.A和N,则一定存在一个整数B,使得A乘B除以N余数为1 实现 定义常量 123 private static final KEY_SIZE=1024;private static final RSA=\"RSA\";private static final MODE=\"RSA/ECB/PCKS1Padding\" 这里注意RSA的加密填充方式,需要两端保持一致。 在Adrioid中默认使用的是RSA/None/NoPadding,在Java中使用的是RSA/None/PCKS1Padding。 创建密钥 1234567 KeyPairGenerator rsa = KeyPairGenerator.getInstance(RSA);rsa.initialize(KEY_SIZE);KeyPair keyPair = rsa.generateKeyPair();Key publicKey = keyPair.getPublic();byte[] publicKeyEncoded = publicKey.getEncoded();Key privateKey = keyPair.getPrivate();byte[] privateKeyEncoded = privateKey.getEncoded(); 把字节数组转换为Key 公钥会以字节数组的形式公开，接收方需要把字节数据转化为公钥。 123456 //公钥公开的方式为X509KeySpec KeySpec = new X509EncodedKeySpec(publicKeyEncoded);PublicKey publicKey = keyFactory.generatePublic(keySpec);//私钥公开的方式为PKCS8KeySpec keySpec = new PKCS8EncodedKeySpec(publicKeyEncoded);PrivateKey privateKey = keyFactory.generatePrivate(keySpec); 加解密 通常使用公钥加密,使用私钥解密.还是使用Cipher类。 1234567 Cipher cipher=Cipher.instance(MODE);//使用公钥加密cipher.init(Cipher.ENCRIPT_MODE,publicKey);cipher.doFinal();//使用私钥解密cipher.init(Cipher.DECRIPT_MODE,privateKey);cipher.doFinal(); SSL 安全套接字（Secure Socket Layer，SSL）协议是Web浏览器与Web服务器之间安全交换信息的协议，提供两个基本的安全服务：鉴别与保密。 鉴别 可选的客户端认证及强制的服务端认证 保密 双方在连接时定义好加密方式,所有传递内容都会加密. SSL是间与应用层与TCP层.应用数据经过SSL层加密并加上SSL头传输给TCP层。 SSL通信流程 握手 握手是在两端连接后数据传输前的协议行为,通过三次握手确定双方的身份,双方的加密方式,以及确定密钥.这个握手过程是通过RAS加密及身份证书完成的. 加密通信 完成握手协议手,双方就按确定的加密方式以对称加密的方式对数据进行加密 握手协议 客户端发送至服务器 一个会话ID,自身SSL版本.一个32位随机数.一自身支持的密码套件列表.一个hello. 服务器接收后会根据客户端提供的列表选择一个密码套件,确定与客户端之间的加密方式 服务器发送至客户端 会话ID,SSL版本,一个32位随机数,一个密码套件.一个servieHello.及自己的证书. 客户端收到证书后可以对证书进行验证.然后生成一个32位随机数.这样总共就有三个随机数了.根据服务端确定的加密方式用这三个随机数生成密钥.然后从证书中获得服务端的公钥对第三个随机数加密 客户端发送至服务器 加密的第三个随机数. 服务器收到加密的随机数后使用私钥解密,然后使用三个随机数生成密钥. 服务器发送至客户端 准备完成,可以开始加密通信 证书 证书是一台服务器对外提供的一个身份证明.需要通过可靠的第三方认证机构(CA)来发布。 一个数字证书通常有以下几项： 证书持有者的公钥 证书的发布机构 CA的签名 签名摘要的算法 证书的验证 一般的浏览器都会有CA根证书,含有所有CA的公钥 CA验证 CA根证书中找到不证书的发布机构 CA签名摘要验证 使用CA的公钥来解密签名摘要,如果解不开说明证书不对 CA签名验证 使用公钥解密签名.然后使用签名摘要算法进行签名摘要,比对解密手的签名摘要.如果不同说明签名被更改 证书过期验证 实现了在线证书状态协议(OCSP)的客户端可以在线查询证书是否过期 TSL TLS 1.0是IETF（Internet Engineering Task Force，Internet工程任务组）制定的一种新的协议，它建立在SSL 3.0协议规范之上，是SSL 3.0的后续版本，可以理解为SSL 3.1 The link of this page is http://home.meng.uno/articles/7f8aa6cf/ . Welcome to reproduce it!","categories":[{"name":"Application","slug":"Application","permalink":"http://home.meng.uno/categories/Application/"}],"tags":[{"name":"Java实现","slug":"Java实现","permalink":"http://home.meng.uno/tags/Java实现/"},{"name":"数据加密","slug":"数据加密","permalink":"http://home.meng.uno/tags/数据加密/"}]},{"title":"Android通知及RemoteViews","slug":"RemoteViews","date":"2017-02-27T08:53:59.000Z","updated":"2020-12-02T02:03:54.000Z","comments":true,"path":"articles/6a064a82/","link":"","permalink":"http://home.meng.uno/articles/6a064a82/","excerpt":"通知 通知创建和使用的流程 * 创建Notification.Builder的实例 * 通过Builder来设置通知的相关属性,并通过builder()方法来获取设置好的通知 * 获取NotificationManager来发送通知. 1 2 3 4 5 Notification.Builder builder = new Notification.Builder(getApplicationContext()); builder.setContentTitle(\"这是标题\"); Notification build = builder.build(); NotificationM","text":"通知 通知创建和使用的流程 创建Notification.Builder的实例 通过Builder来设置通知的相关属性,并通过builder()方法来获取设置好的通知 获取NotificationManager来发送通知. 12345 Notification.Builder builder = new Notification.Builder(getApplicationContext());builder.setContentTitle(\"这是标题\");Notification build = builder.build();NotificationManager nm = (NotificationManager) getSystemService(Context.NOTIFICATION_SERVICE);nm.notify(0,build); 官方的兼容包中提供了兼容工具类,用来在不同版本下创建和使用兼容的通知: 1234 NotificationCompat.Builder builder = new NotificationCompat.Builder(getApplicationContext());builder.setContentTitle(\"这是标题\");Notification nofitication = builder.build();NotificationManagerCompat.from(getApplicationContext()).notify(0,Notification); 使用NotificationCompact和NotificationManagerCompat两个类来提供兼容性.实际上底层的实现还是一样的。 NotificationManagerCompat通过静态方法from(Context Context)创建了一个自身的实例.实例中通过参数Context获取了一个NotificationManager,实际上通知还是通过它来发送。 实际上这两个工具类内部会根据应用运行机器的版本来创建对应的实现类来提供兼容性。 通知的设置 通知是通过Builder来设置的。常见的设置如下： 设置标题 setContentTitle(CharSequence) 设置文字内容 setContentText(CharSequence) 设置副文字 setSubContextText(CharSequence) 设置大图标 setLargeIcon(Bitmap) 不设置时默认为应用图标 设置小图标 setSmallIcon(int) 参数为图片资源Id 设置颜色 setColor(int color) 参数为RGB值 设置声音 setSound 设置震动 setVibrate 设置闪灯 setLight(int,int,int) 第一个int为颜色的资源id,第二个和第三个是闪灯开启和关闭的时间,单位为毫秒 设置点击后是否自动取消 setAutoCancel(boolean) 进度条 从4.1开始通知里自带进度条.通过setProgress(in max,int progress,boolean indeterminate)来设置进度 max 进度最大值 progress 当前进度 indeterminate 是否是无限循环 true表示无限循环进度条,不需要进度及最大值.false反之 当进度条加载完毕后可以能过setProgress(0,0,false)来隐藏进度条。 消息发送 发送通知需要一个id和一个Notification。通过NotificationManager发送到远程的NotificationService上.用Notification里的数据更新远程的UI。 使用notify(int id,Notification notify)可以不断的更新同一个通知.另外此方法可以有个重载的方法可以设置一个Tag。 ID id需要自己设置,一个Id对应一个通知.NotificationService收到Notification后会根据Id去查找对应的View.如果不存在则创建一个。如果存在则更新UI也就是说如果id相同时,后续的通知会把之前的通知替换掉,如果id不同时会弹出多个通知。 PendingIntent PendingIntent用与处理通知的交互事件(也就是点击事件).PendingIntent表示的是一个待定的意图。 可以用来开启一个Activity,Service或广播接收者,它是对Intent的包装.对外部提供这个Intent.通知被点击时系统会按照PendingIntent设置的方法及Intent设置的目标去开启对应的Activity,Service或广播接收者。 创建PendingIntent的方法如下： 123 PendingIntent.startActivity(Context context,int requestCode,Intent intent,int flag);PendingIntent.startService(Context context,int requestCode,Intent intent,int flag);PendingIntent.startBroadcast(Context context,int requestCode,Intent intent,int flag); 所需要的参数都一样,startActivity还可以额外带一个bundle参数。 Context 启动所需上下文.实际上传递到外部的是应用的Application.所以在点击通知时哪怕应用没启动,也可以开启对应的组件. requestCode 请求码,自定义 intent 启动所需组件的intent,因为Context是Application.所以启动Activity需要NEW_TASK标识. flag PendingIntent的标识 对与更新同一个id的通知而言,这两个没有作用,不管PendingIntent是否有变化,始终都是新的通知替换掉原来的。 当每次都使用不同的id来弹出多个通知时,这些flag才会发生作用.这里需要涉及PendingIntent的匹配问题。 两个PendingIntent匹配指的是requestCode相同并且PendingIntent内部的intent都匹配。 当两个intent内部的ComponentName和intent-filter相同,这两个intent就算是匹配的.ComponentName相同指的是这个intent希望开启的组件是相同的。 多个通知的PendingIntent如果不匹配是互不干扰的.如果是匹配的.则根据flag有不同的处理方式。 FLAG_ONE_SHOT PendingIntent只执行一次,也就是说多个含有匹配的PendingIntent的通知,只要有一个被点击了,那么其它的就点击无效.通知里如果还含有其它的PendingIntent,还是可以被点击的. FLAG_NO_CREAT 如果PendingIntent不存在时并不创建新的实例,而是返回null.这个很少用. FLAG_CANCEL_CURRENT 如果PendingIntent已经存在,则取消前者,创建新的实例以保持数据为最新.多个含有匹配PendingIntent的通知,只有最新的可以被点击,其它的点击无效. FLAG_UPDATE_CURRENT 多个含有PendingIntent的通知都会更新其Intent中的Extra数据与最新的通知中保持一至. Action 通知的点击交互被称为Action。 通知自身的点击交互设置方法是setContentIntent(PendingIntent)。 在新版本中通知可以添加不同的按钮来对应不同的Action,添加Action的方法有两种： 12 builer.addAction(Action)builer.addAction(int iconId,CharSequence title,PendingIntent intent) 实际上都是添加Action实例.区别在与Action是否自己来构造。Action在通知上显示为一个带图标的文字按钮.必须的三个参数如下： IconId 图标的资源id Title 按钮的文字 PendingIntent 按钮的点击处理 Style通知样式 在4.0版本后通知可以被拉伸,通知可以设置不同的扩展样式。 1 builer.setStyle(NotificationCompat.style); 系统提供了四种默认样式 MediaStyle 媒体播放器样式.可以最多提供5个Action,用与后台播放媒体文件： 12 builer.setStyle(new NotificationCompat.MediaStyle() .setMediaSession(MediaSession.Token)); BigPictureStyle 大图样式.可以附加一张图片,扩展显示： 12 buider.setStyle(new NotificationCompat.BigPictureStyle() .bigPicture(Bitmap)); InboxStyle 文字式表样式,可以以列表的形式显示最多5行的文字 123 builer.setStyle(new NotificationCompat.InboxStyle() .setLine(CharSequence) .setLine(CharSequence)); BigTextStyle 多文字内容.下拉显示全部文字 12 buider.setStyle(new NotificationCompat.BigTextStyle() .bitText(CharSequence)); RemoteViews RemoteViews是Android提供的一种远程服务,可以跨进程显示及更新UI,通知及桌面小部件就是基与它封装的。 其核心是跨进程传递数据.把UI的布局及对其View的操作封装到RemoteViews里,传递给其它进程.其它进程收到RemoteViews后调用RemoteViews的方法创建或更新UI并显示。 RemoteViews的原理 RemoteViews本身并不是个View.它是个Pacelable,是个可以跨进程传递的一个数据.它携带了UI的布局及对应的数据。 其它进程收到这个RemoteViews后会在其进程中根据这个布局inflat出对应的View.然后根据RemoteViews里的属性反射设置到对应的View上,然后根据设置点击监听.监听的处理就是调用对应的PendingIntent。 因为是跨进程,所以无法直接操作View,所以系统把对view的一个操作定义为Action对象.Action对象本身也是个Pacelable,所以可以跨进程.其封装了操作View的数据.远程进程遍历所有的Action并执行其apply方法通过反射更新View。 向先前说的设置view的属性,设置点击监听,都是一个Action。 RemoteViews的创建 12345 RemoteViews remoteview=new RemoteViews(String packageName,int layoutId);如果通知需要使用自定义UI可以通过创建RemoteView实现builer.setCustomContentView(RemoteViews);或builer.setCustomBigContentView(RemoteViews); packageName表示的是客户端的包名,layoutId表示布局Id。 RemoteViews的设置 RemoteViews对View的操作是在远程进行的.所以客户端只是封装了操作.通过RemoteViews提供的一系列set方法,RemoteViews把所有设置都封装成Action保存到集合中。 设置属性参数为view的id及属性值如： 12 setText(int id,CharSequence text)setColor(int Id,int color) 设置属性参数为view的id,方法名及属性值如 12 setBitmap(int id,String methodName,Bitmap bitmap)setBoolean(int id,String methodName,boolean value) 单个View设置点击事件 1 setOnClickPendingIntent(int id,PendingIntent intent); 集合View设置点击事件 需要 setPendingIntentTemplate及setOnClickFillIntent两个方法结合使用。 因为Action操作View是通过反射的.所以不能使用自定义控件,只能用有限的系统控件。 Layout FramgLayout LinearLayout RelativeLayout GridLayout View Button ImageView TextView ProgressBar ImageButton ListView GridView Chronometer AnalogClock ViewFlipper StackView AdapterViewFlipper ViewStub 发送RemoteViews到其它进程 可以使用任何IPC方式把本地设置好的RemoteViews发送到其它进程 是通过NotificationManager发送到系统进程,小部件是通过AppWidgetManager发送到系统进程.都是通过Binder机制。 因为RemoteViews是个Parcelable,所以可以通过很多系统提供的方式来传递,如Intent,Message,或其它IPC方式。 远程的UI操作 远程收到RemoteViews后通过调用其 apply()或reApply() 方法创建或更新UI。 Apply 1 public View apply(Context context,ViewGroup parent,OnClickHandler handler); 其中parent是父布局,这个方法会创建一个View并根据Action设置属性,返回这个设置好的View： 创建布局 从RemoteViews里获取布局文件的id,根据id填充出对应的View.因为是根据ID来填充布局的,所以必须保证远程进程能根据id获取到布局文件。 系统是根据RemoteViews里的包名来获取包内的布局文件的,同一个应用内不同的线程也可以根据id来获取布局文件.但如果是两个不同的应用是不能根据id来获取布局文件的。 设置属性 RemoteViews会调用perfomApply(View view,ViewGroup parent,OnClickHandler handler)来设置填充好的View 其实就是遍历Action.调用其 apply() 方法操作View.反射设置属性,或通过OnClickHandler设置点击交互。 把填充并设置好的View添加到父布局中。 ReApply 1 public void reapply(Context context, View v, OnClickHandler handler); reApply() 方法只是更新UI,也是调用 perfomApply() 方法。 如果是两个不同的应用间使用RemoteViews,是不能使用 apply() 方法来创建UI的。 这时需要两边约定好布局文件,远程端在需要创建UI时在使用本地的布局文件填充View.然后调用 reApply() 方法来更新UI,最后再添加到父布局。 总结 通知是由RemoteViews来实现的,通过Notification类来封装一个RemoteViews。 设置不同的style实际上是设置RemoteViews里的layoutId. 通过NotificationManager的 notify() 方法发送到系统的NotificationService中。 系统的NotificationService里维护着各种Notification,实际上一个id对应着一个RemoteViews。 系统在自己的进程里创建和更新通知.当通知被点击时,在系统进程中会根据PendingIntent来启动指定的Activity,Service,及BroadcasterReceiver。 The link of this page is http://home.meng.uno/articles/6a064a82/ . Welcome to reproduce it!","categories":[{"name":"Application","slug":"Application","permalink":"http://home.meng.uno/categories/Application/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://home.meng.uno/tags/Android/"},{"name":"RemoteViews","slug":"RemoteViews","permalink":"http://home.meng.uno/tags/RemoteViews/"}]},{"title":"标准模板库（STL）","slug":"stl","date":"2017-02-12T12:18:15.000Z","updated":"2021-01-02T13:11:32.000Z","comments":true,"path":"articles/5ae627d/","link":"","permalink":"http://home.meng.uno/articles/5ae627d/","excerpt":"STL简介 STL（Standard Template Library，标准模板库)是惠普实验室开发的一系列软件的统称。现在虽说它主要出现在C++中，但在被引入C++之前该技术就已经存在了很长的一段时间。 STL的代码从广义上讲分为三类：algorithm（算法），container（容器）和 iterator（迭代器），几乎所有的代码都采用了模板类和模版函数的方式，这相比于传统的由函数和类组成的库来说提供了更好的代码重用机会。在C++标准中，STL被组织为下面的13个头文件：、、、、、","text":"STL简介 STL（Standard Template Library，标准模板库)是惠普实验室开发的一系列软件的统称。现在虽说它主要出现在C++中，但在被引入C++之前该技术就已经存在了很长的一段时间。 STL的代码从广义上讲分为三类：algorithm（算法），container（容器）和 iterator（迭代器），几乎所有的代码都采用了模板类和模版函数的方式，这相比于传统的由函数和类组成的库来说提供了更好的代码重用机会。在C++标准中，STL被组织为下面的13个头文件：&lt;algorithm&gt;、&lt;deque&gt;、&lt;functional&gt;、&lt;iterator&gt;、&lt;vector&gt;、&lt;list&gt;、&lt;map&gt;、&lt;memory&gt;、&lt;numeric&gt;、&lt;queue&gt;、&lt;set&gt;、&lt;stack&gt;和&lt;utility&gt;。 算法 大家都能取得的一个共识是函数库对数据类型的选择对其可重用性起着至关重要的作用。 举例来说，一个求方根的函数，在使用浮点数作为其参数类型的情况下的可重用性肯定比使用整型作为它的参数类性要高。而C++通过模板的机制允许推迟对某些类型的选择，直到真正想使用模板或者说对模板进行特化的时候，STL就利用了这一点提供了相当多的有用算法。它是在一个有效的框架中完成这些算法的——你可以将所有的类型划分为少数的几类，然后就可以在模版的参数中使用一种类型替换掉同一种类中的其他类型。 STL提供了大约100个实现算法的模版函数，比如算法for_each将为指定序列中的每一个元素调用指定的函数，stable_sort以你所指定的规则对序列进行稳定性排序等等。这样一来，只要我们熟悉了STL之后，许多代码可以被大大的化简，只需要通过调用一两个算法模板，就可以完成所需要的功能并大大地提升效率。 算法部分主要由头文件&lt;algorithm&gt;，&lt;numeric&gt;和&lt;functional&gt;组成。&lt;algorithm&gt;是所有STL头文件中最大的一个（尽管它很好理解），它是由一大堆模版函数组成的，可以认为每个函数在很大程度上都是独立的，其中常用到的功能范围涉及到比较、交换、查找、遍历操作、复制、修改、移除、反转、排序、合并等等。&lt;numeric&gt;体积很小，只包括几个在序列上面进行简单数学运算的模板函数，包括加法和乘法在序列上的一些操作。&lt;functional&gt;中则定义了一些模板类，用以声明函数对象。 容器 在实际的开发过程中，数据结构本身的重要性不会逊于操作于数据结构的算法的重要性，当程序中存在着对时间要求很高的部分时，数据结构的选择就显得更加重要。 经典的数据结构数量有限，但是我们常常重复着一些为了实现向量、链表等结构而编写的代码，这些代码都十分相似，只是为了适应不同数据的变化而在细节上有所出入。STL容器就为我们提供了这样的方便，它允许我们重复利用已有的实现构造自己的特定类型下的数据结构，通过设置一些模版类，STL容器对最常用的数据结构提供了支持，这些模板的参数允许我们指定容器中元素的数据类型，可以将我们许多重复而乏味的工作简化。 容器部分主要由头文件&lt;vector&gt;， &lt;list&gt;， &lt;deque&gt;， &lt;set&gt;， &lt;map&gt;， &lt;stack&gt;和&lt;queue&gt;组成。 对于常用的一些容器和容器适配器（可以看作由其它容器实现的容器），可以通过下表总结一下它们和相应头文件的对应关系。 数据结构 描述 实现头文件 向量(vector) 连续存储的元素 &lt;vector&gt; 列表(list) 由节点组成的双向链表，每个结点包含着一个元素 &lt;list&gt; 双队列(deque) 连续存储的指向不同元素的指针所组成的数组 &lt;deque&gt; 集合(set) 由节点组成的红黑树，每个节点都包含着一个元素，节点之间以某种作用于元素对的谓词排列，没有两个不同的元素能够拥有相同的次序 &lt;set&gt; 多重集合(multiset) 允许存在两个次序相等的元素的集合 &lt;set&gt; 栈(stack) 后进先出的值的排列 &lt;stack&gt; 队列(queue) 先进先出的执的排列 &lt;queue&gt; 优先队列(priority_queue) 元素的次序是由作用于所存储的值对上的某种谓词决定的的一种队列 &lt;queue&gt; 映射(map) 由{键，值}对组成的集合，以某种作用于键对上的谓词排列 &lt;map&gt; 多重映射(multimap) 允许键对有相等的次序的映射 &lt;map&gt; 迭代器 下面要说的迭代器从作用上来说是最基本的部分，可是理解起来比前两者都要费力一些。软件设计有一个基本原则，所有的问题都可以通过引进一个间接层来简化，这种简化在STL中就是用迭代器来完成的。概括来说，迭代器在STL中用来将算法和容器联系起来，起着一种黏和剂的作用。几乎STL提供的所有算法都是通过迭代器存取元素序列进行工作的，每一个容器都定义了其本身所专有的迭代器，用以存取容器中的元素。 迭代器部分主要由头文件&lt;utility&gt;， &lt;iterator&gt;和&lt;memory&gt;组成。&lt;utility&gt;是一个很小的头文件，它包括了贯穿使用在STL中的几个模板的声明，&lt;iterator&gt;中提供了迭代器使用的许多方法，而对于&lt;memory&gt;的描述则十分的困难，它以不同寻常的方式为容器中的元素分配存储空间，同时也为某些算法执行期间产生的临时对象提供机制，&lt;memory&gt;中的主要部分是模板类allocator，它负责产生所有容器中的默认分配器。 The link of this page is http://home.meng.uno/articles/5ae627d/ . Welcome to reproduce it!","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://home.meng.uno/categories/Programming-Language/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://home.meng.uno/tags/C/"},{"name":"STL","slug":"STL","permalink":"http://home.meng.uno/tags/STL/"}]},{"title":"信号","slug":"signal","date":"2017-01-12T05:26:36.000Z","updated":"2020-12-08T10:29:15.000Z","comments":true,"path":"articles/1d60a972/","link":"","permalink":"http://home.meng.uno/articles/1d60a972/","excerpt":"信号 信号用来通知进程异步事件，可以把它理解为对中断的一种模拟。它是一个很小的消息，用来达到两个目的： * 告知进程发生了一个特定的事件； * 强迫进程执行自身所包含的信号处理程序。 linux预先定义了一些常规信号，并为它们定义了一些缺省操作。除此之外，还有一类实时信号，它们需要排队进行处理，我们也可以自己定义信号和信号处理方式。 既然信号是和进程相关的，那么task_struct中就必然包含有与信号相关的域了。 1 2 3 4 5 6 7 8 9 task_struct{ ... struct signal_struct *signal; //进程信号描述符 str","text":"信号 信号用来通知进程异步事件，可以把它理解为对中断的一种模拟。它是一个很小的消息，用来达到两个目的： 告知进程发生了一个特定的事件； 强迫进程执行自身所包含的信号处理程序。 linux预先定义了一些常规信号，并为它们定义了一些缺省操作。除此之外，还有一类实时信号，它们需要排队进行处理，我们也可以自己定义信号和信号处理方式。 既然信号是和进程相关的，那么task_struct中就必然包含有与信号相关的域了。 123456789 task_struct&#123; ... struct signal_struct *signal; //进程信号描述符 struct sighand_struct *sighand; //进程信号处理程序描述符 sigset_t blocked; //被阻塞信号掩码 sigset_t real_bloced; //被阻塞信号临时掩码 struct sigpending pending; //存放私有挂起信号 ...&#125; 信号的产生 信号是由内核函数产生的，它们完成信号处理的第一步，也即更新一个/多个进程的描述符。产生的信号并不直接传递，而是根据信号的类型、目标进程的状态唤醒进程，让它们来接收信号。内核提供了一组产生信号的函数，包括为进程、线程组产生信号等，但它们最终都会调用__send_signal()。当然，在调用__send_signal()之前，会检查这个信号是否应该被忽略（进程没有被跟踪、信号被阻塞，显示忽略信号） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 static int __send_signal(int sig, struct siginfo *info, struct task_struct *t, int group, int from_ancestor_ns)&#123;struct sigpending *pending;struct sigqueue *q; int override_rlimit;int ret = 0, result;assert_spin_locked(&amp;t-&gt;sighand-&gt;siglock);result = TRACE_SIGNAL_IGNORED;if (!prepare_signal(sig, t, from_ancestor_ns || (info == SEND_SIG_FORCED))) goto ret;//获取进程或线程组的私有挂起队列pending = group ? &amp;t-&gt;signal-&gt;shared_pending : &amp;t-&gt;pending;//这个信号已经挂起了，忽略它result = TRACE_SIGNAL_ALREADY_PENDING;if (legacy_queue(pending, sig)) goto ret;result = TRACE_SIGNAL_DELIVERED;//如果是kernel内部的某些强制信号，就立马执行if (info == SEND_SIG_FORCED) goto out_set;//如果没有超过挂起信号的上限if (sig &lt; SIGRTMIN) override_rlimit = (is_si_special(info) || info-&gt;si_code &gt;= 0);else override_rlimit = 0;//产生一个sigqueue对象，并把它加入到队列中去q = __sigqueue_alloc(sig, t, GFP_ATOMIC | __GFP_NOTRACK_FALSE_POSITIVE, override_rlimit);if (q) &#123; list_add_tail(&amp;q-&gt;list, &amp;pending-&gt;list); switch ((unsigned long) info) &#123; case (unsigned long) SEND_SIG_NOINFO: q-&gt;info.si_signo = sig; q-&gt;info.si_errno = 0; q-&gt;info.si_code = SI_USER; q-&gt;info.si_pid = task_tgid_nr_ns(current, task_active_pid_ns(t)); q-&gt;info.si_uid = from_kuid_munged(current_user_ns(), current_uid()); break; case (unsigned long) SEND_SIG_PRIV: q-&gt;info.si_signo = sig; q-&gt;info.si_errno = 0; q-&gt;info.si_code = SI_KERNEL; q-&gt;info.si_pid = 0; q-&gt;info.si_uid = 0; break; default: copy_siginfo(&amp;q-&gt;info, info); if (from_ancestor_ns) q-&gt;info.si_pid = 0; break; &#125; //......&#125; 在信号产生之后，linux会调用signal_wake_up()通知进程，告知有新的挂起信号到来，如果当前进程占有了CPU，那么就可以立即执行；否则则要强制进行重新调度。 信号的传递 在信号产生之后，如何确保挂起的信号被正确的处理呢？进程在信号产生时，可能并不在CPU上运行。在进程恢复用户态执行时，会进行检查，如果存在非阻塞的挂起信号，就调用do_signal()，这个函数会逐个助理挂起的非阻塞信号，而信号的处理则进一步调用handle_signal()。 123456789101112131415161718192021222324252627282930313233343536 handle_signal(struct ksignal *ksig, struct pt_regs *regs)&#123; bool stepping, failed; struct fpu *fpu = &amp;current-&gt;thread.fpu; //是否处于系统调用中 if (syscall_get_nr(current, regs) &gt;= 0) &#123; //系统调用被打断了，没有执行完，需要重新执行 switch (syscall_get_error(current, regs)) &#123; case -ERESTART_RESTARTBLOCK: case -ERESTARTNOHAND: regs-&gt;ax = -EINTR; break; case -ERESTARTSYS: if (!(ksig-&gt;ka.sa.sa_flags &amp; SA_RESTART)) &#123; regs-&gt;ax = -EINTR; break; &#125; /* fallthrough */ case -ERESTARTNOINTR: regs-&gt;ax = regs-&gt;orig_ax; regs-&gt;ip -= 2; break; &#125; &#125; //设置栈帧 failed = (setup_rt_frame(ksig, regs) &lt; 0); if (!failed) &#123; regs-&gt;flags &amp;= ~(X86_EFLAGS_DF|X86_EFLAGS_RF|X86_EFLAGS_TF); /* * Ensure the signal handler starts with the new fpu state. */ if (fpu-&gt;fpstate_active) fpu__clear(fpu); &#125; signal_setup_done(failed, ksig, stepping);&#125; 这里存在一个问题：handle_signal()处于内核态中，但信号处理程序是在用户态定义的，因此这里存在着堆栈转换的问题。linux采用的方法是：把内核态堆栈中的硬件上下文，拷贝到当前进程的用户态堆栈中。而当信号处理程序完成时，会自动调用sigreturn()把硬件上下文拷贝回内核态堆栈中，并且恢复用户态堆栈中的内容。这里需要构造一个用户态栈帧： 首先内核需要把内核栈中的内容复制到用户态堆栈中去，把内核态堆栈的返回地址修改为信号处理程序的入口。注意，为了让信号处理程序结束时，能够清除栈上的内容，用户态堆栈还应该放入一个信号处理程序的返回地址，它指向__kernel_sigreturn()，把硬件上下文拷贝到内核态堆栈，然后把这个栈帧删除，随后从内核态返回到用户态继续执行。 信号的接口 kill/tkill/kgill系统调用分别用来给某个进程、线程组发送信号。其中，kill(pid, sig)分别接受一个进程的pid号，以及一个所发送的信号。 实时信号的发送则应该使用rt_sigqueueinfo()来进行发送。如果用户需要为信号指定一个操作，那么则应该使用sigaction(sig, &amp;act, &amp;oact)系统调用，act为指定的操作，而old_act用来记录以前的信号。 The link of this page is http://home.meng.uno/articles/1d60a972/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://home.meng.uno/tags/Linux/"},{"name":"信号","slug":"信号","permalink":"http://home.meng.uno/tags/信号/"}]},{"title":"内核同步","slug":"kernel","date":"2017-01-10T05:11:05.000Z","updated":"2020-12-08T10:15:10.000Z","comments":true,"path":"articles/e41a0c57/","link":"","permalink":"http://home.meng.uno/articles/e41a0c57/","excerpt":"内核同步 对于内核，其实有一个很形象的理解：我们可以把内核理解成一个服务器，它为自身和用户提供各种服务。因此它必须要保证每项服务在处理时，不会互相造成影响，也就是解决“并发”的问题。自身的请求，也即中断；客户的请求，也即用户态的系统调用或异常。内核的同步，就是对内核中的任务进行调度，使它们按照正确的方式运行。 内核抢占 这里，“内核抢占”指的是进程A在内核态运行时，被具有更高优先级的进程B取代，也就是发生了进程上下文的切换。而我们知道，中断上下文是不包括进程信息的，不能被调度。所以只要在中断上下文中，就不能进行“进程切换”。因此硬中断和软中断在执行时都不允许内核抢占；只有在内核执行异常处理程","text":"内核同步 对于内核，其实有一个很形象的理解：我们可以把内核理解成一个服务器，它为自身和用户提供各种服务。因此它必须要保证每项服务在处理时，不会互相造成影响，也就是解决“并发”的问题。自身的请求，也即中断；客户的请求，也即用户态的系统调用或异常。内核的同步，就是对内核中的任务进行调度，使它们按照正确的方式运行。 内核抢占 这里，“内核抢占”指的是进程A在内核态运行时，被具有更高优先级的进程B取代，也就是发生了进程上下文的切换。而我们知道，中断上下文是不包括进程信息的，不能被调度。所以只要在中断上下文中，就不能进行“进程切换”。因此硬中断和软中断在执行时都不允许内核抢占；只有在内核执行异常处理程序（尤其是系统调用），并且内核抢占没有被显示禁用时，才能进行内核抢占。CPU必须打开本地中断，才能完成内核抢占。 从另一个角度来说，CPU在任何情况下，都处于三种上下文情况之一： 运行在用户空间，执行用户进程； 运行在内核空间，处于进程上下文； 运行在内核空间，处于中断上下文。 在关于中断的博文里，我已经写过，中断上下文是不属于任何进程的，它和current没有任何关系。由于没有任何进程背景，在中断上下文中也不能发生睡眠，否则是不能对它进行调度。因此中断上下文中只能用锁进行同步，中断上下文也叫做原子上下文。而异常和系统调用陷入内核时，是出于进程上下文的，因此可以通过current关联相应的任务。所以在进程上下文中，可以发生睡眠，也可以使用信号量；当然也可以使用锁。 ps：以上说的是内核抢占的情况；用户抢占指的是另一个概念，指的是内核即将返回用户空间的时候，如果need_resched标志被设置，就会调用schedule()，选择一个更为合适的进程运行。 内核不能被抢占的情况有这些： 内核正在进行中断处理。在linux下，进程不能抢占中断（注意，中断是可以抢占、中止其他中断的），中断历程中不允许进行进程调度（schedule()会进行判断，如果在中断中会报错）。这也包括软中断的Bottom half部分。 当前的代码段持有自旋锁、读写锁，这些锁保证SMP系统CPU并发的正确性，此时不能进行抢占。 内核正在执行调度程序时，不应该进行抢占。 内核正在对每CPU数据进行操作。 除此之外的情况，都可以发生内核抢占。 每CPU变量 把内核变量，声明为每个CPU所独有的，它是数组结构的数组，每个CPU对应数组的一个元素，CPU直接不能访问其他CPU对应的数组元素，只能读写自身的元素，因此也不会出现竞争条件。但这同样存在着限制：必须确定CPU上的数据是各自独立的。 但是每CPU变量不能解决内核抢占的问题，他只能解决多CPU的问题，因此在访问时应当禁用抢占。 原子操作 通过保证操作在芯片上是原子级的，保证“读－修改－写”指令不会引发竞争。任何一个这样的操作，都必须以单个指令执行，并且不能中断，避免其他CPU访问这个单元。除了常见的0或1次对齐内存访问的汇编指令、单处理器下的“读－修改－写”指令、前缀为lock的指令也是原子操作指令。 优化和内存屏障 优化屏障主要是用来保证编译时，汇编语言指令按照原顺序来执行，而不进行重排。例如在linux中，barrier()的本质就是asm volatile(&quot;&quot;:::&quot;memory&quot;)。而内存屏障则是保证原语前后的指令执行顺序，也即在执行原语后的指令时，原语前的指令必须已经执行完了。 自旋锁 自旋锁是一类特别广泛使用的同步技术，如果内核控制路径必须访问共享数据结构，或者访问临界区，那么就需要为自己获取一个自旋锁；只有资源是空闲时，获取才能成功；当它释放了锁之后，其他内核控制路径就可以进入房间了。那么自旋锁的意义是什么？它是多处理器环境下一种特殊的锁；如果执行路径发现自旋锁是锁着的，或反复在周围进行“旋转”，反复执行循环，直到锁被释放（忙等）。 自旋锁保护的临界区通常是禁止内核抢占的，如果在单CPU环境下，自旋锁仅仅能够禁止或启用内核抢占，并不能起到锁的作用。当然，忙等时还是可以被抢占的，只有上锁后才会禁止抢占。 ps：阿里巴巴的面试官问过我一个问题，**自旋锁的本质是什么？**我当时猜测了一下，回答了原子操作，但没有能够进一步地进行解释。这里应该结合源码进行说明。可以看到对xadd就是一个标准的源子加操作。linux内核使用了两种实现。其一是“标签自旋锁”，raw_spin_lock最后会调用： 12345678910111213141516171819202122232425262728293031323334353637 static inline void __raw_spin_lock(raw_spinlock_t *lock)&#123; preempt_disable(); //禁止了抢占 spin_acquire(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);&#125;arch_spin_lock(arch_spinlock_t *lock)&#123; register struct __raw_tickets inc = &#123;.tail = TICKET_LOCK_INC&#125;;//这个值是0 inc = xadd(&amp;lock-&gt;tickets, inc); //xadd是原子加，在多CPU时会上锁 //获取标签，同时把序号＋1 if(likely(inc.head == inc.tail)) //标签到自己了，取锁成功了 goto out; for(;;)&#123; //否则就不断循环，直到轮到自己 unsigned count = SPIN_THRESHOLD; do&#123; inc.head = READ_ONCE(lock-&gt;tickets.head); if(__tickets_equal(inc.head, inc.tail))//判断是否到自己的标签了 goto clear_slowpath; cpu_relax(); &#125;while(--count); __ticket_lock_spinning(lock, inc.tail); &#125; clear_slowpath: __ticket_check_and_clear_slowpath(lock, inc.head); cout: barrier();&#125;arch_spinlock_t的结构如下，实际上就是一个u16数typedef struct arch_spinlock &#123; union &#123; __ticketpair_t head_tail; struct __raw_tickets &#123; __ticket_t head, tail; &#125; tickets; &#125;; &#125; arch_spinlock_t; 另一种是一种更加复杂的实现，被称为“排队自旋锁”。排队自旋锁基于每CPU变量实现，其实现比基于标签对实现更公平。 读－拷贝－更新 用来保护在多数情况下，被多个CPU读的数据结构，而设计的另一种同步技术，其特点是允许多个读和写并发执行，并且不使用锁。那么它如何在共享数据读前提下，实现同步呢？RCU只保护被动态分配，并且通过指针引用的数据结构，并且在RCU临界区内，禁止睡眠。RCU的做法是，在写操作时，拷贝一份原来的副本，在副本上进行修改，并且在修改完成后进行更新，将旧的指针更新为新的指针。 信号量 在linux中，有两种信号量，一种是给内核使用的内核信号量，另一种是给用户态进程使用的IPC信号量。这里我们只讨论内核信号量。其实信号量和自旋锁在“上锁”这一点上是类似的，如果锁关闭了，那么就不允许内核控制路径继续执行；只不过它不会像自旋锁一样，在原地“忙等”，而是将相应的进程挂起；只有资源可用了，进程才能继续运行。也正因为“睡眠”的特性，信号量不能用在中断处理程序和延迟处理函数上，只有允许睡眠的情况下，才能够使用信号量。 内核信号量的定义在semaphore.h当中： 12345 struct semaphore &#123; raw_spinlock_t lock; //保护信号量的自旋锁 unsigned int count; struct list_head wait_list; &#125;; 很神奇的，这里看到了raw_spinlock_t的影子。这其实是一个由Real-time linux引入的命名问题；这里我们只需要明白：尽可能使用spin_lock；绝对不允许被抢占和休眠的地方，使用raw_spin_lock，否则使用spin_lock，信号量的底层，使用了自旋锁来实现。 信号量的后两个域，count和wait_list分别是现有资源数和等待获取资源的进程序列。对于信号量，内核定义了这些API： 123456 void down(struct semaphore *sem);void up(struct semaphore *sem);int down_interruptible(struct semaphore *sem);int down_killable(struct semaphore *sem);int down_trylock(struct semaphore *sem);int down_timeout(struct semaphore *sem, long jiffies); 这里看看down函数： 12345678910 void down(struct semaphore *sem)&#123; unsigned long flags; raw_spin_lock_irqsave(&amp;sem-&gt;lock, flags); if (likely(sem-&gt;count &gt; 0)) sem-&gt;count--; else __down(sem); raw_spin_unlock_irqrestore(&amp;sem-&gt;lock, flags);&#125; 可以看到，这里自旋锁的作用实际上是保证count不被同时操作；而如果count大于0，则可以减少它的值，表示获取了这个锁，否则会__down_common，这个函数在不发生错误大情况下，会调用这样一段函数： 123 raw_spin_unlock_irq(&amp;sem-&gt;lock);timeout = schedule_timeout(timeout);raw_spin_lock_irq(&amp;sem-&gt;lock); 这个函数是在timer.c代码中定义的。schedule_timeout函数将当前的任务置为休眠到设置的超时为止，这也就是信号量和自旋锁不同之处了，它允许进程的休眠。 而对于up函数来说，释放锁，增加count之后，会马上会检查是否有进程在等待资源： 12345678 static noinline void __sched __up(struct semaphore *sem)&#123; struct semaphore_waiter *waiter = list_first_entry(&amp;sem-&gt;wait_list, struct semaphore_waiter, list); list_del(&amp;waiter-&gt;list); waiter-&gt;up = true; wake_up_process(waiter-&gt;task);&#125; 这样看来，其实信号量和自旋锁最大的不同就只有两个：自旋锁的忙等与信号量的休眠，资源的数量。 互斥量 虽然《深入理解linux内核》这本书中没有写，但是内核中也是有互斥量的；实际上它相当于count ＝ 1的信号量。互斥量的定义为： 1234567891011121314151617 struct mutex &#123; atomic_t count; spinlock_t wait_lock; struct list_head wait_list;#if defined(CONFIG_DEBUG_MUTEXES) || defined(CONFIG_MUTEX_SPIN_ON_OWNER) struct task_struct *owner;#endif#ifdef CONFIG_MUTEX_SPIN_ON_OWNER struct optimistic_spin_queue osq;#endif#ifdef CONFIG_DEBUG_MUTEXES void *magic;#endif#ifdef CONFIG_DEBUG_LOCK_ALLOC struct lockdep_map dep_map;#endif&#125;; 可以看到它同样依赖于自旋锁实现，也包含一个进程的等待队列。我们来看看互斥量的上锁操作： 123456789101112 void __sched mutex_lock(struct mutex *lock)&#123; might_sleep(); __mutex_fastpath_lock(&amp;lock-&gt;count, __mutex_lock_slowpath); mutex_set_owner(lock);&#125;这里__mutex_fastpath_lock最终会调用一段汇编代码：asm_volatile_goto(LOCK_PREFIX &quot; decl %0\\n&quot; &quot; jns %l[exit]\\n&quot; : : &quot;m&quot; (v-&gt;counter) : &quot;memory&quot;, &quot;cc&quot; : exit); 也就是原子操作，修改mutex的counter，而mutex中的自旋锁，是为了保护wait_list而存在的，只是起到一个辅助作用，这点和信号量不太一样。 读写自旋锁/顺序锁/信号量 为了增加内核到并发能力，操作系统还设置了读写自旋锁。读写自旋锁允许多个内存控制路径，同时读同一个数据结构，但如果相对这个结构进行写操作，那么它必须首先获取读写自旋锁的写锁，写锁能让当前的路径独占访问这个资源。 顺序锁则是允许读者在读的同时进行写操作，因此写操作永远不会等待，但这样读操作有时候必须重复读多次，直到读到有效的副本为止。 读写信号量则和读写自旋锁类似，只不过它以挂起代替自旋。 禁止本地中断/可延迟函数 在前面提到的原语中，很多在实现的时候，都禁止了了本地的中断，这就保证了当前内核控制路径能够继续执行，例如raw_spin_lock_irqsave和raw_spin_lock_irqrestore。不过禁止本地中断不能阻止其他CPU 访问共享数据，因此通常和自旋锁结合使用。 而可延迟函数同样可以禁止和激活，这是由preempt_count字段中的值决定的。 The link of this page is http://home.meng.uno/articles/e41a0c57/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://home.meng.uno/tags/Linux/"},{"name":"内核同步","slug":"内核同步","permalink":"http://home.meng.uno/tags/内核同步/"}]},{"title":"HIT操作系统实验总结","slug":"oslab","date":"2017-01-04T12:50:26.000Z","updated":"2021-01-01T18:56:39.000Z","comments":true,"path":"articles/86743755/","link":"","permalink":"http://home.meng.uno/articles/86743755/","excerpt":"哈工大《操作系统》六次实验每次需要修改的文件见：修改文件列表本实验总结源自github项目：MIC 操作系统引导 bootsect.s * 实现屏幕输出 * 修改打印的字符串（空白也算作一个字符） * 读入setup.s代码（包括：设置驱动器、磁头，读取setup.s的磁道和扇区，并跳到相应位置开始执行） setup.s * （和bootsect.s中部分代码相同）打印相关信息 * （原代码已经可以部分打印硬件信息）需要在相关位置嵌入msg实现打印提示信息功能 build.c * 将bootsect.s、setup.s、system.s编译、链接生成Image文件 系统调用","text":"哈工大《操作系统》六次实验每次需要修改的文件见：修改文件列表 本实验总结源自github项目：MIC 操作系统引导 bootsect.s 实现屏幕输出 修改打印的字符串（空白也算作一个字符） 读入setup.s代码（包括：设置驱动器、磁头，读取setup.s的磁道和扇区，并跳到相应位置开始执行） setup.s （和bootsect.s中部分代码相同）打印相关信息 （原代码已经可以部分打印硬件信息）需要在相关位置嵌入msg实现打印提示信息功能 build.c 将bootsect.s、setup.s、system.s编译、链接生成Image文件 系统调用 unistd.h文件：添加系统调用功能号 sys.h声明新的系统调用处理函数；添加系统调用处理程序索引值到指针数组表中 system_call.s中增加系统调用总数 makefile添加新的系统调用所在文件的编译链接规则（依赖关系） 进程运行轨迹的跟踪与统计 process.c 涉及到fork()和wait()系统调用 主要实现了一个函数——cpuio_bound() 用fork()建立若干个同时运行的子程序 父P等待所有子P退出后才退出，每个子P性质通过cpuio_bound()控制性质 fork.c fork系统调用函数 main.c 内核的入口函数main()，对它的修改是增加日志创建语句，并将log文件关联到文件描述符log文件记录进程状态转换轨迹 kernel 主要寻找进程状态转换点： printk.c sched.c exit.c 信号量的实现和应用 sem_open 打开信号量 sem_wait 信号量P操作——value– sem_post 信号量V操作——value++ sem_unlink 释放信号量 地址映射与共享 shm.c shmget()：得到一个共享内存标识符或创建一个共享内存对象并返回共享内存标识符 shmat()：连接共享内存标识符为shmid的共享内存，连接成功后把共享内存区对象映射到调用进程的地址空间，随后可像本地空间一样访问 sem.c 实现信号量的四种操作，与实验四相同 字符显示的控制 keyboard.S 添加对字符F12的输入判断 console.c 添加输出到控制台的字符控制 file_dev.c 添加输出到文件的字符控制 The link of this page is http://home.meng.uno/articles/86743755/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://home.meng.uno/tags/操作系统/"}]},{"title":"使用SVM识别手写数字","slug":"svm_number_recog","date":"2016-12-17T04:08:13.000Z","updated":"2021-01-01T16:42:30.000Z","comments":true,"path":"articles/7dac38a/","link":"","permalink":"http://home.meng.uno/articles/7dac38a/","excerpt":"调用第三方库实现 在此我选用的是sk-learn的关于svm的库，其关于此次实验的svm函数定义为：svm.SVC(C=8.0, kernel='rbf', gamma=0.1)。svm.SVC()函数的几个重要超参在其官方的介绍文档中有如下的解释： * C :误差项的惩罚参数，浮点型，可选 (默认=1.0)； * kernel : 指定核函数类型，字符型，可选 (默认=‘rbf’)，如果使用自定义的核函数，需要预先计算核矩阵； * gamma : 浮点型, 可选 (默认=0.0)，’rbf’核函数的系数，需要注意的是，此处的gamma与课本中的sigma是互为倒数的关系（所以其可以为","text":"调用第三方库实现 在此我选用的是sk-learn的关于svm的库，其关于此次实验的svm函数定义为：svm.SVC(C=8.0, kernel='rbf', gamma=0.1)。svm.SVC()函数的几个重要超参在其官方的介绍文档中有如下的解释： C :误差项的惩罚参数，浮点型，可选 (默认=1.0)； kernel : 指定核函数类型，字符型，可选 (默认=‘rbf’)，如果使用自定义的核函数，需要预先计算核矩阵； gamma : 浮点型, 可选 (默认=0.0)，’rbf’核函数的系数，需要注意的是，此处的gamma与课本中的sigma是互为倒数的关系（所以其可以为0）。 代码 123456789101112131415161718192021222324252627282930313233 #!/usr/bin/env python3# -*- coding: utf-8 -*-\"\"\"Created on Thu Dec 1 13:30:21 2016@author: kuangmeng使用SVM分类器，从MNIST数据集中进行手写数字识别的分类程序\"\"\"import cPickleimport gzipfrom sklearn import svmimport timedef load_data(): \"\"\" 返回包含训练数据、验证数据、测试数据的元组的模式识别数据 \"\"\" f = gzip.open('data.gz', 'rb') training_data, validation_data, test_data = cPickle.load(f) f.close() return (training_data, validation_data, test_data)def Svm(): print (\"开始时间：\",time.strftime('%Y-%m-%d %H:%M:%S')) training_data, validation_data, test_data = load_data() # 传递训练模型的参数，这里用默认的参数 clf = svm.SVC(C=10.0, kernel='rbf', gamma=0.10,cache_size=8000,probability=False) # 进行模型训练 clf.fit(training_data[0], training_data[1]) # 测试集测试预测结果 predictions = [int(a) for a in clf.predict(test_data[0])] num_correct = sum(int(a == y) for a, y in zip(predictions, test_data[1])) print (\"%s 中的 %s 测试正确。\" % (num_correct, len(test_data[1]))) print (\"结束时间：\",time.strftime('%Y-%m-%d %H:%M:%S'))if __name__ == \"__main__\": Svm() 自己编程实现 在自己编程实现过程中，我也借鉴了很多其他人写的很成熟的方案，最终从数据结构、逻辑结构以及特征计算等方面得到比较合理的一组答案。 逻辑结构 本次实验的程序分为“训练”和“测试”两部分，两部分分别进行的工作如下： 训练 加载数据 初始化模型 更新标签 初始化预测误差 迭代每个样本（用KT优化） 得到每个样本的模型 对步骤5的解释： 对于svm我们要求解a（数组），如果 a的所有分量满足svm对偶问题的KKT条件，那么这个问题的解就求出来了，我们svm模型学习也就完成了。如果没有满足KKT，那么我们就在 a中找两个分量 ai和 aj，其中 ai 是违反KKT条件最严重的分量，通过计算，使得 ai 和 aj满足KKT条件，直到a的所有分量都满足KKT条件。而且这个计算过程是收敛的，因为每次计算出来的新的两个分量，使得对偶问题中要优化的目标函数值更小。因为每次求解的那两个分量，是要优化问题在这两个分量上的极小值，所以每一次优化，都会使目标函数比上一次的优化结果的值变小。 测试 加载数据 对每个数据预测 计算正确率与相关信息逻辑结构 特征计算 仿照KKT的优化方法，在本次试验中，我将每张图片作为一个数据。由此得到对每一个测试样本的预测（如果在某个分类的计算时结果为正，则说明该测试样本属于该类别，结果为0则不属于此类别）。 其他杂项 核函数选择：按照传统，选择的是RBF核函数，函数形式与教材完全相同； 数据来源：MNIST(http://yann.lecun.com/exdb/mnist/) SMO优化算法： 取初始值a(0)=0，令K=0； 选取优化变量a1(k) , a2(k) , 针对优化问题，求得最优解 a1(k+1) , a2(k+1) 更新 a(k) 为 a(k+1) ； 在精度条件范围内是否满足停机条件，即是否有变量违反KKT条件，如果违反了，则令k=k+1，跳转2，否则4； 求得近似解â =a(k+1) 其中第3步中，是否违反KKT条件，对于a(k)的每个分量按照以下的违反KKT条件的公式进行验算即可。变量选取分为两步，第一步是选取违反KKT条件最严重的ai，第二步是根据已经选取的第一个变量，选择优化程度最大的第二个变量。违反KKT条件最严重的变量可以按照这样的规则选取，首先看0&lt;ai&lt;C的那些分量中，是否有违反KKT条件的，如果有，则选取yig(xi)最小的那个做为a1。如果没有则遍历所有的样本点，在违反KKT条件的分量中选取yig(xi)最小的做为a1。当选择了a1后，如果a1对应的E1为正，选择Ei最小的那个分量最为a2，如果E1为负，选择Ei最大的那个分量最为a2，这是因为anew2依赖于|E1−E2|。 如果选择的a2，不能满足下降的最小步长，那么就遍历所有的支持向量点做为a2进行试用，如果仍然都不能满足下降的最小步长，那么就遍历所有的样本点做为a2试用。如果还算是不能满足下降的最小步长，那么就重新选择a1。 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237 #!/usr/bin/env python3# -*- coding: utf-8 -*-\"\"\"Created on Sun Dec 4 14:24:53 2016@author: kuangmeng\"\"\"import timeimport osimport mathclass model: def __init__(self): self.a = [] self.b = 0.0 class DATA: def __init__(self): self.samples = [] # 样本数据 self.tests = [] # 测试数据 self.models = [] # 训练的模型 self.forecasterror = [] # 预测知与真实y之差Ei self.modelnum = 0 # 当前正使用或训练的模型 self.cache= [] # 缓存kernel函数的计算结果 self.sigma = 10 # sigma def init_models(self): for i in range(0, 10): m = model() for j in range(len(self.samples)): m.a.append(0) self.models.append(m) def init_cache(self): i = 0 for x in self.samples: print (\"正在计算第\",i+1,\"个样本的RBF核\") self.cache.append([]) j = 0 for z in self.samples: if i &gt; j: self.cache[i].append(self.cache[j][i]) else: self.cache[i].append(RBF(x,z)) j += 1 i += 1 class image: def __init__(self): self.data = [] self.num = 0 self.label = [] self.filename = \"\"gv = DATA()# RBF核函数def RBF(j, i): if j == i: return math.exp(0) sigma = gv.sigma ret = 0.0 for m in range(len(j.data)): for n in range(len(j.data[m])): ret += math.pow(int(j.data[m][n]) - int(i.data[m][n]), 2) ret = math.exp(-ret/sigma) return ret#加载测试与训练数据def loaddata(dirpath, name): files = os.listdir(dirpath) for file in files: img = image() img.data = images(dirpath + file) img.num = int(file[0]) img.filename = file name.append(img)#图片分列 def images(path): img = [] file = open(path, \"r\") for line in file: line = line[:-2] img.append(line) return img #更新样本标签，正在训练啥就将啥的标签定为1，其他的定为-1 def update_samples_label(num): for img in gv.samples: if img.num == num: img.label.append(1) else: img.label.append(-1) #初始化DATA.forecasterrordef init_forecasterror(): gv.forecasterror = [] for i in range(len(gv.samples)): diff = 0.0 for j in range(len(gv.samples)): if gv.models[gv.modelnum].a[j] != 0: diff += gv.models[gv.modelnum].a[j] * gv.samples[j].label[gv.modelnum] * gv.cache[j][i] diff += gv.models[gv.modelnum].b diff -= gv.samples[i].label[gv.modelnum] gv.forecasterror.append(diff)#更新DATA.forecasterror def update_forecasterror(i, new_ai, j, new_bj, new_b): for idx in range(len(gv.samples)): diff = (new_ai - gv.models[gv.modelnum].a[i])* gv.samples[i].label[gv.modelnum] * gv.cache[i][idx] diff += (new_bj - gv.models[gv.modelnum].a[j])* gv.samples[j].label[gv.modelnum] * gv.cache[j][idx] diff += new_b - gv.models[gv.modelnum].b diff += gv.forecasterror[idx] gv.forecasterror[idx] = diff# g(x)def predict(m): pred = 0.0 for j in range(len(gv.samples)): if gv.models[gv.modelnum].a[j] != 0: pred += gv.models[gv.modelnum].a[j] * gv.samples[j].label[gv.modelnum] * RBF(gv.samples[j],m) pred += gv.models[gv.modelnum].b return preddef save_models(): for i in range(10): fn = open(\"models/\" + str(i) + \"_a.model\", \"w\") for ai in gv.models[i].a: fn.write(str(ai)) fn.write('\\n') fn.close() fn = open(\"models/\" + str(i) + \"_b.model\", \"w\") fn.write(str(gv.models[i].b)) fn.close()def load_models(): for i in range(10): fn = open(\"models/\" + str(i) + \"_a.model\", \"r\") j = 0 for line in fn: gv.models[i].a[j] = float(line) j += 1 fn.close() fn = open(\"models/\" + str(i) + \"_b.model\", \"r\") gv.models[i].b = float(fn.readline()) fn.close()#### T: tolerance 误差容忍度(精度)# times: 迭代次数# 优化方法：SMO# C: 惩罚系数# modelnum: 模型序号0到9# step: aj移动的最小步长###def train(T, times, C, modelnum, step): time = 0 gv.modelnum = modelnum update_samples_label(modelnum) init_forecasterror() updated = True while time &lt; times and updated: updated = False time += 1 for i in range(len(gv.samples)): ai = gv.models[gv.modelnum].a[i] Ei = gv.forecasterror[i] #计算违背KKT的点 if (gv.samples[i].label[gv.modelnum] * Ei &lt; -T and ai &lt; C) or (gv.samples[i].label[gv.modelnum] * Ei &gt; T and ai &gt; 0): for j in range(len(gv.samples)): if j == i: continue kii = gv.cache[i][i] kjj = gv.cache[j][j] kji = kij = gv.cache[i][j] eta = kii + kjj - 2 * kij if eta &lt;= 0: continue new_aj = gv.models[gv.modelnum].a[j] + gv.samples[j].label[gv.modelnum] * (gv.forecasterror[i] - gv.forecasterror[j]) / eta # f 7.106 L = 0.0 H = 0.0 a1_old = gv.models[gv.modelnum].a[i] a2_old = gv.models[gv.modelnum].a[j] if gv.samples[i].label[gv.modelnum] == gv.samples[j].label[gv.modelnum]: L = max(0, a2_old + a1_old - C) H = min(C, a2_old + a1_old) else: L = max(0, a2_old - a1_old) H = min(C, C + a2_old - a1_old) if new_aj &gt; H: new_aj = H if new_aj &lt; L: new_aj = L if abs(a2_old - new_aj) &lt; step: # print (\"j = %d, is not moving enough\" % j) continue new_ai = a1_old + gv.samples[i].label[gv.modelnum] * gv.samples[j].label[gv.modelnum] * (a2_old - new_aj) # f 7.109 new_b1 = gv.models[gv.modelnum].b - gv.forecasterror[i] - gv.samples[i].label[gv.modelnum] * kii * (new_ai - a1_old) - gv.samples[j].label[gv.modelnum] * kji * (new_aj - a2_old) # f7.115 new_b2 = gv.models[gv.modelnum].b - gv.forecasterror[j] - gv.samples[i].label[gv.modelnum]*kji*(new_ai - a1_old) - gv.samples[j].label[gv.modelnum]*kjj*(new_aj-a2_old) # f7.116 if new_ai &gt; 0 and new_ai &lt; C: new_b = new_b1 elif new_aj &gt; 0 and new_aj &lt; C: new_b = new_b2 else: new_b = (new_b1 + new_b2) / 2.0 update_forecasterror(i, new_ai, j, new_aj, new_b) gv.models[gv.modelnum].a[i] = new_ai gv.models[gv.modelnum].a[j] = new_aj gv.models[gv.modelnum].b = new_b updated = True print (\"迭代次数: %d, 修改组合: i: %d 与 j:%d\" %(time, i, j)) break# 测试数据def test(): record = 0 record_correct = 0 for img in gv.tests: print (\"正在测试：\", img.filename) for modelnum in range(10): gv.modelnum = modelnum if predict(img) &gt; 0: print (\"测试结果：\",modelnum) record += 1 if modelnum == int(img.filename[0]): record_correct += 1 break print (\"相关记录数量:\", record) print (\"正确识别数量:\", record_correct) print (\"正确识别比例:\", record_correct/record) print (\"测试数据总量:\", len(gv.tests))if __name__ == \"__main__\": print (\"开始时间：\",time.strftime('%Y-%m-%d %H:%M:%S')) training = True loaddata(\"train/\", gv.samples) loaddata(\"test/\", gv.tests) print (\"训练数据个数：\",len(gv.samples)) print (\"测试数据个数：\",len(gv.tests)) if training == True: gv.init_cache() gv.init_models() print (\"模型初始化成功！\") print (\"当前时间：\",time.strftime('%Y-%m-%d %H:%M:%S')) T = 0.0001 C = 10 step = 0.001 gv.sigma = 1 if training == True: for i in range(10): print (\"正在训练模型:\", i) train(T, 10, C, i, step) save_models() else: load_models() for i in range(10): update_samples_label(i) print (\"训练完成时间：\",time.strftime('%Y-%m-%d %H:%M:%S')) test() print (\"测试完成时间：\",time.strftime('%Y-%m-%d %H:%M:%S')) The link of this page is http://home.meng.uno/articles/7dac38a/ . Welcome to reproduce it!","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://home.meng.uno/categories/Machine-Learning/"}],"tags":[{"name":"模式识别","slug":"模式识别","permalink":"http://home.meng.uno/tags/模式识别/"},{"name":"SVM","slug":"SVM","permalink":"http://home.meng.uno/tags/SVM/"},{"name":"手写数字","slug":"手写数字","permalink":"http://home.meng.uno/tags/手写数字/"},{"name":"分类","slug":"分类","permalink":"http://home.meng.uno/tags/分类/"}]},{"title":"主成分分析（PCA）","slug":"pca","date":"2016-12-17T03:52:21.000Z","updated":"2021-01-01T16:46:15.000Z","comments":true,"path":"articles/6c0d033f/","link":"","permalink":"http://home.meng.uno/articles/6c0d033f/","excerpt":"目标 实现一个PCA模型，能够对给定数据进行降维（即找到其中的主成分） 实验准备 降维的必要 多重共线性–预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。 高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有0.02%。 过多的变量会妨碍查找规律的建立。 仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。 降维的目的 * 减少预测变量的个数 * 确保这些变量是相互独立的 * 提供一个框架来解释结果 * 降维的方法 * 主成分分析 * 因子分析 * 用","text":"目标 实现一个PCA模型，能够对给定数据进行降维（即找到其中的主成分） 实验准备 降维的必要 多重共线性–预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。 高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有0.02%。 过多的变量会妨碍查找规律的建立。 仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。 降维的目的 减少预测变量的个数 确保这些变量是相互独立的 提供一个框架来解释结果 降维的方法 主成分分析 因子分析 用户自定义复合 有关PCA PCA概念 主成分分析 （ Principal Component Analysis ， PCA ）或者主元分析。是一种掌握事物主要矛盾的统计分析方法，它可以从多元事物中解析出主要影响因素，揭示事物的本质，简化复杂的问题。计算主成分的目的是将高维数据投影到较低维空间。给定 n 个变量的 m 个观察值，形成一个 n * m 的数据矩阵， n 通常比较大。对于一个由多个变量描述的复杂事物，人们难以认识，那么是否可以抓住事物主要方面进行重点分析呢？如果事物的主要方面刚好体现在几个主要变量上，我们只需要将这几个变量分离出来，进行详细分析。但是，在一般情况下，并不能直接找出这样的关键变量。这时我们可以用原有变量的线性组合来表示事物的主要方面， PCA 就是这样一种分析方法。 PCA作用范围 PCA 主要用于数据降维，对于一系列例子的特征组成的多维向量，多维向量里的某些元素本身没有区分性，比如某个元素在所有的例子中都为1，或者与1差距不大，那么这个元素本身就没有区分性，用它做特征来区分，贡献会非常小。所以我们的目的是找那些变化大的元素，即方差大的那些维，而去除掉那些变化不大的维，从而使特征留下的都是“精品”，而且计算量也变小了。 对于一个K维的特征来说，相当于它的每一维特征与其他维都是正交的（相当于在多维坐标系中，坐标轴都是垂直的），那么我们可以变化这些维的坐标系，从而使这个特征在某些维上方差大，而在某些维上方差很小。 PCA的算法步骤 设有m条n维数据。 将原始数据按列组成n行m列矩阵X 将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值 求出协方差矩阵C=1/mXX’ 求出协方差矩阵的特征值及对应的特征向量 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P Y=PX即为降维到k维后的数据 小结 PCA的应用分析 对于一个训练集，100个对象模板，特征是10维，那么它可以建立一个100*10的矩阵，作为样本。求这个样本的协方差矩阵，得到一个10*10的协方差矩阵，然后求出这个协方差矩阵的特征值和特征向量，应该有10个特征值和特征向量，我们根据特征值的大小，取前四个特征值所对应的特征向量，构成一个10*4的矩阵，这个矩阵就是我们要求的特征矩阵，100*10的样本矩阵乘以这个10*4的特征矩阵，就得到了一个100*4的新的降维之后的样本矩阵，每个特征的维数下降了。当给定一个测试的特征集之后，比如1*10维的特征，乘以上面得到的10*4的特征矩阵，便可以得到一个1*4的特征，用这个特征去分类。所以做PCA实际上是求得这个投影矩阵，用高维的特征乘以这个投影矩阵，便可以将高维特征的维数下降到指定的维数。 在进行基因表达数据分析时，一个重要问题是确定每个实验数据是否是独立的，如果每次实验数据之间不是独立的，则会影响基因表达数据分析结果的准确性。对于利用基因芯片所检测到的基因表达数据，如果用 PCA 方法进行分析，可以将各个基因作为变量，也可以将实验条件作为变量。当将基因作为变量时，通过分析确定一组“主要基因元素”，它们能够很好地说明基因的特征，解释实验现象；当将实验条件作为变量时，通过分析确定一组“主要实验因素”，它们能够很好地刻画实验条件的特征，解释基因的行为。 PCA作为基础的数学分析方法，其实际应用十分广泛，比如人口统计学、数量地理学、分子动力学模拟、数学建模、数理分析等学科中均有应用，是一种常用的多变量分析方法。 PCA优缺点 优点 以方差衡量信息的无监督学习，不受样本标签限制； 各主成分之间正交，可消除原始数据成分间的相互影响； 可减少指标选择的工作量； 用少数指标代替多数指标，利用PCA降维是最常用的算法； 计算方法简单，易于实现。 缺点 主成分解释其含义往往具有一定的模糊性，不如原始样本完整； 贡献率小的主成分往往可能含有对样本差异的重要信息； 特征值矩阵的正交向量空间是否唯一有待讨论； 属于无监督学习。 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 #!/usr/bin/env python3# -*- coding: utf-8 -*-\"\"\"Created on Tue Nov 29 15:51:25 2016@author: kuangmeng\"\"\"import numpy as npimport matplotlib.pyplot as plt#全局变量定义区XLabel = list()YLabel = list()phi = [0., 0.]#自己定义的矩阵转换函数def Transport(matrix): temp = list() for i in range(len(matrix[0])): temp.append(list()) for j in range(len(matrix)): temp[i].append(matrix[j][i]) return temp#加载文件（可以通过更改文件名来加载不同的测试数据）data_set = open('testSet.txt', 'r')for line in data_set.readlines(): data_line = line.strip().split() tmpx = float(data_line[0]) tmpy = float(data_line[1]) phi[0] += tmpx phi[1] += tmpy XLabel.append(tmpx) YLabel.append(tmpy)phi[0] = phi[0]/100.0phi[1] = phi[1]/100.0data_set.close()#加载结束temp_x = list()for i in range(100): temp_x.append([XLabel[i]-phi[0], YLabel[i]-phi[1]])temp_x_ = Transport(temp_x)sigma = np.dot(temp_x_, temp_x)D,V= np.linalg.eig(sigma)for i in range(2): for j in range(2): V.real[i][j] *= -1temp_v_ = Transport(V.real)tr1 = list()tr1.append(XLabel)tr1.append(YLabel)tr1 = Transport(tr1)xr1 = np.dot(tr1, temp_v_[0])xr2 = np.dot(phi, temp_v_[1])xr = tr1# print xrfor i in range(len(XLabel)): xr[i][0] = np.dot(xr1[i], V.real[0][0])+np.dot(xr2, V.real[0][1]) xr[i][1] = np.dot(xr1[i], V.real[1][0])+np.dot(xr2, V.real[1][1])# print xrplt.plot(XLabel, YLabel, 'r+')temp_xr = Transport(xr)plt.plot(temp_xr[0], temp_xr[1], 'b*')for i in range(len(XLabel)): plt.plot([XLabel[i],xr[i][0]], [YLabel[i],xr[i][1]])plt.axis([-8,6,-5,5])plt.xlabel = 'x'plt.ylabel = 'y'plt.show() The link of this page is http://home.meng.uno/articles/6c0d033f/ . Welcome to reproduce it!","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://home.meng.uno/categories/Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://home.meng.uno/tags/机器学习/"},{"name":"主成分分析","slug":"主成分分析","permalink":"http://home.meng.uno/tags/主成分分析/"},{"name":"PCA","slug":"PCA","permalink":"http://home.meng.uno/tags/PCA/"}]},{"title":"使用EM算法优化的GMM","slug":"gmm","date":"2016-12-17T03:16:53.000Z","updated":"2021-01-01T18:41:38.000Z","comments":true,"path":"articles/177fbbcc/","link":"","permalink":"http://home.meng.uno/articles/177fbbcc/","excerpt":"先上代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 1","text":"先上代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113 # -*- coding: utf-8 -*-import numpy as npimport matplotlib.pyplot as matimport matplotlib.mlab as mlab#EM算法def EM(dataSet,K): (N, M) = np.shape(dataSet) W = np.zeros([N, K]) P= N/K for k in range(K): W[np.floor(k*P):np.floor((k+1)*P), k] = 1 A,M,S = Mstep(dataSet,W) return W, A, M, S#M的跨度def Mstep(data,W): (N, M) = np.shape(data) K = np.size(W,1) Nk = np.sum(W,0) A = Nk/np.sum(Nk) Mm = data.T.dot(W).dot(np.diag(np.reciprocal(Nk))) S = np.zeros([M,M,K]) for k in range(K): datMean = data.T - Mm[0:,k][None].T.dot(np.ones([1,N])) S[:,:,k] = (datMean.dot(np.diag(W[0:,k])).dot(datMean.T))/Nk[k] return A,Mm,S#E的跨度def Estep(data,A,M,S): N = np.size(data,0) K = np.size(A) W = np.zeros([N,K]) for k in range(K): for i in range(N): W[i,k] = A[k]*multivariate(data[i,:][None].T, \\ M[:,k][None].T,S[:,:,k]) W = W*np.reciprocal(np.sum(W,1)[None].T) return Wdef multivariate(x, m, s): if len(x) == len(m) and (len(x), len(x)) == s.shape: det = np.linalg.det(s) const = 1.0/(np.math.pow((2*np.pi), float(len(x))/2) * np.math.pow(det, 1.0/2)) x_m = np.matrix(x - m) inv_ = np.linalg.inv(s) result = np.math.pow(np.math.e, -0.5 * (x_m.T * inv_ * x_m)) return const * result else: return -1#GMM主程序def GMM(): # 加载文件 input_file = open('points.dat') lines = input_file.readlines() Data = np.array([line.strip().split() for line in lines]).astype(np.float) (x, y) = np.shape(Data) mat.draw() mat.pause(0.01) mat.subplot(111) mat.plot(x, y, 'b*') learn = Data[np.math.ceil(x*0.8):x, 0:] train = Data[:np.math.floor(x*0.8), 0:] trainnum = 16 (W, Alpha, Mu, Sigma) = EM(train,trainnum) m = np.arange(-4.0, 4.0, 0.1) n = np.arange(-4.0, 4.0, 0.1) ax, ay = np.meshgrid(m, n) i = 0 prev = -9999 mat.clf() while(True): if(False): SigmaSum = np.sum(Sigma,2) for k in range(trainnum): Sigma[:,:,k] = SigmaSum W = Estep(train,Alpha,Mu,Sigma) Alpha,Mu,Sigma = Mstep(train,W) # trains = logLike(train,Alpha,Mu,Sigma) N,M = np.shape(train) P = np.zeros([N,len(Alpha)]) for k in range(len(Alpha)): for j in range(N): P[j,k] = multivariate(train[j,:][None].T,Mu[0:,k][None].T,Sigma[:,:,k]) trains = np.sum(np.log(P.dot(Alpha))) i = i + 1 #画图，训练和测试样本 mat.subplot(211) mat.scatter(train[0:,0],train[0:,1]) mat.hold(True) for k in range(0, trainnum): az = mlab.bivariate_normal(ax, ay, Sigma[0, 0, k], Sigma[1, \\ 1, k], Mu[0,k], Mu[1,k], Sigma[1, 0, k]) try: mat.contour(ax, ay, az) except: continue mat.hold(False) # Render these mat.draw() mat.pause(0.01) mat.subplot(212) mat.scatter(learn[0:,0],learn[0:,1]) mat.hold(True) for k in range(0, trainnum): az = mlab.bivariate_normal(ax, ay, Sigma[0, 0, k], Sigma[1, \\ 1, k], Mu[0,k], Mu[1,k], Sigma[1, 0, k]) try: mat.contour(ax, ay, az) except: continue mat.hold(False) if(i&gt;150 or abs(trains - prev)&lt; 0.01): break prev = trainsif __name__ == '__main__': GMM() 实验要求 实验目标 实现一个混合高斯模型，并且用EM算法估计模型中的参数。 实验过程 用混合高斯模型产生k个高斯分布的数据（其中参数自己设定），然后用你实现的EM算法估计参数，看看每次迭代后似然值变化情况，考察EM算法是否可以获得正确的结果（与你设定的结果比较）。 可以UCI上找一个简单问题数据，用你实现的GMM进行聚类。 算法原理 GMM算法 高斯模型就是用高斯概率密度函数（正态分布曲线）精确地量化事物，将一个事物分解为若干的基于高斯概率密度函数（正态分布曲线）形成的模型。 对图像背景建立高斯模型的原理及过程：图像灰度直方图反映的是图像中某个灰度值出现的频次，也可以认为是图像灰度概率密度的估计。如果图像所包含的目标区域和背景区域相比比较大，且背景区域和目标区域在灰度上有一定的差异，那么该图像的灰度直方图呈现双峰-谷形状，其中一个峰对应于目标，另一个峰对应于背景的中心灰度。对于复杂的图像，尤其是医学图像，一般是多峰的。通过将直方图的多峰特性看作是多个高斯分布的叠加，可以解决图像的分割问题。 在智能监控系统中，对于运动目标的检测是中心内容，而在运动目标检测提取中，背景目标对于目标的识别和跟踪至关重要。而建模正是背景目标提取的一个重要环节。 我们首先要提起背景和前景的概念，前景是指在假设背景为静止的情况下，任何有意义的运动物体即为前景。建模的基本思想是从当前帧中提取前景，其目的是使背景更接近当前视频帧的背景。即利用当前帧和视频序列中的当前背景帧进行加权平均来更新背景,但是由于光照突变以及其他外界环境的影响，一般的建模后的背景并非十分干净清晰，而高斯混合模型是是建模最为成功的方法之一。 混合高斯模型使用K（基本为3到5个）个高斯模型来表征图像中各个像素点的特征,在新一帧图像获得后更新混合高斯模型, 用当前图像中的每个像素点与混合高斯模型匹配,如果成功则判定该点为背景点, 否则为前景点。 通观整个高斯模型，主要是有方差和均值两个参数决定，对均值和方差的学习，采取不同的学习机制,将直接影响到模型的稳定性、精确性和收敛性 。由于我们是对运动目标的背景提取建模，因此需要对高斯模型中方差和均值两个参数实时更新。为提高模型的学习能力,改进方法对均值和方差的更新采用不同的学习率;为提高在繁忙的场景下,大而慢的运动目标的检测效果,引入权值均值的概念,建立背景图像并实时更新,然后结合权值、权值均值和背景图像对像素点进行前景和背景的分类。 具体实现过程： 为图像的每个像素点指定一个初始的均值、标准差以及权重。 收集N（一般取200以上，否则很难得到像样的结果）帧图像利用在线EM算法得到每个像 素点的均值、标准差以及权重。 从N+1帧开始检测，检测的方法： 对每个像素点： 将所有的高斯核按照 ω / σ 降序排序 选择满足下式的前M个高斯核：M = arg min(ω / σ &gt; T) 如果当前像素点的像素值在中有一个满足：就可以认为其为背景点。 更新背景图像，用EM算法。 EM算法 EM 算法是 Dempster，Laind，Rubin 于 1977 年提出的求参数极大似然估计的一种方法，它可以从非完整数据集中对参数进行 MLE 估计，是一种非常简单实用的学习算法。这种方法可以广泛地应用于处理缺损数据，截尾数据，带有噪声等所谓的不完全数据(incomplete data)。 最大期望算法经过两个步骤交替进行计算： 第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值； 第二步是最大化（M），最大化在 E 步上求得的最大似然值来计算参数的值。 M 步上找到的参数估计值被用于下一个 E 步计算中，这个过程不断交替进行。 通过交替使用这两个步骤，EM算法逐步改进模型的参数，使参数和训练样本的似然概率逐渐增大，最后终止于一个极大点。直观地理解EM算法，它也可被看作为一个逐次逼近算法：事先并不知道模型的参数，可以随机的选择一套参数或者事先粗略地给定某个初始参数λ0 ，确定出对应于这组参数的最可能的状态，计算每个训练样本的可能结果的概率，在当前的状态下再由样本对参数修正，重新估计参数λ，并在新的参数下重新确定模型的状态，这样，通过多次的迭代，循环直至某个收敛条件满足为止，就可以使得模型的参数逐渐逼近真实参数。 具体实现步骤： 给出种类数k,初始化每一类高斯分布的均值μk，方差∑k以及每一类的概率πk； 执行EM； 计算似然，如果没有达到预期效果，则返回第2步； 计算每个数据点对于k个高斯分布的似然，选择似然最大的一类作为数据的最终分类。 其他的混合模型，例如朴素贝叶斯混合模型也是可以使用EM算法推出使用的，这一算法虽然在GMM中作为参数使用，但是其仍然可以单独发挥作用。我觉得EM算法就是相互迭代（毕竟其由E和M两部分组成嘛），求出一个稳定值，而这种相互迭代的方法用的范围挺广的，例如混合模型，K-means等都需要使用。 与K-means的区别 在上文我也已经提到了EM算法可以用在K-means等其他需要迭代的方法上的事实，其实，我觉得GMM 和 K-means 很像，只不过后者要简单，而且相对来说实现并不是很高效。不过 GMM 是学习出一些概率密度函数来（所以 GMM 除了用在 clustering 上之外，还经常被用于 density estimation ），简单地说，K-means 的结果是每个数据点被 assign 到其中某一个 cluster 了，而 GMM 则给出这些数据点被 assign 到每个 cluster 的概率，又称作 soft assignment。 实验环境 Spyder 作为Python开发的集成开发环境； 编程语言：Python 3.5； 操作系统：macOS Sierra。 小结 GMM算法作为EM算法族的一个例子，它指定了各个参与杂合的分布都是高斯分布，即分布参数表现为均值Mu和方差Sigma。通过EM算法作为计算使用的框架，迭代地算出各个高斯分布的参数。 GMM与K-means的思考 提到GMM不得不提K-means，总结了网上的资料以及老师上课的课件，我将两者的区别与联系陈述如下： 两者的联系: 都是迭代执行的算法，且迭代的策略也相同：算法开始执行时先对需要计算的参数赋初值，然后交替执行两个步骤，一个步骤是对数据的估计（k-means是估计每个点所属簇；GMM是计算隐含变量的期望）；第二步是用上一步算出的估计值重新计算参数值，更新目标参数（k-means是计算簇心位置；GMM是计算各个高斯分布的中心位置和协方差矩阵） 两者的区别: 首先，两者需要计算的参数不同：K-means是簇心位置；GMM是各个高斯分布的参数；其次，两者计算目标参数的方法不同：K-means是计算当前簇中所有元素的位置的均值；GMM是基于概率的算法，是通过计算似然函数的最大值实现分布参数的求解的。 关于GMM引发的过拟合的思考 首先我想提到这样的一个“人辨认其他生物（例如鱼）”的例子。当我们被告知水里游的那个生物是鱼之后，我们会使用“在同样的地方生活的是同一种东西”这类似的假设，归纳出“在水里游的都是鱼”这样一个结论。当然这个过程是完全“本能”的，如果不仔细去想，我们也不会了解自己是怎样“认识鱼”的。另一个值得注意的地方是这样的假设并不总是完全正确的，甚至可以说总是会有这样那样的缺陷的，因为我们有可能会把虾、龟、甚至是潜水员当做鱼。也许你觉得可以通过修改前提假设来解决这个问题，例如，基于“生活在同样的地方并且穿着同样衣服的是同一种东西”这个假设，你得出结论：在水里有并且身上长有鳞片的是鱼。可是这样还是有问题，因为有些没有长鳞片的鱼现在又被你排除在外了。 机器在识别方面面临着和人一样的问题，在机器学习中，一个学习算法也会有一个前提假设，这里被称作“归纳偏执”。例如线性回归，目的是要找一个函数尽可能好地拟合给定的数据点，它的归纳偏执就是“满足要求的函数必须是线性函数”。一个没有归纳偏执的学习算法从某种意义上来说毫无用处，就像一个完全没有归纳能力的人一样，在第一次看到鱼的时候有人告诉他那是鱼，下次看到另一条鱼了，他并不知道那也是鱼，因为两条鱼总有一些地方不一样的，或者就算是同一条鱼，在河里不同的地方看到，或者只是看到的时间不一样，也会被他认为是不同的，因为他无法归纳，无法提取主要矛盾、忽略次要因素，只好要求所有的条件都完全一样──然而哲学家已经告诉过我们了：世界上不会有任何样东西是完全一样的，所以这个人即使是有无比强悍的记忆力，也绝学不到任何一点知识。 于是有了上面的铺垫，我们就可以引出论题——“过拟合 ”，就像前面的回归的问题，如果去掉“线性函数”这个归纳偏执，因为对于 N 个点，我们总是可以构造一个 N-1 次多项式函数，让它完美地穿过所有的这 N 个点，或者如果我用任何大于 N-1 次的多项式函数的话，我们甚至可以构造出无穷多个满足条件的函数出来。如果假定特定领域里的问题所给定的数据个数总是有个上限的话，我可以取一个足够大的 N ，从而得到一个（或者无穷多个）“超级函数”，能够拟合这个领域内所有的问题。 没有归纳偏执或者归纳偏执太宽泛会导致过拟合 ，然而另一个极端──限制过大的归纳偏执也是有问题的：如果数据本身并不是线性的，强行用线性函数去做回归通常并不能得到好结果（例如我在“实验一：多项式拟合曲线”中就做过相应的测试）。难点正在于在这之间寻找一个临界点。不过我们在这里相对于机器来说有一个很大的优势：人通常不会孤立地用某一个独立的系统和模型去处理问题，一个人每天都会从各个来源获取大量的信息，并且通过各种手段进行整合处理，归纳所得的所有知识最终得以统一地存储起来，并能有机地组合起来去解决特定的问题。 以上就是我关于“过拟合”的一点不全面的思考！ The link of this page is http://home.meng.uno/articles/177fbbcc/ . Welcome to reproduce it!","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://home.meng.uno/categories/Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://home.meng.uno/tags/机器学习/"},{"name":"EM","slug":"EM","permalink":"http://home.meng.uno/tags/EM/"},{"name":"GMM","slug":"GMM","permalink":"http://home.meng.uno/tags/GMM/"}]},{"title":"Android新闻软件编写","slug":"androidnews","date":"2016-12-16T07:01:14.000Z","updated":"2020-12-02T01:40:58.000Z","comments":true,"path":"articles/8fb3f6d8/","link":"","permalink":"http://home.meng.uno/articles/8fb3f6d8/","excerpt":"当我开始学安卓开发时，我发现网上最多的教程就是关于Android上的新闻客户端开发的（而且课时特别长），我本人觉得完全是那些上传网课的人想拉时长牟利，在写完listview之后，因为我们的《软设》项目需要，我也来做做“新闻页”，我只写显示过程（不涉及爬虫），只是为了记录下开发过程供初学者及日后自己回顾。 首先，我在values/string目录加上如下条目，用作显示（内容无关）： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36","text":"当我开始学安卓开发时，我发现网上最多的教程就是关于Android上的新闻客户端开发的（而且课时特别长），我本人觉得完全是那些上传网课的人想拉时长牟利，在写完listview之后，因为我们的《软设》项目需要，我也来做做“新闻页”，我只写显示过程（不涉及爬虫），只是为了记录下开发过程供初学者及日后自己回顾。 首先，我在values/string目录加上如下条目，用作显示（内容无关）： 123456789101112131415161718192021222324252627282930313233343536 &lt;string name=\"title\"&gt;是上帝除四害是的次数不多是彻底那就是地产表示比不对是不死都今?&lt;/string&gt; &lt;string name=\"text\"&gt;我是测手术储备货币结算你说的内存金士顿内存就剧场那就看到手残你从从今年刷卡才能加内存显卡才能收到你今年的你朝鲜才能常出现从侠客行朝鲜&lt;/string&gt; &lt;string-array name=\"text_arr\"&gt; &lt;item&gt;bids比貂蝉死不打算比赛的buds不v电话苏帮你吧报错还加班猝死地产表示出版社u白崇禧必须&lt;/item&gt; &lt;item&gt;但是不撒手撒就行字数限制到6个字，多的用省略号，是设置什么属学校决定停止晚自习，我和同学一起回荚冬 我是在那次孝雅之星的事迹报告会上认识她的。当轮到她上场的时候，主持人这样说道：“她是一个外表刚强，内心柔弱的人。”话音刚落，她班上的同学一阵哄堂大笑。 我用疑惑的眼神看了看身旁的同学，她告诉我，在她们班上谁都知道她是一个标准的女汉子。她的话勾起了我的好奇心，使我有了想听听她的故事的欲望。 她步履坚定的走上了演讲台，先是向我们深深地鞠了一躬，便开始娓娓道来她的事迹。 她的父母都在外打拼，忙于事业，所以很少有空去照顾她，所以家中的一切只能由她一人包办。后来，她又有了一个妹妹，致使她身上的担子又加重了许多，但她依然十分 坚强的承担这一切。她说妹妹的到来对于她来说像一个天使一样美好，而并不是觉得她是一个负担。当我们回到家中扑在父母怀里撒娇时，她在床头哄着妹妹入睡；当我作文网 http://wwW.zuoWen8.Com/们安稳的进入梦乡时，她却还在奋笔疾书。夜的黑暗与漫长，只有她才知道；思念的感受有多浓稠，只有她才知道；内心的压抑有多难受，只有她才知 道。 当她说到妹妹因调皮将很烫的饭菜洒到她的手臂上，她还得继续给妹妹喂饭时，当她提及妹妹做坏事后她忍气吞声的到别人家中道歉时，她哽咽了，将头扭到一边独自抹眼泪。 我们沉默了，低头不语。忧伤的气息迅速在全场蔓延，每个人的心都在和她共鸣着，有好几次，她正准备开口时，却都卡在了喉咙，全场为她响起了雷鸣般经久不息的掌声。 &lt;/item&gt; &lt;item&gt;看到他我想到了爸爸，幸好他今天不上班，不用冒那么大的雪，假如哪个人是我爸爸，我多么希看有一个好心人上前伸出一只手，帮他一把。假如那是你，&lt;/item&gt; &lt;item&gt;我停了车子，想往帮助他，可我也象那些来来往往的行人一样，脚步并没有动，的确有很多人同情他，同情也的确对他没用，他还是站不起来，一遍一遍看他起来又摔倒，只好转过头， 不看他，疼痛无奈，一个中年男子的窘态在众人眼前暴露无遗，这时的他没有一点男子汉的心胸。&lt;/item&gt; &lt;item&gt;走到一个小岔路口时，我看到路的另一边一个中年男子坐在地上，他穿着青色衣服，双手扶地，似乎挣扎着坐起来，一次又一次尝试着。旁边躺着他的尽看的大梁自行车，等待着主人扶 起它，在这路上最难过的就是他们了吧！也只有他们可以相互安慰。&lt;/item&gt; &lt;item&gt;春天，春姑娘带来了蒙蒙细雨和柔和的春风，并把它们化作一只大画笔，把绿色涂在草坪上。这时，无数只小燕子从远方飞来，在草坪上飞来飞去。&lt;/item&gt; &lt;item&gt;夏天，草坪旁的花坛里，月季花欣然怒放，引来了勤劳的小蜜蜂和翩翩起舞的蝴蝶，热闹极了。&lt;/item&gt; &lt;item&gt;冬天，雪花落到草坪上，给草坪盖上了一层厚厚的棉被，来年，小草更加茁壮成长。&lt;/item&gt; &lt;item&gt;草坪就像一个氧气袋，它通过光合作用，净化空气，美化环境。我们要爱护学校的草坪。&lt;/item&gt; &lt;/string-array&gt; &lt;string-array name=\"title_arr\"&gt; &lt;item&gt;死地产表示出版社u白崇禧必须&lt;/item&gt; &lt;item&gt;停止晚自习，我和同学一起回荚冬&lt;/item&gt; &lt;item&gt;假如哪个人是我爸爸，我多么希看有一个好心人上前伸出一只手，帮他一把。假如那是你，&lt;/item&gt; &lt;item&gt;我停了的行人一样，脚步并没有动，的确有很多人同情他，同情也的确对他没用，他还是站不起来，一遍一遍看他起来又摔倒，只好转过头，他没有一点男子汉的心胸。&lt;/item&gt; &lt;item&gt;他穿着青色衣服，双手扶地，似乎挣扎着坐起来，一次又一次尝试着。旁边躺着他的尽看的大梁自行车，等待着主人扶&lt;/item&gt; &lt;item&gt;春天。&lt;/item&gt; &lt;item&gt;夏天极了。&lt;/item&gt; &lt;item&gt;冬天，雪花落到&lt;/item&gt; &lt;item&gt;草坪就像的草坪。&lt;/item&gt; &lt;/string-array&gt; 接着编写显示主界面： 123456789101112131415161718192021222324252627 public class MainActivity extends AppCompatActivity&#123; private ListView listview; //private ArrayAdapter&lt;String&gt;arr_adapter; private SimpleAdapter simp_Adapter; private List&lt;Map&lt;String,String&gt;&gt;datalist; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); listview = (ListView)findViewById(R.id.listview); datalist = new ArrayList&lt;Map&lt;String,String&gt;&gt;(); simp_Adapter = new SimpleAdapter(this,getData(),R.layout.item, new String[]&#123;\"title\",\"text\"&#125;,new int[]&#123;R.id.title,R.id.text&#125;); listview.setAdapter(simp_Adapter); &#125; private List&lt;Map&lt;String,String&gt;&gt; getData()&#123; String[] data_text = getResources().getStringArray(R.array.text_arr); String[] data_title = getResources().getStringArray(R.array.title_arr); for(int i=0;i&lt;data_text.length;i++)&#123; Map&lt;String,String&gt;map = new HashMap&lt;String,String&gt;(); map.put(\"title\",data_title[i]); map.put(\"text\",data_text[i]); datalist.add(map); &#125; return datalist; &#125;&#125; 其对应的activity_main.xml文件为： 12345678910111213141516171819 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:id=\"@+id/activity_main\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:paddingBottom=\"@dimen/activity_vertical_margin\" android:paddingTop=\"@dimen/activity_vertical_margin\" tools:context=\"com.example.lpf.test.MainActivity\"&gt; &lt;ListView android:id=\"@+id/listview\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:layout_alignParentBottom=\"true\" android:layout_alignParentStart=\"true\" android:background=\"@color/bg\" android:divider=\"@color/item_item\" android:dividerHeight=\"10dp\"/&gt;&lt;/RelativeLayout&gt; 再然后，编写点击后的后台跳转逻辑： 1234567891011121314 public class ShowActivity extends AppCompatActivity &#123; protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_show); Bundle bundle = getIntent().getExtras(); String title = bundle.getString(\"title\"); String text = bundle.getString(\"text\"); TextView title_view = (TextView) findViewById(R.id.title); title_view.setText(title); TextView text_view = (TextView) findViewById(R.id.text); text_view.setMovementMethod(ScrollingMovementMethod.getInstance()); text_view.setText(text); &#125;&#125; 对应的activity_show.xml文件如下： 123456789101112131415161718192021222324252627 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:orientation=\"vertical\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:paddingBottom=\"@dimen/activity_vertical_margin\" android:paddingLeft=\"@dimen/activity_horizontal_margin\" android:paddingRight=\"@dimen/activity_horizontal_margin\" android:paddingTop=\"@dimen/activity_vertical_margin\"&gt; &lt;TextView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:id=\"@+id/title\" android:text=\"@string/title\" android:textSize=\"22sp\" android:typeface=\"monospace\" android:background=\"@color/itembg\"/&gt; &lt;TextView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:id=\"@+id/text\" android:textSize=\"18sp\" android:textColor=\"@color/textbg\" android:typeface=\"normal\" android:scrollbars=\"vertical\" android:text=\"@string/text\"/&gt;&lt;/LinearLayout&gt; 最后就是负责点击跳转的任务的后台程序了： 1234567891011121314151617181920212223 public class TestActivity extends ListActivity &#123; String[] data = &#123;\"北京\",\"西安\",\"广州\",\"上海\"&#125;; ListView lstview; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); lstview = (ListView)findViewById(R.id.listview); lstview.setOnItemClickListener(new AdapterView.OnItemClickListener()&#123; @Override public void onItemClick(AdapterView&lt;?&gt; parent, View view, int position, long id) &#123; String s = data[position]; &#125; &#125;); ArrayAdapter&lt;String&gt; adapter = new ArrayAdapter&lt;String&gt;( this, R.layout.item, R.id.listview, data ); lstview.setAdapter(adapter); &#125;&#125; 附加一个item.xml用于接收显示： 1234567891011121314151617181920212223242526272829 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:orientation=\"vertical\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:paddingBottom=\"@dimen/activity_vertical_margin\" android:paddingLeft=\"@dimen/activity_horizontal_margin\" android:paddingRight=\"@dimen/activity_horizontal_margin\" android:paddingTop=\"@dimen/activity_vertical_margin\" android:background=\"@drawable/white_bg\"&gt; &lt;TextView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:id=\"@+id/title\" android:text=\"@string/title\" android:textSize=\"20sp\" android:typeface=\"monospace\" android:background=\"@color/itembg\"/&gt; &lt;TextView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:id=\"@+id/text\" android:textSize=\"15sp\" android:textColor=\"@color/textbg\" android:typeface=\"normal\" android:maxLines=\"3\" android:ellipsize=\"end\" android:text=\"@string/text\"/&gt;&lt;/LinearLayout&gt; 至此一个新闻客户端基本框架就已经编写完毕！ The link of this page is http://home.meng.uno/articles/8fb3f6d8/ . Welcome to reproduce it!","categories":[{"name":"Application","slug":"Application","permalink":"http://home.meng.uno/categories/Application/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://home.meng.uno/tags/Android/"},{"name":"新闻软件","slug":"新闻软件","permalink":"http://home.meng.uno/tags/新闻软件/"}]},{"title":"Linux 0.11启动引导","slug":"os","date":"2016-12-16T06:52:23.000Z","updated":"2020-12-02T02:00:27.000Z","comments":true,"path":"articles/6c138ff9/","link":"","permalink":"http://home.meng.uno/articles/6c138ff9/","excerpt":"Linux引导启动程序程序在boot目录下，有bootset.s, head.s和setup.s（编译后），其中： bootset.s 系统启动时首先是进入实模式，从地址0xffff0（这地址映射的rom-bios在内存的地址）处开始执行bios代码，然后执行系统检测（也就是自检过程）,然后初始化实模式的中断向量表(实模式中断向量在内存物理地址0处)。然后将启动设备的第一个扇区（512字节，也就是bootset.s编译完成的内容）内容读取到内存0x7c00(31kB)处，并且跳转到这里。 跳转到bootset.s后，bootset.s主要做如下工作： bootset.s在最前面的几句代码先将","text":"Linux引导启动程序程序在boot目录下，有bootset.s, head.s和setup.s（编译后），其中： bootset.s 系统启动时首先是进入实模式，从地址0xffff0（这地址映射的rom-bios在内存的地址）处开始执行bios代码，然后执行系统检测（也就是自检过程）,然后初始化实模式的中断向量表(实模式中断向量在内存物理地址0处)。然后将启动设备的第一个扇区（512字节，也就是bootset.s编译完成的内容）内容读取到内存0x7c00(31kB)处，并且跳转到这里。 跳转到bootset.s后，bootset.s主要做如下工作： bootset.s在最前面的几句代码先将自己移动到内存0x90000（576kB）处； bootset.s将启动设备第2个扇区到第五个扇区内容（4个扇区里面存放的是setup.s的内容）读取到内存0x90200处，也就是bootset.s后面； 将内核其他模块读取到0x10000（64KB）处，读取的大小为192KB，对于当时的内核来说确实是足够大了； 在bootset.s偏移508处定义了根文件系统的设备号，并且根据编译选项进行了赋值操代码默认启动驱动器是软盘a，然后就是bootset.s,setup.s,和内核镜像都成放在软盘a中; setup.s 将系统的一些参数存放在0x90000处,覆盖之前的bootset.s,参数主要包括，内存大小，硬盘参数，显存的参数信息以及根文件系统的设备号; 定义了GDT表，最后加载了gdtr和ldtr，最后跳到保护模式GDT表定义在setup.s,也就是在0x90200的那段内存中，LDT还没有定义。 head.s 其被编译到内核镜像中。重新定义了GDT表(目前就一个第二个段描述符有效)和并定义LDT表（表中中断处理程序目前还是指向一个默认的处理程序），并加载相应的寄存器； 内存开始处设置页目录表，一共有4个叶目录，初始化页目录，然后开启分页，最后跳到主函数main()。 至此，系统启动引导完成。 The link of this page is http://home.meng.uno/articles/6c138ff9/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://home.meng.uno/tags/Linux/"},{"name":"操作系统","slug":"操作系统","permalink":"http://home.meng.uno/tags/操作系统/"},{"name":"启动引导","slug":"启动引导","permalink":"http://home.meng.uno/tags/启动引导/"}]},{"title":"Android开发中的小细节","slug":"android","date":"2016-12-15T05:58:11.000Z","updated":"2021-01-07T06:59:53.000Z","comments":true,"path":"articles/5d6c9819/","link":"","permalink":"http://home.meng.uno/articles/5d6c9819/","excerpt":"Android的ListView的使用 可能我们在手机APP上使用的最多的视图就是列表了，那么Android列表（ListView）该怎么使用呢？ 首先还是显示界面activity_main.xml: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19","text":"Android的ListView的使用 可能我们在手机APP上使用的最多的视图就是列表了，那么Android列表（ListView）该怎么使用呢？ 首先还是显示界面activity_main.xml: 12345678910111213141516171819 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:id=\"@+id/activity_main\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:paddingBottom=\"@dimen/activity_vertical_margin\" android:paddingTop=\"@dimen/activity_vertical_margin\" tools:context=\"com.example.lpf.test.MainActivity\"&gt; &lt;ListView android:id=\"@+id/listview\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:layout_alignParentBottom=\"true\" android:layout_alignParentStart=\"true\" android:background=\"@color/bg\" android:divider=\"@color/item_item\" android:dividerHeight=\"10dp\"/&gt;&lt;/RelativeLayout&gt; 之后是其对应的MainActivity.java文件： 123456789101112131415161718192021222324252627 public class MainActivity extends AppCompatActivity&#123; private ListView listview; //private ArrayAdapter&lt;String&gt;arr_adapter; private SimpleAdapter simp_Adapter; private List&lt;Map&lt;String,String&gt;&gt;datalist; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); listview = (ListView)findViewById(R.id.listview); datalist = new ArrayList&lt;Map&lt;String,String&gt;&gt;(); simp_Adapter = new SimpleAdapter(this,getData(),R.layout.item, new String[]&#123;\"title\",\"text\"&#125;,new int[]&#123;R.id.title,R.id.text&#125;); listview.setAdapter(simp_Adapter); &#125; private List&lt;Map&lt;String,String&gt;&gt; getData()&#123; String[] data_text = getResources().getStringArray(R.array.text_arr); String[] data_title = getResources().getStringArray(R.array.title_arr); for(int i=0;i&lt;data_text.length;i++)&#123; Map&lt;String,String&gt;map = new HashMap&lt;String,String&gt;(); map.put(\"title\",data_title[i]); map.put(\"text\",data_text[i]); datalist.add(map); &#125; return datalist; &#125;&#125; 其他文件保持不变即可。 至此，一个Android列表程序就实现了。 Android实现输入框回车输入 用惯了iOS的各位在开发安卓程序或者使用安卓手机时，都会遇到这样一个问题：原本在iOS上都是回车输入，而到了Android上却需要点击按钮完成输入（对比两个系统上的QQ就发现了）。我一直在使用iOS系统，因为《软设》才着手Android开发，所以我就想能不能像iOS上的那样实现一个输入框+回车符完成输入呢？经过我查找资料，发现确实可以： 12345678910111213141516 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:orientation=\"vertical\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:weightSum=\"1\"&gt; &lt;EditText android:id=\"@+id/edit_message\" android:layout_margin=\"30dp\" android:layout_width=\"match_parent\" android:layout_height=\"80dp\" android:hint=\"请输入文本信息 ...\" android:imeOptions=\"actionSearch\" android:singleLine=\"true\"/&gt;&lt;/LinearLayout&gt; 在EditText中加入了imeOptions就可以将回车符转变成各种各样的功能： actionDone——回车符–&gt;完成 actionSend——回车符–&gt;发送 actionGo——回车符–&gt;前进 actionNext——回车符–&gt;下一项 actionNone——回车符–&gt;无动作 actionPrevious——回车符–&gt;上一项 actionSearch——回车符–&gt;搜索 actionUnspecified——回车符–&gt;未指定 actionSend——回车符–&gt;发送 又查阅资料发现：ime是Input Method Editors的缩写，也就是输入法编辑器，原来如此，不过想使用这个属性，必须加上android:inputType 或者 android:singleline=&quot;true&quot; 至此，就完成了Android回车符向iOS的转化！ Android页面跳转 前情提要 开发安卓单页面程序久了，必然会思考怎么开发像现在一般Android应用程序那样的多页面（指页面有跳转）程序，我也是在搜索了别人的博客之后，才总结出如下的这点精华步骤！ 编写AndroidManifest.xml 首先，我们要确定我们需要怎样的跳转，既然跳转，无非就是自动跳转或者点击按钮，无论哪种，首先我们必须有两个界面（至少），所以在AndroidManifest.xml中，我们需要这样写： 123456789101112131415161718192021222324 &lt;manifest xmlns:android=\"http://schemas.android.com/apk/res/android\" package=\"uno.meng.download\"&gt; &lt;application android:allowBackup=\"true\" android:icon=\"@mipmap/ic_launcher\" android:theme=\"@style/AppTheme\"&gt; &lt;activity android:name=\".MainActivity\" android:label=\"@string/app_name\"&gt; &lt;intent-filter&gt; &lt;action android:name=\"android.intent.action.MAIN\" /&gt; &lt;category android:name=\"android.intent.category.LAUNCHER\" /&gt; &lt;/intent-filter&gt; &lt;/activity&gt; &lt;activity android:name=\".ResultActivity\" android:label=\"@string/comeback\" android:parentActivityName=\".MainActivity\" &gt; &lt;meta-data android:name=\"android.support.PARENT_ACTIVITY\" android:value=\".MainActivity\"/&gt; &lt;/activity&gt; &lt;/application&gt;&lt;/manifest&gt; 其中，每个对应一个界面，从代码中可见，我将后一个页面加了一个返回前一个页面的“返回符”。 编写跳转前界面search.xml 由于我在此将介绍怎么使用按钮跳转（带输入），所以直接在主界面search.xml（名称随意）中声明这两个组件（按钮，输入框）： 123456789101112131415161718192021 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:orientation=\"vertical\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:weightSum=\"1\"&gt; &lt;EditText android:id=\"@+id/edit_message\" android:layout_margin=\"30dp\" android:layout_width=\"match_parent\" android:layout_height=\"80dp\" android:hint=\"请输入文本信息 ...\"/&gt; &lt;Button android:id=\"@+id/button\" android:text=\"点击提交 \" android:layout_margin=\"100dp\" android:layout_width=\"127dp\" android:layout_height=\"wrap_content\" android:onClick=\"sendMessage\" /&gt;&lt;/LinearLayout&gt; 我对按钮加了一个onClick事件。 编写对应的MainActivity.java 在search.xml对应的MainActivity.java文件中我们写好onCreate方法（每个文件都会有）以及sendMessage方法： 123456789101112131415 public class MainActivity extends AppCompatActivity &#123; public final static String EXTRA_MESSAGE = \"uno.meng.download.MESSAGE\"; public void sendMessage()&#123; EditText editText = (EditText)findViewById(R.id.edit_message); String message = editText.getText().toString(); Intent intent = new Intent(this, ResultActivity.class); intent.putExtra(EXTRA_MESSAGE,message); startActivity(intent); &#125; @Override protected void onCreate(Bundle savedInstanceState)&#123; super.onCreate(savedInstanceState); setContentView(R.layout.search); &#125;&#125; 编写接收界面result.xml 然后到result.xml接收（我用的一个框来接收）： 123456789101112 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:orientation=\"vertical\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:weightSum=\"1\"&gt; &lt;TextView android:layout_margin=\"30dp\" android:layout_width=\"match_parent\" android:layout_height=\"80dp\"/&gt;&lt;/LinearLayout&gt; 编写接收对应的ResultActivity.java 编写对应的ResultActivity.java文件， 将从MainActivity.java接收来的文字打印到result.xml的框中： 123456789101112131415 public class ResultActivity extends AppCompatActivity&#123; private Intent intent; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.result); intent = getIntent(); String message = intent.getStringExtra(MainActivity.EXTRA_MESSAGE); System.out.println(message); TextView textview = new TextView(this); textview.setTextSize(100); textview.setText(message); setContentView(textview); &#125;&#125; 到此为止，已经完成了Android页面跳转！ Android 自定义toolBar上的 action item 自定义的view，action_view_auto_like.xml 123456789101112131415 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;FrameLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\"&gt; &lt;ImageButton android:id=\"@+id/action_auto_like_button\" android:layout_width=\"70dp\" android:layout_height=\"30dp\" android:scaleType=\"centerInside\" android:background=\"@android:color/transparent\" android:src=\"@drawable/btn_nav_autoliker\"&gt; &lt;/ImageButton&gt;&lt;/FrameLayout&gt; 自定义的menu，menu_tinder_liker.xml 123456789101112 &lt;menu xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tinder=\"http://schemas.android.com/apk/res-auto\"&gt; &lt;item android:id=\"@+id/action_auto\" android:actionLayout=\"@layout/action_view_auto_like\" android:icon=\"@drawable/btn_nav_autoliker\" android:orderInCategory=\"1\" android:title=\"@string/action_auto_like\" tinder:actionLayout=\"@layout/action_view_auto_like\" tinder:showAsAction=\"always\"/&gt;&lt;/menu&gt; 在fragment 中配置 现在onCreateView中加上setHasOptionsMenu(true);，让系统在fragment中初始化menu。 12345678910 @Overridepublic void onCreateOptionsMenu(Menu menu, MenuInflater inflater) &#123; inflater.inflate(R.menu.menu_tinder_like, menu); MenuItem searchItem = menu.findItem(R.id.action_auto); FrameLayout layout = (FrameLayout) MenuItemCompat .getActionView(searchItem); layout.findViewById(R.id.action_auto_like_button) .setOnClickListener(this); super.onCreateOptionsMenu(menu, inflater);&#125; 然后在onClick中加入你的逻辑。 The link of this page is http://home.meng.uno/articles/5d6c9819/ . Welcome to reproduce it!","categories":[{"name":"Application","slug":"Application","permalink":"http://home.meng.uno/categories/Application/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://home.meng.uno/tags/Android/"},{"name":"ListView","slug":"ListView","permalink":"http://home.meng.uno/tags/ListView/"},{"name":"toolBar","slug":"toolBar","permalink":"http://home.meng.uno/tags/toolBar/"},{"name":"ActionItem","slug":"ActionItem","permalink":"http://home.meng.uno/tags/ActionItem/"},{"name":"页面跳转","slug":"页面跳转","permalink":"http://home.meng.uno/tags/页面跳转/"},{"name":"回车输入","slug":"回车输入","permalink":"http://home.meng.uno/tags/回车输入/"}]},{"title":"Java导出Excel","slug":"java2excel","date":"2016-12-06T14:16:48.000Z","updated":"2020-12-02T01:50:27.000Z","comments":true,"path":"articles/7725d215/","link":"","permalink":"http://home.meng.uno/articles/7725d215/","excerpt":"完成这个实验，你需要下载jxljar包，具体方法自行百度。 接下来我将直接使用具体代码进行讲解我的实现过程。 文件名：ExcelAction.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 7","text":"完成这个实验，你需要下载jxljar包，具体方法自行百度。 接下来我将直接使用具体代码进行讲解我的实现过程。 文件名：ExcelAction.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 package net.kuangmeng.excel;/*这里是所有需要导入的库，不用担心当你写好其他代码时，编辑器会提示或者自动帮你补全！*/import java.io.File;import java.io.FileOutputStream;import java.io.IOException;import java.util.ArrayList;import java.util.List;import com.opensymphony.xwork2.ActionSupport;import jxl.Workbook;import jxl.format.Alignment;import jxl.format.Colour;import jxl.format.UnderlineStyle;import jxl.format.VerticalAlignment;import jxl.write.Label;import jxl.write.WritableCellFormat;import jxl.write.WritableFont;import jxl.write.WritableSheet;import jxl.write.WritableWorkbook;import jxl.write.WriteException;import jxl.write.biff.RowsExceededException;@SuppressWarnings(\"serial\")public class ExcelAction extends ActionSupport&#123; public static void main(String[] args) &#123; //主函数调用 //listth为导出excel的表头信息 list&lt;String&gt; listth = new ArrayList&lt;String&gt;; //listtd为导出的excel表项 list&lt;String&gt; listtd = new ArrayList&lt;String&gt;; //num为表的列数 int num ; exportExcel(tablename,listth,listtd,num); &#125;//真正的导出excel方法 public static void exportExcel(String fileName,List&lt;String&gt; listth,List&lt;String&gt; listtd,int num) &#123; //设置保存文件具体位置及文件名 String excelName =\"C:\\\\Users\\\\meng\\\\Desktop\\\\\"+fileName+\".xls\"; try &#123; File excelFile = new File(excelName); // 如果文件存在就删除它 if (excelFile.exists()) excelFile.delete(); // 打开文件 WritableWorkbook book = Workbook.createWorkbook(excelFile); // 生成名为“第一页”的工作表，参数0表示这是第一页 WritableSheet sheet = book.createSheet(\"Up2U导出表格 \", 0); // 文字样式 jxl.write.WritableFont wfc = new jxl.write.WritableFont( WritableFont.ARIAL, 10, WritableFont.NO_BOLD, false, UnderlineStyle.NO_UNDERLINE, jxl.format.Colour.BLACK); jxl.write.WritableCellFormat wcfFC = new jxl.write.WritableCellFormat( wfc); jxl.write.WritableCellFormat wcfF = new jxl.write.WritableCellFormat(wfc); wcfF.setBackground(jxl.format.Colour.BLACK); // 设置单元格样式 wcfFC.setBackground(jxl.format.Colour.GRAY_25);// 单元格颜色 wcfFC.setAlignment(jxl.format.Alignment.CENTRE);// 单元格居中 // 在Label对象的构造子中指名单元格位置是第一列第一行(0,0) // 以及单元格内容为 for(int i=0;i&lt;listth.size()/(num-2);i++)&#123; for(int j=0;j&lt;num-2;j++)&#123; sheet.addCell(new Label(j,i,listth.get(i*(num-2)+j),wcfFC)); &#125; &#125; for(int i=listth.size()/(num-2);i&lt;(listth.size()+listtd.size())/(num-2);i++)&#123; for(int j=0;j&lt;num-2;j++)&#123; sheet.addCell(new Label(j,i,listtd.get((i-1)*(num-2)+j),wcfF)); &#125; &#125; // 写入数据并关闭文件 book.write(); book.close(); System.out.println(\"Excel创建成功\"); &#125; catch (Exception e) &#123; System.out.println(e); &#125; &#125;&#125; The link of this page is http://home.meng.uno/articles/7725d215/ . Welcome to reproduce it!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://home.meng.uno/categories/Software-Engineering/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://home.meng.uno/tags/软件工程/"}]},{"title":"Some errors when using macOS","slug":"mac_errors","date":"2016-12-06T13:24:05.000Z","updated":"2021-01-06T18:03:04.000Z","comments":true,"path":"articles/115ed5e0/","link":"","permalink":"http://home.meng.uno/articles/115ed5e0/","excerpt":"Mac sudo命令无法使用 在之前好长一段时间，不知道因为我改动了哪个文件的权限，导致sudo命令无法使用，每次启动sudo总会报什么权限不对的错误，在网上找了好久都没找到解决办法，包括stackoverflow上面问题采纳的方案都无济于事，今天闲来无事，又想解决这个问题，这次我是直接进苹果的“Mac 支持”上看的，发现Mac有个单用户模式（在此给出连接），我进入单用户模式，然后就是一个黑框框，在里面输入以下几条命令： 1 2 3 mount -uw / chown root:wheel /etc/sudoers chmod 440 /etc/sudoers 大致就是恢复文件权限之","text":"Mac sudo命令无法使用 在之前好长一段时间，不知道因为我改动了哪个文件的权限，导致sudo命令无法使用，每次启动sudo总会报什么权限不对的错误，在网上找了好久都没找到解决办法，包括stackoverflow上面问题采纳的方案都无济于事，今天闲来无事，又想解决这个问题，这次我是直接进苹果的“Mac 支持”上看的，发现Mac有个单用户模式（在此给出连接），我进入单用户模式，然后就是一个黑框框，在里面输入以下几条命令： 123 mount -uw /chown root:wheel /etc/sudoerschmod 440 /etc/sudoers 大致就是恢复文件权限之类的吧，结果reboot之后，居然就好了😝。 特此记录以下，给出现同种问题的小伙伴提供下。 Mac MySQL无法启动 正像这次博客的日期那样，《软工》大项目接近尾声了，然而直到今天我才真正解决了这个大难题——Mac MySQL无法使用！！！ 解决方案 检查MySQL是否成功安装 1 mysql --version 关闭MySQL连接（即使没连也无妨） 1 sudo /usr/local/mysql/support-files/mysql.server stop 登录管理员 12 cd /usr/local/mysql/bin/sudo su 禁止MySQL验证来登录（此时不验证密码） 1 ./mysqld_safe --skip-grant-tables &amp; （此时应该成功进入mysql&gt;）设置密码 1 UPDATE mysql.user SET authentication_string=PASSWORD('*****') WHERE User='root'; （若显示密码过期）设置密码永不过期 1 ALTER USER 'root'@'localhost' PASSWORD EXPIRE NEVER; 刷新MySQL的系统权限 1 flush privileges; 至此应该来说MySQL应该好使了。 The link of this page is http://home.meng.uno/articles/115ed5e0/ . Welcome to reproduce it!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://home.meng.uno/categories/Software-Engineering/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://home.meng.uno/tags/软件工程/"},{"name":"MySQL","slug":"MySQL","permalink":"http://home.meng.uno/tags/MySQL/"},{"name":"sudo","slug":"sudo","permalink":"http://home.meng.uno/tags/sudo/"}]},{"title":"Java发送邮件","slug":"javamail","date":"2016-12-04T14:30:34.000Z","updated":"2020-12-02T01:50:41.000Z","comments":true,"path":"articles/21f3b9c2/","link":"","permalink":"http://home.meng.uno/articles/21f3b9c2/","excerpt":"我觉得学会Java mail是一件很自豪的事，怎么说呢，邮箱这么有逼格的东西都能被你玩的很溜的话，一定不一般。 本次试验使用了javax.mail.jarjar包，请自行百度下载。 我实现的Java mail主要包括4个部分： 1. 发送邮件使用的基本信息 2. 邮件发送器 3. 发件人设置 4. 实际发送 四个部分组成。 发送邮件使用的基本信息 文件名：MailSenderInfo.java代码如下，我仍然以备注的形式讲解： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 2","text":"我觉得学会Java mail是一件很自豪的事，怎么说呢，邮箱这么有逼格的东西都能被你玩的很溜的话，一定不一般。 本次试验使用了javax.mail.jarjar包，请自行百度下载。 我实现的Java mail主要包括4个部分： 发送邮件使用的基本信息 邮件发送器 发件人设置 实际发送 四个部分组成。 发送邮件使用的基本信息 文件名：MailSenderInfo.java 代码如下，我仍然以备注的形式讲解： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495 package net.kuangmeng.mail; /** * 发送邮件需要使用的基本信息 */ import java.util.Properties; public class MailSenderInfo &#123; // 发送邮件的服务器的IP和端口 private String mailServerHost; private String mailServerPort = \"25\"; // 邮件发送者的地址 private String fromAddress; // 邮件接收者的地址 private String toAddress; // 登陆邮件发送服务器的用户名和密码 private String userName; private String password; // 是否需要身份验证 private boolean validate = false; // 邮件主题 private String subject; // 邮件的文本内容 private String content; // 邮件附件的文件名 private String[] attachFileNames; /** * 获得邮件会话属性 */ public Properties getProperties()&#123; Properties p = new Properties(); p.put(\"mail.smtp.host\", this.mailServerHost); p.put(\"mail.smtp.port\", this.mailServerPort); p.put(\"mail.smtp.auth\", validate ? \"true\" : \"false\"); return p; &#125; public String getMailServerHost() &#123; return mailServerHost; &#125; public void setMailServerHost(String mailServerHost) &#123; this.mailServerHost = mailServerHost; &#125; public String getMailServerPort() &#123; return mailServerPort; &#125; public void setMailServerPort(String mailServerPort) &#123; this.mailServerPort = mailServerPort; &#125; public boolean isValidate() &#123; return validate; &#125; public void setValidate(boolean validate) &#123; this.validate = validate; &#125; public String[] getAttachFileNames() &#123; return attachFileNames; &#125; public void setAttachFileNames(String[] fileNames) &#123; this.attachFileNames = fileNames; &#125; public String getFromAddress() &#123; return fromAddress; &#125; public void setFromAddress(String fromAddress) &#123; this.fromAddress = fromAddress; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public String getToAddress() &#123; return toAddress; &#125; public void setToAddress(String toAddress) &#123; this.toAddress = toAddress; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getSubject() &#123; return subject; &#125; public void setSubject(String subject) &#123; this.subject = subject; &#125; public String getContent() &#123; return content; &#125; public void setContent(String textContent) &#123; this.content = textContent; &#125; &#125; 邮件发送器 文件名：SimpleMailSender.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105 package net.kuangmeng.mail;import java.util.Date; import java.util.Properties; import javax.mail.Address; import javax.mail.BodyPart; import javax.mail.Message; import javax.mail.MessagingException; import javax.mail.Multipart; import javax.mail.Session; import javax.mail.Transport; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeBodyPart; import javax.mail.internet.MimeMessage; import javax.mail.internet.MimeMultipart; /** * 简单邮件（不带附件的邮件）发送器 */ public class SimpleMailSender &#123; /** * 以文本格式发送邮件 * @param mailInfo 待发送的邮件的信息 */ public boolean sendTextMail(MailSenderInfo mailInfo) &#123; // 判断是否需要身份认证 MyAuthenticator authenticator = null; Properties pro = mailInfo.getProperties(); if (mailInfo.isValidate()) &#123; // 如果需要身份认证，则创建一个密码验证器 authenticator = new MyAuthenticator(mailInfo.getUserName(), mailInfo.getPassword()); &#125; // 根据邮件会话属性和密码验证器构造一个发送邮件的session Session sendMailSession = Session.getDefaultInstance(pro,authenticator); try &#123; // 根据session创建一个邮件消息 Message mailMessage = new MimeMessage(sendMailSession); // 创建邮件发送者地址 Address from = new InternetAddress(mailInfo.getFromAddress()); // 设置邮件消息的发送者 mailMessage.setFrom(from); // 创建邮件的接收者地址，并设置到邮件消息中 Address to = new InternetAddress(mailInfo.getToAddress()); mailMessage.setRecipient(Message.RecipientType.TO,to); // 设置邮件消息的主题 mailMessage.setSubject(mailInfo.getSubject()); // 设置邮件消息发送的时间 mailMessage.setSentDate(new Date()); // 设置邮件消息的主要内容 String mailContent = mailInfo.getContent(); mailMessage.setText(mailContent); // 发送邮件 Transport.send(mailMessage); return true; &#125; catch (MessagingException ex) &#123; ex.printStackTrace(); &#125; return false; &#125; /** * 以HTML格式发送邮件 * @param mailInfo 待发送的邮件信息 */ public static boolean sendHtmlMail(MailSenderInfo mailInfo)&#123; // 判断是否需要身份认证 MyAuthenticator authenticator = null; Properties pro = mailInfo.getProperties(); //如果需要身份认证，则创建一个密码验证器 if (mailInfo.isValidate()) &#123; authenticator = new MyAuthenticator(mailInfo.getUserName(), mailInfo.getPassword()); &#125; // 根据邮件会话属性和密码验证器构造一个发送邮件的session Session sendMailSession = Session.getDefaultInstance(pro,authenticator); try &#123; // 根据session创建一个邮件消息 Message mailMessage = new MimeMessage(sendMailSession); // 创建邮件发送者地址 Address from = new InternetAddress(mailInfo.getFromAddress()); // 设置邮件消息的发送者 mailMessage.setFrom(from); // 创建邮件的接收者地址，并设置到邮件消息中 Address to = new InternetAddress(mailInfo.getToAddress()); // Message.RecipientType.TO属性表示接收者的类型为TO mailMessage.setRecipient(Message.RecipientType.TO,to); // 设置邮件消息的主题 mailMessage.setSubject(mailInfo.getSubject()); // 设置邮件消息发送的时间 mailMessage.setSentDate(new Date()); // MiniMultipart类是一个容器类，包含MimeBodyPart类型的对象 Multipart mainPart = new MimeMultipart(); // 创建一个包含HTML内容的MimeBodyPart BodyPart html = new MimeBodyPart(); // 设置HTML内容 html.setContent(mailInfo.getContent(), \"text/html; charset=utf-8\"); mainPart.addBodyPart(html); // 将MiniMultipart对象设置为邮件内容 mailMessage.setContent(mainPart); // 发送邮件 Transport.send(mailMessage); return true; &#125; catch (MessagingException ex) &#123; ex.printStackTrace(); &#125; return false; &#125; &#125; 发件人设置 文件名：MyAuthenticator.java 1234567891011121314151617 package net.kuangmeng.mail;import javax.mail.*; public class MyAuthenticator extends Authenticator&#123; String userName=null; String password=null; public MyAuthenticator()&#123; &#125; public MyAuthenticator(String username, String password) &#123; this.userName = username; this.password = password; &#125; protected PasswordAuthentication getPasswordAuthentication()&#123; return new PasswordAuthentication(userName, password); &#125; &#125; 实际发送 文件名：MailAction.java package net.kuangmeng.mail; import net.kuangmeng.*; public class MailAction { @SuppressWarnings(&quot;static-access&quot;) public static void main(String[] args){ //这个类主要是设置邮件 MailSenderInfo mailInfo = new MailSenderInfo(); mailInfo.setMailServerHost(&quot;smtp.yeah.net&quot;); mailInfo.setMailServerPort(&quot;25&quot;); mailInfo.setValidate(true); mailInfo.setUserName(&quot;*****@yeah.net&quot;); mailInfo.setPassword(&quot;******&quot;);//您的邮箱密码 mailInfo.setFromAddress(&quot;*****@yeah.net&quot;); mailInfo.setToAddress(&quot;****@qq.com&quot;); mailInfo.setSubject(&quot;你好！&quot;);//邮件主题 mailInfo.setContent(&quot;这是一个测试&quot;);//邮件内容 //这个类主要来发送邮件 SimpleMailSender sms = new SimpleMailSender(); sms.sendTextMail(mailInfo);//发送文体格式 sms.sendHtmlMail(mailInfo);//发送html格式 } } The link of this page is http://home.meng.uno/articles/21f3b9c2/ . Welcome to reproduce it!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://home.meng.uno/categories/Software-Engineering/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://home.meng.uno/tags/软件工程/"}]},{"title":"Git小结与Git Large File Storage","slug":"use_git","date":"2016-10-21T07:50:24.000Z","updated":"2021-01-01T16:49:52.000Z","comments":true,"path":"articles/5850713f/","link":"","permalink":"http://home.meng.uno/articles/5850713f/","excerpt":"Git指令目录 进入git教程官网我们可以发现，主要从这几个方面来讲解的（几乎所有你能搜到的博客都是这么一成不变！）： 建立项目 1. init 2. clone 基本操作 3. add 4. status 5. diff 6. commit 7. reset 8. rm 9. mv 分支管理 10. branch 11. checkout 12. merge 13. mergetool 14. log 15. stash 16. tag 分享与更新 17. fetch 18. pull 19. push 20. remote 21. subm","text":"Git指令目录 进入git教程官网我们可以发现，主要从这几个方面来讲解的（几乎所有你能搜到的博客都是这么一成不变！）： 建立项目 init clone 基本操作 add status diff commit reset rm mv 分支管理 branch checkout merge mergetool log stash tag 分享与更新 fetch pull push remote submodule Git Large File Storage 基本没有什么人不知道Git了吧，也没有多少人不知道GitHub，但是谈到GitHub如何存储大文件（100MB以上），又有多少人知道呢？ 今天，我要给大家介绍一种，不用分割文件即可实现让GitHub存储我们的大文件的方案 —— Git Large File Storage。 首先，给出官网：Git Large File Storage。 使用方法： 下载并安装：（Mac下：brew install git-lfs） 进入Git仓库，安装lfs：git lfs install 设置要跟踪的大文件：git lfs track &quot;*.file&quot; 添加.gitattributes进Git仓库：git add .gitattributes 正常的Git提交到GitHub！ 其业务逻辑： The link of this page is http://home.meng.uno/articles/5850713f/ . Welcome to reproduce it!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://home.meng.uno/categories/Software-Engineering/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://home.meng.uno/tags/软件工程/"},{"name":"Git","slug":"Git","permalink":"http://home.meng.uno/tags/Git/"},{"name":"GitHub","slug":"GitHub","permalink":"http://home.meng.uno/tags/GitHub/"},{"name":"超大文件","slug":"超大文件","permalink":"http://home.meng.uno/tags/超大文件/"}]},{"title":"Unicode和UTF-8的区别","slug":"utf-8","date":"2016-10-21T04:50:24.000Z","updated":"2020-12-02T02:09:13.000Z","comments":true,"path":"articles/97858b96/","link":"","permalink":"http://home.meng.uno/articles/97858b96/","excerpt":"很久很久以前，有一群人，他们决定用8个可以开合的晶体管来组合成不同的状态，以表示世界上的万物。他们看到8个开关状态是好的，于是他们把这称为\"字节\"。 再后来，他们又做了一些可以处理这些字节的机器，机器开动了，可以用字节来组合出很多状态，状态开始变来变去。他们看到这样是好的，于是它们就这机器称为\"计算机\"。 开始计算机只在美国用。八位的字节一共可以组合出256(2的8次方)种不同的状态。 他们把其中的编号从0开始的32种状态分别规定了特殊的用途，一但终端、打印机遇上约定好的这些字节被传过来时，就要做一些约定的动作。遇上00x10, 终端就换行，遇上0x07, 终端就向人们嘟嘟叫，例好遇上0","text":"很久很久以前，有一群人，他们决定用8个可以开合的晶体管来组合成不同的状态，以表示世界上的万物。他们看到8个开关状态是好的，于是他们把这称为&quot;字节&quot;。 再后来，他们又做了一些可以处理这些字节的机器，机器开动了，可以用字节来组合出很多状态，状态开始变来变去。他们看到这样是好的，于是它们就这机器称为&quot;计算机&quot;。 开始计算机只在美国用。八位的字节一共可以组合出256(2的8次方)种不同的状态。 他们把其中的编号从0开始的32种状态分别规定了特殊的用途，一但终端、打印机遇上约定好的这些字节被传过来时，就要做一些约定的动作。遇上00x10, 终端就换行，遇上0x07, 终端就向人们嘟嘟叫，例好遇上0x1b, 打印机就打印反白的字，或者终端就用彩色显示字母。他们看到这样很好，于是就把这些0x20以下的字节状态称为&quot;控制码&quot;。 他们又把所有的空格、标点符号、数字、大小写字母分别用连续的字节状态表示，一直编到了第127号，这样计算机就可以用不同字节来存储英语的文字了。大家看到这样，都感觉很好，于是大家都把这个方案叫做 ANSI 的&quot;Ascii&quot;编码（American Standard Code for Information Interchange，美国信息互换标准代码）。当时世界上所有的计算机都用同样的ASCII方案来保存英文文字。 后来，就像建造巴比伦塔一样，世界各地的都开始使用计算机，但是很多国家用的不是英文，他们的字母里有许多是ASCII里没有的，为了可以在计算机保存他们的文字，他们决定采用127号之后的空位来表示这些新的字母、符号，还加入了很多画表格时需要用下到的横线、竖线、交叉等形状，一直把序号编到了最后一个状态255。从128到255这一页的字符集被称&quot;扩展字符集&quot;。从此之后，贪婪的人类再没有新的状态可以用了，美帝国主义可能没有想到还有第三世界国家的人们也希望可以用到计算机吧！ 等中国人们得到计算机时，已经没有可以利用的字节状态来表示汉字，况且有6000多个常用汉字需要保存呢。但是这难不倒智慧的中国人民，我们不客气地把那些127号之后的奇异符号们直接取消掉, 规定：一个小于127的字符的意义与原来相同，但两个大 于127的字符连在一起时，就表示一个汉字，前面的一个字节（他称之为高字节）从0xA1用到0xF7，后面一个字节（低字节）从0xA1到0xFE，这样我们就可以组合出大约7000多个简体汉字了。在这些编码里，我们还把数学符号、罗马希腊的字母、日文的假名们都编进去了，连在 ASCII 里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的&quot;全角&quot;字符，而原来在127号以下的那些就叫&quot;半角&quot;字符了。 中国人民看到这样很不错，于是就把这种汉字方案叫做 “GB2312”。GB2312 是对 ASCII 的中文扩展。 但是中国的汉字太多了，我们很快就就发现有许多人的人名没有办法在这里打出来，特别是某些很会麻烦别人的国家领导人。于是我们不得不继续把 GB2312 没有用到的码位找出来老实不客气地用上。 后来还是不够用，于是干脆不再要求低字节一定是127号之后的内码，只要第一个字节是大于127就固定表示这是一个汉字的开始，不管后面跟的是不是扩展字符集里的内容。结果扩展之后的编码方案被称为 GBK 标准，GBK 包括了 GB2312 的所有内容，同时又增加了近20000个新的汉字（包括繁体字）和符号。 后来少数民族也要用电脑了，于是我们再扩展，又加了几千个新的少数民族的字，GBK 扩成了 GB18030。从此之后，中华民族的文化就可以在计算机时代中传承了。 中国的程序员们看到这一系列汉字编码的标准是好的，于是通称他们叫做 “DBCS”（Double Byte Charecter Set 双字节字符集）。在DBCS系列标准里，最大的特点是两字节长的汉字字符和一字节长的英文字符并存于同一套编码方案里，因此他们写的程序为了支持中文处理，必须要注意字串里的每一个字节的值，如果这个值是大于127的，那么就认为一个双字节字符集里的字符出现了。那时候凡是受过加持，会编程的计算机僧侣们都要每天念下面这个咒语数百遍： “一个汉字算两个英文字符！一个汉字算两个英文字符……” 因为当时各个国家都像中国这样搞出一套自己的编码标准，结果互相之间谁也不懂谁的编码，谁也不支持别人的编码，连大陆和台湾这样只相隔了150海里，使用着同一种语言的兄弟地区，也分别采用了不同的 DBCS 编码方案——当时的中国人想让电脑显示汉字，就必须装上一个&quot;汉字系统&quot;，专门用来处理汉字的显示、输入的问题，但是那个台湾的愚昧封建人士写的算命程序就必须加装另一套支持 BIG5 编码的什么&quot;倚天汉字系统&quot;才可以用，装错了字符系统，显示就会乱了套！这怎么办？而且世界民族之林中还有那些一时用不上电脑的穷苦人民，他们的文字又怎么办？ 真是计算机的巴比 伦塔命题啊！ 正在这时，大天使加百列及时出现了——一个叫 ISO （国际标谁化组织）的国际组织决定着手解决这个问题。他们采用的方法很简单：废了所有的地区性编码方案，重新搞一个包括了地球上所有文化、所有字母和符号的编码！他们打算叫它&quot;Universal Multiple-Octet Coded Character Set&quot;，简称 UCS, 俗称 “UNICODE”。 UNICODE 开始制订时，计算机的存储器容量极大地发展了，空间再也不成为问题了。于是 ISO 就直接规定必须用两个字节，也就是16位来统一表示所有的字符，对于ascii里的那些“半角”字符，UNICODE 包持其原编码不变，只是将其长度由原来的8位扩展为16位，而其他文化和语言的字符则全部重新统一编码。由于&quot;半角&quot;英文符号只需要用到低8位，所以其高8位永远是0，因此这种大气的方案在保存英文文本时会多浪费一倍的空间。 这时候，从旧社会里走过来的程序员开始发现一个奇怪的现象：他们的strlen函数靠不住了，一个汉字不再是相当于两个字符了，而是一个！是的，从 UNICODE 开始，无论是半角的英文字母，还是全角的汉字，它们都是统一的&quot;一个字符&quot;！同时，也都是统一的&quot;两个字节&quot;，请注意&quot;字符&quot;和&quot;字节&quot;两个术语的不同，“字节”是一个8位的物理存贮单元，而“字符”则是一个文化相关的符号。在UNICODE 中，一个字符就是两个字节。一个汉字算两个英文字符的时代已经快过去了。 从前多种字符集存在时，那些做多语言软件的公司遇上过很大麻烦，他们为了在不同的国家销售同一套软件，就不得不在区域化软件时也加持那个双字节字符集咒语，不仅要处处小心不要搞错，还要把软件中的文字在不同的字符集中转来转去。UNICODE 对于他们来说是一个很好的一揽子解决方案，于是从 Windows NT 开始，MS 趁机把它们的操作系统改了一遍，把所有的核心代码都改成了用 UNICODE 方式工作的版本，从这时开始，WINDOWS 系统终于无需要加装各种本土语言系统，就可以显示全世界上所有文化的字符了。 但是，UNICODE 在制订时没有考虑与任何一种现有的编码方案保持兼容，这使得 GBK 与UNICODE 在汉字的内码编排上完全是不一样的，没有一种简单的算术方法可以把文本内容从UNICODE编码和另一种编码进行转换，这种转换必须通过查表来进行。 如前所述，UNICODE 是用两个字节来表示为一个字符，他总共可以组合出65535不同的字符，这大概已经可以覆盖世界上所有文化的符号。如果还不够也没有关系，ISO已经准备了UCS-4方案，说简单了就是四个字节来表示一个字符，这样我们就可以组合出21亿个不同的字符出来（最高位有其他用途），这大概可以用到银 河联邦成立那一天吧！ UNICODE 来到时，一起到来的还有计算机网络的兴起，UNICODE 如何在网络上传输也是一个必须考虑的问题，于是面向传输的众多 UTF（UCS Transfer Format）标准出现了，顾名思义，UTF8就是每次8个位传输数据，而UTF16就是每次16个位，只不过为了传输时的可靠性，从UNICODE到UTF时并不是直接的对应，而是要过一些算法和规则来转换。 受到过网络编程加持的计算机僧侣们都知道，在网络里传递信息时有一个很重要的问题，就是对于数据高低位的解读方式，一些计算机是采用低位先发送的方法，例如我们PC机采用的 INTEL 架构，而另一些是采用高位先发送的方式，在网络中交换数据时，为了核对双方对于高低位的认识是否是一致的，采用了一种很简便的方法，就是在文本流的开始时向对方发送一个标志符——如果之后的文本是高位在位，那就发送&quot;FEFF&quot;，反之，则发送&quot;FFFE&quot;。不信你可以用二进制方式打开一个UTF-X格式的文件，看看开头两个字节是不是这两个字节？ 讲到这里，我们再顺便说说一个很著名的奇怪现象：当你在 windows 的记事本里新建一个文件，输入&quot;联通&quot;两个字之后，保存，关闭，然后再次打开，你会发现这两个字已经消失了，代之的是几个乱码！呵呵，有人说这就是联通之所以拼不过移动的原因。 其实这是因为GB2312编码与UTF8编码产生了编码冲撞的原因。 从网上引来一段从UNICODE到UTF8的转换规则： Unicode UTF-8 0000 - 007F 0xxxxxxx 0080 - 07FF 110xxxxx 10xxxxxx 0800 - FFFF 1110xxxx 10xxxxxx 10xxxxxx 例如&quot;汉&quot;字的Unicode编码是6C49。6C49在0800-FFFF之间，所以要用3字节模板：1110xxxx 10xxxxxx 10xxxxxx。将6C49写成二进制是：0110 1100 0100 1001，将这个比特流按三字节模板的分段方法分为0110 110001 001001，依次代替模板中的x，得到：1110-0110 10-110001 10-001001，即E6 B1 89，这就是其UTF8的编码。 而当你新建一个文本文件时，记事本的编码默认是ANSI, 如果你在ANSI的编码输入汉字，那么他实际就是GB系列的编码方式，在这种编码下，&quot;联通&quot;的内码是： c1 1100 0001 aa 1010 1010 cd 1100 1101 a8 1010 1000 注意到了吗？第一二个字节、第三四个字节的起始部分的都是&quot;110&quot;和&quot;10&quot;，正好与UTF8规则里的两字节模板是一致的，于是再次打开记事本时，记事本就误认为这是一个UTF8编码的文件，让我们把第一个字节的110和第二个字节的10去掉，我们就得到了&quot;00001 101010&quot;，再把各位对齐，补上前导的0，就得到了&quot;0000 0000 0110 1010&quot;，不好意思，这是UNICODE的006A，也就是小写的字母&quot;j&quot;，而之后的两字节用UTF8解码之后是0368，这个字符什么也不是。这就是只有&quot;联通&quot; 两个字的文件没有办法在记事本里正常显示的原因。 而如果你在&quot;联通&quot;之后多输入几个字，其他的字的编码不见得又恰好是110和10开始的字节，这样再次打开时，记事本就不会坚持这是一个utf8编码的文件，而会用ANSI的方式解读之，这时乱码又不出现了。 好了，终于可以回答NICO的问题了，在数据库里，有n前缀的字串类型就是UNICODE类型，这种类型中，固定用两个字节来表示一个字符，无论这个字符是汉字还是英文字母，或是别的什么。 如果你要测试&quot;abc汉字&quot;这个串的长度，在没有n前缀的数据类型里，这个字串是7个字符的长度，因为一个汉字相当于两个字符。而在有n前缀的数据类型里，同样的测试串长度的函数将会告诉你是5个字符，因为一个汉字就是一个字符。 The link of this page is http://home.meng.uno/articles/97858b96/ . Welcome to reproduce it!","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://home.meng.uno/categories/Programming-Language/"}],"tags":[{"name":"UTF-8","slug":"UTF-8","permalink":"http://home.meng.uno/tags/UTF-8/"},{"name":"Unicode","slug":"Unicode","permalink":"http://home.meng.uno/tags/Unicode/"}]},{"title":"Java-Struts2-MySQL配置","slug":"profile_javaweb","date":"2016-10-07T11:18:54.000Z","updated":"2021-01-01T17:01:46.000Z","comments":true,"path":"articles/61cdd944/","link":"","permalink":"http://home.meng.uno/articles/61cdd944/","excerpt":"在Eclipse中配置Struts2 为了配置Struts2，首先我明确了，配置其所需要的各个部分的文件，我的理解是，一个web.xml(配置监听器)、一个struts.xml(配置“action”)、导入必要的jar包，现将配置好的文件结构截图展示如下： 其中，web.xml与struts.xml具体内容将在以后篇幅具体展开！ 在Eclipse中配置MySQL 通过对该实验的理解，我发现eclipse配置数据库，并不是针对某一个项目，而是针对整个集成开发环境，所以，相应地配置MySQL也是整个IDE的事，在软件的总配置中，如下图所示位置： 此为我成功地加入了MySQL之后的界面","text":"在Eclipse中配置Struts2 为了配置Struts2，首先我明确了，配置其所需要的各个部分的文件，我的理解是，一个web.xml(配置监听器)、一个struts.xml(配置“action”)、导入必要的jar包，现将配置好的文件结构截图展示如下： 其中，web.xml与struts.xml具体内容将在以后篇幅具体展开！ 在Eclipse中配置MySQL 通过对该实验的理解，我发现eclipse配置数据库，并不是针对某一个项目，而是针对整个集成开发环境，所以，相应地配置MySQL也是整个IDE的事，在软件的总配置中，如下图所示位置： 此为我成功地加入了MySQL之后的界面，如果没有加入，需要点击右侧的“Add…”进行添加，如下图： 由于我添加的是最新的MySQL 5.1，所以在具体的项目中，我也需要导入相应版本的jar包，用来加载MySQL驱动： 添加的具体方法是将本地的此jar文件拖动到“WebContent-&gt;WEB-INF-&gt;lib”文件夹下，然后右键，将其添加到“Build Path”。 在Eclipse中配置Tomcat Tomcat是一个开源的web应用服务器，与MySQL一样，它也是对整个集成开发环境而言的，所以关于其的配置，也在eclipse的设置中，已经配置好的环境如下： 如果是第一次配置，仍然需要点击右侧的“Add…”来选择你要安装的版本： 再下一步就是要选择安装的tomcat的地址与安装名称了： 点击结束，就会自动安装好，每次对着web项目点击右键，在“Run As”选项下，第一个，就是：Run On Server，即在tomcat上运行该程序。 至此，开发环境已经完全搭建好了，接下来就是实际的开发过程了! The link of this page is http://home.meng.uno/articles/61cdd944/ . Welcome to reproduce it!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://home.meng.uno/categories/Software-Engineering/"}],"tags":[{"name":"Eclipse","slug":"Eclipse","permalink":"http://home.meng.uno/tags/Eclipse/"},{"name":"MySQL","slug":"MySQL","permalink":"http://home.meng.uno/tags/MySQL/"},{"name":"配置","slug":"配置","permalink":"http://home.meng.uno/tags/配置/"},{"name":"Struts2","slug":"Struts2","permalink":"http://home.meng.uno/tags/Struts2/"}]},{"title":"VT-x/EPT解读","slug":"VT-x","date":"2016-09-10T04:07:20.000Z","updated":"2020-12-02T02:10:14.000Z","comments":true,"path":"articles/18c2ca64/","link":"","permalink":"http://home.meng.uno/articles/18c2ca64/","excerpt":"1 VT-x VT-x是intel针对硬件辅助虚拟化的技术，它解决x86指令集不能被虚拟化的问题，并且简化了VMM软件，减少了软件虚拟化的需求。Virtual Machine Extensions定义了一系列新的操作，称为VMX操作，来提供处理器级别的支持。同时它提供了一个新的特权等级VMX root给VMM，从而避免了ring deprivileging方法(让操作系统运行于ring 1，VMM使用ring 0)带来的虚拟化漏洞。VMX操作可以分为两类： * root:VMM执行的VMX root 操作 * non-root:Guest执行的VMX non-root操作 对于这两种模","text":"1 VT-x VT-x是intel针对硬件辅助虚拟化的技术，它解决x86指令集不能被虚拟化的问题，并且简化了VMM软件，减少了软件虚拟化的需求。Virtual Machine Extensions定义了一系列新的操作，称为VMX操作，来提供处理器级别的支持。同时它提供了一个新的特权等级VMX root给VMM，从而避免了ring deprivileging方法(让操作系统运行于ring 1，VMM使用ring 0)带来的虚拟化漏洞。VMX操作可以分为两类： root:VMM执行的VMX root 操作 non-root:Guest执行的VMX non-root操作 对于这两种模式之间的转换，VMX提供了确切的说法： VM Entry:转换到VMX non-root操作 VM Exit:从VMX non-root操作转换到VMX root操作 实际上，这个操作过程也就是VMM和虚拟机之间的转换过程。VMCS是一个用来管理VMX non-root操作和VMX转换的数据结构。它由VMM配置，指定guest OS状态，并在VM exits发生时进行控制。 2 MMU虚拟化 第一代VT-x在每次VMX转换都会进行TLB冲洗，这会造成在所有VM exits和大部分VM entries时的性能流失，因此对TLB清洗必须有更好的VMM软件控制。VPID(virtual Processor Identifier)是VMCS中的一个16bit域，它用来缓存线性翻译的结果。VPID启动时，就不需要冲洗TLB，不同虚拟机的TLB项能在TLB中共存。 既往的VMM维持一个shadow page table，将guest的virtual pages，直接映射到machine pages。同时，guest中的V-&gt;P表，与VMM中V-&gt;M shadow page table同步。为了维持guest page table和shadow page table之间的关系，会因为VMM traps造成额外的代价，每次切换都会丢失性能，并且为了复制guest page table，在内存上也会有额外花费。 3 EPT(Extended Page Table) EPT这样的硬件支持(在AMD架构中类似的技术是NPT，nested page table)能够有效解决传统shadow page table的花销问题。在KVM最新的内存虚拟化技术中，采用了两级页表映射。第一级页表，客户虚拟机采用的是传统操作系统的页表，也即guest page table，记录着客户机虚拟地址(GVA)到客户机物理地址(GPA)的映射。而在KVM中，维护的第二级页表是extended page table(EPT)，记录的是虚拟机物理地址(GPA)到宿主机物理地址(HPA)的映射。 在图中可以看到，包括guest CR3在内，一共有5个GPA，它们都要通过硬件走一次EPT，得到下一个HPA。那么如何通过EPT计算出对应的HPA呢，KVM是如何操作的呢？EPT和传统的页表一样，也分为4层(PML4、PDPT、PD、PT)，一个gpa通过四级页表的寻址，再加上gpa最后12位的offset，得到了hpa。 在这个架构中，页可以分为两种：物理页(physical page)和页表页(MMU page)。物理页就是真正存放数据的页，页表页是存放EPT页表的页。这两种页创建的方式也不同，物理页可以通过内核提供的__get_free_page来创建，而页表页则是通过mmu_page_cache获得。这个page cache是在KVM初始化vcpu时通过linux内核中的slab机制分配的，它作为之后的MMU pages的cache来使用。在KVM中，每个MMU page对应一个数据结构kvm_mmu_page，在EPT处理过程中，它是极为重要的一个数据结构。 一条地址如何翻译？首先non-root状态下的CPU加载guest CR3，由于guest CR3是一条GPA，CPU需要通过EPT来实现GPA-&gt;HPA的转换。但首先，MMU会先查询硬件的TLB，来判断有没有GPA到HPA的映射。如果没有GPA到HPA的映射，那么在cache中查询EPT/NPT。如果cache里面没有缓存，则逐层向下层存储查询，最终获得guest CR3所映射的物理地址单元内容，作为下一级guest页表的索引基址。当CPU访问EPT页表，查找HPA，发现相应的页表项不存在时，就会抛出EPT Violation，由VMM截获处理它。随后通过GVA的偏移量，计算出下一条GPA，依次循环下去，直到最终获得客户机请求的页。整个过程如下图所示。 The link of this page is http://home.meng.uno/articles/18c2ca64/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Intel","slug":"Intel","permalink":"http://home.meng.uno/tags/Intel/"},{"name":"虚拟化","slug":"虚拟化","permalink":"http://home.meng.uno/tags/虚拟化/"},{"name":"VT-x","slug":"VT-x","permalink":"http://home.meng.uno/tags/VT-x/"},{"name":"EPT","slug":"EPT","permalink":"http://home.meng.uno/tags/EPT/"}]},{"title":"VBA常用语法","slug":"VBA","date":"2016-08-29T08:14:52.000Z","updated":"2020-12-02T02:09:32.000Z","comments":true,"path":"articles/c779c554/","link":"","permalink":"http://home.meng.uno/articles/c779c554/","excerpt":"1. 数组下标是从0开始的 2. sheet下标是从1开始的 3. 获取dict的方法 1 2 Dim dic setdic = CreateObject(\"Scripting.Dictionary\") 4. dict添加数值方法 1 dic.Add key, value 5. 遍历dict 方法 1 2 3 For Each Data In dict MsgBox (dict.Item(Data) & \":\" & Data) Next 6. vba用 '&'来连接字符串 7. 正则表达式使用方法 1 2 3 4 5 Dim regEx","text":"数组下标是从0开始的 sheet下标是从1开始的 获取dict的方法 12 Dim dic setdic = CreateObject(\"Scripting.Dictionary\") dict添加数值方法 1 dic.Add key, value 遍历dict 方法 123 For Each Data In dict MsgBox (dict.Item(Data) &amp; \":\" &amp; Data)Next vba用 '&amp;'来连接字符串 正则表达式使用方法 12345 Dim regExSet regEx = CreateObject(\"vbscript.regexp\")regEx.Global = TrueregEx.Pattern = \".?[0-9A-F]+$\"result = regEx.test(content) '验证content 获取某张数据表(sheet)已经使用的行列数方法 123 Dim rows, columns rows = sheet.UsedRange.rows.Count colums = sheet.UsedRange.Columns.Count 获取某张数据表 123 Sheets(i)'根据id获取表 Sheets(\"map\")'根据名称获取表Sheets(i).name'获取表的名称 获得数据表的某行内容 12 Sheets(i).Cells(c, 1)Sheets(i).Range(\"A17\") UBound获取数组长度(数组最后一位的下标,数组从0开始).1表示有两个数 当函数从中间处理错误要退出可以使用 Exit Function不要用Return，不然返回值会没有 使用 Function关键字能有返回值，返回值应该赋给函数名。 The link of this page is http://home.meng.uno/articles/c779c554/ . Welcome to reproduce it!","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://home.meng.uno/categories/Programming-Language/"}],"tags":[{"name":"VBA","slug":"VBA","permalink":"http://home.meng.uno/tags/VBA/"}]},{"title":"LLVM与LLVM Pass","slug":"llvm","date":"2016-08-11T06:48:40.000Z","updated":"2021-01-04T07:24:41.000Z","comments":true,"path":"articles/c0258bc3/","link":"","permalink":"http://home.meng.uno/articles/c0258bc3/","excerpt":"LLVM简介 传统的编译器，采用的是针对单语言源代码，生成对应平台机器码的方式，类似于： 而LLVM则采用了一种多前端，多后端的方式，如下： 在LLVM当中，LLVM IR是一种low-level的，虚拟指令集。所有的前端语言都能够生成LLVM，从而能够被统一的进行处理。在LLVM当中，还提供了对LLVM IR Optimization进行优化的方式，能够对现有的源码进行搜索，匹配对应的pattern，从而进行指令的替换、修改。 LLVM中，每个过程都是从Pass继承来的，LLVM优化器提供了许多passes，每个都写的很简洁，它们被编译成了库文件，并且在编译的时候被调用。这些库提","text":"LLVM简介 传统的编译器，采用的是针对单语言源代码，生成对应平台机器码的方式，类似于： 而LLVM则采用了一种多前端，多后端的方式，如下： 在LLVM当中，LLVM IR是一种low-level的，虚拟指令集。所有的前端语言都能够生成LLVM，从而能够被统一的进行处理。在LLVM当中，还提供了对LLVM IR Optimization进行优化的方式，能够对现有的源码进行搜索，匹配对应的pattern，从而进行指令的替换、修改。 LLVM中，每个过程都是从Pass继承来的，LLVM优化器提供了许多passes，每个都写的很简洁，它们被编译成了库文件，并且在编译的时候被调用。这些库提供了分析和变换的能力，并且既能独立运作，又能合作。 代码生成 那么LLVM这种“多对多”的编译方式，是如何将LLVM IR转化为机器码的呢？LLVM将代码的生成划分成了多个独立的过程：指令选择、寄存器生成，调度，代码布局优化，生成汇编代码。这样不同的平台，也能够利用自己的优势，对相同的LLVM IR进行优化。 LLVM采用了一种“mix and match”的方式，允许target作者，对于架构做出明确的指示，例如对寄存器的使用、限制做出明确的指示。LLVM利用Target-8description文件来指定对应架构的特性、使用的指令、寄存器。 而LLVM利用tblgen工具从这些.md文件当中，能够读取出足够的信息，并且在instruction selection、register allocator等过程中，选择处理的过程。而.cpp文件，则是实现一些特定的过程，例如浮点指针栈。 LLVM Pass LLVM Pass完成编译器的变换、优化工作；它们是Pass的子类，根据不同的需要，可以选择去继承ModulePass，CallGraphSCCPass，FunctionPass，或者LoopPass，RegionPass和BasicBlockPass等。 llvm是需要编写、编译、加载和执行的，它相当于一个可以加载的模块。 如果想编写一个模块，可以再llvm/lib/Transform当中创建对应的目录，并且在Transform以及目标文件夹下同时修改两个CMakeLists。在编译时，llvm的编译链会自动生成对应的pass。 pass是基于中间语言llvm IR来进行的。因此它用来对.bc文件进行优化，例如： 1 opt -load /lib/LLVMLbpass.so -lbpass &lt;foo.bc&gt; /dev/null The link of this page is http://home.meng.uno/articles/c0258bc3/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"LLVM","slug":"LLVM","permalink":"http://home.meng.uno/tags/LLVM/"},{"name":"编译安全","slug":"编译安全","permalink":"http://home.meng.uno/tags/编译安全/"}]},{"title":"Linux文件系统","slug":"linux-file","date":"2016-08-01T08:54:49.000Z","updated":"2020-12-08T10:19:45.000Z","comments":true,"path":"articles/a4b4101b/","link":"","permalink":"http://home.meng.uno/articles/a4b4101b/","excerpt":"虚拟文件系统(VFS)是linux的内核软件层，它能够为各种文件系统提供通用的接口，例如linux，unix，windows系统。它是一个位于应用程序和具体文件之间的中间层。VFS引入了一个通用文件模型，它能够表示所有支持的文件系统，它包含有超级块对象、索引节点对象、文件对象、目录项对象。 超级块对象 super_block存放已安装文件系统的有关信息，对基于磁盘的文件系统，它通常对应于存放在磁盘上的文件系统控制块。对于一个特定的文件系统，超级块的格式是固定的。每一个文件系统都对应一个超级块，它们都会链接到一个超级块链表，而文件系统中的每个文件在打开时都需要在内存分配一个inode结构它们都","text":"虚拟文件系统(VFS)是linux的内核软件层，它能够为各种文件系统提供通用的接口，例如linux，unix，windows系统。它是一个位于应用程序和具体文件之间的中间层。VFS引入了一个通用文件模型，它能够表示所有支持的文件系统，它包含有超级块对象、索引节点对象、文件对象、目录项对象。 超级块对象 super_block存放已安装文件系统的有关信息，对基于磁盘的文件系统，它通常对应于存放在磁盘上的文件系统控制块。对于一个特定的文件系统，超级块的格式是固定的。每一个文件系统都对应一个超级块，它们都会链接到一个超级块链表，而文件系统中的每个文件在打开时都需要在内存分配一个inode结构它们都要链接到超级块。 索引节点对象 文件系统处理文件所需要的所有信息，都放在索引节点的数据结构中。对于每个文件来说，索引节点对文件是唯一的,其数据结构是inode。要访问一个文件时，一定要通过它的索引才知道它是什么类型的文件(inode有一个union包含了这个信息)。inode包含了文件的各种信息。它还包含一个dentry结构的队列，可以通过它找到与这个文件相联系的所有dentry结构，指向超级块的指针等。每个索引节点对象也包含在文件系统的双向循环链表中，这个链表的头保存在超级块对象中。同时，inodes也存放在一个散列表中，用来加快对索引对象的搜索。 文件对象 文件对象用来描述一个进程怎样与一个打开的文件进行交互，它是在文件被打开的时候创建的，由一个file数据结构来描述。其存放的主要信息是文件指针，即文件中当前的位置。使用中的文件对象，包含在超级块所确立的链表中。其中的f_list字段包含了前一个/后一个对象的指针。 目录项对象 VFS把每个目录看作由若干子目录和文件组成的一个普通文件。在目录项被读入内存后，VFS就用一个dentry结构来表示它。对于进程查找路径中的每一个分量，都为其创建一个目录项分量。每个分量都和其对应的索引节点相联系。由于目录项对象可能会经常使用，因此linux使用目录项高速缓存，它包含一个不同状态的目录项对象集合，以及一个用于快速的散列表。 文件操作函数的调用 由于每个文件系统都有其自身的文件操作集合，VFS将inode从磁盘装入内存时，会把文件操作的指针存放在数据结构file_operation中(定义在fs.h中)，而该结构的地址存放在索引节点对象的i_fop字段中。进程打开文件时，VFS就用存放在索引节点中的地址初始化新文件对象的f_op字段，使得后续操作能够继续调用这些文件操作函数。f_op是指向文件操作表的指针。事实上除了file_operations,还有dentry_operations和inode_operations，super_operations，但它们通常只在打开文件的过程中使用，不像file_operations结构中那些函数常用。 进程与文件 已打开的文件，是属于进程的一项“财产”，归具体的进程所有。在进程的task_struct中，包含两个数据结构。 12 struct fs_struct *fs;struct files_struct *files; 其中，fs_struct是关于文件系统的信息，它反映的主要是带有全局性的，对具体进程而言的信息，与具体打开的文件关系不大。fs_struct里面包含有一个指针pwd，它总是指向进程的“当前工作目录”，每当进程进入不同目录时，内核就使进程的pwd指向这个目录在内存中的dentry，而root指针则指向进程的根目录(还有一个altroot，指向“替换跟目录”)。files_struct是关于已打开文件的信息，它的主体是一个file结构数组，每打开一个文件后，进程就通过一个fid来访问这个文件，这个fid实际上就是它在file数组中的下标。 文件系统的安装和拆卸 每当将一个存储设备安装到现有文件系统中的某个节点时，内核都要为之建立一个vfsmount结构，它既包含了该设备的信息，又包含了有关安装点的信息。因此fs_struct中的pwdmnt总是指向一个vfsmount结构。 与传统的Unix内核不同，linuc允许同一个文件系统被安装许多次。对于每个安装操作，内核通过一个vfsmount数据结构来保存安装点和安装标志等信息。这个vfsmount数据结构保存在几个双向循环链表中：父文件系统vfsmount描述符的地址和安装点dentry的地址索引散列表；属于每一命名空间的已安装文件系统描述符形成的双向循环链表；每一个已安装文件系统已安装的文件形成等双向循环链表。 安装普通文件系统 mount系统调用用来安装一个普通文件系统。它的服务例程sys_mount()向下调用了do_mount，并最后由do_kern_mount()函数完成了安装操作的核心。do_kern_mount首先会查找对应的文件系统类型，然后分配一个新的已安装文件系统的描述符mnt，并初始化一个新的超级块(如果get_sb能够在链表中找到对应的超级块对象，说明这个设备被安装了多次，直接返回这个已有超级块的地址)，以及mnt中的一些字段。最后，它会把这个描述符插入到合适的链表中去。在完成这些工作后，该函数将mnt返回。 安装根文件系统 根文件系统的安装与普通文件系统安装是不同的，它是系统初始化的关键部分。它分为两个部分，首先是安装特殊rootfs文件系统，它提供一个作为初始安装点的空目录；随后内核在空目录上安装实际根文件系统。为什么要先安装rootfs？因为它允许内核轻易地改变实际根文件系统，而内核会逐个安装卸载许多个根文件系统。 在rootfs安装阶段，do_kern_mount会调用rootfs_get_sb，为其分配特殊的超级块。随后为进程0分配namespace，将其根目录和当前工作目录设置为根文件系统。 在实际安装阶段，内核调用mount_root函数，在rootfs初始根文件系统中创建设备文件/dev/root。调用sys_chdr(&quot;root&quot;)改变进程的当前目录，然后移动rootfs上的安装文件系统和安装点。而rootfs也并没有被卸载，只是隐藏在根目录下而已。 卸载文件系统 unmount系统调用由来卸载一个文件系统。相应的sys_unmount例程对文件名和一组标志进行处理。首先需要对安装点路径名进行查找，随后调用do_unount，根据标志位进行相应的处理。unmount_tree会卸载文件系统(及其所有子文件系统)。最后，内核会减少相应文件系统根目录的目录项对象和已安装文件系统描述符的引用计数器值。 路径名查找 进程识别一个文件时，需要分析路径名，并且把它拆分成一个文件名序列。首先需要区分这个路径是相对路径还是绝对路径。这个可以通过路径名的第一个字符是否是“/”来确定。绝对路径从进程的根目录开始搜索，否则从进程的当前目录开始搜索。 在这个过程中，内核会检查依次检查序列中的每一项，找到与它匹配的目录项，以获得相应的索引节点；再读取这个索引节点的目录文件，继续检查下去。但这个过程没有想象的那么简单：每个目录的访问权都需要检查，文件名可能是与任意一个路径名对应的符号链接，符号链接会包括循环，查找可能会延伸到其他的文件系统。 路径名查找接受一个文件名指针，一个标志，以及一个nameidata数据结构的地址，这个nameidata存放了查找操作的结果。nameidata中的dentry和mnt分别指向所解析的最后一个路径分量的目录项对象和已安装文件系统对象。 文件打开/读写 文件打开和关闭，核心是对一个fd进行操作。这个fd也就是指向文件对象的指针数组task_struct-&gt;files-&gt;fd中分配给新文件的索引。创建一个新的文件对象，然后将它放在fd数组中。 文件读写是对于文件自身来说的，因此其相关的信息存储在inode中。linux中，对于文件的读写，实际上是对缓冲区直接进行的，而不是直接在文件上操作。对文件的操作由内核中的线程kflushd来完成，它们相当于一道流水线上的两道工序。 The link of this page is http://home.meng.uno/articles/a4b4101b/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://home.meng.uno/tags/Linux/"},{"name":"文件系统","slug":"文件系统","permalink":"http://home.meng.uno/tags/文件系统/"}]},{"title":"完全动态最大匹配的简单确定性算法","slug":"biggest-match","date":"2016-07-16T12:40:27.000Z","updated":"2020-12-02T01:41:28.000Z","comments":true,"path":"articles/8e9f2014/","link":"","permalink":"http://home.meng.uno/articles/8e9f2014/","excerpt":"Simple Deterministic Algorithms for Fully Dynamic Maximal Matching 解决的问题 这篇文章介绍了什么是“完全动态最大匹配”，然后介绍了他们提供的在最坏情况下更新时间为平摊O(√m)（m表示图中的边数）时间复杂度、O(n+m)空间复杂度内完成的“3/2-近似最大基数匹配（MCM）”算法。这篇文章是为了处理图论中的“匹配”问题，作者将“图”分成[1]一般图，[2]低荫度[4]图 两种来处理。而对于每种图，处理步骤与方法基本一致，每种图都有——“插入”、“删除”两种操作，所不同的就是每种图所对应的情况（Case）不同而已。 采用的思","text":"Simple Deterministic Algorithms for Fully Dynamic Maximal Matching 解决的问题 这篇文章介绍了什么是“完全动态最大匹配”，然后介绍了他们提供的在最坏情况下更新时间为平摊O(√m)（m表示图中的边数）时间复杂度、O(n+m)空间复杂度内完成的“3/2-近似最大基数匹配（MCM）”算法。这篇文章是为了处理图论中的“匹配”问题，作者将“图”分成[1]一般图，[2]低荫度[4]图 两种来处理。而对于每种图，处理步骤与方法基本一致，每种图都有——“插入”、“删除”两种操作，所不同的就是每种图所对应的情况（Case）不同而已。 采用的思想 这篇算法总体采用了分治的思想在每个小的情况下又有贪心算法的影子，本来“3/2-近似最大基数匹配（MCM）”这个问题相当难解决，而且已知算法中，要么时间复杂度为O(n)，要么为O((n+m)√2/2)而且相当复杂，情况相当多。这篇算法一个很大的优点就是，思路清晰，作者将自己设计算法的思路，无遗地展现在读者面前。说起思路清晰，也就是分的情况条例清晰，也就是我们所说的分治思想。本来一个大问题不好求解，但我们可以通过将其分解为等价的几个小问题分别求解。 就像这篇算法，将“完全动态最大匹配”分为一般图与低荫度图两个部分来分别阐述，让算法在一般情况以及特殊情况都能游刃有余。 在应对每一大类图的各种操作（插入或删除）时，由于仍旧很复杂，所以作者又一次运用分治思想，将他们每个具体的操作继续分解为更小的几种情况，同时做到面面俱到，条理清晰。 在每一个小的情况下，遍历所有情况，找出能使匹配最大的情况，属于贪心的范畴，对于贪心算法，由于每种情况实在所要找的节点较少，所以我也认为贪心是比较不错的选择，而且，我们没必要在这样的小点上过于纠结。 基本算法描述 一般图 定义V={1,2……n}表示图的顶点的集合 为了简单处理，假定√n为整数； 定义G = (G0,G1……)表示图的集合，而且定义G0为空图，Gi由Gi-1得到； 对于每一个时间跨度i,Gi=(V,Ei),mi=|Ei| （m表示图中的边数）。 数据结构 M 存储在AVL树中（支持O(logn)时间复杂度的插入、删除）每个节点v处保存mate(v)返回当前匹配中的邻接顶点； N(v) 存储在AVL树中（v∈V）用于存储节点v的邻接顶点，变量deg(v)存储v的度（无向图）（支持O(logn)时间复杂度的插入、删除以及取出r邻接顶点在O®时间复杂度内）； F(v)（v∈V）用于保存v的自由邻接节点（支持O(1)时间复杂度下的插入、删除以及如果有自由邻接顶点就返回TRUE的has-free(v)操作，O(√n)时间复杂度下的返回任意自由邻接顶点的get-free(v)操作），为了很好的得出F(v)，还另外加了一个长度为n的Boolean型数组来表明当前自由邻接顶点、一个长度为√n的范围在[√nj+1,√n(j+1)]的计数数组来保存位置j处的自由邻接顶点的数目、一个变量来存储总的自由邻接节点数。 一个最大堆Fmax存储所有的以自身的度为权的自由节点（支持O(logn)时间内的插入、删除、update-key、find-max操作）。 具体算法 初始化图后，定义三个不变量，每次程序循环运行结束都不会突破这些不变量： 不变量1：所有节点度不超过√(2m+2n)； 不变量2：在第i次循环变为自由节点的节点度不超过√(2m)； 不变量3：M是最大匹配，而且没有长度为3的增光路径。 然后进行循环，每次循环对边ei进行操作。 插入边ei={u,v}的操作 首先更新相关的数据结构：N(u),N(v),deg(u),deg(v),Fmax以及u、v的值。 然后分如下四种情况讨论： u、v都已匹配，此时无需操作； u、v都是自由节点，这种情况下涉及更新M与Fmax，将u、v从F(x)中移除，花费O(√n+m)时间； u是自由的而v已被匹配，这种境况下，找到最大匹配集M中与v匹配的节点x=mate(v),再找到与x邻接的自由顶点w，如果能找到，则将匹配(x,v)移除，而将（u,v)、(x,w)加入。操作时间也在O(√n+m)内； v是自由的而v已被匹配，这种情况与上一种相对称。 删除边ei={u,v}的操作 删除操作主要分为两种情况： ei ∉ M 唯一要做的事就是，将u从F(v)，v从F(u)中删去； ei∈M 由分为两种情况讨论： deg(u)≤√(2m) 时，我们必须通过aug-path(u)来所搜一条以u为起点的长度为3的增广路径，如果没有找到，那么直接标定v为自由节点即可，如果找到，定义u的自由邻接节点为w，w的邻接顶点为y，y的自由临界顶点为x，将{u,w}和{y,x}添加到匹配中而将{w,y}删除，同样的方法处理v。 deg(u)&gt;√(2m) 时，此时u不能自由，因为它的度太高。保持u匹配，我们通过surrogate(u)来获取u的代替节点，我们称呼为su，改变它的状态为自由，然后像在a)中处理u一样处理su。 对于低荫度图 在这部分中，考虑的是荫度不大于c的图，而c由下式确定： 其中E(U)表示U中的边数。 由于图较稀疏，所以将每条边都带上方向，构成△-方向无向图 数据结构 与一般图基本相同。 具体算法 主要从：定向、插入、删除，几个部分来阐述： 定向 在每一步中，都通过运行算法A，来保证当前图是△-方向的。像如下算法中那样更新F和D，如果有t条边在定向，我们需要O(t)时间。 插入边ei={u,v}的操作 由于出度最多为△，所以插入时间O(△)。 删除边ei={u,v}的操作 未匹配的边被删除没什么可以操作的，但是当匹配的边被删除就有趣了，这里必须找到u(v)的匹配节点，时间复杂度仍为O(△)。 算法分析的结论 分析这个算法，其时间复杂度确实是平摊条件[4]下O(√m)(m表示图的边数)； 其空间复杂度为O(m+n)。 这个算法确实能在最短的时间内的出“3/2-近似最大基数匹配（MCM）”。 用一个例子说明相关算法 为了较好的说明这个算法，我仅就自己画的一个图做些简要的分析，有遗漏之处实属算法没理解透： （图如下，分别由6个顶点、7条边组成，由于算法本身要求图中点、边数目较多，而简短的语言无法描述清楚，所以选择这样一个简单的图来叙述） 以下模拟算法过程：（只举例说明） 由文章知，该算法要处理动态图的最大匹配问题，首先初始化G0为空，我们按e0—e6的顺序向图中添加边，然后删除边。 将e0（AB）加入，直接将此边加入最大匹配M； 将e1（AC）加入，不操作； 将e2（BC）加入，不操作； 将e3（CE）加入，直接将此边加入M； 将e4（BF）加入，不操作； 将e5（EF）加入，不操作； 将e6加入，将此边加入M，同时将e3从M中删除，将e4加入，将e0删除，将e1加入M。 得到如下的最大匹配： 删除边时： （由于删边不容易看出来，所以仅将出现可见变化时的情况列出。） 删除e0，不操作； 删除e1,将e1从最大匹配M中删除； 删除e4，将e2加入M，将e4删除。 得到的匹配如下： 通过以上两个实例（一个删除、一个插入），基本能模拟本文关于动态最大匹配的一般算法。 认识与看法 已有算法的问题所在 算法提到对于低荫度图，给每条边改成有向边处理，然后用贪心算法求解，但是算法并未给出如何判断一个图是否是低荫度图，而且即使有了一个标准，那么当一个图先是低荫度图，后来因为添加一条边之后，成了算法判断的非低荫度图，之后再添加边……如此一来，后面添加边的匹配操作，仍然会按低荫度图处理，这样一来时间复杂度就明显升高了。与此相反，当一个图先是普通图，当减去一条边后，成了低荫度图，再继续减去边……后来虽然形成了低荫度图，但是仍然按普通图来处理的。——我的意思是，当加边/减边（一个更新操作）在普通图与低荫度图之间轮换时，算法会因每次都错开最好情况而用的时间急剧升高。在实际应用中，这种情况应该会很常见，所以我认为这个问题还是相当严峻的。 算法一直在说向图G，但这是一个“抽象”的概念，文中并没有提及这个图是怎么存储的，也没有提及添加/删除边是怎么进行的。我觉得这虽然是不起眼的一部分，但是它实现的好坏，却是后来算法的高效进行与否的保证。（在第四次作业中我将详细分析不同存储结构存储的图的不同之处，以及哪种存储对此算法的实现最有益。） 已有算法可改进之处 算法中提到对点的度按降序排列，但如果两个点的度完全一样，或者所有点的度完全一样时，这样的处理难免有些草率，和普通匹配算法没有这一处理操作的算法过程相似，甚至多了偶尔的不必要交换（当度相同时的交换），于是我想到，能不能通过增加一个标志位来改进交换同时由于考虑到一般情况下，顶点的度有大部分是相同的，所以可以考虑改进一下排序操作，找一个比较稳定的排序是比较好的。 算法用AVL树来存储匹配，这一点感觉是比较好的操作，但是当存储每个点的邻接点时，不仅又用AVL树来存储，而且需要用另外一个变量存储该顶点的度，而且每次插入/删除边（一个更新操作）都可能需要调整AVL树，于是，我想能不能用更好的适用于随机搜索（插入、删除）的数据结构，例如：散列表来存储邻接顶点呢？这样一来，搜索的时间由O(logN)减小到O(1)，空间基本不会增加，反而减少一个整型空间（每个顶点减少一个，共N个），插入/删除边的时间大约也是O(1)，这样一来就比以前的算法要改进了一些（具体实现，见作业四）。 算法中提到用一个数据结构F(v)来存储顶点v的邻接顶点中未匹配的顶点，及其信息。然后算法提到一个长度为n（n表示所有顶点数量）的数组以及一个长度√n的计数数组。这种处理，在原则上是没有什么错误的，但是长度为n或者√n的数组没法实现每次插入顶点时就没法用了，为了实现完全动态匹配，我们需要用一个更好的数据结构来完成这一操作（当然对于本算法，这一改进并没有什么作用，但是想到可以为算法进行拓展，所以这个改进是必要的），我想到的数据结构是动态表——一种最省空间，且扩充/缩减时间O(1)（由摊还分析可证）的数据结构。 当数据量较大，短时间有较多条边需要插入/删除时，（通过插入操作来说明）每次操作，都是先将边加入存储边的数据结构，然后再依次更新其他相邻节点的邻接信息，以及边的信息。最后再分情况（动态规划）依次判断以及做出调整，简单来讲，整个判断过程分为三个部分，而且这三个部分是相互独立的，所以假如有N次插入，那么就有3N步，正常情况（该算法原来描述那样）。在一次插入边的操作结束后，存储边的数据结构便空闲下来，而第二次边的插入还没有开始。中间这部分时间浪费严重（当大数据来临时）。 已有算法不适用之处 算法一开始是通过一个空的数据结构开始插入边，来建立匹配，假如已有一个大数据的图（数据在外存，一般来说数据量过大，没法一次性装入内存）或者，由于数据量过大，没法普通存储（或者为了省空间、减小出错率）而用压缩图来存储，如果仍然用现有的算法来做，是极其困难的。也许可以实现，但是也困难重重，我仅将其归为不适用的一类。 改进意见 针对第一处问题。为了在每一步都使算法做到最优，那么在低荫度图与普通图的转换接口处，就应该更加重视，为了解决这个问题，我觉得，可以在每增加/删除一条边（一个更新操作）时增加一次判断，即：max[|E(U)|/(|U|-1)]&gt;C时，就进行普通图的操作，否则进行低荫度图的操作，其中C为预设的荫度的分界线。同时，首先对普通图的操作，或者对低荫度图的操作，当第一次出现两种图混合时，再初始化另一种图的存储结构，这样一来如果一直是基于两者中一种图的更新操作，就不需要那么大的空间开销。 在原算法，没有这一判断时，相对现在的改进，在每一次更新操作的时间上先是少一步，但当更新操作在多次往往复复地在两种图之间转换时，虽然改进多了一次判断，但是避免了两种图用同一种算法实现的时间消耗，算法改进的正确性及合理性得证。 针对第二处问题。我觉得这里的问题是作者的一个疏忽，当然在这个算法中，选用哪种数据结构来存储图，确实不是那么好选择的。选用邻接矩阵时，虽然边的信息清晰明了，但是，当数据量增大时，空间开销也是挺大的。若选用邻接表，减少了空间开销，但是，当每次统计某个节点的邻接顶点时，显得有些麻烦，邻接多重表，十字链表等弊端就更多与优点了。于是，在此改进中，我仅仅提出“邻接矩阵”与“邻接表”这两种存储图的数据结构来存储这个算法有关的图，这样，相对来说，比其他数据结构要简单，以及开销较小（时间/空间上）。 这两种图的存储结构，都可以实现边的插入/删除在O(1)时间内，并且相对其他存储结构还比较好实现，于是正确性得证。 针对第一处不足。我觉得可以增加一个标志位（整型）来实现对同一个度的不同节点做一下区分，第一次度为某个值的顶点标记为1，以后如果再有和这个点度相同的标记为2，……以此类推，每次为了保证度越高的节点越先匹配，不仅要判断度的大小，同时当度相等时，还要判断标志位，标志位越小（越大也行）就越优先保持匹配。 这一改进，使该算法对多顶点度相等的图，效果明显，当然当顶点度基本都不相同时，这一改进显得一无是处，反而增加空间开销。但就对程序改进角度讲，这样做着实可以在某些情况下，增加算法正确性，减少算法时间复杂性，我觉得这可以作为改进合理性的证明。 针对第二处不足。我觉得可以用散列表来存储每个顶点的邻接点信息，这样一来，可以减少一个整型变量空间，同时使搜索某个点是否与另一个点邻接能够在O(1)时间内完成（这也是散列表的最大的用处）。当然，散列表的构造也有很多方式，这里可以假设之前每个顶点都有一个ID或者有一个单独的可区分的标识。当以顺序ID来标记每个顶点时，可以用直接寻址法；当以单独的可区分标识来标记每个顶点时，可以用数据分析法来确定散列函数。 该改进的正确性，可以通过散列表在搜索上的正确性来证明，同时在空间复杂性上可能比以前算法要多（当然也可能相同），但在搜索时间上，对原有算法改进是十分显著的。由此，改进的合理性由此得证。 针对第三处不足。这一点属于“个人爱好”，为什么这么说呢？因为这一改进对原有算法不会产生一丁点的影响，但是却为程序可拓展性做了一点贡献。用动态表代替原有长度固定的数组，当顶点个数增多时，可以避免每次不够用又重新手动申请空间的弊端。可以说，动态表这一数据结构完美适应了顶点可变的情况，当边减少时，那些独立的顶点，可以通过某种方式，将其删去，以减少空间复杂性，同时，顶点减少，搜索更快；当边增多时，如果某些顶点原来没有记录，再在动态表中将其加入，丝毫不用担心空间分配麻烦问题。 动态表实现简单，效率高，事实上它和普通数组相比基本没有效率损失。我觉得即使是原算法思想（即使没边，也有顶点）也可以用动态表代替数组。 针对第四处不足。当大数据来临时，我将每次的匹配过程看成三部分，这让我想起了《计算机组成原理》中关于指令流水线的介绍，我发现该算法中三部分之间互相无关联，于是，我就想能不能仿照指令流水线的方式，来改进已有算法的三段匹配过程呢？也就是说，当第一条边插入时，我顺序开始执行那三段操作，当执行完第二段时，第二次插入已经可以开始执行了……依次类推，当大量数据进行同一个操作（插入/删除边）时，就可以成倍地减少时间复杂性（将三段融合成一段）。 这一改进符合并行性要求，而且可以证明，原来程序的三段互不影响，那么这个改进就显得在大数据上大有用武之处。 针对第一处不适。由于该算法是每次（动态）加入边，而现实是需要先初始化一个图，而且还可能是大量的数据的图，所以该算法不适用之处就显现出来了。虽然如果将已经存在的大数据图里的边一个个挑出来再加入，可以完成此操作，但是考虑时间将会是特别大的。于是想到，能不能给该算法在加一点能够对静态图匹配的内容，以便其能够在大数据上发挥作用。 如今，数据已进入海量时代，应对大数据冲击，已经成为考验所有算法好坏的一个标准，于是对此算法往大数据上适应，是十分有必要的。 The link of this page is http://home.meng.uno/articles/8e9f2014/ . Welcome to reproduce it!","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://home.meng.uno/categories/Algorithm/"}],"tags":[{"name":"算法设计","slug":"算法设计","permalink":"http://home.meng.uno/tags/算法设计/"},{"name":"最大匹配","slug":"最大匹配","permalink":"http://home.meng.uno/tags/最大匹配/"},{"name":"确定性算法","slug":"确定性算法","permalink":"http://home.meng.uno/tags/确定性算法/"},{"name":"动态","slug":"动态","permalink":"http://home.meng.uno/tags/动态/"}]},{"title":"汇编指令与代码规范","slug":"asm","date":"2016-07-16T12:18:01.000Z","updated":"2021-01-04T07:33:26.000Z","comments":true,"path":"articles/711069a/","link":"","permalink":"http://home.meng.uno/articles/711069a/","excerpt":"基本指令 数据传输指令 通用数据传送指令 MOV 传送字或字节. MOVSX 先符号扩展,再传送. MOVZX 先零扩展,再传送. PUSH 把字压入堆栈. POP 把字弹出堆栈. PUSHA 把AX,CX,DX,BX,SP,BP,SI,DI依次压入堆栈. POPA 把DI,SI,BP,SP,BX,DX,CX,AX依次弹出堆栈. PUSHAD 把EAX,ECX,EDX,EBX,ESP,EBP,ESI,EDI依次压入堆栈. POPAD 把EDI,ESI,EBP,ESP,EBX,EDX,ECX,EAX依次弹出堆栈. BSWAP 交换32位寄存器里字节的顺序 XCHG 交换字或字","text":"基本指令 数据传输指令 通用数据传送指令 MOV 传送字或字节. MOVSX 先符号扩展,再传送. MOVZX 先零扩展,再传送. PUSH 把字压入堆栈. POP 把字弹出堆栈. PUSHA 把AX,CX,DX,BX,SP,BP,SI,DI依次压入堆栈. POPA 把DI,SI,BP,SP,BX,DX,CX,AX依次弹出堆栈. PUSHAD 把EAX,ECX,EDX,EBX,ESP,EBP,ESI,EDI依次压入堆栈. POPAD 把EDI,ESI,EBP,ESP,EBX,EDX,ECX,EAX依次弹出堆栈. BSWAP 交换32位寄存器里字节的顺序 XCHG 交换字或字节.( 至少有一个操作数为寄存器,段寄存器不可作为操作数) CMPXCHG 比较并交换操作数.( 第二个操作数必须为累加器AL/AX/EAX ) XADD 先交换再累加.( 结果在第一个操作数里 ) XLAT 字节查表转换. BX 指向一张 256 字节的表的起点, AL 为表的索引值 (0-255,即0-FFH); 返回 AL 为查表结果. ( [BX+AL]-&gt;AL ) 输入输出端口传送指令 IN I/O端口输入. ( 语法: IN 累加器, {端口号│DX} ) OUT I/O端口输出. ( 语法: OUT {端口号│DX},累加器 ) 输入输出端口由立即方式指定时, 其范围是 0-255; 由寄存器 DX 指定时,其范围是 0-65535. 目的地址传送指令. LEA 装入有效地址. 例: LEA DX,string ;把偏移地址存到DX. LDS 传送目标指针,把指针内容装入DS. 例: LDS SI,string ;把段地址:偏移地址存到DS:SI. LES 传送目标指针,把指针内容装入ES. 例: LES DI,string ;把段地址:偏移地址存到ES:DI. LFS 传送目标指针,把指针内容装入FS. 例: LFS DI,string ;把段地址:偏移地址存到FS:DI. LGS 传送目标指针,把指针内容装入GS. 例: LGS DI,string ;把段地址:偏移地址存到GS:DI. LSS 传送目标指针,把指针内容装入SS. 例: LSS DI,string ;把段地址:偏移地址存到SS:DI. 标志传送指令. LAHF 标志寄存器传送,把标志装入AH. SAHF 标志寄存器传送,把AH内容装入标志寄存器. PUSHF 标志入栈. POPF 标志出栈. PUSHD 32位标志入栈. POPD 32位标志出栈. 算术运算指令 ADD 加法. ADC 带进位加法. INC 加 1. AAA 加法的ASCII码调整. DAA 加法的十进制调整. SUB 减法. SBB 带借位减法. DEC 减 1. NEC 求反(以 0 减之). CMP 比较.(两操作数作减法,仅修改标志位,不回送结果). AAS 减法的ASCII码调整. DAS 减法的十进制调整. MUL 无符号乘法. IMUL 整数乘法. 以上两条,结果回送AH和AL(字节运算),或DX和AX(字运算) AAM 乘法的ASCII码调整. DIV 无符号除法. IDIV 整数除法. 以上两条,结果回送: 商回送AL,余数回送AH, (字节运算); 或商回送AX,余数回送DX, (字运算). AAD 除法的ASCII码调整. CBW 字节转换为字. (把AL中字节的符号扩展到AH中去) CWD 字转换为双字. (把AX中的字的符号扩展到DX中去) CWDE 字转换为双字. (把AX中的字符号扩展到EAX中去) CDQ 双字扩展. (把EAX中的字的符号扩展到EDX中去) 逻辑运算指令 AND 与运算. or 或运算. XOR 异或运算. NOT 取反. TEST 测试.(两操作数作与运算,仅修改标志位,不回送结果). SHL 逻辑左移. SAL 算术左移.(=SHL) SHR 逻辑右移. SAR 算术右移.(=SHR) ROL 循环左移. ROR 循环右移. RCL 通过进位的循环左移. RCR 通过进位的循环右移. 以上八种移位指令,其移位次数可达255次. 移位一次时, 可直接用操作码. 如 SHL AX,1.移位&gt;1次时, 则由寄存器CL给出移位次数.如 MOV CL,04 SHL AX,CL 串指令 DS:SI 源串段寄存器 :源串变址. ES:DI 目标串段寄存器:目标串变址. CX 重复次数计数器. AL/AX 扫描值. D标志 0表示重复操作中SI和DI应自动增量; 1表示应自动减量. Z标志 用来控制扫描或比较操作的结束. MOVS 串传送. ( MOVSB 传送字符. MOVSW 传送字. MOVSD 传送双字. ) CMPS 串比较. ( CMPSB 比较字符. CMPSW 比较字. ) SCAS 串扫描. 把AL或AX的内容与目标串作比较,比较结果反映在标志位. LODS 装入串.把源串中的元素(字或字节)逐一装入AL或AX中. ( LODSB 传送字符. LODSW 传送字. LODSD 传送双字. ) STOS 保存串.是LODS的逆过程. REP 当CX/ECX&lt;&gt;0时重复. REPE/REPZ 当ZF=1或比较结果相等,且CX/ECX&lt;&gt;0时重复. REPNE/REPNZ 当ZF=0或比较结果不相等,且CX/ECX&lt;&gt;0时重复. REPC 当CF=1且CX/ECX&lt;&gt;0时重复. REPNC 当CF=0且CX/ECX&lt;&gt;0时重复. 程序转移指令 无条件转移指令 (长转移) JMP 无条件转移指令 CALL 过程调用 RET/RETF过程返回. 条件转移指令 (短转移,-128到+127的距离内) 当且仅当(SF XOR OF)=1时,OP1&lt;OP2 JA/JNBE 不小于或不等于时转移. JAE/JNB 大于或等于转移. JB/JNAE 小于转移. JBE/JNA 小于或等于转移. 以上四条,测试无符号整数运算的结果(标志C和Z). JG/JNLE 大于转移. JGE/JNL 大于或等于转移. JL/JNGE 小于转移. JLE/JNG 小于或等于转移. 以上四条,测试带符号整数运算的结果(标志S,O和Z). JE/JZ 等于转移. JNE/JNZ 不等于时转移. JC 有进位时转移. JNC 无进位时转移. JNO 不溢出时转移. JNP/JPO 奇偶性为奇数时转移. JNS 符号位为 “0” 时转移. JO 溢出转移. JP/JPE 奇偶性为偶数时转移. JS 符号位为 “1” 时转移. 循环控制指令(短转移) LOOP CX不为零时循环. LOOPE/LOOPZ CX不为零且标志Z=1时循环. LOOPNE/LOOPNZ CX不为零且标志Z=0时循环. JCXZ CX为零时转移. JECXZ ECX为零时转移. 中断指令 INT 中断指令 INTO 溢出中断 IRET 中断返回 处理器控制指令 HLT 处理器暂停, 直到出现中断或复位信号才继续. WAIT 当芯片引线TEST为高电平时使CPU进入等待状态. ESC 转换到外处理器. LOCK 封锁总线. NOP 空操作. STC 置进位标志位. CLC 清进位标志位. CMC 进位标志取反. STD 置方向标志位. CLD 清方向标志位. STI 置中断允许位. CLI 清中断允许位. 伪指令 DW 定义字(2字节). PROC 定义过程. ENDP 过程结束. SEGMENT 定义段. ASSUME 建立段寄存器寻址. ENDS 段结束. END 程序结束. 处理机控制指令 标志处理指令 CLC（进位位置0指令） CMC（进位位求反指令） STC（进位位置为1指令） CLD（方向标志置1指令） STD（方向标志位置1指令） CLI（中断标志置0指令） STI（中断标志置1指令） NOP（无操作） HLT（停机） WAIT（等待） ESC（换码） LOCK（封锁） 破解中常用的汇编指令 基本上多数破解的思路是一样的，就是将本来判断为true的时候干的事情改为逻辑值为false就做，因此常常需要替换一些汇编命令： 12345678910111213141516 cmp a,b 比较a与bmov a,b 把b的值送给aret 返回主程序nop 无作用,英文“no operation”的简写，意思是“do nothing”(机器码90)call 调用子程序je 或jz 若相等则跳(机器码74 或0F84)jne或jnz 若不相等则跳(机器码75或0F85)jmp 无条件跳(机器码EB)jb 若小于则跳ja 若大于则跳jg 若大于则跳jge 若大于等于则跳jl 若小于则跳jle 若小于等于则跳pop 出栈push 压栈 常见机器码的修改 12 74=&gt;75 74=&gt;90 74=&gt;EB75=&gt;74 75=&gt;90 75=&gt;EB jnz -&gt; nop 1 75-&gt;90(相应的机器码修改) jnz -&gt; jmp 1 75 -&gt; EB(相应的机器码修改) jnz -&gt; jz 12 75-&gt;74 (正常) 0F 85 -&gt; 0F 84(特殊情况下,有时,相应的机器码修改) 修改为jmp je(jne,jz,jnz) =&gt;jmp相应的机器码EB （出错信息向上找到的第一个跳转）jmp的作用是绝对跳，无条件跳，从而跳过下面的出错信息。 出错信息，例如：注册码不对，sorry,未注册版不能…，“Function Not Avaible in Demo” 或 “Command Not Avaible” 或 &quot;Can’t save in Shareware/Demo&quot;等 （我们希望把它跳过，不让它出现）。 修改为nop je(jne,jz,jnz) =&gt;nop相应的机器码90 （正确信息向上找到的第一个跳转） nop的作用是抹掉这个跳转，使这个跳转无效，失去作用，从而使程序顺利来到紧跟其后的正确信息处。 正确信息，例如：注册成功，谢谢您的支持等（我们希望它不被跳过，让它出现，程序一定要顺利来到这里）。 出错信息（我们希望不要跳到这里，不让它出现）它们在存贮器和寄存器、寄存器和输入输出端口之间传送数据。 例如使用windbg时候： 12 0:000&gt; dd 0c366b28 l40c366b28 7e830c74 940f0108 c0b60fc0 01b805eb 执行ed 0c366b28 7e830c75 修改为: 12 0:000&gt; dd 0c366b28 l40c366b28 7e830c75 940f0108 c0b60fc0 01b805eb 代码规范 随着程序功能的增加和版本的提高，程序越来越复杂，源文件也越来越多，风格规范的源程序会对软件的升级、修改和维护带来极大的方便，要想开发一个成熟的软件产品，必须在编写源程序的时候就有条不紊，细致严谨。 在编程中，在程序排版、注释、命名和可读性等问题上都有一定的规范，虽然编写可读性良好的代码并不是必然的要求（世界上还有难懂代码比赛，看谁的代码最不好读懂！），但好的代码风格实际上是为自己将来维护和使用这些代码节省时间。 变量和函数的命名 匈牙利表示法 匈牙利表示法主要用在变量和子程序的命名，这是现在大部分程序员都在使用的命名约定。“匈牙利表示法”这个奇怪的名字是为了纪念匈牙利籍的Microsoft程序员Charles Simonyi，他首先使用了这种命名方法。 匈牙利表示法用连在一起的几个部分来命名一个变量，格式是类型前缀加上变量说明，类型用小写字母表示，如用h表示句柄，用dw表示double word，用sz表示以0结尾的字符串等，说明则用首字母大写的几个英文单词组成，如TimeCounter，NextPoint等，可以令人一眼看出变量的含义来，在汇编语言中常用的类型前缀有： 123456789 b 表示bytew 表示worddw 表示dwordh 表示句柄lp 表示指针sz 表示以0结尾的字符串lpsz 表示指向0结尾的字符串的指针f 表示浮点数st 表示一个数据结构 这样一来，变量的意思就很好理解： 12345 hWinMain 主窗口的句柄dwTimeCount 时间计数器，以双字定义szWelcome 欢迎信息字符串，以0结尾lpBuffer 指向缓冲区的指针stWndClass WNDCLASS结构 由于匈牙利表示法既描述了变量的类型，又描述了变量的作用，所以能帮助程序员及早发现变量的使用错误，如把一个数值当指针来使用引发的内存页错误等。 对于函数名，由于不会返回多种类型的数值，所以命名时一般不再用类型开头，但名称还是用表示用途的单词组成，每个单词的首字母大写。Windows API是这种命名方式的绝好例子，当人们看到ShowWindow，GetWindowText，DeleteFile和GetCommandLine之类的API函数名称时，恐怕不用查手册，就能知道它们是做什么用的。比起int 21h/09h和int 13h/02h之类的中断调用，好处是不必多讲的。 对匈牙利表示法的补充 使用匈牙利表示法已经基本上解决了命名的可读性问题，但相对于其他高级语言，汇编语言有语法上的特殊性，考虑下面这些汇编语言特有的问题： 对局部变量的地址引用要用lea指令或用addr伪操作，全局变量要用offset；对局部变量的使用要特别注意初始化问题。如何在定义中区分全局变量、局部变量和参数？ 汇编的源代码占用的行数比较多，代码行数很容易膨胀，程序规模大了如何分清一个函数是系统的API还是本程序内部的子程序？ 实际上上面的这些问题都可以归纳为区分作用域的问题。为了分清变量的作用域，命名中对全局变量、局部变量和参数应该有所区别，所以我们需要对匈牙利表示法做一些补充，以适应Win32汇编的特殊情况，下面的补充方法是笔者提出的，读者可以参考使用： 全局变量的定义使用标准的匈牙利表示法，在参数的前面加下划线，在局部变量的前面加@符号，这样引用的时候就能随时注意到变量的作用域。 在内部子程序的名称前面加下划线，以便和系统API区别。 如下面是一个求复数模的子程序，子程序名前面加下划线表示这是本程序内部模块，两个参数——复数的实部和虚部用_dwX和_dwY表示，中间用到的局部变量@dwResult则用@号开头： 123456789101112131415 _Calc proc _dwX,_dwY local @dwResult finit fild _dwX fld st(0) fmul ;i * i fild _dwY fld st(0) fmul ;j * j fadd ;i * i + j * j fsqrt ;sqrt(i * i + j * j) fistp @dwResult ;put result mov eax,@dwResult ret_Calc endp 代码的书写格式 排版方式 程序的排版风格应该遵循以下规则。 首先是大小写的问题，汇编程序中对于指令和寄存器的书写是不分大小写的，但小写代码比大写代码便于阅读，所以程序中的指令和寄存器等要采用小写字母，而用equ伪操作符定义的常量则使用大写，变量和标号使用匈牙利表示法，大小写混合。 其次是使用Tab的问题。汇编源程序中Tab的宽度一般设置为8个字符。在语法上，指令和操作数之间至少有一个空格就可以了，但指令的助记符长度是不等长的，用Tab隔开指令和操作数可以使格式对齐，便于阅读。如： 123 xor eax,eaxfistp dwNumberxchg eax,ebx 上述代码的写法就不如下面的写法整齐： 123 xor eax, eax fistp dwNumberxchg eax, ebx 还有就是缩进格式的问题。程序中的各部分采用不同的缩进，一般变量和标号的定义不缩进，指令用两个Tab缩进，遇到分支或循环伪指令再缩进一格，如： 12345678910111213 .datadwFlag dd ?.codestart: mov eax,dwFlag .if dwFlag == 1 call _Function1 .else call _Function2 .endif 合适的缩进格式可以明显地表现出程序的流程结构，也很容易发现嵌套错误，当缩进过多的时候，可以意识到嵌套过深，该改进程序结构了。 The link of this page is http://home.meng.uno/articles/711069a/ . Welcome to reproduce it!","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://home.meng.uno/categories/Programming-Language/"}],"tags":[{"name":"代码规范","slug":"代码规范","permalink":"http://home.meng.uno/tags/代码规范/"},{"name":"汇编","slug":"汇编","permalink":"http://home.meng.uno/tags/汇编/"},{"name":"ASM","slug":"ASM","permalink":"http://home.meng.uno/tags/ASM/"}]},{"title":"I/O体系结构","slug":"io-system","date":"2016-07-15T04:28:52.000Z","updated":"2020-12-08T10:13:10.000Z","comments":true,"path":"articles/d0c0e94a/","link":"","permalink":"http://home.meng.uno/articles/d0c0e94a/","excerpt":"I/O体系结构 虚拟文件系统利用底层函数，调用每个设备的操作，那么这些操作是如何在设备上执行的，操作系统又是如何知道设备的操作是什么的呢？这些是由操作系统决定的。 我们知道，操作系统的工作，是依赖于数据通路的，它们让信息得以在CPU、RAM、I/O设备之间传递。这些数据通路称为总线。这就包括数据总线（PCI、ISA、EISA、SCSI等）、地址总线、控制总线。I/O总线，指的就是用于CPU和I/O设备之间通信的数据总线。I/O体系的通用结构如图所示： 那么CPU是如何通过I/O总线和I/O设备交互呢？这首先得从内存和外设的编址方式说起。第一种是“独立编址”，也就是内存和外设分开编址，I/","text":"I/O体系结构 虚拟文件系统利用底层函数，调用每个设备的操作，那么这些操作是如何在设备上执行的，操作系统又是如何知道设备的操作是什么的呢？这些是由操作系统决定的。 我们知道，操作系统的工作，是依赖于数据通路的，它们让信息得以在CPU、RAM、I/O设备之间传递。这些数据通路称为总线。这就包括数据总线（PCI、ISA、EISA、SCSI等）、地址总线、控制总线。I/O总线，指的就是用于CPU和I/O设备之间通信的数据总线。I/O体系的通用结构如图所示： 那么CPU是如何通过I/O总线和I/O设备交互呢？这首先得从内存和外设的编址方式说起。第一种是“独立编址”，也就是内存和外设分开编址，I/O端口有独立的地址空间，这也被称为I/O映射方式。每个连接到I/O总线上的设备，都分配了自己的I/O地址集（在I/O地址空间中），它被称为I/O端口。in、out等指令用语CPU对I/O端口进行读写。在执行其中一条指令时，CPU使用地址总线选择所请求的I/O端口，使用数据总线在CPU寄存器和端口之间传送数据。这种方式编码逻辑清晰，速度快，但空间有限。 第二种是“统一编址”，也被称为内存映射方式，I/O端口还可以被映射到内存地址空间（这也正是现代硬件设备倾向于使用的方式），这样CPU就可以通过对内存操作的指令，来访问I/O设备，并且和DMA结合起来。这种方式更加统一，易于使用。它实际上使用了ioremap()。自从PCI总线出现后，不论采用I/O映射还是内存映射方式，都需要将I/O端口映射到内存地址空间。 每个I/O设备的I/O端口都是一组寄存器：控制寄存器、状态寄存器、输入寄存器和输出寄存器。内核会纪录分配给每个硬件设备的I/O端口。 设备驱动程序模型 在内核中，设备不仅仅需要完成相应的操作，还要对其电源管理、资源分配、生命周期等等行为进行统一的管理。因此，内核建立了一个统一的设备模型，提取设备操作的共同属性，进行抽象，并且为添加设备、安装驱动提供统一的接口。它们本身并不代表具体的对象，只是用来维持对象间的层次关系。 这里首先要提的是sysfs文件系统。和/proc类似，安装于/sys目录，其目的是表现出设备驱动程序模型间的层次关系。在驱动程序模型当中，有三种重要的数据结构（旧版本），自上到下分别是subsystem、kset、kobject。如果要理解这个模型中，每个数据结构的作用，就必须理解它们和操作系统中的什么东西相对应。它们均对应着**/sys中的目录**。kobject是这个对象模型中，所有对象的基类。kset本身首先是一个kobject，而它又承担着一个kobject容器的作用，它把kobject组织成有序的目录；subsys则是更高的一层抽象，它本身首先是一个kset。驱动、总线、设备都能够用设备驱动程序模型中的对象表示。 设备驱动程序模型中的组件 设备驱动程序模型建立在几个基本数据结构之上，它们描述了总线、设备、设备驱动器等等。这里，我们来看看它们的数据结构。首先，device用来描述设备驱动程序模型中的设备。 12345678910111213141516171819202122232425262728293031323334 struct device &#123;struct device *parent;//父设备struct kobject kobj; //对应的kobjectconst char *init_name; //初始化名const struct device_type *type;//设备的类型struct mutex mutex; //驱动的互斥量struct bus_type *bus; //设备在什么类型的总线struct device_driver *driver; //设备的驱动void *driver_data; //驱动私有数据指针struct dev_pm_info power;struct dev_pm_domain *pm_domain;//dma相关变量u64 *dma_mask; u64 coherent_dma_mask; unsigned long dma_pfn_offset;struct device_dma_parameters *dma_parms;struct list_head dma_pools; struct dma_coherent_mem *dma_mem; dev_t devt; //dev目录下的描述符u32 id; spinlock_t devres_lock;struct list_head devres_head;struct klist_node knode_class;struct class *class; //类void (*release)(struct device *dev);//释放设备描述符时候的回调函数&#125;; 首先，可以看到device中包含有一个kobject，还包含有它相关驱动对象。所有的device对象，全部收集在devices_kset中，它对应着/sys/devices中。设备的引用计数则是由kobject来完成的。device还会被嵌入到一个更大的描述符中，例如pci_dev，它除了包含dev之外，还有PCI所特有的一些数据结构。device_add完成了新的device的添加工作。我注意到，error = bus_add_device(dev);，也就是说device的添加会把它和bus关联起来。 再来看看驱动程序的结构。其数据结构为device_driver。相对于设备的数据结构来说，它相对较为简单：对于每个设备驱动，都有几个通用的方法，分别用语处理热插拔、即插即用、电源管理、探查设备等。同样，驱动也会被嵌入到一个更大的描述符中，例如pci_driver。 1234567891011121314151617181920212223 struct device_driver &#123;const char *name; //驱动名struct bus_type *bus; //总线描述符struct module *owner;const char *mod_name; //模块名bool suppress_bind_attrs; /* disables bind/unbind via sysfs */const struct of_device_id *of_match_table;const struct acpi_device_id *acpi_match_table;int (*probe) (struct device *dev); //探测设备int (*remove) (struct device *dev); //移除设备void (*shutdown) (struct device *dev); //断电方法int (*suspend) (struct device *dev, pm_message_t state);//低功率int (*resume) (struct device *dev); //恢复方法const struct attribute_group **groups;const struct dev_pm_ops *pm; //电源管理的操作struct driver_private *p;&#125;; 为什么这里没有kobject呢？它实际上保存在了driver_private当中，这个结构和device_driver是双向链接的。 1234567 struct driver_private &#123;struct kobject kobj;struct klist klist_devices;struct klist_node knode_bus;struct module_kobject *mkobj;struct device_driver *driver;&#125;; driver的添加，通过调用driver_register()来完成，它同样包含一个函数：bus_add_driver()，也就是将driver添加到某个bus。 再来看看总线的结构。bus是连接device和driver的桥梁，bus中的很多代码，都是为了让device找到driver来设计的。总线的数据结构如下： 123456789101112131415161718192021222324252627 struct bus_type &#123;const char *name;const char *dev_name;struct device *dev_root;struct device_attribute *dev_attrs; /* use dev_groups instead */const struct attribute_group **bus_groups;const struct attribute_group **dev_groups;const struct attribute_group **drv_groups;//检查驱动是否支持特定设备int (*match)(struct device *dev, struct device_driver *drv); //回调事件，在kobject状态改变时调用int (*uevent)(struct device *dev, struct kobj_uevent_env *env);//探测设备int (*probe)(struct device *dev);//从总线移除设备int (*remove)(struct device *dev);//掉电void (*shutdown)(struct device *dev);int (*online)(struct device *dev);int (*offline)(struct device *dev);//改变电源状态和恢复int (*suspend)(struct device *dev, pm_message_t state);int (*resume)(struct device *dev);const struct dev_pm_ops *pm;const struct iommu_ops *iommu_ops;struct subsys_private *p;struct lock_class_key lock_key;&#125;; 同样，总线也有一个subsys_private，它保存了kobject。but_type中定义了一系列的方法。例如，当内核检查一个给定的设备是否可以由给定的驱动程序处理时，就会执行match方法。可以用bus_for_each_drv()和bus_for_each_dev()函数分别循环扫描drivers和device两个链表中的所有元素，来进行match。 设备文件 设备驱动程序使得硬件设备，能以特定方式，响应控制设备的编程接口（一组规范的VFS函数，open，read，lseek，ioctl等），这些函数都是由驱动程序来具体实现的。在设备文件上发出的系统调用，都会由内核转化为对应的设备驱动程序函数，因此设备驱动必须被注册，也即构造一个device_driver，并且加入到设备驱动程序模型中。在注册时，内核会试图进行一次match。注意，这个注册的过程基本driver_register通常不会在驱动中直接调用，但我们但驱动通常都会间接的调用它来完成注册。 遵循linux“一切皆文件”的原则，I/O设备同样可以当作设备文件来处理，它和磁盘上的普通文件的交互方式一样，例如都可以通过write()系统调用写入数据。设备文件可以通过mknod()节点来创建，它们保存在/dev/目录下。 linux当中，硬件设备可以花费为两种：字符设备和块设备。其中，块设备指的是可以随机访问的设备，例如硬盘、软盘等；而字符设备则指的是声卡、键盘这样的设备。设备文件同样在VFS当中，但它的索引节点没有指向磁盘数据的指针，相反地它对应一个标识符（包含一个主设备号和一个次设备号）。VFS会在设备文件打开时，改变一个设备文件的缺省文件操作，让它去调用和设备相关的操作。 字符设备驱动程序 这里我们以字符设备驱动程序为例。首先，字符设备的驱动，在linux系统中，是以cdev结构来表示的： 12345678 struct cdev &#123;struct kobject kobj;struct module *owner;const struct file_operations *ops;struct list_head list; //包括的inode的devicesdev_t dev;unsigned int count;&#125;; 现在让我们回顾一下inode的数据结构： 123456789 struct inode &#123; ... union &#123; struct pipe_inode_info *i_pipe; struct block_device *i_bdev; struct cdev *i_cdev;&#125;; ...&#125; 我们看到了cdev指针的影子，可见cdev和inode确实是直接相关的。要实现驱动，首先就要对cdev进行初始化，注册字符设备。驱动的安装，首先要分配cdev结构体、申请设备号并初始化cdev。注意，驱动程序是如何和刚才我们所说的设备驱动模型建立联系的呢？实际上在初始化cdev的时候，就调用了kobject_init()，在模型中添加了一个kobject。 随后，驱动要注册cdev，也即调用cdev_add()函数。这个工作主要是由kobj_map()来实现的，它是一个数组。对于每一类设备，都有一个全局变量，例如字符设备的cdev_map，块设备的bdev_map。最后要进行硬件资源的初始化。 123456789101112 int cdev_add(struct cdev *p, dev_t dev, unsigned count)&#123; int error; p-&gt;dev = dev; p-&gt;count = count; error = kobj_map(cdev_map, dev, count, NULL, exact_match, exact_lock, p); if (error) return error; kobject_get(p-&gt;kobj.parent); return 0;&#125; kobj_map的结构如下，它用来保存设备号和kobject的对应关系 123456789101112 struct kobj_map &#123; struct probe &#123; struct probe *next; dev_t dev; unsigned long range; struct module *owner; kobj_probe_t *get; int (*lock)(dev_t, void *); void *data; &#125; *probes[255]; struct mutex *lock;&#125;; 不过到现在为止，我们都还没有说明，程序在访问字符设备时，是如何去调用正确的方法的。我们曾提到过，open()系统调用会改变字符文件对象的f_op字段，将默认文件操作替换为驱动的操作。在字符设备文件创建时，会调用init_special_inode来进行索引节点对象的初始化。其inode的操作(def_chr_fops)只包含一个默认的文件打开操作，也即chrdev_open。它会根据inode，首先利用cdev_map，找到对应的kobject，随后再进一步找到cdev，然后从中提取出文件操作的函数fops，并把它填充到file当中去。 123456789101112131415161718192021222324252627282930313233343536373839 static int chrdev_open(struct inode *inode, struct file *filp)&#123; const struct file_operations *fops; struct cdev *p; struct cdev *new = NULL; int ret = 0; spin_lock(&amp;cdev_lock); p = inode-&gt;i_cdev; if (!p) &#123; struct kobject *kobj; int idx; spin_unlock(&amp;cdev_lock); kobj = kobj_lookup(cdev_map, inode-&gt;i_rdev, &amp;idx);//获取对应的kobject if (!kobj) return -ENXIO; new = container_of(kobj, struct cdev, kobj); spin_lock(&amp;cdev_lock); /* Check i_cdev again in case somebody beat us to it while we dropped the lock. */ p = inode-&gt;i_cdev; if (!p) &#123; inode-&gt;i_cdev = p = new; list_add(&amp;inode-&gt;i_devices, &amp;p-&gt;list);//将device加入到cdev的list中去 new = NULL; &#125; else if (!cdev_get(p)) ret = -ENXIO; &#125; else if (!cdev_get(p)) ret = -ENXIO; spin_unlock(&amp;cdev_lock); cdev_put(new); if (ret) return ret; ret = -ENXIO; fops = fops_get(p-&gt;ops) if (!fops) goto out_cdev_put; replace_fops(filp, fops);//替换file当中的fops return ret;&#125; 这里很奇怪的是，我们并没有看到类似前面提到的driver_register()、device_register()这样的函数。实际上这里并没有真正创建一个设备，而只是说创建了一个接口，所以有这样一个这个问题：为什么cdev_add没有产生设备节点？对于这个问题，我们应该理解为cdev和driver/device二者是配套工作的，cdev用来和用户交互，而device则是内核中的结构。 另一个问题是，在上面的过程中，似乎没有提及设备文件的创建。实际上，作为一个rookie，那么设备文件常常是用mknod命令手动创建的。当然，linux自然也提供了自动创建的借口，那就是利用udev来实现，调用device_create()函数。 当然，这个例子只是为了说明，操作系统的驱动程序是如何工作的，为什么对I/O设备的操作可以抽象成对设备文件的操作，程序在操作I/O文件时，是如何使用正确的操作的。 块设备的驱动 和字符设备类似，操作系统中的块设备，也是以文件的形式来访问。这里有一个很拗口的问题：磁盘是一个块设备，块设备有一个块设备文件。那么访问块设备文件和访问普通的磁盘上的文件有什么关系呢？ 不论是块设备文件还是普通的文件，它们都是通过VFS来统一访问的。只不过对于一个普通文件，它可能已经在RAM中了（高速缓存机制），因此它的访问可能会直接在RAM中进行；但如果说要修改磁盘上的内容，或者文件内容不在RAM中，则也会间接地，通过块设备文件进行访问。这个驱动模型可以用这样一个图表示： 这里我们只考虑最底层的情况：内核从块设备读取数据。为了从块设备中读取数据，内核必须知道数据的物理位置，而这正是映射层的工作。映射层的工作包括两步： 根据文件所在文件系统的块，将文件拆分成块，然后内核能够确定请求数据所在的块号； 映射层调用文件系统具体的函数，找到数据在磁盘上的位置，也就是完成文件块号，到逻辑块号的映射关系。 随后的工作在通用块层进行，内核在这一层，启动I/O操作。通常一个I/O操作对应一组连续的块，我们把它称为bio，它用来搜集底层需要的信息。 I/O调度层负责根据内核中的各种策略，把待处理的I/O数据传送请求，进行归类。它的作用是把物理介质上相邻的数据请求，进行合并，一并处理。 最后一层也就是通过块设备的驱动来完成了，它向I/O接口发送适当的命令，从而进行实际的数据传送。 通用块层 通用块层负责处理所有块设备的请求，其核心数据结构就是bio。它代表一次块设备I/O请求。 1234567891011121314151617 struct bio &#123;struct bio *bi_next; //请求队列中的下一个biostruct block_device *bi_bdev; //块设备描述符指针unsigned long bi_flags; /* status, command, etc */unsigned long bi_rw; //rw位struct bvec_iter bi_iter; unsigned int bi_phys_segments;//合并后有多少个段unsigned int bi_seg_front_size;unsigned int bi_seg_back_size;atomic_t bi_remaining;//剩余的bio_vecbio_end_io_t *bi_end_io;//bio结束的回调函数void *bi_private;unsigned short bi_vcnt; //bio中biovec的数量unsigned short bi_max_vecs;//最多能有多少个atomic_t bi_cnt; //结构体的使用计数struct bio_vec *bi_io_vec; //bio_vec数组&#125;; 在这个数据结构中，还包含了一个bio_vec。这是什么意思呢？在linux中，相邻数据块被称为一个段，每个bio_vec对应一个内存页中的段。在io操作期间，bio是会一直更新的，其中的bi_iter用来在数组中遍历，按每个段来执行下一步的操作。 那么当通用块层收到一个I/O请求操作时，会发生什么呢？首先内核会为这次操作分配bio描述符，并对它进行填充。随后通用块层会调用generic_make_request，这个函数的作用很明确：它会进行一系列检查和设置，保证bio中的信息是针对整个磁盘，而不是磁盘分区的；随后获取这个块设备相关的请求队列q，调用q-&gt;make_request_fn，把bio插入请求队列中去。 I/O调度层 在块设备上，每个I/O请求操作都是异步处理的，通用块层的请求会被加入块设备的请求队列中，每个块设备都会单独地进行I/O的调度，这样能够有效提高磁盘的性能。 前面提到，通用块层会调用一个q-&gt;make_request_fn，向I/O调度程序发送一个请求，该函数会进一步调用__make_request()。这个函数的目的，就是把bio放进请求队列当中：（1）如果请求队列是空的，就构造一个新的请求插入；（2）如果请求队列不是空的，但是bio不能合并（不能合并到某个请求的头和尾），也构造一个新的请求插入；（3）请求队列不是空的，并且bio可以合并，就合并到对应的请求中去。注意，bio，请求和请求队列的关系如下： 1234567891011121314151617181920 -- request_queue |-- request1 |-- bio0 |-- request2 |-- bio1 |-- bio2``` 而I/O的调度，就是对请求队列进行排序，针对磁盘的特点，降低寻道的次数。这里说说几个常见的算法：- CFQ完全公平队列：默认的调度算法，完全公平排队。每个进程/线程都单独创建一个队列，并且用上面提到的策略进行管理。队列间采用时间片的方式来分配I/O。- Deadline最后期限算法：在电梯调度的基础上，根据读写请求的“最后期限”进行排序，并通过读期限短于写期限来保证写操作不被饿死。 - 预期I/O算法：与最后期限类似，但是在读操作时，会预先判断当前的进程是否马上会有读操作，并且优先地进行处理。 - NOOP：适用于固态硬盘，不进行任何优化。 总而言之，I/O调度层的作用，就是把请求的队列重新排序，并逐个交给块设备驱动程序进行处理。# 块设备驱动程序I/O调度层排序好的请求，会由块设备的驱动程序来处理。同样，块设备也遵循着我们前面提到的驱动程序模型：块设备对应一个`device`，而驱动程序对应了一个`device_driver`。对于块设备来说，驱动程序也要通过`register_blkdev()`注册一个设备号。随后，驱动程序要初始化`gendisk`描述符，以及它所包含的设备操作表`fops`。在此之后，是“请求队列”的初始化，以及中断程序的设置：要为设备注册IRQ线。最后要把磁盘注册到内核（`add_disk`）,并把它激活。 当一个块设备文件被`open()`时，内核同样也要为它初始化操作。对于块设备来说，其默认的文件操作如下： const struct file_operations def_blk_fops = { .open = blkdev_open, .release = blkdev_close, .llseek = block_llseek, .read = new_sync_read, .write = new_sync_write, .read_iter = blkdev_read_iter, .write_iter = blkdev_write_iter, .mmap = generic_file_mmap, .fsync = blkdev_fsync, .unlocked_ioctl = block_ioctl, #ifdef CONFIG_COMPAT .compat_ioctl = compat_blkdev_ioctl, #endif .splice_read = generic_file_splice_read, .splice_write = iter_file_splice_write, }; `dentry_open()`方法会调用`blkdev_open()`。它（1）首先会获取块设备的描述符：如果块设备已经打开，则可以通过inode-&gt;i_bdev直接获取，否则则需要根据设备号去查找块设备描述符。（2）获取块设备相关的`gendisk`地址，`get_gendisk`是通过设备号来找到gendisk的。（3）如果是第一次打开块设备，则要根据它是整盘还是分区，进行相应的设置和初始化。（4）如果不是第一次打开，只需要按需要执行自定义的`open()`函数就行了。 # 补充：I/O的监控方式 - 轮询：CPU重复检查设备的状态寄存器，直到寄存器的值表明I/O操作已经完成了。 - 中断：设备发出中断信号，告知I/O操作已经完成了，数据放在对应的端口，当数据缓冲满了时，由CPU去取，CPU需要控制数据传输的过程。 - DMA：由CPU的DMA电路来辅助数据的传输，CPU不需要参与内存和IO之间的传输过程，只需要通过DMA的中断来获取信息。DMA能够在所有数据处理完时才通知CPU处理。 &lt;br&gt;&lt;br&gt;The link of this page is [http://home.meng.uno/articles/d0c0e94a/](http://home.meng.uno/articles/d0c0e94a/) . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://home.meng.uno/tags/Linux/"},{"name":"I/O","slug":"I-O","permalink":"http://home.meng.uno/tags/I-O/"}]},{"title":"rIOMMU：Efficient IOMMU for I/O Devices that Employ Ring Buffers","slug":"rIOMMU","date":"2016-06-22T12:28:00.000Z","updated":"2020-12-02T02:04:19.000Z","comments":true,"path":"articles/51c782dd/","link":"","permalink":"http://home.meng.uno/articles/51c782dd/","excerpt":"文章内容理解 作者写作背景 在I/O设备开始与CPU异步直接与主存交换信息（DMA时代）开始时，DMA使用物理地址直接存取，这就给系统带来了很多诸如劣质甚至恶意设备影响，造成系统崩溃……等麻烦，在这种情况下，对I/O设备的存取统一管理的单元——输入输出存储管理单元（IOMMU）应运而生。随着设备带宽的提高，那些像网卡和PCIe SSD控制器等高带宽设备可以与系统通过一个环型缓冲器相互影响，于是一种带有环形的缓冲器能够分层、平滑地替换虚拟地址的rIOMMU成了研究的重点。这种rIOMMU能高达7.56倍地提高普通IOMMU的效率，是没有IOMMU的0.77—1.00倍。 文章结构 该文章从以下","text":"文章内容理解 作者写作背景 在I/O设备开始与CPU异步直接与主存交换信息（DMA时代）开始时，DMA使用物理地址直接存取，这就给系统带来了很多诸如劣质甚至恶意设备影响，造成系统崩溃……等麻烦，在这种情况下，对I/O设备的存取统一管理的单元——输入输出存储管理单元（IOMMU）应运而生。随着设备带宽的提高，那些像网卡和PCIe SSD控制器等高带宽设备可以与系统通过一个环型缓冲器相互影响，于是一种带有环形的缓冲器能够分层、平滑地替换虚拟地址的rIOMMU成了研究的重点。这种rIOMMU能高达7.56倍地提高普通IOMMU的效率，是没有IOMMU的0.77—1.00倍。 文章结构 该文章从以下七个部分论述： 摘要：简述本次论文的主要内容； 介绍：向读者介绍什么是IOMMU以及什么是Riommu其他缩写的概念； 背景：在什么情况下，作者提出用rIOMMU改进原有IOMMU的： 首先讲OS的DMA保护； 接着讲IOMMU的设计与提升作用； 最后提出rIOMMU的概念。 安全代价：讲解OS与IO设备的关系与保护方式 设计：这是作者主要论述的部分，也是本文的核心，从以下三个方面论述： 数据结构 硬件实现 软件实现 评估：从7个模式分别测试该设计的可行性，又分为方法与结果两个部分论述 总结 下面直接进入文章的设计部分： 数据结构 作者从软件、硬件两种方式来组合实现rIOMMU,所以数据结构不得不是最先介绍的东西了。我将以代码与文字结合的方式介绍： 以上四个结构体所展示的数据结构，都是硬件软件公用的，他们都被rIOMMU用来转换虚拟地址，被操作系统调用。接下来定义一个只有硬件会使用的结构体： ## 硬件处理 首先，在与rRING相连的rIOTLB_entry的IOVA中寻找e，如果e不存在rIOMMU就搜索表，用上面定义的数据结构，找到rPTE，并插入rIOTLB一个匹配的entry，同时使表移动确保e.rpte是rPTE中给定的rIOVA。 其次，如果e已经初始化在rIOTLB被找到，rIOMMU匹配每一个IOVA和e，并且实时更新e。 然后，检查IOVA.offset如果出错，会造成rIOMMU启动I/O默认页（IOPF）,当然这并不是所希望的，未出错时，最终该offset会加到rPTE.phys_addr上形成虚拟地址的转换。 最后还有关于错误的一些处理，在此不再赘述。 软件处理 说完了硬件上的处理，接下来该软件上的操作了，说是软件其实就是设备的驱动程序、映射函数等底层的软件，这部分的处理和Linux中IOMMU的基准处理相似。 这一部分主要是处理映射的问题，具体就是将数据结构中定义的每个结构体的各个域之间建立联系。映射给每个设备一个ring ID，一个映射的物理地址，它包含两个部分： 在ring的尾部分配一个环入口rPTE，然后更新上面数据结构提到的tail/nmapped域； 当rPTE初始化后，首先确保其内容更新对rIOMMU是可见的，它的返回状态是入口的索引，而且这个ring ID是由IOVA所得到的。 这一部分也同样有错误处理，在此不再赘述。 可行性分析 可行性评估 在读了这一篇关于优化IOMMU的文章后，我在网上寻找了一下，相关的研究进展情况，发现早在很多年前像Intel、AMD等处理器生产商早就有关于IOMMU的应用，但是在像本文所讲的rIOMMU还没有得到应用过，也就是说文章现在所言皆是理想的情况。 在文章的后半部分，作者也从“方法”、“实验执行”、“基准”等方面做了很多测试，其中在“方法”模块采用：①strict，②strict+，③defer，④defer+，⑤riommu-，⑥riommu，⑦none其中模式，可以说是比较准确的了下图及下表展示所有测试结果： 通过结果可见，当运行“需求-响应”（RR）模式是，相对于普通IOMMU，提升不是多么的明显，但是在其他方面，结果还是挺理想的。 创新之处 作者在测试部分将模拟方法分成7个等级，基本上模拟了所有的正常可能情况，有力证明了本次实验的成功。 作者在进行设计之初，分析了现有的DMA与IOMMU，然后提出自己的设计，使我们对总体设计有一个整体认识，也使的作者的设计比较容易接受。 在设计上，作者不仅考虑到硬件，同时对软件也进行了设计，而且设计过程相当明细，用实际的代码来解释，使设计极具可行性。 设计存在的不足分析 我认为作者在设计时并未考虑这样设计所带来的结构复杂性，也未对实际的能耗做评估，也许可能会由于总线分配问题而没法嵌入系统。 作者虽然提到rIOMMU在对有错误的DMA设备的保护的作用，但是没有给出具体的保护方案。 我想到既然通过环形缓冲器能解决速度的问题，那么会不会带来其他的弊端，例如：某次传输没有传输完就被后一次的覆盖掉或者当环形缓冲器中有错误，没法处理时会不会发生DMA总是在那里占用总线而又没有实际的数据传输。 既然是环形缓冲器，那么当传输数据是暂时性的，而且又没能在有效的时间范围内进入环形缓冲器，那么是不是就有冲掉的风险。 针对不足的改进意见 在上一部分我浅显地提出了自己认为是不足的地方，在此仅仅做些自己认为合理的改进意见。不能保证我的观点是完全正确，但是这确实是我自己对相关问题的认识所得。 作者可以增加关于能耗的测试，作者提到这样的rIOMMU确实在某些方面比IOMM要有效得多，但是能耗是不是也如此呢？作者应该往这方面做些必要的测试（由于水平有限，所以只能借助作者之手完成这项测试）。 既然rIOMMU对设备有保护作用，那么其作用具体体现在哪呢？仅仅是在对错误数据的处理上，远远不足以满足现在设备的要求，例如，当某设备出错，重复发送某信息时，作为向主存传输信息的接口，应该如何处理呢？我认为可以从软件上进行相关判断，我觉得这将是这类IOMMU值得改进的地方。 我觉得环缓冲器还应该增加错误信息清除能力，当出现出错信息时，能有效地将其清除并且不影响其他设备的正常传输。 每个环形缓冲器都会有数量“size”的限制，像用尽这种现象应该是尽量避免的。也许可以通过取一个大大的“buffer”值来基本解决这一问题，但是将造成空间消耗严重，我觉得更好的解决办法是，修改设计，使设计能够容纳多个环缓冲器，当然不是每个环每次都会使用，当第一个环占满并且还有数据需要缓冲时，才会调用第二个环……在这种设计上，我觉得将会增加很多判断，设计空间的动态分配等问题。 附录 在此将本文所有用到的英文缩写做一下梳理： IOMMU: Input/Output Memory Management Unit 输入输出存储管理单元 rIOMMU: IOMMU Employed Ring Buffers 带环缓冲器的IOMMU IOTLB: I/O Translation Lookaside Buffer 用于IOMMU的块表 IOVA: I/O Visual Address IO虚拟地址 The link of this page is http://home.meng.uno/articles/51c782dd/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"IOMMU","slug":"IOMMU","permalink":"http://home.meng.uno/tags/IOMMU/"},{"name":"Buffer","slug":"Buffer","permalink":"http://home.meng.uno/tags/Buffer/"}]},{"title":"HTTPS","slug":"HTTPS","date":"2016-06-16T09:31:04.000Z","updated":"2021-01-01T16:54:04.000Z","comments":true,"path":"articles/87267df0/","link":"","permalink":"http://home.meng.uno/articles/87267df0/","excerpt":"HTTPS HTTP在安全方面的不足 * 通信使用明文（不加密），内容可能会被窃听 * 不验证通信方的身份，因此有可能遭遇伪装 * 无法证明报文的完整性，所以有可能已遭篡改 这些问题不仅在 HTTP 上出现，其他未加密的协议中也会存在这类问题 窃听 由于 HTTP 本身不具备加密的功能，所以也无法做到对通信整体（使用 HTTP 协议通信的请求和响应的内容）进行加密。即，HTTP 报文使用明文（指未经过加密的报文）方式发送。 伪装 HTTP 协议中的请求和响应不会对通信方进行确认。也就是说存在“服务器是否就是发送请求中 URI 真正指定的主机，返回的响应是否真的返回到实际提出","text":"HTTPS HTTP在安全方面的不足 通信使用明文（不加密），内容可能会被窃听 不验证通信方的身份，因此有可能遭遇伪装 无法证明报文的完整性，所以有可能已遭篡改 这些问题不仅在 HTTP 上出现，其他未加密的协议中也会存在这类问题 窃听 由于 HTTP 本身不具备加密的功能，所以也无法做到对通信整体（使用 HTTP 协议通信的请求和响应的内容）进行加密。即，HTTP 报文使用明文（指未经过加密的报文）方式发送。 伪装 HTTP 协议中的请求和响应不会对通信方进行确认。也就是说存在“服务器是否就是发送请求中 URI 真正指定的主机，返回的响应是否真的返回到实际提出请求的客户端”等类似问题。 篡改 由于 HTTP 协议无法证明通信的报文完整性，因此，在请求或响应送出之后直到对方接收之前的这段时间内，即使请求或响应的内容遭到篡改，也没有办法获悉。响应在传输途中，遭攻击者拦截并篡改内容的攻击称为中间人攻击（Man-in-the-Middle attack，MITM） HTTP+ 加密 + 认证 + 完整性保护 =HTTPS 通常，HTTP 直接和 TCP 通信。当使用 SSL 时，则演变成先和 SSL 通信，再由 SSL 和 TCP 通信了。简言之，所谓 HTTPS，其实就是身披 SSL 协议这层外壳的 HTTP。 SSL 是独立于 HTTP 的协议，所以不光是 HTTP 协议，其他运行在应用层的 SMTP 和 Telnet 等协议均可配合 SSL 协议使用。可以说 SSL 是当今世界上应用最为广泛的网络安全技术。 共享密钥（对称密钥） &amp; 公开密钥（非对称密钥） 了解HTTPS必须要了解到一些常用的加密手段，以及其优势和劣势，因为HTTPS 采用共享密钥加密和公开密钥加密两者并用的混合加密机制。 共享密钥 加密和解密都会用到密钥。没有密钥就无法对密码解密，反过来说，任何人只要持有密钥就能解密了。如果密钥被攻击者获得，那加密也就失去了意义。但是与公开密钥相比，其加密、揭秘性能高。 公开密钥 公开密钥加密使用一对非对称的密钥。一把叫做私有密钥（private key），另一把叫做公开密钥（public key）。顾名思义，私有密钥不能让其他任何人知道，而公开密钥则可以随意发布，任何人都可以获得。 使用公开密钥加密方式，发送密文的一方使用对方的公开密钥进行加密处理，对方收到被加密的信息后，再使用自己的私有密钥进行解密。利用这种方式，不需要发送用来解密的私有密钥，也不必担心密钥被攻击者窃听而盗走。 HTTPS 采用混合加密机制 看得出，服务器在提供给客户端public key时如果被第三发截获、掉包那么就有安全问题了，所有又有了第三发验证机构：数字证书认证机构（CA，Certificate Authority） 首先，服务器的运营人员向数字证书认证机构提出公开密钥的申请。数字证书认证机构在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公钥证书后绑定在一起。 服务器会将这份由数字证书认证机构颁发的公钥证书发送给客户端，以进行公开密钥加密方式通信。公钥证书也可叫做数字证书或直接称为证书。 HTTPS 的安全通信机制 一切都准备好了，下一步就是利用上面的知识点做一个可靠、安全的通行链路了，具体细节可以从这个流程图例获得 请注意第2步时，当服务器给客户端返回自己的证书时，证书包含三部分内容，公钥、名称、数字签名等信息；注意数字签名是加密的，数字签名是用颁发机构的私钥对本证书的公钥，名称以及其他信息做hash散列加密而成的，所以客户端需要解密数字签名来验证该证书是否是合法可靠的，那怎么解密呢，客户端浏览器会找到该证书的根证书颁发机构，然后在本机上的证书管理器里寻找 那些受信任的根证书颁发机构列表是否有该证书的根证书颁发机构，如果有，则用该根证书的公钥解密服务器下发的证书 如果不能正常解密，则服务器下发的证书则被认为是伪造的，浏览器弹出提示框 如果能正常解密，则获取到公钥，名称，数字签名信息跟本身的公钥等其他信息比对一下，确认公钥没有被篡改，如果公钥不一致，则依然被认为是不可信的 因此客户端验证服务器的合法性取决于公钥，而公钥的合法性取决于ca证书颁发机构的合法性，这里会形成一个信任链，而终点则是CA根证书，根证书是CA机构自己办法给自己的，根证书是一个特殊的数字证书，公钥是公开的，而私钥是被CA机构保存在硬件中的，所以证书的安全性取决于你对该CA机构的信任，反过来说，加入CA机构的密钥被窃取，那么该CA机构颁发的所有证书将会存在灾难性安全问题； 就像你验证身份证是否真实，肯定去公安局验证，那么谁来保证公安局是合法可靠的呢，没人能保证，公安局自己生命自己是合法可靠的，就这么简单 SSL/TLS HTTPS 使用 SSL（Secure Socket Layer） 和 TLS（Transport Layer Security）这两个协议。 SSL 技术最初是由浏览器开发商网景通信公司率先倡导的，开发过 SSL3.0 之前的版本。目前主导权已转移到 IETF（Internet Engineering Task Force，Internet 工程任务组）的手中。 IETF 以 SSL3.0 为基准，后又制定了 TLS1.0、TLS1.1 和 “TLS1.2。TSL 是以 SSL 为原型开发的协议，有时会统一称该协议为 SSL。当前主流的版本是 SSL3.0 和 TLS1.0。 由于 SSL1.0 协议在设计之初被发现出了问题，就没有实际投入使用。SSL2.0 也被发现存在问题，所以很多浏览器直接废除了该协议版本。 HTTPS or Not 虽然HTTPS在安全上做到了保障，但是一份付出一分收获。 速度 SSL 的慢分两种。一种是指通信慢。另一种是指由于大量消耗 CPU 及内存等资源，导致处理速度变慢。 和使用 HTTP 相比，网络负载可能会变慢 2 到 100 倍。除去和 TCP 连接、发送 HTTP 请求 • 响应以外，还必须进行 SSL 通信，因此整体上处理通信量不可避免会增加。 价格 要进行 HTTPS 通信，证书是必不可少的。而使用的证书必须向认证机构（CA）购买。证书价格可能会根据不同的认证机构略有不同。通常，一年的授权需要600人民币。 HTTPS真的可靠吗？ 没有绝对的安全 一个合法有效的SSL证书误签发给了假冒者 破解SSL证书签发CA的私钥 SSL证书签发CA的私钥泄露 破解SSL证书的私钥 SSL证书的私钥泄露 伪造一个合法有效的SSL证书 认证机构主动为假冒网站签发合法有效的服务器证书 利用可信的SSL服务器证书进行中间人攻击 在用户主机中植入伪造的根CA证书（或一个完整的CA证书链） 旁路证书可信性的验证 The link of this page is http://home.meng.uno/articles/87267df0/ . Welcome to reproduce it!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://home.meng.uno/categories/Software-Engineering/"}],"tags":[{"name":"web","slug":"web","permalink":"http://home.meng.uno/tags/web/"},{"name":"https","slug":"https","permalink":"http://home.meng.uno/tags/https/"},{"name":"ssl","slug":"ssl","permalink":"http://home.meng.uno/tags/ssl/"}]},{"title":"Linux中的Namespace","slug":"namespace","date":"2016-06-10T03:44:56.000Z","updated":"2020-12-08T10:24:18.000Z","comments":true,"path":"articles/e81ea9b1/","link":"","permalink":"http://home.meng.uno/articles/e81ea9b1/","excerpt":"当前，linux实现了6种不同类型的namespaces。每种namespace，都用来包含一类特定的系统资源，这样从命名空间内部的进程来看，它们就拥有了隔离的全局资源。namespaces的一个目标就是容器，一种轻量级的虚拟化工具，让一组进程认为它们是系统上仅有的一组进程。 Mount namespaces mount namespace(CLONE_NEWNS)隔离一组进程所能看到文件系统mount点。在不同mount namespaces中的进程，对于文件系统有不同的视图。在使用了mount namespaces之后，mount和umount系统调用不再对所有进程可见的，全局的moun","text":"当前，linux实现了6种不同类型的namespaces。每种namespace，都用来包含一类特定的系统资源，这样从命名空间内部的进程来看，它们就拥有了隔离的全局资源。namespaces的一个目标就是容器，一种轻量级的虚拟化工具，让一组进程认为它们是系统上仅有的一组进程。 Mount namespaces mount namespace(CLONE_NEWNS)隔离一组进程所能看到文件系统mount点。在不同mount namespaces中的进程，对于文件系统有不同的视图。在使用了mount namespaces之后，mount和umount系统调用不再对所有进程可见的，全局的mount points进行操作，而是只会影响和发起调用的进程相关的mount namespace。 利用主从关系，还可以让一个mount namespace自动拥有另一个mount namespace的内容，例如一个硬盘设备挂在到某个namespace中后会自动显示在另一个namespace中。 mount namespace是linux上实现的第一种namespace。 UTS namespaces UTS namespace(CLONE_NEWUTS)隔离两种系统标识符：nodename和domainname。在容器的上下文环境中，UTS namespaces特性允许每个容器拥有自身的hostname和NIS domain name。这允许了根据容器的name来定义它们的行为，uts指的是UNIX Time-sharing System，它是传递给uname系统调用的参数。 IPC namespaces IPC namespaces(CLONE_NEWIPC)隔离inter-process communication resources，也即跨进程的通讯资源，System V IPC，以及POSIX message queues。这些IPC机制的共性时，IPC objects是由特殊机制来进行识别的，而不是文件系统的路径。在每个namespaces当中，又有其自身所拥有的System V IPC标识符和POSIX message queue filesystem。 PID namespaces PID namespaces(CLONE_NEWPID)隔离进程ID空间，也就是说，不同PID命名空间的进程，可以拥有相同的PID。这样做的一个好处是，容器能够在不同的hosts之间转移，但是又能够保持其中的进程ID不变。而且PID namespace能够允许每个容器拥有自己的init(pid 1)，对初始化、孤儿进程等事件进行处理。 从一个PID namespace的角度来看，一个进程拥有两个PID：namespace内部的PID，以及namespace外部的，host上的PID。PID namespaces也是可以层叠的，从进程所归属的PID命名空间开始，一直到根PID namespace，它都有一个PID；一个进程只能看到处于它所在PID namespace当中的，以及更下层的其他进程。 Userspace API 为了创建一个新的namespace，进程需要调用clone系统调用，并且使用CLONE_NEWPID标识位。 在一个新的namespace当中，第一个task的PID是1，它也就是这个namespace的init，以及child_reaper。但这个init是可以死亡的，此时这个namespace都会终止。 在把tasks分割出来之后，还必须对proc进行处理，让它只显示当前task可见的PID。为了实现这个目的，procfs应该在每个namespace被使用一次。 Internal API 一个task所拥有的所有PID都在struct pid中被描述了。这个数据结构如下： 123456789101112 struct upid &#123;int nr; /* moved from struct pid */struct pid_namespace *ns; /* the namespace this value is visible in */struct hlist_node pid_chain; /* moved from struct pid */ &#125;; struct pid &#123;atomic_t count;struct hlist_head tasks[PIDTYPE_MAX];struct rcu_head rcu;int level; /* the number of upids */struct upid numbers[0]; &#125;; 这里，struct upid表示PID值，它储存在hash当中，并且拥有PID值。为了转换得到这个pid值，可以使用task_pid_nr,pid_nr_ns(),find_task_by_vpid等函数。 这些函数的后缀有一些规律： __nr()：对“全局”的PID进行操作，这里全局指的是在整个系统中也是独一无二的。pid_nr会告诉你struct pid的global PID，这只在PID值不会离开kernel时使用。 __vnr()：对“virtual”PID进行操作，也就是进程可见的ID，例如task_pid_vnr会告诉你一个task的PID。 _nr_ns()：对指定namespace中的PID进行处理，如果希望得到某个task的PID，可以通过task_pid_nr_ns来获得pid number，在用find_task_by_pid_ns来找到这个task。这个方法在系统调用中很常见，特别是当PID来自用户空间时。在这种情况下，task可能是在另一个namespace中的。 network namespaces network namespaces(CLONE_NEWNET)将系统中与网络相关的资源隔离。也就是说，每个namespace当中拥有自身的网络设备、IP地址、IP路由表，端口号等。 network namespaces让containers能够被应用到网络的层面上。每个container能够拥有自身的网络设备、并且其应用能够被绑定到namespace中特有的端口号上，对于特定的container，还可以设置特殊的路由规则。例如，可以在同一个host系统上，运行多个用container包含的servers，并且它们都绑定了80端口。 User namespaces user namespaces(CLONE_NEWUSER)将用户和group ID空间隔离。这也就是说，在一个user namespace内外，同一个进程点user和group id可以是不同的。例如，一个进程可以在一个user namespace外部，拥有一个普通的、无特权的user ID；而在在namespace中拥有UID 0。也就是说在namespace当中拥有root权限，但在namespace外部则不行。 从Linux 3.8开始，无特权的进程能够创建它们自身的user namespaces，这为应用提供了新的可能：由于一个进程能够在其user namespaces中拥有root权限，那么它们就能够去使用那些本身只能由root用户使用的功能。但这确实会带来一些安全问题。 C++中的namespace 编程语言中的namespace，虽然拥有相同的名称，其含义是完全不同的。但主要的思想是一致的，这里的命名空间也就是将空间内定义的内容放在一个盒子里，而命名空间也就是这个区域，using namespace 空间名，就将区域引入到了操作范围之内。 这里，namespace是一种描述逻辑分组的机制，比如可以将某些属于同一个任务的类声明在同一个命名空间当中。标准C++库当中的所有内容，都被定义在命名空间std当中了。 Namespace API namespace的API包含3个系统调用——clone，unshare，setns，以及一系列的/proc文件。为了指定操作的namespace类型，这3个系统调用都使用了一个CLONE_NEW常量(CLONE_NEWIPC,CLONE_NEWNS,etc)。 Clone 通过clone，可以创建一个namespace，它是一个创建新的process的系统调用。其函数原型为： 1 int clone(int (*child_func)(void *), void *child_stack, int flags, void *arg); clone可以看作fork()的通用版本，其功能能够通过flags参数CLONE_*来控制，这些参数包含了parent和child是否共享虚拟内存、打开文件描述符等。而如果参数中CLONE_NEW位被指定了，那么就会创建一个新的，对应类型的namespace，而新的进程则成为这个namespace中的一个成员。 和大多数其他的namespaces一样，创建一个UTS namespace是需要特权的，例如CAP_SYS_ADMIN，这对于避免需要设置user ID的应用来说是有必要的：如果能够使用任意的hostname，那么一个非特权用户就能够破坏lock file的作用，或者能够改变应用的行为。 /proc/里的文件 对于每一个进程来说，都有一个/proc/PID/ns目录，这其中每一种类型的namespace，都对应了一个文件。从linux 3.9开始，这些文件都被符号链接，作为处理这个进程相关namespace的handler。 12345678 $ ls -l /proc/$$/ns # $$ is replaced by shell&apos;s PID total 0 lrwxrwxrwx. 1 mtk mtk 0 Jan 8 04:12 ipc -&gt; ipc:[4026531839] lrwxrwxrwx. 1 mtk mtk 0 Jan 8 04:12 mnt -&gt; mnt:[4026531840] lrwxrwxrwx. 1 mtk mtk 0 Jan 8 04:12 net -&gt; net:[4026531956] lrwxrwxrwx. 1 mtk mtk 0 Jan 8 04:12 pid -&gt; pid:[4026531836] lrwxrwxrwx. 1 mtk mtk 0 Jan 8 04:12 user -&gt; user:[4026531837] lrwxrwxrwx. 1 mtk mtk 0 Jan 8 04:12 uts -&gt; uts:[4026531838] 这些符号链接的作用之一，就是用来检查两个进程是否处于同一个命名空间当中。kernel保证如果两个进程在同一个namespace当中，那么/proc/PID/ns中的inode number就会是一致的。inode numbers能够通过stat()系统调用来得到。 但是，kernel还是会构造/proc/PID/ns的符号链接，并使得它指向字符串，这个字符串包含了namespace的类型和inode number。 如果这个符号链接被打开，那么即使namespace中的进程全部终止了，namespace也不会消被清除。 setns setns可以被用来加入一个已存在的namespace。保持一个没有任何进程的namespace，是因为随时可以加入新的进程到这个namespace当中去，这也是setns系统调用的作用。其函数原型为： 1 int setns(int fd, int nstype); 更准确的说，setns解除一个进程和之前对应nstype的namespace的联系，并且将其关联到新的，对应类型的namespace中去。这里，fd指定了对应的namespace，它是/proc/PID/ns目录下的一个文件描述符。而nstype则会用来检查fd指向的namespace的类型。 利用setns和execve，能够构造一个很有效的工具：一个加入指定namespace然后再namespace中执行一条命令的程序。 从linux 3.8开始，setns能够加入任何类型的namespace。 unshare unshare用来离开namespace。 unshare的功能类似于clone，它创建一个新的namespaces，并且让调用者称为这个命名空间的一部分。它的主要目的，是在不创建新的进程或线程的前提下，完成namespace的分离工作。 12345 clone()和if(fork() == 0) unshare()是等价的 The link of this page is http://home.meng.uno/articles/e81ea9b1/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://home.meng.uno/tags/Linux/"},{"name":"Namespace","slug":"Namespace","permalink":"http://home.meng.uno/tags/Namespace/"}]},{"title":"Matlab基本函数","slug":"matlab-func","date":"2016-06-07T11:02:10.000Z","updated":"2020-12-02T01:55:05.000Z","comments":true,"path":"articles/dbf56532/","link":"","permalink":"http://home.meng.uno/articles/dbf56532/","excerpt":"求矩阵行数/列数/维数的函数 ndims(A) 返回A的维数 size(A) 返回A各个维的最大元素个数 length(A) 返回max(size(A)) [m,n]=size(A) 如果A是二维数组，返回行数和列数 nnz(A) 返回A中非0元素的个数 取整函数 fix(x) 截尾取整 floor(x) 不超过x 的最大整数(高斯取整) ceil(x) 大于x 的最小整数 生成随机数函数 rand(n):生成0到1之间的n阶随机数方阵 rand(m,n):生成0到1之间的m×n的随机数矩阵 其他随机数生成函数 betarnd 贝塔分布的随机数生成器 binornd 二项","text":"求矩阵行数/列数/维数的函数 ndims(A) 返回A的维数 size(A) 返回A各个维的最大元素个数 length(A) 返回max(size(A)) [m,n]=size(A) 如果A是二维数组，返回行数和列数 nnz(A) 返回A中非0元素的个数 取整函数 fix(x) 截尾取整 floor(x) 不超过x 的最大整数(高斯取整) ceil(x) 大于x 的最小整数 生成随机数函数 rand(n):生成0到1之间的n阶随机数方阵 rand(m,n):生成0到1之间的m×n的随机数矩阵 其他随机数生成函数 betarnd 贝塔分布的随机数生成器 binornd 二项分布的随机数生成器 chi2rnd 卡方分布的随机数生成器 exprnd 指数分布的随机数生成器 frndf分布的随机数生成器 gamrnd 伽玛分布的随机数生成器 geornd 几何分布的随机数生成器 hygernd 超几何分布的随机数生成器 lognrnd 对数正态分布的随机数生成器 nbinrnd 负二项分布的随机数生成器 ncfrnd 非中心f分布的随机数生成器 nctrnd 非中心t分布的随机数生成器 ncx2rnd 非中心卡方分布的随机数生成器 normrnd 正态（高斯）分布的随机数生成器 poissrnd 泊松分布的随机数生成器 raylrnd 瑞利分布的随机数生成器 trnd 学生氏t分布的随机数生成器 unidrnd 离散均匀分布的随机数生成器 unifrnd 连续均匀分布的随机数生成器 weibrnd 威布尔分布的随机数生成器 基本数学函数 abs(x)：纯量的绝对值或向量的长度 angle(z)：复数z的相角(Phase angle) sqrt(x)：开平方 real(z)：复数z的实部 imag(z)：复数z的虚部 conj(z)：复数z的共轭复数 round(x)：四舍五入至最近整数 fix(x)：无论正负，舍去小数至最近整数 floor(x)：地板函数，即舍去正小数至最近整数 ceil(x)：天花板函数，即加入正小数至最近整数 rat(x)：将实数x化为分数表示 rats(x)：将实数x化为多项分数展开 sign(x)：符号函数 当x&lt;0时，sign(x)=-1 当x=0时，sign(x)=0 当x&gt;0时，sign(x)=1 rem(x,y)：求x除以y的馀数 gcd(x,y)：整数x和y的最大公因数 lcm(x,y)：整数x和y的最小公倍数 exp(x)：自然指数 pow2(x)：2的指数 log(x)：以e为底的对数，即自然对数或 log2(x)：以2为底的对数 log10(x)：以10为底的对数 常用的三角函数 sin(x)：正弦函数 cos(x)：馀弦函数 tan(x)：正切函数 asin(x)：反正弦函数 acos(x)：反馀弦函数 atan(x)：反正切函数 atan2(x,y)：四象限的反正切函数 sinh(x)：超越正弦函数 cosh(x)：超越馀弦函数 tanh(x)：超越正切函数 asinh(x)：反超越正弦函数 acosh(x)：反超越馀弦函数 atanh(x)：反超越正切函数 向量的常用函数 min(x): 向量x的元素的最小值 max(x): 向量x的元素的最大值 mean(x): 向量x的元素的平均值 median(x): 向量x的元素的中位数 std(x): 向量x的元素的标准差 diff(x): 向量x的相邻元素的差 sort(x): 对向量x的元素进行排序（Sorting） length(x): 向量x的元素个数 norm(x): 向量x的欧氏（Euclidean）长度 sum(x): 向量x的元素总和 prod(x): 向量x的元素总乘积 cumsum(x): 向量x的累计元素总和 cumprod(x): 向量x的累计元素总乘积 dot(x, y): 向量x和y的内积 cross(x, y): 向量x和y的外积 永久常数 i或j：基本虚数单位（即） eps：系统的浮点（Floating-point）精确度 inf：无限大， 例如1/0 nan或NaN：非数值（Not a number），例如0/0 pi：圆周率 realmax：系统所能表示的最大数值 realmin：系统所能表示的最小数值 nargin: 函数的输入引数个数 nargout: 函数的输出引数个数 基本绘图函数 plot: x轴和y轴均为线性刻度（Linear scale） loglog: x轴和y轴均为对数刻度（Logarithmic scale） semilogx: x轴为对数刻度，y轴为线性刻度 semilogy: x轴为线性刻度，y轴为对数刻度 plot绘图函数的参数 字元 颜色 字元 图线型态 y 黄色 . 点 k 黑色 o 圆 w 白色 x x b 蓝色 + + g 绿色 * * r 红色 - 实线 c 亮青色 : 点线 m 锰紫色 -. 点虚线 -- 虚线 注解 xlabel(‘Input Value’); % x轴注解 ylabel(‘Function Value’); % y轴注解 title(‘Two Trigonometric Functions’); % 图形标题 legend(‘y = sin(x)’,‘y = cos(x)’); % 图形注解 grid on; % 显示格线 二维绘图函数 bar 长条图 errorbar 图形加上误差范围 fplot 较精确的函数图形 polar 极座标图 hist 累计图 rose 极座标累计图 stairs 阶梯图 stem 针状图 fill 实心图 feather 羽毛图 compass 罗盘图 quiver 向量场图 The link of this page is http://home.meng.uno/articles/dbf56532/ . Welcome to reproduce it!","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://home.meng.uno/categories/Programming-Language/"}],"tags":[{"name":"MATLAB","slug":"MATLAB","permalink":"http://home.meng.uno/tags/MATLAB/"}]},{"title":"Virtio原理","slug":"virtio","date":"2016-05-10T03:41:13.000Z","updated":"2020-12-02T01:44:06.000Z","comments":true,"path":"articles/1ae8cec4/","link":"","permalink":"http://home.meng.uno/articles/1ae8cec4/","excerpt":"Virtio简介 virtio是KVM虚拟环境下，针对I/O虚拟化的通用框架。virtio是一个半虚拟化驱动。首先说明一下全虚拟化和半虚拟化的区别。全虚拟化是指guest操作系统运行在物理机器上的hypervisor上，它不知道自己已被虚拟化，不需要任何更改就可以工作。半虚拟化指的是guest操作系统不仅知道它运行在hypervisor上，还包括让guest操作系统更高效与hyperviosr交互的代码(驱动程序)。 QEMU模拟I/O 如果使用QEMU模拟I/O，当guest中的设备驱动程序发起I/O操作请求时，KVM中的I/O操作捕获代码会将这次I/O请求拦截，在经过处理后将这次I/O请","text":"Virtio简介 virtio是KVM虚拟环境下，针对I/O虚拟化的通用框架。virtio是一个半虚拟化驱动。首先说明一下全虚拟化和半虚拟化的区别。全虚拟化是指guest操作系统运行在物理机器上的hypervisor上，它不知道自己已被虚拟化，不需要任何更改就可以工作。半虚拟化指的是guest操作系统不仅知道它运行在hypervisor上，还包括让guest操作系统更高效与hyperviosr交互的代码(驱动程序)。 QEMU模拟I/O 如果使用QEMU模拟I/O，当guest中的设备驱动程序发起I/O操作请求时，KVM中的I/O操作捕获代码会将这次I/O请求拦截，在经过处理后将这次I/O请求的信息放在I/O共享页，然后通知QEMU程序。QEMU获得I/O操作的具体信息后，交由硬件模拟代码模拟出本次I/O操作，完成后，将结果放回I/O共享页，并通知KVM模块中的I/O操作捕获代码。最后，KVM模块中的捕获代码读取I/O共享页中的操作结果，把结果返回到客户机中。倘若guest通过DMA访问大块I/O时，QEMU不会把操作结果放在I/O共享页中，而是通过内存映射的方式将结果直接写到guest的内存中。 Virtio模拟I/O 下图中，最上面一排(virtio_blk等)是前端驱动，它们是在客户机中存在的驱动程序模块，而后端处理程序是在QEMU中实现的。在前端和后端之间，定义了两层来支持guest和QEMU之间的通信。virtio层是虚拟队列借口，一个前端驱动程序可以使用多个队列。虚拟队列实际上是guest操作系统和hyperviosr的衔接点。而virtio-ring实现了环形缓冲区，它用来保存前端驱动和后端处理程序执行的信息，并且它可以一次性保存前端驱动的多次I/O请求，并且交由后端驱动去批量处理，最后实际调用host中设备驱动实现物理上的I/O操作，这样做就可以实现批量处理，而不是客户机中的每次I/O请求都需要处理一次，从而提高了guest和hypervisor信息交换的效率。 virtio_blk 在linux中，对于块设备的访问，通常是用一个I/O队列，来维护一系列的bio数据结构，通常一个请求可能包含多个bio结构。bio是上层内核vfs与下层驱动连接的纽带。 virtio_blk结构体中的gendisk结构多request_queue队列接收block层的bio请求，按照request_queue队列默认处理过程，bio请求会在io调度层转化为request，然后进入request_queue队列，最后调用virtblk_request将request转化为vbr结构，最后由QEMU接管处理。 QEMU处理过vdr之后，会将它加入到virtio_ring的request队列，并发一个中断给队列，队列的中断响应函数vring_interrupt调用队列的回调函数virtblk_done。 最后由request_queue注册的complete函数virtblk_request_done处理，通过blk_mq_end_io通告块设备层IO结束。 The link of this page is http://home.meng.uno/articles/1ae8cec4/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://home.meng.uno/tags/Linux/"},{"name":"虚拟化","slug":"虚拟化","permalink":"http://home.meng.uno/tags/虚拟化/"},{"name":"Virtio","slug":"Virtio","permalink":"http://home.meng.uno/tags/Virtio/"}]},{"title":"页缓存（Page Cache）","slug":"page-cache","date":"2016-04-16T04:22:47.000Z","updated":"2020-12-02T02:00:52.000Z","comments":true,"path":"articles/4314a663/","link":"","permalink":"http://home.meng.uno/articles/4314a663/","excerpt":"buffer cache/page cache linux中存在有两个缓存，buffer cache是针对设备的缓存，而page cache是针对文件的缓存。对于一个ext4文件系统来说，每个文件都有一棵radix树管理文件的缓存页，这些缓存页就是page cache；而对于每个块设备来说，都有一棵radix树来管理数据的缓存块，这些缓存块被称为buffer cache。在常见的linux系统中，page cache通常以4kb为单位，而buffer cache的大小由块设备来决定，通常是512B。总的来说，page cache是对文件数据的缓存，而buffer cache是对设备数据的缓存。","text":"buffer cache/page cache linux中存在有两个缓存，buffer cache是针对设备的缓存，而page cache是针对文件的缓存。对于一个ext4文件系统来说，每个文件都有一棵radix树管理文件的缓存页，这些缓存页就是page cache；而对于每个块设备来说，都有一棵radix树来管理数据的缓存块，这些缓存块被称为buffer cache。在常见的linux系统中，page cache通常以4kb为单位，而buffer cache的大小由块设备来决定，通常是512B。总的来说，page cache是对文件数据的缓存，而buffer cache是对设备数据的缓存。 在linux 2.4之前，这两个cache是有区别的，但这明显会产生一些浪费。因此在2.4之后的内核版本中，这两个cache就被统一化了：使用page cache。如果一个缓存数据既代表文件又代表块，那么buffer cache就直接指向page cache。 但是buffer cache依然是保留的。因为内核依然需要进行block的I/O。由于大部分block表示的是文件数据，因此它们都通过page cache的形式来缓存。但是剩下的小部分数据不是文件：它们是metadata活着原始的block I/O，这一部分依然由buffer cache来保存。 linux当中，所有的文件I/O操作，都是通过page cache来实现的。写操作是通过将page cache中对应的页标记为脏页来实现的；读操作是通过从page cache中返回数据来实现的。如果数据还不在cache中，就先把它读到cache里面。 如果只是研究一般文件的读写，那么就只需要在意page cache，不用去关心buffer cache。 关系 现在我们知道，在linux中，大部分文件都采用了page cache的形式来进行缓存。但是块设备的读写，却是以块的形式来进行的。前面有提到，page cache通常以4kb为单位，而buffer cache则通常是512B的。实际上，一个或多个buffer cache组成了一个page cache。 linux支持的文件系统，大多以块的形式组织文件。在文件以块的形式调入内存后，就以buffer cache的形式，对它们进行管理。buffer cache由两个部分组成，分别是缓冲区的首部buffer_head，和实际的缓冲区内容。buffer_head中，有一个指向数据的指针，和一个缓冲区长度的字段，这两个部分并不相邻。每当以块的形式，将数据读入内存时，它就要被存储在一个缓冲区当中，而buffer_head则起到一个描述符的作用。 在从块设备中读写文件页的时候，会根据不同情况，来构造bio。bio中，io_vec中，bv_page字段，会指向page。在2.6版本后，buffer_head只给上层提供有关其描述的块的当前状态，描述磁盘块到物理内存的映射关系，而bio则负责所有块I/O操作。 在linux中，mpage_readpage试图读取文件中一个page大小的数据。最理想的情况下，这个page大小的数据都在连续的物理磁盘上吗，函数只需要提交一个bio就可以获取所有的数据。这里使用get block函数，检查物理块是否连续。如果连续，则直接调用mpage_bio_submit函数请求整个page的数据，不连续则调用block_read_full_page逐个block读取，建立bh和bio之间的关系。mpage从来不回把不完整的页放进bio中，除非是文件的结尾。 页高速缓存到用户空间 所谓的页高速缓存到用户空间，实际上分为两种：一种是read到用户空间，也就是复制到用户空间中的堆中去；第二种是映射，mmap是在堆外的空间。 读取，要经过两次复制： 第一次是从磁盘中读取来填充页缓存中的页； 第二次是将是从内存中的页缓存，读取到进程堆空间的内存中。 映射，只有一次复制：从磁盘中复制到缓存中。mmap会创建一个虚拟内存区域vm_area_struct，进程的task_struct包含了进程页表项，让这些页表项指向页缓存所在的物理页page。 由于程序的代码段必然是通过mmap来实现的，因此它们在使用时，其实是保存在页缓存中的。 The link of this page is http://home.meng.uno/articles/4314a663/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://home.meng.uno/tags/Linux/"},{"name":"I/O","slug":"I-O","permalink":"http://home.meng.uno/tags/I-O/"}]},{"title":"Linux系统参数传递","slug":"64bitslinux","date":"2016-04-10T03:16:31.000Z","updated":"2020-12-08T10:08:12.000Z","comments":true,"path":"articles/bc46dabc/","link":"","permalink":"http://home.meng.uno/articles/bc46dabc/","excerpt":"x64寄存器 x64体系提供了16个通用寄存器，以及16个通用寄存器，以及16个浮点寄存器XMM/YMM寄存器。这些寄存器分为两类： * 易失寄存器：由调用方假想的临时寄存器，并要在调用过程中销毁。 * 非易失寄存器：需要在整个函数调用过程中保留其值，一旦使用，必须由调用方保存。 也就是说，易失寄存器被定义为随时会改变，不用恢复它的初始值。但是如果要嵌入一些汇编语句，还是要对它们进行保护和恢复。而易失寄存器一旦使用，必须由调用方来对它们进行保存。也就是说在任何情况下使用它们，都必须进行保存。 寄存器 使用 是否在调用前保存 RAX 临时寄存器传递参数寄存器数量，第一返回值寄存器 否","text":"x64寄存器 x64体系提供了16个通用寄存器，以及16个通用寄存器，以及16个浮点寄存器XMM/YMM寄存器。这些寄存器分为两类： 易失寄存器：由调用方假想的临时寄存器，并要在调用过程中销毁。 非易失寄存器：需要在整个函数调用过程中保留其值，一旦使用，必须由调用方保存。 也就是说，易失寄存器被定义为随时会改变，不用恢复它的初始值。但是如果要嵌入一些汇编语句，还是要对它们进行保护和恢复。而易失寄存器一旦使用，必须由调用方来对它们进行保存。也就是说在任何情况下使用它们，都必须进行保存。 寄存器 使用 是否在调用前保存 RAX 临时寄存器传递参数寄存器数量，第一返回值寄存器 否 RBX 被调用者保存寄存器，选择性的基址指针 是 RCX 传递第四个参数 否 RDX 传递第三个参数，第二返回值寄存器 否 RSP 栈指针 是 RBP 被调用者保存寄存器，选择性的栈帧寄存器 是 RSI 传递第二个参数 否 RDI 传递第一个参数 否 R8 传递第五个参数 否 R9 传递第六个参数 否 R10 临时寄存器，用于传递函数的静态链指针 否 R11 临时寄存器 否 R12-R15 被调用者保护寄存器 是 xmm0-xmm1 传递和返回浮点参数 否 xmm2-xmm7 传递浮点参数 否 xmm8-xmm15 临时寄存器 否 mmx0-mmx7 临时寄存器 否 st0-st1 临时寄存器，用来保存long double返回值 否 st2-st7 临时寄存器 否 fs 系统预留(线程特殊寄存器) 否 mxcsr SSE2控制和状态子寄存器 部分 x87 SW x87状态字 否 x87 CW x87控制字 是 参数传递 可以看出，在Linux中，前6个参数都是利用寄存器来进行传递的。那么参数多于6个的情况下，是如何传递的呢？首先参数按照从左到右的顺序，依次使用寄存器，在寄存器被使用完后，参数从右到左依次入栈，使用堆栈进行参数的传递。此处有一个例子： 123456789101112131415 typedef struct &#123;int a, b;double d;&#125; structparm;structparm s;int e, f, g, h, i, j, k;long double ld;double m, n;__m256 y;extern void func (int e, int f,structparm s, int g, int h,long double ld, double m,__m256 y,double n, int i, int j, int k);func (e, f, s, g, h, ld, m, y, n, i, j, k); 那么，在这个函数的调用中，寄存器的使用情况如下： 通用寄存器 浮点寄存器 栈帧偏移 %rdi:e %xmm0:s.d 0:ld %rsi:f %xmm1:m 16:j %rdx:s.a,s.b %xmm2:y 24:k %rcx:g %xmm3:n %r8:h %r9:i 此处存在两个疑问:第一、s.a,s.b为什么使用同一个寄存器；第二、ld为什么直接使用了栈帧传递？第一个是在结构体中，s.a，s.b是对齐可合并的，因此可以使用一个寄存器来传递这两个参数（此处存在疑问，是我自己的理解）；第二个是因为long double被归为X87类，这类参数是必须通过内存来传递的。 Red zone 在linux中，red zone是函数栈帧中，返回地址之下的一片区域，被调用函数可以使用red zone来储存局部变量，来避免对栈指针进行过多的修改。这大概就是在某些函数中，rsp直接被sub某个很大值的原因。 The link of this page is http://home.meng.uno/articles/bc46dabc/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"64bits","slug":"64bits","permalink":"http://home.meng.uno/tags/64bits/"},{"name":"Linux","slug":"Linux","permalink":"http://home.meng.uno/tags/Linux/"}]},{"title":"内存管理","slug":"memory","date":"2016-03-11T05:15:43.000Z","updated":"2020-12-08T10:21:19.000Z","comments":true,"path":"articles/41e39f2b/","link":"","permalink":"http://home.meng.uno/articles/41e39f2b/","excerpt":"页框管理与伙伴系统 这里的内存管理，指的是内核如何分配（为自己）动态内存。linux把页框作为一个管理的基本单位，用数据结构page对其进行描述。而所有的page则放在一个mem_map数组当中，进行管理。但计算机存在着一些限制，因此linux把内存划分为了几个管理区，包括ZONE_DMA、ZONE_NORMAL、ZONE_HIGHMEM等；而对页框的分配和释放，也是按照分区来进行管理的： 在每个分区之内，页框由伙伴系统来进行处理。伙伴系统主要是为了解决“外碎片”的问题：当请求和释放不断发生的时候，就很有可能导致操作系统中发生存在空闲的小块页框，但是没有大块连续页框的问题。伙伴系统把空闲","text":"页框管理与伙伴系统 这里的内存管理，指的是内核如何分配（为自己）动态内存。linux把页框作为一个管理的基本单位，用数据结构page对其进行描述。而所有的page则放在一个mem_map数组当中，进行管理。但计算机存在着一些限制，因此linux把内存划分为了几个管理区，包括ZONE_DMA、ZONE_NORMAL、ZONE_HIGHMEM等；而对页框的分配和释放，也是按照分区来进行管理的： 在每个分区之内，页框由伙伴系统来进行处理。伙伴系统主要是为了解决“外碎片”的问题：当请求和释放不断发生的时候，就很有可能导致操作系统中发生存在空闲的小块页框，但是没有大块连续页框的问题。伙伴系统把空闲页分组成11个块链表，分别包含1，2，4，6,…,1024个连续的页框。每当有两个连续的大小为b的页框出现时（并且起始地址满足一个倍数条件），它们就被视为伙伴，伙伴系统就会把它们合并成大小为2b的页框。在页分配时，如果当前大小b的free_list中找不到空闲的页框，就会从2b的链表中寻找空闲页块，并且进行分割，将它分为两个大小为b的页块。 每个伙伴系统，管理的是mem_map的一个子集。在管理区描述符中，有一个struct free_area，它用来辅助伙伴系统： 1234 struct free_area &#123; struct list_head free_list[MIGRATE_TYPES]; unsigned long nr_free;&#125;; free_list是用来连接空闲页的链表数组，而nr_free则是当前内存区中空闲页块的个数。 反碎片 当然，上面说到的只是最基本的伙伴系统，但它并没有完全解决碎片的问题。linux中还采用了一种反碎片的机制，它根据已内存页的类型来工作： 不可移动页：在内存中有固定的位置，不能移动到其他地方（kernel的大多数内存页） 可移动页：用户空间的页，只要更新页表项即可 可回收页：在内存缺少时，可以进行回收的页，例如从文件映射的页 （以及一些其他类型） 如果根据页的可移动性，将其进行分组，避免可回收页和不可回收页的交叉组织（例如在可移动页中间有不可移动页），并且在某个类型的页分配失败时，会从备用列表中寻找相应的页，这个顺序定义在page_alloc.c当中。 内存分配方法 分配内存通常可以调用一下几个函数： alloc_pages/alloc_page：分配若干个页，返回第一个struct page get_zeroed_page：分配一个struct page，并且将内存填0 get_free_pages/get_free_page：返回值是虚拟地址 get_dma_pages：分配一个适用于DMA的页 还有一些基于伙伴系统的方法，它们可能会借助页表进行映射，例如vmalloc，kmalloc。 内存分配时，通常要指定一个掩码gfp_mask，它定义了页所位于的区域、页在I/O和vfs上的操作，以及对分配操作的规定（阻塞、I/O、文件系统等）。 释放不再使用的页，同样可以采用struct page或者虚拟地址作为参数： free_page/free_pages：以struct page为参数 __free_page/__free_pages：以虚拟地址为参数 页框高速缓存 （为了避免混淆，我把所有硬件的高速缓存称为cache） 内核经常会请求、释放单个页框，为了提高系统的性能，每个内存管理区都有一个每CPU的页框高速缓存，它包含一些预先分配的页框，能够用来满足CPU发出的单个页框请求。注意，这个页框高速缓存，和硬件上的cache的概念不同，但它们有一点小小的关联。由于每个CPU有自己的cache，那么假设一个进程刚刚释放了一个页，那么这个页就有很大概率还在cache当中。页框高速缓存保存热页（刚释放的，很可能在cache当中的页）和冷页（释放时间比较长的页）。其实对于分配热页来说，很好理解：用在cache中的页可以减少开销；但如果说是DMA设备使用，就要分配冷页了，因为它不会用到cache。 slab分配器 前面所说的伙伴系统，是用“页”为单位来进行，显然太大了；所以需要把页进一步拆分，变成更小的单位。slab分配器不仅仅提供小内存块，它还作为一个缓存使用，主要是针对那些经常分配、释放的对象：例如内核中的fs_struct数据结构，可能经常会分配和释放；那么slab就将释放的内存块保存在一个列表里面，而不是返回给伙伴系统。这样一来，再次分配新的内存块时，就不需要经过伙伴系统了，而且这些内存块还很可能在cache里面。 slab分配器包含几个部分：高速缓存kmem_cache，slab，以及slab中所包含的对象。每个高速缓存只负责一种对象类型，它由多个slab构成。kmem_cache当中有三个slab链表，分别对应用尽的slab、部分空闲的slab，和空闲的slab，还有一个array_cache *数组，它保存cpu最后释放的那些很可能处于“热”状态的对象。 而对于每个slab，则组织了一系列的object；它包含了空闲对象，正在使用的对象。那么为什么不直接用kmem_cache管理对象，要增加出slab这一层呢？这明显是为了更好的管理内存：通过slab，可以让内存的使用更平均，或者能够更好的管理空闲的页。 在新版本的内核中，slab由kmem_cache_node来管理，它包含3个链表slabs_partial，slabs_full和slabs_free。每个slab是一个或多个连续页帧的集合，每个objects由链表串联，现在slab中的object直接由page中的freelist来管理了。 123456789101112131415161718192021222324 struct kmem_cache_node &#123;spinlock_t list_lock;#ifdef CONFIG_SLABstruct list_head slabs_partial; /* partial list first, better asm code */struct list_head slabs_full;struct list_head slabs_free;unsigned long free_objects;unsigned int free_limit;unsigned int colour_next; /* Per-node cache coloring */struct array_cache *shared; /* shared per node */struct alien_cache **alien; /* on other nodes */unsigned long next_reap; /* updated without locking */int free_touched; /* updated without locking */#endif#ifdef CONFIG_SLUBunsigned long nr_partial;struct list_head partial;#ifdef CONFIG_SLUB_DEBUGatomic_long_t nr_slabs;atomic_long_t total_objects;struct list_head full;#endif#endif&#125;; 值得一提的是，kmalloc的实现也是也是基于slab来实现的，它包含一个数组，存放了一些用于不同长度的slab缓存，这也就是我们所说的“内存池”。 slab着色 slab着色与颜色并没有关系，它要解决的问题与硬件高速缓存有关。硬件高速缓存倾向于把大小一样的对象放在高速缓存内的相同偏移位置；而不同slab当中相同偏移量的对象，就会映射在高速缓存的同一行当中；这样高速缓存可能就会频繁的对同一高速缓存行进行更新，从而造成性能损失。 slab着色就是给每个slab分配一个随机的“颜色”，把它作为slab中对象需要移动的特定偏移量来使用，这样对象就会被放置到不同的缓存行。 The link of this page is http://home.meng.uno/articles/41e39f2b/ . Welcome to reproduce it!","categories":[{"name":"Operation System","slug":"Operation-System","permalink":"http://home.meng.uno/categories/Operation-System/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://home.meng.uno/tags/Linux/"},{"name":"内存","slug":"内存","permalink":"http://home.meng.uno/tags/内存/"}]},{"title":"神奇的Latex","slug":"latex-lang","date":"2016-03-04T13:43:28.000Z","updated":"2020-12-02T01:43:09.000Z","comments":true,"path":"articles/67934c43/","link":"","permalink":"http://home.meng.uno/articles/67934c43/","excerpt":"或许很多人都知道这个软件、语言的名字，也知道他至今只有两版（基本无错），但不是人人都会使用。 注释 所有的注释行以%开头，没有多行注释语法。 命令 每一个LaTeX命令由反斜线\\开始 LaTeX 文档以对编译对象文档的定义开始 这些文档包括书籍，报告，演示等 文档的选项出现在中括号里 字体 我们设定文章字体为12pt 1 \\documentclass[12pt]{article} 定义使用的库 如果想要引入图片，彩色字，或是其他语言的源码在您的文档中 需要增强 LaTeX 的功能。这将通过添加库来实现 下例中将要为展示数据引入 float 和 caption 库，为超","text":"或许很多人都知道这个软件、语言的名字，也知道他至今只有两版（基本无错），但不是人人都会使用。 注释 所有的注释行以%开头，没有多行注释语法。 命令 每一个LaTeX命令由反斜线\\开始 LaTeX 文档以对编译对象文档的定义开始 这些文档包括书籍，报告，演示等 文档的选项出现在中括号里 字体 我们设定文章字体为12pt 1 \\documentclass[12pt]&#123;article&#125; 定义使用的库 如果想要引入图片，彩色字，或是其他语言的源码在您的文档中 需要增强 LaTeX 的功能。这将通过添加库来实现 下例中将要为展示数据引入 float 和 caption 库，为超链接引入 hyperref 库 123 \\usepackage&#123;caption&#125;\\usepackage&#123;float&#125;\\usepackage&#123;hyperref&#125; 我们还可以定义其他文档属性！ 123 \\author&#123;Chaitanya Krishna Ande, Sricharan Chiruvolu \\&amp; Svetlana Golubeva&#125;\\date&#123;\\today&#125;\\title&#123;Learn \\LaTeX \\hspace&#123;1pt&#125; in Y Minutes!&#125; 开始正文 这一行之前都是“序章” 1 \\begin&#123;document&#125; 如果想设定作者，时间，标题字段我们可使用 LaTeX 来建立标题页 1 \\maketitle 分章节时，可以建立目录 我们需要编译文档两次来保证他们顺序正确 使用目录来分开文档是很好的做法 这里我们使用 \\newpage 操作符 12 \\newpage\\tableofcontents 1 \\newpage 许多研究论文有摘要部分。这可以使用预定义的指令来实现 它应被放在逻辑上正确的位置，即顶部标题等的下面和文章主体的上面 该指令可以在报告和文章中使用 123 \\begin&#123;abstract&#125; \\LaTeX \\hspace&#123;1pt&#125; documentation written as \\LaTeX! How novel and totally not my idea!\\end&#123;abstract&#125; 章节指令 所有章节标题会自动地添加到目录中 123456789101112 \\section&#123;Introduction&#125;Hello, my name is Colton and together we&apos;re going to explore \\LaTeX!\\section&#123;Another section&#125;This is the text for another section. I think it needs a subsection.\\subsection&#123;This is a subsection&#125; % 子章节同样非常直观I think we need another one\\subsubsection&#123;Pythagoras&#125;Much better now.\\label&#123;subsec:pythagoras&#125; 使用型号我们可以借助 LaTeX 内置的编号功能 这一技巧也在其他指令中有效 1 \\section*&#123;This is an unnumbered section&#125; 然而并不是所有章节都要被标序号 1234567891011121314151617181920212223 \\section&#123;Some Text notes&#125;%\\section&#123;Spacing&#125; % 需要增加有关空白间隔的信息\\LaTeX \\hspace&#123;1pt&#125; is generally pretty good about placing text where it shouldgo. If a line \\\\ needs \\\\ to \\\\ break \\\\ you add \\textbackslash\\textbackslash \\hspace&#123;1pt&#125; to the source code. \\\\ \\section&#123;Lists&#125;Lists are one of the easiest things to create in \\LaTeX! I need to go shoppingtomorrow, so let&apos;s make a grocery list.\\begin&#123;enumerate&#125; % 此处创建了一个“枚举”环境 % \\item 使枚举增加一个单位 \\item Salad. \\item 27 watermelon. \\item A single jackrabbit. % 我们甚至可以通过使用 [] 覆盖美剧的数量 \\item[how many?] Medium sized squirt guns. Not a list item, but still part of the enumerate.\\end&#123;enumerate&#125; % 所有环境都有终止符\\section&#123;Math&#125; 使用LaTex的一个最主要的方面是学术论文和技术文章，通常在数学和科学的领域。 插入特殊符号！ 数学符号极多，远超出你能在键盘上找到的那些； 集合关系符，箭头，操作符，希腊字符等等，集合与关系在数学文章中很重要，如声明所有“ x 属于 X” $\\forall$ x $\\in$ X. 注意我们需要在这些符号之前和之后增加 $ 符号，因为在编写时我们处于 text-mode，然而数学符号只在 math-mode 中存在。 text mode 进入 math-mode 使用 $ 操作符，反之亦然，变量同时会在 math-mode 中被渲染。 我们也可以使用 \\[ \\] 来进入 math mode。 1 \\[a^2 + b^2 = c^2 \\] 123 My favorite Greek letter is $\\xi$. I also like $\\beta$, $\\gamma$ and $\\sigma$.I haven&apos;t found a Greek letter yet that \\LaTeX \\hspace&#123;1pt&#125; doesn&apos;t knowabout! 常用函数操作符同样很重要： 123 trigonometric functions ($\\sin$, $\\cos$, $\\tan$), logarithms 和 exponentials ($\\log$, $\\exp$), limits ($\\lim$), etc. 在 LaTeX 指令中预定义 让我们写一个等式看看发生了什么： 1 $\\cos(2\\theta) = \\cos^&#123;2&#125;(\\theta) - \\sin^&#123;2&#125;(\\theta)$ 分数可以写成以下形式： 12 % 10 / 7$$ ^&#123;10&#125;/_&#123;7&#125; $$ 相对比较复杂的分数可以写成 12 % \\frac&#123;numerator&#125;&#123;denominator&#125;$$ \\frac&#123;n!&#125;&#123;k!(n - k)!&#125; $$ \\\\ 我们同样可以插入公式（equations）在环境 “equation environment” 下。 展示数学相关时，使用方程式环境 1234 \\begin&#123;equation&#125; % 进入 math-mode c^2 = a^2 + b^2. \\label&#123;eq:pythagoras&#125; % 为了下一步引用\\end&#123;equation&#125; % 所有 \\begin 语句必须有end语句对应 引用我们的新等式！ 123 Eqn.~\\ref&#123;eq:pythagoras&#125; is also known as the Pythagoras Theorem which is alsothe subject of Sec.~\\ref&#123;subsec:pythagoras&#125;. A lot of things can be labeled: figures, equations, sections, etc. 求和（Summations）与整合（Integrals）写作 sum 和 int。 一些编译器会提醒在等式环境中的空行 12345678 \\begin&#123;equation&#125; \\sum_&#123;i=0&#125;^&#123;5&#125; f_&#123;i&#125;\\end&#123;equation&#125; \\begin&#123;equation&#125; \\int_&#123;0&#125;^&#123;\\infty&#125; \\mathrm&#123;e&#125;^&#123;-x&#125; \\mathrm&#123;d&#125;x\\end&#123;equation&#125; \\section&#123;Figures&#125; 插入 让我们插入图片，图片的放置非常微妙，我在每次使用时都会查找可用选项。 12345678 \\begin&#123;figure&#125;[H] % H 是放置选项的符号 \\centering % 图片在本页居中 % 宽度放缩为页面的0.8倍 %\\includegraphics[width=0.8\\linewidth]&#123;right-triangle.png&#125; % 需要使用想象力决定是否语句超出编译预期 \\caption&#123;Right triangle with sides $a$, $b$, $c$&#125; \\label&#123;fig:right-triangle&#125;\\end&#123;figure&#125; 插入表格与插入图片方式相同 1 \\subsection&#123;Table&#125; 1234567891011 \\begin&#123;table&#125;[H] \\caption&#123;Caption for the Table.&#125; % 下方的 &#123;&#125; 描述了表格中每一行的绘制方式 % 同样，我在每次使用时都会查找可用选项。 \\begin&#123;tabular&#125;&#123;c|cc&#125; Number &amp; Last Name &amp; First Name \\\\ % 每一列被 &amp; 分开 \\hline % 水平线 1 &amp; Biggus &amp; Dickus \\\\ 2 &amp; Monty &amp; Python \\end&#123;tabular&#125;\\end&#123;table&#125; 现在增加一些源代码在 LaTex 文档中， 我们之后需要 LaTex 不翻译这些内容而仅仅是把他们打印出来而已。 这里使用 verbatim environment。 也有其他库存在 (如. minty, lstlisting, 等)，但是 verbatim 是最基础和简单的一个。 1234567 \\begin&#123;verbatim&#125; print(&quot;Hello World!&quot;) a%b; % 在这一环境下我们可以使用 % random = 4; #decided by fair random dice roll\\end&#123;verbatim&#125;\\section&#123;Compiling&#125; 现在你大概想了解如何编译这个美妙的文档，然后得到饱受称赞的\\LaTeX \\hspace{1pt} pdf文档 LaTex组合 12345678 \\begin&#123;enumerate&#125; \\item Write the document in plain text (the ``source code&apos;&apos;). \\item Compile source code to produce a pdf. The compilation step looks like this (in Linux): \\\\ \\begin&#123;verbatim&#125; &gt; pdflatex learn-latex.tex \\end&#123;verbatim&#125;\\end&#123;enumerate&#125; 许多 LaTex编译器把步骤1和2在同一个软件中进行了整合。所以你可以只看步骤1完全不看步骤2 步骤2同样在以下情境中使用情景 \\footnote 以防万一，当你使用引用时(如 Eqn.~\\ref{eq:pythagoras})，你将需要多次运行步骤2来生成一个媒介文件 *.aux 。同时这也是在文档中增加脚标的方式。 在步骤1中，用普通文本写入格式化信息，步骤2的编译阶段则注意在步骤1 中定义的格式信息。 1 \\section&#123;Hyperlinks&#125; 同样可以在文档中加入超链接 使用如下命令在序言中引入库： 123 \\begin&#123;verbatim&#125; \\usepackage&#123;hyperref&#125;\\end&#123;verbatim&#125; 超链接 12 \\url&#123;https://learnxinyminutes.com/docs/latex/&#125;， 或 \\href&#123;https://learnxinyminutes.com/docs/latex/&#125;&#123;shadowed by text&#125; 你不可以增加特殊空格和符号，因为这将会造成编译错误 这个库同样在输出PDF文档时制造略缩的列表，或在目录中激活链接 1 \\section&#123;End&#125; 这就是全部内容了！ 引用部分 最简单的建立方式是使用书目提要章节 1234567 \\begin&#123;thebibliography&#125;&#123;1&#125; % 与其他列表相同， \\bibitem 命令被用来列出条目 % 每个记录可以直接被文章主体引用 \\bibitem&#123;latexwiki&#125; The amazing \\LaTeX \\hspace&#123;1pt&#125; wikibook: &#123;\\em https://en.wikibooks.org/wiki/LaTeX&#125; \\bibitem&#123;latextutorial&#125; An actual tutorial: &#123;\\em http://www.latex-tutorial.com&#125;\\end&#123;thebibliography&#125; 结束文档 1 \\end&#123;document&#125; The link of this page is http://home.meng.uno/articles/67934c43/ . Welcome to reproduce it!","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://home.meng.uno/categories/Programming-Language/"}],"tags":[{"name":"Latex","slug":"Latex","permalink":"http://home.meng.uno/tags/Latex/"},{"name":"Tex","slug":"Tex","permalink":"http://home.meng.uno/tags/Tex/"},{"name":"排版","slug":"排版","permalink":"http://home.meng.uno/tags/排版/"}]},{"title":"在Gulp中使用BrowserSync","slug":"browsersync","date":"2016-02-24T11:58:51.000Z","updated":"2021-01-04T07:27:36.000Z","comments":true,"path":"articles/70440106/","link":"","permalink":"http://home.meng.uno/articles/70440106/","excerpt":"BrowserSync可以同时同步刷新多个浏览器，更神奇的是你在一个浏览器中滚动页面、点击按钮、输入框中输入信息等用户行为也会同步到每个浏览器中。 安装browser-sync模块 1 npm install browser-sync -g 命令行直接使用 1 browser-sync start --server --files \"css/*.css\" 使用上面命令会开启一个迷你服务器，自动帮你打开浏览器，默认地址localhost:3000，默认打开index.html，如果没有，需要手动加上你要打开的页面，如localhost:3000/test.html。 通常你不","text":"BrowserSync可以同时同步刷新多个浏览器，更神奇的是你在一个浏览器中滚动页面、点击按钮、输入框中输入信息等用户行为也会同步到每个浏览器中。 安装browser-sync模块 1 npm install browser-sync -g 命令行直接使用 1 browser-sync start --server --files &quot;css/*.css&quot; 使用上面命令会开启一个迷你服务器，自动帮你打开浏览器，默认地址localhost:3000，默认打开index.html，如果没有，需要手动加上你要打开的页面，如localhost:3000/test.html。 通常你不会需要默认的地址，所以需要使用代理模式： 1 browser-sync start --proxy &quot;localhost:8080&quot; --files &quot;css/*.css&quot; Browsersync + Gulp 12345678910111213141516171819202122232425 var gulp = require(&apos;gulp&apos;), sass = require(&apos;gulp-ruby-sass&apos;), autoprefixer = require(&apos;gulp-autoprefixer&apos;), minifycss = require(&apos;gulp-minify-css&apos;), rename = require(&apos;gulp-rename&apos;), notify = require(&apos;gulp-notify&apos;);var browserSync = require(&apos;browser-sync&apos;).create();gulp.task(&apos;sass&apos;, function() &#123; return sass(&apos;sass/style.scss&apos;, &#123;style: &quot;expanded&quot;&#125;) //.pipe(sass(&#123;style: &quot;expanded&quot;&#125;)) .pipe(autoprefixer(&apos;last 2 version&apos;, &apos;safari 5&apos;, &apos;ie 8&apos;, &apos;ie 9&apos;, &apos;opera 12.1&apos;, &apos;ios 6&apos;, &apos;android 4&apos;)) .pipe(rename(&#123;suffix: &apos;.min&apos;&#125;)) .pipe(minifycss()) .pipe(gulp.dest(&apos;css&apos;)) .pipe(notify(&#123; message: &apos;Styles task complete&apos; &#125;)) .pipe(browserSync.stream());&#125;);gulp.task(&apos;serve&apos;, [&apos;sass&apos;], function() &#123; browserSync.init(&#123; server: &quot;./&quot; &#125;); gulp.watch(&quot;sass/*.scss&quot;, [&apos;sass&apos;]); gulp.watch(&quot;*.html&quot;).on(&apos;change&apos;, browserSync.reload);&#125;);gulp.task(&apos;default&apos;, [&apos;serve&apos;]); 其中 1 gulp.watch(&quot;sass/*.scss&quot;, [&apos;sass&apos;]); 会在编译完sass后，以无刷新方式更新页面。 1 gulp.watch(&quot;*.html&quot;).on(&apos;change&apos;, browserSync.reload); 会在修改html文件后刷新页面。 如果需要在修改js后刷新页面，可以像下面这样： 1234567891011121314151617181920212223242526 gulp.task(&apos;sass&apos;, function() &#123; return sass(&apos;sass/style.scss&apos;, &#123;style: &quot;expanded&quot;&#125;) //.pipe(sass(&#123;style: &quot;expanded&quot;&#125;)) .pipe(autoprefixer(&apos;last 2 version&apos;, &apos;safari 5&apos;, &apos;ie 8&apos;, &apos;ie 9&apos;, &apos;opera 12.1&apos;, &apos;ios 6&apos;, &apos;android 4&apos;)) .pipe(rename(&#123;suffix: &apos;.min&apos;&#125;)) .pipe(minifycss()) .pipe(gulp.dest(&apos;css&apos;)) .pipe(notify(&#123; message: &apos;Styles task complete&apos; &#125;)) .pipe(browserSync.stream());&#125;);gulp.task(&apos;js&apos;, function () &#123; return gulp.src(&apos;js/*js&apos;) .pipe(browserify()) .pipe(uglify()) .pipe(gulp.dest(&apos;dist/js&apos;)) .pipe(browserSync.stream());;&#125;);gulp.task(&apos;serve&apos;, [&apos;sass&apos;, &apos;js&apos;], function() &#123; browserSync.init(&#123; server: &quot;./&quot; &#125;); gulp.watch(&quot;sass/*.scss&quot;, [&apos;sass&apos;]); gulp.watch(&quot;*.html&quot;).on(&apos;change&apos;, browserSync.reload); gulp.watch(&quot;js/*.js&quot;, [&apos;js&apos;])&#125;);gulp.task(&apos;default&apos;, [&apos;serve&apos;]); The link of this page is http://home.meng.uno/articles/70440106/ . Welcome to reproduce it!","categories":[{"name":"Application","slug":"Application","permalink":"http://home.meng.uno/categories/Application/"}],"tags":[{"name":"Gulp","slug":"Gulp","permalink":"http://home.meng.uno/tags/Gulp/"},{"name":"BrowserSync","slug":"BrowserSync","permalink":"http://home.meng.uno/tags/BrowserSync/"}]},{"title":"虚函数","slug":"virtaul-func","date":"2016-02-20T04:18:29.000Z","updated":"2020-12-02T02:09:41.000Z","comments":true,"path":"articles/dcffa0c0/","link":"","permalink":"http://home.meng.uno/articles/dcffa0c0/","excerpt":"虚函数与多态 继承和多态，是面向对象中老生常谈的话题。C++中，我们也可以经常看到virtaul、override这样的关键字；这正是虚函数的标志。虚函数就是为了解决多态的问题：如果要使用一个基类的指针，根据对象的不同类型去调用相应的函数，就需要使用虚函数了。通俗的说也就是同一个入口，却能够调用不同的方法。 通常，对于虚函数的调用，往往在运行时才能确定调用哪个版本的函数。这是由于基类的指针或者引用，其动态类型必须在运行的时候才能确定（它具体指向了什么类型）。而“动态绑定”就指的在运行时，根据对象的类型，调用具体方法的过程，这个过程正是通过虚函数表实现的。 抽象基类指的是有纯虚函数的类。纯虚函","text":"虚函数与多态 继承和多态，是面向对象中老生常谈的话题。C++中，我们也可以经常看到virtaul、override这样的关键字；这正是虚函数的标志。虚函数就是为了解决多态的问题：如果要使用一个基类的指针，根据对象的不同类型去调用相应的函数，就需要使用虚函数了。通俗的说也就是同一个入口，却能够调用不同的方法。 通常，对于虚函数的调用，往往在运行时才能确定调用哪个版本的函数。这是由于基类的指针或者引用，其动态类型必须在运行的时候才能确定（它具体指向了什么类型）。而“动态绑定”就指的在运行时，根据对象的类型，调用具体方法的过程，这个过程正是通过虚函数表实现的。 抽象基类指的是有纯虚函数的类。纯虚函数，指的是没有函数体的函数，通常通过在函数体的位置写上=0来表示。对于抽象基类，是不能直接创建一个对象的；但是可以创建它它们的派生类的对象：只要它们覆盖了纯虚函数。纯虚函数表示这个函数的具体实现全部交给派生类去做。 虚函数表与虚函数的调用 那么，“动态绑定”是如何实现的呢？这便是借助于虚函数表来实现。对于每个具有虚函数的类，都会有一个对应的虚函数表vtable，其代码和对应内存结构如下所示： class A { int varA; public: virtual int vAfoo(int a, int *b){ return a + (*b); } virtual int vAbar(int a){ return a + 1; } virtual bool vAduh(){ return true; } virtual int vAtest(int a){ return 0; } void Afoo(){ this-&gt;vAduh(); } }; class B { int varB; public: virtual int vBfoo(int a) = 0; virtual bool vBbar(int b){ return b == 0; } char *Bfoo(char *c){ return c; } }; class C : public A, public B { int varC; public: int vAtest(int a){ return -(a); } int vAfoo(int a, int *b){ return *b; } int vBfoo(int a){ return a - 1; } virtual void vCfoo(){} bool vAduh(){ return false; } }; 这个表中的每一项，都是一个虚函数的地址，也就是虚函数的指针。而每个对象的第一个值都是虚标指针，它指向了了所对应虚函数表的第一个表项（也就是虚函数表的基址）。每次调用虚函数时，都会首先通过这个虚表指针，找到虚函数表，然后再在虚函数表中，找到真正的虚函数的地址，并进行调用。假设存在有多继承的情况，那么就会有多个vptr，分别放在对应的基类对象的开头位置。 虚继承 对于“菱形继承”情况（也即两个子类继承同一个父类，而新的子类又同时继承这两个子类），则可能产生二义性问题。例如下面的情况，那么D中就会保存两次A中的变量和函数，并且在使用时也会很不方便，必须利用域作用符来使用变量和函数。 A / \\ B1 B2 \\ / D 虚继承是在继承时，在基类类型前面加上virtual关键字。虚继承能够解决基类多副本的问题：在任何派生类当中，虚基类都是通过一个共享对象来表示的，它们通过指针去访问这个基类中的内容；它不用去保存多份基类的拷贝，而是只需要多出一个指向基类子对象的指针。从内存布局上来说，在虚表的负offset位置，会保存一个指针指向虚基类对象。 也就是说继承自A的虚函数和对象，全部只保存一份在D自身的子对象中，相比不使用虚继承，它删除了B1和B2当中的（2份）基类成员；它自己则需要保存一份基类成员和偏移指针；而如果要用B1和B2的指针或者引用去访问一个D对象时，那么访问A的成员则需要通过间接引用来访问；也就是说子对象需要有一个偏移量，指示在内存中，基类的位置。其内存布局一般如下： 内存 B1的虚表指针 B1的偏移指针 B1的数据成员 B2的虚表指针 B2的偏移指针 B2的数据成员 D的虚表指针 D的偏移指针 D的数据成员 A的虚表指针 A的数据成员 虚表与劫持攻击 在C程序中，%90以上的间接调用都是vcall。篡改程序中的虚函数调用，是劫持C程序的一种常见手段。这里简单说说常见手段。 一种方法是虚表注入。众所周知，虚表保存在程序的.rodata段中，它是可读，不可写的；而对象当中的虚表指针却是可读写的状态；因此篡改虚表指针是较为直接的方式。 如图，如果利用漏洞（overflow、use-after-free等）在内存中构造一个虚假的虚表，并且将对象中的虚函数指针指向注入的虚假的虚表，那么在虚函数调用时，就会调用虚假的虚函数。甚至只需要一次虚函数调用就能够通过shellcode完成攻击。 当然，如果程序进行了一定程度的保护，例如检查虚表指针是否属于.rodata段，攻击就只能依赖于现有的虚表来构造了。Counterfeit Object-oriented Programming就提出了这样一种方法。 可以看到，这种方法没有注入新的虚表，而是将vptr的值，指向了虚表中的不同位置（而不是虚表的起始地址）。如果能够构造一系列的虚假对象，那么就可以在一次循环中（比如某个对象数组的依次析构），在调用同一个虚函数时，实际上调用不同的函数，从而构造一个虚假的执行链。看到这里，也许你会有疑问：仅仅用有限的虚函数，能够构造图灵计算的攻击吗？答案是肯定的：有兴趣的话可以阅读一下原文，通过拼凑虚函数，是能够组合出各种语义的。 小结 可见，虚函数是面向对象语言中，十分巧妙而又必不可少的设计；但它的特点也使得它成为黑客滥用、攻击的目标。指的庆幸的是，目前已经有一些开销较小的方法，能够保护虚表和虚函数了。 The link of this page is http://home.meng.uno/articles/dcffa0c0/ . Welcome to reproduce it!","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://home.meng.uno/categories/Programming-Language/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://home.meng.uno/tags/C/"},{"name":"虚函数","slug":"虚函数","permalink":"http://home.meng.uno/tags/虚函数/"},{"name":"内存安全","slug":"内存安全","permalink":"http://home.meng.uno/tags/内存安全/"}]},{"title":"判断是否手机登陆","slug":"phone","date":"2016-01-11T12:17:29.000Z","updated":"2020-12-02T02:01:54.000Z","comments":true,"path":"articles/d23d13ae/","link":"","permalink":"http://home.meng.uno/articles/d23d13ae/","excerpt":"需求 项目的网站时做的手机网页，但是没有考虑到自适应pc端，只是在chrome中固定了手机屏幕大小，所以在pc端查看会很丑，布局是乱的；但是重写又很麻烦，所以有必要做一下提醒，让用户手动将网页缩小到手机屏幕大小。 用到 非手机检验和cookie记录 方法 这段代码是使用了cookie来控制的 1. 使用cookie让浏览器记住页面已经打开过一次，当前页面刷新不会弹出提醒。 2. 浏览器一旦关闭，保存这个记录的cookie文件将被删除。重新打开浏览器弹出窗口会再次出现，从而确保了在原有的窗口基础上只打开一次。 判断非手机登陆方式 1 2 if (!/Android|web","text":"需求 项目的网站时做的手机网页，但是没有考虑到自适应pc端，只是在chrome中固定了手机屏幕大小，所以在pc端查看会很丑，布局是乱的；但是重写又很麻烦，所以有必要做一下提醒，让用户手动将网页缩小到手机屏幕大小。 用到 非手机检验和cookie记录 方法 这段代码是使用了cookie来控制的 使用cookie让浏览器记住页面已经打开过一次，当前页面刷新不会弹出提醒。 浏览器一旦关闭，保存这个记录的cookie文件将被删除。重新打开浏览器弹出窗口会再次出现，从而确保了在原有的窗口基础上只打开一次。 判断非手机登陆方式 12 if (!/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) alert(&quot;为了提高体验效果，请把页面缩小成手机屏幕大小。\\n谢谢合作！&quot;); 1234567891011121314151617181920212223242526272829303132333435363738 &lt;script type=&quot;text/javascript&quot;&gt; try&#123; var alertmessage=&quot;为了提高体验效果，请把页面缩小成手机屏幕大小。\\n谢谢合作！&quot;; var once_per_session=1 function get_cookie(Name) &#123; var search = Name + &quot;=&quot; var returnvalue = &quot;&quot;; if (document.cookie.length &gt; 0) &#123; offset = document.cookie.indexOf(search) if (offset != -1) &#123; offset += search.length; end = document.cookie.indexOf(&quot;;&quot;, offset); if (end == -1) end = document.cookie.length; returnvalue=unescape(document.cookie.substring(offset, end)) &#125; &#125; return returnvalue; &#125; function alertornot()&#123; if (get_cookie(&apos;alerted&apos;)==&apos;&apos;)&#123; loadalert(); document.cookie=&quot;alerted=yes&quot;; &#125; &#125; function loadalert()&#123; if (!/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) alert(alertmessage); &#125; if (once_per_session==0) loadalert(); else alertornot(); &#125; catch(e) &#123; alert(e); &#125;&lt;/script&gt; The link of this page is http://home.meng.uno/articles/d23d13ae/ . Welcome to reproduce it!","categories":[{"name":"Application","slug":"Application","permalink":"http://home.meng.uno/categories/Application/"}],"tags":[{"name":"Phone","slug":"Phone","permalink":"http://home.meng.uno/tags/Phone/"},{"name":"登录","slug":"登录","permalink":"http://home.meng.uno/tags/登录/"}]},{"title":"Listview Item进入和删除动画","slug":"listview-animation","date":"2015-11-24T14:08:59.000Z","updated":"2021-01-01T18:29:58.000Z","comments":true,"path":"articles/fb92c8a1/","link":"","permalink":"http://home.meng.uno/articles/fb92c8a1/","excerpt":"从右边划入的动画 * 定义xml动画 1 2 3 4 5 6 * Adapter里只需要写如下代码 1 2 3 4 5 6 7 8 9 1","text":"从右边划入的动画 定义xml动画 123456 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;translate xmlns:android=\"http://schemas.android.com/apk/res/android\" android:duration=\"@integer/animTime\" android:fromXDelta=\"100%p\" android:toXDelta=\"0%p\" &gt;&lt;/translate&gt; Adapter里只需要写如下代码 123456789101112 @Overridepublic View getChildView(final int groupPosition, int childPosition, boolean isLastChild, View convertView, ViewGroup parent) &#123; .... if (!junk.isAnimatedBefore()) &#123; junk.setAnimatedBefore(true); convertView.startAnimation(AnimationUtils.loadAnimation(mContext, R.anim.slide_left_in)); &#125;else&#123; convertView.clearAnimation(); &#125; return convertView;&#125; 从左边划出的动画 定义xml动画 1234567891011 &lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;set xmlns:android=\"http://schemas.android.com/apk/res/android\" android:fillEnabled=\"true\" android:fillAfter=\"true\" android:duration=\"@integer/animTime\"&gt; &lt;translate android:fromXDelta=\"0%p\" android:toXDelta=\"-100%p\"&gt; &lt;/translate&gt;&lt;/set&gt; Adapter里不需要做什么，需要在外面操作ListView. 在外面操作ListView 开始动画 123456789101112131415161718192021222324252627282930313233 private void startBoost() &#123; mIsDeleting = true; boolean hasAnimation = false; long offset = 0; mAnimCount = 0; int groupPosition = -1; int childPosition = -1; Iterator&lt;ArrayList&lt;Junk&gt;&gt; listIterator = mChildren.iterator(); while (listIterator.hasNext()) &#123; groupPosition++; final Iterator&lt;Junk&gt; junkIterator = listIterator.next().iterator(); while (junkIterator.hasNext()) &#123; childPosition++; Junk junk = junkIterator.next(); if (junk.isChecked()) &#123; ... junkIterator.remove(); final View itemView = ListViewUtil.getChildItemView(this, mListView, groupPosition, childPosition); if (itemView != null) &#123; hasAnimation = true; ((Animation) itemView.getTag(R.id.anim)).setAnimationListener(mAnimationListener); ((Animation) itemView.getTag(R.id.anim)).setStartOffset(offset); offset += 100; itemView.startAnimation(((Animation) itemView.getTag(R.id.anim))); &#125; &#125; &#125; &#125; if (!hasAnimation) &#123; deleteCompleted(); &#125;&#125; mAnimationListener定义如下： 12345678910111213141516171819 private Animation.AnimationListener mAnimationListener = new Animation.AnimationListener() &#123; @Override public void onAnimationStart(Animation animation) &#123; mAnimCount++; &#125; @Override public void onAnimationEnd(Animation animation) &#123; mAnimCount--; if (mAnimCount == 0) &#123; deleteCompleted(); &#125; &#125; @Override public void onAnimationRepeat(Animation animation) &#123; &#125;&#125;; 删除完成后需要调用ListViewUtil.clearListViewAnim(this, mListView);,否则原来动画的地方会显示空白。 动画时要禁用OnTouch事件，因为我们做动画时并没有调用mAdapter.notifyDataSetChanged()，所以如果滚动，由于数据没有同步，肯定会挂掉。 禁用OnTouch事件的方法如下 123456789 @Overridepublic boolean dispatchTouchEvent(MotionEvent ev) &#123; // intercept touch event when deleting ,avoid listview get view. if (mIsDeleting) &#123; return true; &#125; return super.dispatchTouchEvent(ev);&#125; ListView的工具方法 12345678910111213141516171819202122232425262728293031323334353637383940 public static View getChildItemView(Context context, FloatingGroupExpandableListView listView, int groupPosition, int childPosition) &#123; long packedPosition = ExpandableListView.getPackedPositionForChild(groupPosition, childPosition); int flatPosition; try &#123; flatPosition = listView.getFlatListPosition(packedPosition); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; int firstPosition = listView.getFirstVisiblePosition(); int wantedChild = flatPosition - firstPosition; if (wantedChild &lt; 0 || wantedChild &gt;= listView.getChildCount()) &#123; return null; &#125; View childItemView = listView.getChildAt(wantedChild); childItemView.setTag(R.id.anim, AnimationUtils.loadAnimation(context, R.anim.slide_left_out)); return childItemView;&#125;public static View getItemView(Context context, ListView listView, int position) &#123; int firstPosition = listView.getFirstVisiblePosition() - listView.getHeaderViewsCount(); int wantedChild = position - firstPosition; if (wantedChild &lt; 0 || wantedChild &gt;= listView.getChildCount()) &#123; return null; &#125; View wantedView = listView.getChildAt(wantedChild); wantedView.setTag(R.id.anim, AnimationUtils.loadAnimation(context, R.anim.slide_left_out)); return wantedView;&#125;public static void clearListViewAnim(Context context, ListView listView) &#123; int first = listView.getFirstVisiblePosition(); int count = listView.getChildCount() + 2; for (int i = first; i &lt; first + count; i++) &#123; View itemView = getItemView(context, listView, i); if (itemView != null) &#123; itemView.clearAnimation(); &#125; &#125;&#125; The link of this page is http://home.meng.uno/articles/fb92c8a1/ . Welcome to reproduce it!","categories":[{"name":"Application","slug":"Application","permalink":"http://home.meng.uno/categories/Application/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://home.meng.uno/tags/Android/"},{"name":"listview","slug":"listview","permalink":"http://home.meng.uno/tags/listview/"},{"name":"animation","slug":"animation","permalink":"http://home.meng.uno/tags/animation/"}]},{"title":"数学建模常用的十大算法","slug":"cumum-func","date":"2015-02-03T12:03:33.000Z","updated":"2020-12-02T01:42:27.000Z","comments":true,"path":"articles/dd0e4c80/","link":"","permalink":"http://home.meng.uno/articles/dd0e4c80/","excerpt":"概述 蒙特卡罗算法 该算法又称随机性模拟算法，是通过计算机仿真来解决问题的算法，同时可以通过模拟来检验自己模型的正确性，几乎是比赛时必用的方法。 数据拟合、参数估计、插值等数据处理算法 比赛中通常会遇到大量的数据需要处理，而处理数据的关键就在于这些算法，通常使用MATLAB 作为工具。 线性规划、整数规划、多元规划、二次规划等规划类算法 建模竞赛大多数问题属于最优化问题，很多时候这些问题可以用数学规划算法来描述，通常使用Lindo、Lingo 软件求解。 图论算法 这类算法可以分为很多种，包括最短路、网络流、二分图等算法，涉及到图论的问题可以用这些方法解决，需要认真准备。 动态规划、回","text":"概述 蒙特卡罗算法 该算法又称随机性模拟算法，是通过计算机仿真来解决问题的算法，同时可以通过模拟来检验自己模型的正确性，几乎是比赛时必用的方法。 数据拟合、参数估计、插值等数据处理算法 比赛中通常会遇到大量的数据需要处理，而处理数据的关键就在于这些算法，通常使用MATLAB 作为工具。 线性规划、整数规划、多元规划、二次规划等规划类算法 建模竞赛大多数问题属于最优化问题，很多时候这些问题可以用数学规划算法来描述，通常使用Lindo、Lingo 软件求解。 图论算法 这类算法可以分为很多种，包括最短路、网络流、二分图等算法，涉及到图论的问题可以用这些方法解决，需要认真准备。 动态规划、回溯搜索、分治算法、分支定界等计算机算法 这些算法是算法设计中比较常用的方法，竞赛中很多场合会用到。 最优化理论的三大非经典算法：模拟退火算法、神经网络算法、遗传算法 这些问题是用来解决一些较困难的最优化问题的，对于有些问题非常有帮助，但是算法的实现比较困难，需慎重使用。 网格算法和穷举法 两者都是暴力搜索最优点的算法，在很多竞赛题中有应用，当重点讨论模型本身而轻视算法的时候，可以使用这种暴力方案，最好使用一些高级语言作为编程工具。 一些连续数据离散化方法 很多问题都是实际来的，数据可以是连续的，而计算机只能处理离散的数据，因此将其离散化后进行差分代替微分、求和代替积分等思想是非常重要的。 数值分析算法 如果在比赛中采用高级语言进行编程的话，那些数值分析中常用的算法比如方程组求解、矩阵运算、函数积分等算法就需要额外编写库函数进行调用。 图象处理算法 赛题中有一类问题与图形有关，即使问题与图形无关，论文中也会需要图片来说明问题，这些图形如何展示以及如何处理就是需要解决的问题，通常使用MATLAB 进行处理。 以下将结合历年的竞赛题，对这十类算法进行详细地说明。 十类算法的详细说明 蒙特卡罗算法 大多数建模赛题中都离不开计算机仿真，随机性模拟是非常常见的算法之一。举个例子就是97 年的A 题，每个零件都有自己的标定值，也都有自己的容差等级，而求解最优的组合方案将要面对着的是一个极其复杂的公式和108 种容差选取方案，根本不可能去求解析解，那如何去找到最优的方案呢？随机性模拟搜索最优方案就是其中的一种方法，在每个零件可行的区间中按照正态分布随机的选取一个标定值和选取一个容差值作为一种方案，然后通过蒙特卡罗算法仿真出大量的方案，从中选取一个最佳的。另一个例子就是去年的彩票第二问，要求设计一种更好的方案，首先方案的优劣取决于很多复杂的因素，同样不可能刻画出一个模型进行求解，只能靠随机仿真模拟。 数据拟合、参数估计、插值等算法 数据拟合在很多赛题中有应用，与图形处理有关的问题很多与拟合有关系，一个例子就是98 年美国赛A 题，生物组织切片的三维插值处理，94 年A 题逢山开路，山体海拔高度的插值计算，还有吵的沸沸扬扬可能会考的“非典”问题也要用到数据拟合算法，观察数据的走向进行处理。此类问题在MATLAB中有很多现成的函数可以调用，熟悉MATLAB，这些方法都能游刃有余的用好。 规划类问题算法 竞赛中很多问题都和数学规划有关，可以说不少的模型都可以归结为一组不等式作为约束条件、几个函数表达式作为目标函数的问题，遇到这类问题，求解就是关键了，比如98年B 题，用很多不等式完全可以把问题刻画清楚，因此列举出规划后用Lindo、Lingo 等软件来进行解决比较方便，所以还需要熟悉这两个软件。 图论问题 98年B题、00年B题、95年锁具装箱等问题体现了图论问题的重要性，这类问题算法有很多，包括：Dijkstra、Floyd、Prim、Bellman-Ford，最大流，二分匹配等问题。每一个算法都应该实现一遍，否则到比赛时再写就晚了。 计算机算法设计中的问题 计算机算法设计包括很多内容：动态规划、回溯搜索、分治算法、分支定界。比如92 年B 题用分枝定界法，97 年B 题是典型的动态规划问题，此外98 年B 题体现了分治算法。这方面问题和ACM 程序设计竞赛中的问题类似，推荐看一下《计算机算法设计与分析》（电子工业出版社）等与计算机算法有关的书。 最优化理论的三大非经典算法 这十几年来最优化理论有了飞速发展，模拟退火法、神经网络、遗传算法这三类算法发展很快。近几年的赛题越来越复杂，很多问题没有什么很好的模型可以借鉴，于是这三类算法很多时候可以派上用场，比如：97 年A 题的模拟退火算法，00 年B 题的神经网络分类算法，象01 年B 题这种难题也可以使用神经网络，还有美国竞赛89 年A 题也和BP 算法有关系，当时是86 年刚提出BP 算法，89 年就考了，说明赛题可能是当今前沿科技的抽象体现。03 年B 题伽马刀问题也是目前研究的课题，目前算法最佳的是遗传算法。 网格算法和穷举算法 网格算法和穷举法一样，只是网格法是连续问题的穷举。比如要求在N 个变量情况下的最优化问题，那么对这些变量可取的空间进行采点，比如在[a; b] 区间内取M +1 个点，就是a; a+(b-a)/M; a+2 (b-a)/M; …… ; b 那么这样循环就需要进行(M + 1)N 次运算，所以计算量很大。比如97 年A 题、99 年B 题都可以用网格法搜索，这种方法最好在运算速度较快的计算机中进行，还有要用高级语言来做，最好不要用MATLAB 做网格，否则会算很久的。穷举法大家都熟悉，就不说了。 一些连续数据离散化的方法 大部分物理问题的编程解决，都和这种方法有一定的联系。物理问题是反映我们生活在一个连续的世界中，计算机只能处理离散的量，所以需要对连续量进行离散处理。这种方法应用很广，而且和上面的很多算法有关。事实上，网格算法、蒙特卡罗算法、模拟退火都用了这个思想。 数值分析算法 这类算法是针对高级语言而专门设的，如果你用的是MATLAB、Mathematica，大可不必准备，因为象数值分析中有很多函数一般的数学软件是具备的。 图象处理算法 01 年A 题中需要你会读BMP 图象、美国赛98 年A 题需要你知道三维插值计算，03 年B 题要求更高，不但需要编程计算还要进行处理，而数模论文中也有很多图片需要展示，因此图象处理就是关键。做好这类问题，重要的是把MATLAB 学好，特别是图象处理的部分。 The link of this page is http://home.meng.uno/articles/dd0e4c80/ . Welcome to reproduce it!","categories":[{"name":"Mathematical Modeling","slug":"Mathematical-Modeling","permalink":"http://home.meng.uno/categories/Mathematical-Modeling/"}],"tags":[{"name":"数学建模","slug":"数学建模","permalink":"http://home.meng.uno/tags/数学建模/"},{"name":"算法","slug":"算法","permalink":"http://home.meng.uno/tags/算法/"}]},{"title":"兔子，胡萝卜与OAuth的故事","slug":"oauth-rabbit","date":"2014-08-20T04:55:00.000Z","updated":"2020-12-02T01:56:57.000Z","comments":true,"path":"articles/a4df422f/","link":"","permalink":"http://home.meng.uno/articles/a4df422f/","excerpt":"简单的故事，就别用复杂的方式传诵 讲几个故事 从前，有只老兔子，在仓库里存了一万根胡萝卜，作为给小兔子的遗产。而后他就去周游世界了。小兔子有天想去把萝卜拿出来，却被仓库外的一只兔子拦住了。一问才知道，这是老兔子安排的仓库守卫。和所有故事中的守护者一样，他正直而古板，八字眉下面有着睡眠不足的熊猫眼，世人一般称他为兔门神。兔子想要拿到萝卜，就得说服兔门神呀，于是他走了上前… #兔子与OAuth1.0的故事 1. 兔子首先得证明自己是只兔子，不是狗熊也不是狼，于是他向兔门神出示了身份证 2. 兔门神说：哦，你是只兔子。但你还得证明你是老兔子的兔崽子呀。兔子说我爸旅游去了，怎么证明呢？兔门神","text":"简单的故事，就别用复杂的方式传诵 讲几个故事 从前，有只老兔子，在仓库里存了一万根胡萝卜，作为给小兔子的遗产。而后他就去周游世界了。小兔子有天想去把萝卜拿出来，却被仓库外的一只兔子拦住了。一问才知道，这是老兔子安排的仓库守卫。和所有故事中的守护者一样，他正直而古板，八字眉下面有着睡眠不足的熊猫眼，世人一般称他为兔门神。兔子想要拿到萝卜，就得说服兔门神呀，于是他走了上前… #兔子与OAuth1.0的故事 兔子首先得证明自己是只兔子，不是狗熊也不是狼，于是他向兔门神出示了身份证 兔门神说：哦，你是只兔子。但你还得证明你是老兔子的兔崽子呀。兔子说我爸旅游去了，怎么证明呢？兔门神说，这样吧，我把你的身份证拍下来，发送给你爸，让他看下这是不是你。于是兔门神打开了微信…… 正在休假的老兔子看了下照片，回复说证件照好难看毁三观啊，但勉强认得出这货就是我儿子 兔门神确认这信息后，说，你老爸还是认你这个儿子的 兔子问，那我可以去拿胡萝卜了没？ 兔门神说，可以了，这样吧，我发你个通行证，以后拿这个来我就不用这么麻烦了。 兔子与OAuth1.0a的故事 这种貌似天衣无缝的形式，却被一只坏兔子看出了破绽。他注意到一个细节，在最后的一步，兔门神都是习惯性的把通行证交给了面前的兔子,而不管这只兔子是不是当初的那只。于是，坏兔子趁兔门神正在和老兔子聊微信的时候，一个劲站在了兔子前面，最后兔门神居然把通行证塞给了他！这怎么可以？于是在第一步和第六步又有了修改。 兔子出示身份证的同时，也出示了自己的私房照，说，门神大哥呀，后面你记得把通行证给照片上的帅哥！ …… …… …… …… 兔门神看了下面前的兔子，私房照上的明显P过嘛但勉强认得出是本人，于是才交出了通行证 兔子与Oauth2.0的故事 兔门神回家后，向他的老婆兔女神汇报了今天的工作，更安全的方案使他得意洋洋，没想到被兔女神骂了一顿。兔女神说，兔子证明自己还得带个身份证，你不知道在天朝办个身份证多麻烦吗？让小兔子跟老兔子去聊下微信就可以了干嘛要你插手？兔门神哑口无言，兔女神高贵冷艳的说我有四种方案，给你先说说最常用的一种吧。 兔子一开始就跟他老爸聊微信了。当然他得明确告诉老爸，他需要打开哪个仓库（因为老兔子有很多儿子，每个儿子去拿萝卜的仓库不一样，兔子要指定一下具体是哪个，问他可不可以） 老兔子回复说：“just do IT”… 兔子然后去拿胡萝卜，首先被兔女神拦住了。女神告诉他，你要给我四样东西：老兔子的回复，你的私房照，身份证，还要给我一个密码。兔子愣愣的想了个密码，把这四样东西交了过去 兔女神把这四种东西混在一起，用魔法变出了两件法宝：一封情书和一撮猴子毛…然后她解释说：拿着我的情书去找我老公，他就让你进仓库了；但是这情书会过期，是出于安全考虑啦，过期后你得召唤我再写一封，召唤出我的步骤就是吹一下猴子毛，像孙悟空那样你就别在意这些细节好伐？ 兔子拿着情书去找兔门神时，发现他由于被妻子分担了压力，明显睡眠好多了… 演员表 兔子-消费者，也就是第三方应用 老兔子-用户，也就是我们，记住，我们永远是第三方的亲爹 仓库-Oauth提供者，这里有我们保存的资料，比如说新浪微博，qq空间，人人… 兔门神-在前两个故事中，由授权服务器和资源服务器共同扮演，在最后的故事中，只由资源服务器扮演 兔女神-授权服务器，只管授权，不管取资源 重要道具 身份证-签名，将一个http请求以及相应参数字符串化 拍下的身份证照片-Request Token，服务器进行认证 通行证-Access Token，获取资源的凭证 私房照-重定向地址 坏兔子(我把它当成道具而不是演员)-重定向地址劫持 仓库的名称-appId,即对应具体哪个第三方 just do it-Auth code，用户授权号 第三个故事的身份证-client id 客户端帐号 密码-client secret 客户端密码 魔法-将client id，client secket，重定向地址，Auth code生成Access Token 情书-Access Token，获取资源的凭证 猴子毛-Refresh Token，用来在Access Token过期后将其刷新，刷新需带上client id和client secret 说书人说 Oauth2.0比起Oauth1.0，没有了第一步的签名，将服务器分开为授权服务器与资源服务器。这是最大的两个特征。开放平台必须得做到对第三方友好，才有利于接入。像Oauth1.0签名的操作，就难倒了许多第三方。也许你知道了Oauth2.0接入步骤简化了些，但也知道其内部实现要更复杂，抛去安全方面的考虑，我认为这是正确的方向。因为，Oauth2.0在某种意义上说，向第三方做到了——“把悲伤留给自己，你的美丽让你带走”。 The link of this page is http://home.meng.uno/articles/a4df422f/ . Welcome to reproduce it!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://home.meng.uno/categories/Software-Engineering/"}],"tags":[{"name":"web","slug":"web","permalink":"http://home.meng.uno/tags/web/"},{"name":"OAuth","slug":"OAuth","permalink":"http://home.meng.uno/tags/OAuth/"}]}]}