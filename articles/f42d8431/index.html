<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">

  <meta http-equiv="X-Frame-Options" content="DENY">
  <title>《深度学习》问题解答 | 欢迎来到匡盟盟的博客！</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta name="keywords" content="匡盟盟, Colyn Kuang, Blog, 博客" />


  <meta name="google-site-verification" content="3YclHsmiu1_poywYScAg4jt4RGqoHUoIXQJFV5vEZ1I" />


  <meta name="baidu-site-verification" content="d6tIQA0tgL" />


  <meta name="description" content="如何设置网络的初始值？* 《深度学习》 8.4 参数初始化策略  一般总是使用服从（截断）高斯或均匀分布的随机值，具体是高斯还是均匀分布影响不大，但是也没有详细的研究。  但是，初始值的大小会对优化结果和网络的泛化能力产生较大的影响。  更大的初始值有助于避免冗余的单元；但如果初始值太大，又会造成梯度爆炸。  一些启发式初始化策略通常是根据输入与输出的单元数来决定初始权重的大小，比如 Glorot">
  <meta name="keywords" content="深度学习,AI">
  <meta property="og:type" content="article">
  <meta property="og:title" content="《深度学习》问题解答">
  <meta property="og:url" content="http://www.meng.uno/articles/f42d8431/index.html">
  <meta property="og:site_name" content="欢迎来到匡盟盟的博客！">
  <meta property="og:description" content="如何设置网络的初始值？* 《深度学习》 8.4 参数初始化策略  一般总是使用服从（截断）高斯或均匀分布的随机值，具体是高斯还是均匀分布影响不大，但是也没有详细的研究。  但是，初始值的大小会对优化结果和网络的泛化能力产生较大的影响。  更大的初始值有助于避免冗余的单元；但如果初始值太大，又会造成梯度爆炸。  一些启发式初始化策略通常是根据输入与输出的单元数来决定初始权重的大小，比如 Glorot">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610212719.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611172657.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213145.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213218.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213257.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213349.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213428.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608171710.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608172312.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213451.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608212808.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214123.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214502.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214846.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608195851.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214926.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215029.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215150.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215222.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608204913.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215308.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608205223.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215339.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215417.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215445.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215522.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215554.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215623.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215656.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215732.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610131620.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610132602.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610194353.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610203703.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215812.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611150734.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611152559.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611171103.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611171213.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180611194550.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180611194704.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611194754.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611195027.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180611195622.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611203427.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180611210928.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611204215.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611204503.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611211649.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611211753.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611214508.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611215422.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611215923.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611220109.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612104740.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612120851.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612121301.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612154333.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612213144.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/conv_no_padding_no_strides_transposed.gif">
  <meta property="og:image" content="http://www.meng.uno/images/assets/conv_dilation.gif">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613101802.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613150111.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613150711.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613151014.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613164438.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613164637.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613170734.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613171757.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613200023.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613201829.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613211004.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613211437.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613211935.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614103628.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614104032.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614111241.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614111712.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614203447.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180615111903.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610203703.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180615150955.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180617140439.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180617141056.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180617141502.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619104551.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619104649.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619104914.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619113501.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619114844.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619193332.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619193540.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619205103.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619214629.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619214735.png">
  <meta property="og:updated_time" content="2018-08-16T04:44:14.686Z">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="《深度学习》问题解答">
  <meta name="twitter:description" content="如何设置网络的初始值？* 《深度学习》 8.4 参数初始化策略  一般总是使用服从（截断）高斯或均匀分布的随机值，具体是高斯还是均匀分布影响不大，但是也没有详细的研究。  但是，初始值的大小会对优化结果和网络的泛化能力产生较大的影响。  更大的初始值有助于避免冗余的单元；但如果初始值太大，又会造成梯度爆炸。  一些启发式初始化策略通常是根据输入与输出的单元数来决定初始权重的大小，比如 Glorot">
  <meta name="twitter:image" content="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610212719.png">
  <meta name="twitter:creator" content="@kuangmengmeng">
  <link rel="publisher" href="mengmengkuang">
  <meta property="fb:admins" content="kuangmengmeng">
  <meta property="fb:app_id" content="1559086807462632">

  <link rel="alternate" href="/atom.xml" title="欢迎来到匡盟盟的博客！" type="application/atom+xml">




  <link rel="icon" id="myid" href="/css/images/logo.png">
  <link rel="apple-touch-icon" id="myid" href="/css/images/logo.png">

  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">

  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face {
      font-family: futura-pt;
      src: url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");
      font-weight: 500;
      font-style: normal;
    }

    @font-face {
      font-family: futura-pt;
      src: url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");
      font-weight: 500;
      font-style: normal;
    }

    @font-face {
      font-family: futura-pt;
      src: url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");
      font-weight: lighter;
      font-style: normal;
    }

    @font-face {
      font-family: futura-pt;
      src: url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");
      font-weight: 400;
      font-style: italic;
    }
  </style>
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>
  <script src="/js/bootstrap.js"></script>
  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css">
  <link rel="stylesheet" href="/css/prism-duotone-sea.css">


  <link rel="stylesheet" href="/css/dialog.css">





  <link rel="stylesheet" href="/css/header-post.css">





  <link rel="stylesheet" href="/css/vdonate.css">




  <!-- 
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":["copy","weixin","linkedin","sqq","tsina","twi","fbook","mail"],"bdPic":"","bdStyle":"0","bdSize":"16"},"slide":{"type":"slide","bdImg":"0","bdPos":"left","bdTop":"100"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["qzone","tsina","tqq","renren","weixin"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

 -->





  <link rel="canonical" href="http://www.meng.uno/articles/f42d8431/" />

  <link rel="stylesheet" href="/css/animate.css">
  <link rel="stylesheet" href="/css/loading.css">

  <script>
    var linkEle = document.getElementById("myid");
    var tmplink = linkEle.href;

    var tmptitle = document.title;
    document.addEventListener('visibilitychange', function() {
      var isHidden = document.hidden;
      if (isHidden) {
        document.title = '喔唷，崩溃啦！';
        linkEle.href = '/css/images/avatar.png';
      } else {
        document.title = tmptitle;
        linkEle.href = tmplink;

      }
    });


    function Hide() {
      var mychar = document.getElementById("homelogoback").style.display = "none";
    }
  </script>

  <script defer src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"></script>




</head>


<body data-spy="scroll" data-target="#toc" data-offset="50">



  <div id="container">
    <div id="wrap">

      <header>
        <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
          <div class="navbar-inner">
            <div class="container">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>


              <a class="brand" style="
                 border-width: 0px;  margin-top: 0px;" href="#" data-toggle="modal" data-target="#myModal">
                  <img class="img-rotate" style="box-shadow:1px 1px 10px 3px #e5e5e5; border-radius: 50%;" width="124px" height="124px" alt="匡盟盟的博客" src="/css/images/logo.png">
              </a>


              <div class="navbar-collapse collapse">
                <ul class="hnav navbar-nav">

                  <li> <a class="main-nav-link" href="/">首页</a> </li>

                  <li> <a class="main-nav-link" href="/archives">所有文章</a> </li>

                  <li> <a class="main-nav-link" href="/categories">分类</a> </li>

                  <li> <a class="main-nav-link" href="/tags">标签</a> </li>

                  <li> <a class="main-nav-link" href="/about">关于我</a> </li>

                  <li> <a class="main-nav-link" href="/comments">留言板</a> </li>

                  <li>
                    <div id="search-form-wrap">

                      <form class="search-form">
                        <input type="text" style="width=0;" class="ins-search-input search-form-input" placeholder="" />
                        <button type="submit" class="search-form-submit"></button>
                      </form>
                      <div class="ins-search">
                        <div class="ins-search-mask"></div>
                        <div class="ins-search-container">
                          <div class="ins-input-wrapper">
                            <input type="text" class="ins-search-input" placeholder="请输入关键词..." />
                            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
                          </div>
                          <div class="ins-section-wrapper">
                            <div class="ins-section-container"></div>
                          </div>
                        </div>
                      </div>
                      <script>
                        (function(window) {
                          var INSIGHT_CONFIG = {
                            TRANSLATION: {
                              POSTS: '文章',
                              PAGES: '页面',
                              CATEGORIES: '分类',
                              TAGS: '标签',
                              UNTITLED: '空标题',
                            },
                            ROOT_URL: '/',
                            CONTENT_URL: '/content.json',
                          };
                          window.INSIGHT_CONFIG = INSIGHT_CONFIG;
                        })(window);
                      </script>
                      <script src="/js/insight.js"></script>

                    </div>
                  </li>

                </ul>
              </div>
            </div>

          </div>
        </div>

      </header>

      <script>
        (function(w, i, d, g, e, t, s) {
          w[d] = w[d] || [];
          t = i.createElement(g);
          t.async = 1;
          t.src = e;
          s = i.getElementsByTagName(g)[0];
          s.parentNode.insertBefore(t, s);
        })(window, document, '_gscq', 'script', '//widgets.getsitecontrol.com/125646/script.js');
      </script>


      <div id="content" class="outer">

        <section id="main" style="float:none;">
          <article id="post-DL-《深度学习》整理" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost">
            <div id="articleInner" class="article-inner">


              <header class="article-header">
                <h1 class="thumb" class="article-title" itemprop="name">
                  《深度学习》问题解答
                </h1>
              </header>

              <div class="article-meta">
                <a href="/articles/f42d8431/" class="article-date">
	  <time datetime="2018-05-03T18:09:21.000Z" itemprop="datePublished">2018-05-04</time>
	</a>
                <a class="article-category-link" href="/categories/DeepLearning/">DeepLearning</a>
                <a class="article-views">
	<span id="busuanzi_container_page_pv">
		阅读量<span id="busuanzi_value_page_pv"></span>
	</span>
	<span class="post-count"> | 字数24,496</span>
	<span class="post-count"> | 预计时间87分钟</span>
	</a>
              </div>
              <div class="article-entry" itemprop="articleBody">

                <h1>如何设置网络的初始值？*</h1>
                <blockquote>
                  <p>《深度学习》 8.4 参数初始化策略</p>
                </blockquote>
                <p>一般总是使用服从（截断）高斯或均匀分布的随机值，具体是高斯还是均匀分布影响不大，但是也没有详细的研究。</p>
                <p>但是，<strong>初始值的大小</strong>会对优化结果和网络的泛化能力产生较大的影响。</p>
                <p>更大的初始值有助于避免冗余的单元；但如果初始值太大，又会造成梯度爆炸。</p>
                <p>一些启发式初始化策略通常是根据输入与输出的单元数来决定初始权重的大小，比如 Glorot and Bengio (2010) 中建议建议使用的标准初始化，其中 m 为输入数，n 为输出数</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=W_%7Bi,j%7D&amp;space;%5Csim&amp;space;U(-%5Csqrt%7B%5Cfrac%7B6%7D%7Bm+n%7D%7D,%5Csqrt%7B%5Cfrac%7B6%7D%7Bm+n%7D%7D)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610212719.png" alt=""></a></p>
                <p>还有一些方法推荐使用随机正交矩阵来初始化权重 (Saxe et al., 2013)。</p>
                <blockquote>
                  <p>常用的初始化策略可以参考 Keras 中文文档：<a href="http://keras-cn.readthedocs.io/en/latest/other/initializations/" target="_blank" rel="noopener">初始化方法Initializers</a></p>
                </blockquote>
                <h1>梯度爆炸的解决办法</h1>
                <p><strong>梯度爆炸</strong>：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611172657.png" alt=""></p>
                <ol>
                  <li>
                    <p><strong>梯度截断</strong>（gradient clipping）——如果梯度超过某个阈值，就对其进行限制</p>
                    <blockquote>
                      <p>《深度学习》 10.11.1 截断梯度</p>
                    </blockquote>
                    <p>下面是 Tensorflow 提供的几种方法：</p>
                    <ul>
                      <li><code>tf.clip_by_value(t, clip_value_min, clip_value_max)</code></li>
                      <li><code>tf.clip_by_norm(t, clip_norm)</code></li>
                      <li><code>tf.clip_by_average_norm(t, clip_norm)</code></li>
                      <li><code>tf.clip_by_global_norm(t_list, clip_norm)</code></li>
                    </ul>
                    <p>这里以<code>tf.clip_by_global_norm</code>为例：</p>
                    <figure class="highlight plain">
                      <table>
                        <tr>
                          <td class="gutter">
                            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                          </td>
                          <td class="code">
                            <pre><span class="line">To perform the clipping, the values `t_list[i]` are set to:</span><br><span class="line"></span><br><span class="line">    t_list[i] * clip_norm / max(global_norm, clip_norm)</span><br><span class="line"></span><br><span class="line">where:</span><br><span class="line"></span><br><span class="line">    global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))</span><br></pre>
                          </td>
                        </tr>
                      </table>
                    </figure>
                    <p>用法：</p>
                    <figure class="highlight plain">
                      <table>
                        <tr>
                          <td class="gutter">
                            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>
                          </td>
                          <td class="code">
                            <pre><span class="line">train_op = tf.train.AdamOptimizer()</span><br><span class="line">params = tf.trainable_variables()</span><br><span class="line">gradients = tf.gradients(loss, params)</span><br><span class="line"></span><br><span class="line">clip_norm = 100</span><br><span class="line">clipped_gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm)</span><br><span class="line"></span><br><span class="line">optimizer_op = train_op.apply_gradients(zip(clipped_gradients, params))</span><br></pre>
                          </td>
                        </tr>
                      </table>
                    </figure>
                    <blockquote>
                      <p>clip_norm 的设置视 loss 的大小而定，如果比较大，那么可以设为 100 或以上，如果比较小，可以设为 10 或以下。</p>
                    </blockquote>
                  </li>
                  <li>
                    <p>良好的参数初始化策略也能缓解梯度爆炸问题（权重正则化）</p>
                  </li>
                  <li>
                    <p>使用线性整流激活函数，如 ReLU 等</p>
                  </li>
                </ol>
                <h1>神经网络（MLP）的万能近似定理*</h1>
                <blockquote>
                  <p>《深度学习》 6.4.1 万能近似性质和深度</p>
                </blockquote>
                <p>一个前馈神经网络如果具有至少一个非线性输出层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。</p>
                <h1>神经网络中，深度与宽度的关系，及其表示能力的差异**</h1>
                <blockquote>
                  <p>《深度学习》 6.4 - 架构设计；这一节的内容比较分散，想要更好的回答这个问题，需要理解深度学习的本质——学习多层次组合（ch1.2），这才是现代深度学习的基本原理。</p>
                </blockquote>
                <p>隐藏层的数量称为模型的<strong>深度</strong>，隐藏层的维数（单元数）称为该层的<strong>宽度</strong>。</p>
                <p><strong>万能近似定理</strong>表明一个单层的网络就足以表达任意函数，但是该层的维数可能非常大，且几乎没有泛化能力；此时，使用更深的模型能够减少所需的单元数，同时增强泛化能力（减少泛化误差）。参数数量相同的情况下，浅层网络比深层网络更容易过拟合。</p>
                <h1>在深度神经网络中，引入了隐藏层（非线性单元），放弃了训练问题的凸性，其意义何在？**</h1>
                <blockquote>
                  <p>《深度学习》 6 深度前馈网络（引言） &amp; 6.3 隐藏单元</p>
                </blockquote>
                <p>放弃训练问题的凸性，简单来说，就是放弃寻求问题的最优解。</p>
                <p><strong>非线性单元</strong>的加入，使训练问题不再是一个<strong>凸优化</strong>问题。这意味着神经网络很难得到最优解，即使一个只有两层和三个节点的简单神经网络，其训练优化问题仍然是 NP-hard 问题 (Blum &amp; Rivest, 1993).</p>
                <blockquote>
                  <p><a href="http://baijiahao.baidu.com/s?id=1561255903377484&amp;wfr=spider&amp;for=pc%EF%BC%89" target="_blank" rel="noopener">深度学习的核心问题——NP-hard问题</a> - 百家号</p>
                </blockquote>
                <p>但即使如此，使用神经网络也是利大于弊的：</p>
                <ul>
                  <li>人类设计者只需要寻找正确的<strong>函数族</strong>即可，而不需要去寻找精确的函数。</li>
                  <li>使用简单的梯度下降优化方法就可以高效地找到足够好的局部最小值</li>
                  <li>增强了模型的学习/拟合能力，如原书中所说“ maxout 单元可以以任意精度近似任何凸函数”。至于放弃凸性后的优化问题可以在结合工程实践来不断改进。 “似乎传统的优化理论结果是残酷的，但我们可以通过<strong>工程方法</strong>和<strong>数学技巧</strong>来尽量规避这些问题，例如启发式方法、增加更多的机器和使用新的硬件（如GPU）。”</li>
                </ul>
                <blockquote>
                  <p><a href="https://github.com/elviswf/DeepLearningBookQA_cn/issues/1#issuecomment-396061806" target="_blank" rel="noopener">Issue #1</a> · elviswf/DeepLearningBookQA_cn</p>
                </blockquote>
                <h1>稀疏表示，低维表示，独立表示*</h1>
                <blockquote>
                  <p>《深度学习》 5.8 无监督学习算法</p>
                </blockquote>
                <p>无监督学习任务的目的是找到数据的“最佳”表示。“最佳”可以有不同的表示，但是一般来说，是指该表示在比本身表示的信息更简单的情况下，尽可能地保存关于 x 更多的信息。</p>
                <p>低维表示、稀疏表示和独立表示是最常见的三种“简单”表示：1）低维表示尝试将 x 中的信息尽可能压缩在一个较小的表示中；2）稀疏表示将数据集嵌入到输入项大多数为零的表示中；3）独立表示试图分开数据分布中变化的来源，使得表示的维度是统计独立的。</p>
                <p>这三种表示不是互斥的，比如主成分分析（PCA）就试图同时学习低维表示和独立表示。</p>
                <p>表示的概念是深度学习的核心主题之一。</p>
                <h1>局部不变性（平滑先验）及其在基于梯度的学习上的局限性*</h1>
                <blockquote>
                  <p>《深度学习》 5.11.2 局部不变性与平滑正则化</p>
                </blockquote>
                <p>局部不变性：函数在局部小区域内不会发生较大的变化。</p>
                <p>为了更好地<strong>泛化</strong>，机器学习算法需要由一些先验来引导应该学习什么类型的函数。</p>
                <p>其中最广泛使用的“隐式先验”是平滑先验（smoothness prior），也称局部不变性先验（local constancy prior）。许多简单算法完全依赖于此先验达到良好的（局部）泛化，一个极端例子是 k-最近邻系列的学习算法。</p>
                <p>但是仅依靠平滑先验<strong>不足以</strong>应对人工智能级别的任务。简单来说，区分输入空间中 O(k) 个区间，需要 O(k) 个样本，通常也会有 O(k) 个参数。最近邻算法中，每个训练样本至多用于定义一个区间。类似的，决策树也有平滑学习的局限性。</p>
                <p>以上问题可以总结为：是否可以有效地表示复杂的函数，以及所估计的函数是否可以很好地泛化到新的输入。该问题的一个关键观点是，只要我们通过额外假设生成数据的分布来建立区域间的依赖关系，那么 O(k) 个样本足以描述多如 O(2^k) 的大量区间。通过这种方式，能够做到<strong>非局部的泛化</strong>。</p>
                <blockquote>
                  <p>一些其他的机器学习方法往往会提出更强的，针对特定问题的假设，例如周期性。通常，神经网络不会包含这些很强的针对性假设——深度学习的核心思想是假设数据由因素或特征组合产生，这些因素或特征可能来自一个层次结构的多个层级。许多其他类似的通用假设进一步提高了深度学习算法。这些很温和的假设允许了样本数目和可区分区间数目之间的<strong>指数增益</strong>。深度的分布式表示带来的指数增益有效地解决了维数灾难带来的挑战</p>
                  <blockquote>
                    <p>指数增益：《深度学习》 ch6.4.1、ch15.4、ch15.5</p>
                  </blockquote>
                </blockquote>
                <h1>为什么交叉熵损失相比均方误差损失能提高以 sigmoid 和 softmax 作为激活函数的层的性能？**</h1>
                <blockquote>
                  <p>《深度学习》 6.6 小结 中提到了这个结论，但是没有给出具体原因（可能在前文）。</p>
                </blockquote>
                <p>简单来说，就是使用均方误差（MSE）作为损失函数时，会导致大部分情况下<strong>梯度偏小</strong>，其结果就是权重的更新很慢，且容易造成“梯度消失”现象。而交叉熵损失克服了这个缺点，当误差大的时候，权重更新就快，当误差小的时候，权重的更新才慢。</p>
                <p>具体推导过程如下：</p>
                <blockquote>
                  <p><a href="https://blog.csdn.net/guoyunfei20/article/details/78247263" target="_blank" rel="noopener">https://blog.csdn.net/guoyunfei20/article/details/78247263</a> - CSDN 博客</p>
                  <p>这里给出了一个具体的<a href="https://blog.csdn.net/shmily_skx/article/details/53053870" target="_blank" rel="noopener">例子</a></p>
                </blockquote>
                <h1>分段线性单元（如 ReLU）代替 sigmoid 的利弊</h1>
                <ul>
                  <li>
                    <p>当神经网络比较小时，sigmoid 表现更好；</p>
                  </li>
                  <li>
                    <p>在深度学习早期，人们认为应该避免具有不可导点的激活函数，而 ReLU 不是全程可导/可微的</p>
                  </li>
                  <li>
                    <p>sigmoid 和 tanh 的输出是有界的，适合作为下一层的输入，以及整个网络的输出。实际上，目前大多数网络的输出层依然使用的 sigmoid（单输出） 或 softmax（多输出）。</p>
                    <blockquote>
                      <p>为什么 ReLU 不是全程可微也能用于基于梯度的学习？——虽然 ReLU 在 0 点不可导，但是它依然存在左导数和右导数，只是它们不相等（相等的话就可导了），于是在实现时通常会返回左导数或右导数的其中一个，而不是报告一个导数不存在的错误。</p>
                      <blockquote>
                        <p>一阶函数：可微==可导</p>
                      </blockquote>
                    </blockquote>
                  </li>
                  <li>
                    <p>对于小数据集，使用整流非线性甚至比学习隐藏层的权重值更加重要 (Jarrett et al., 2009b)</p>
                  </li>
                  <li>
                    <p>当数据增多时，在深度整流网络中的学习比在激活函数具有曲率或两侧<strong>饱和</strong>的深度网络中的学习更容易 (Glorot et al., 2011a)：传统的 sigmoid 函数，由于两端饱和，在传播过程中容易丢弃信息</p>
                  </li>
                  <li>
                    <p>ReLU 的过程更接近生物神经元的作用过程</p>
                    <blockquote>
                      <p>饱和（saturate）现象：在函数图像上表现为变得很平，对输入的微小改变会变得不敏感。</p>
                    </blockquote>
                  </li>
                </ul>
                <blockquote>
                  <p><a href="https://blog.csdn.net/code_lr/article/details/51836153" target="_blank" rel="noopener">https://blog.csdn.net/code_lr/article/details/51836153</a> - CSDN博客</p>
                  <blockquote>
                    <p>答案总结自该知乎问题：<a href="https://www.zhihu.com/question/29021768" target="_blank" rel="noopener">https://www.zhihu.com/question/29021768</a></p>
                  </blockquote>
                </blockquote>
                <h1>在做正则化过程中，为什么只对权重做正则惩罚，而不对偏置做权重惩罚*</h1>
                <blockquote>
                  <p>《深度学习》 7.1 参数范数惩罚</p>
                </blockquote>
                <p>在神经网络中，参数包括每一层仿射变换的<strong>权重</strong>和<strong>偏置</strong>，我们通常只对权重做惩罚而不对偏置做正则惩罚。</p>
                <p>精确拟合偏置所需的数据通常比拟合权重少得多。每个权重会指定两个变量如何相互作用。我们需要在各种条件下观察这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。另外，正则化偏置参数可能会导致明显的欠拟合。</p>
                <h1>列举常见的一些范数及其应用场景，如 L0、L1、L2、L∞、Frobenius等范数**</h1>
                <blockquote>
                  <p>《深度学习》 2.5 范数（介绍）</p>
                </blockquote>
                <p>L0: 向量中非零元素的个数</p>
                <p>L1: 向量中所有元素的绝对值之和</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cleft&amp;space;%7C&amp;space;x&amp;space;%5Cright&amp;space;%7C_1=%5Csum_i%7B%5Cleft&amp;space;%7C&amp;space;x_i&amp;space;%5Cright&amp;space;%7C%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213145.png" alt=""></a></p>
                <p>L2: 向量中所有元素平方和的开放</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cleft&amp;space;%7C&amp;space;x&amp;space;%5Cright&amp;space;%7C_2=%5Csqrt%7B%5Csum_i%7B%5Cleft&amp;space;%7C&amp;space;x_i&amp;space;%5Cright&amp;space;%7C%5E2%7D%7D" target="_blank"
                    rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213218.png" alt=""></a></p>
                <p>其中 L1 和 L2 范数分别是 Lp (p&gt;=1) 范数的特例：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cleft&amp;space;%7C&amp;space;x&amp;space;%5Cright&amp;space;%7C_p=(%5Csum_i%7B%5Cleft&amp;space;%7C&amp;space;x_i&amp;space;%5Cright&amp;space;%7C%5E2%7D)%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D"
                    target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213257.png" alt=""></a></p>
                <p>L∞: 向量中最大元素的绝对值，也称最大范数</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cleft&amp;space;%7C&amp;space;x&amp;space;%5Cright&amp;space;%7C_%5Cinfty=%5Cmax_i%5Cleft&amp;space;%7C&amp;space;x&amp;space;%5Cright&amp;space;%7C" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213349.png" alt=""></a></p>
                <p>Frobenius 范数：相当于作用于矩阵的 L2 范数</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cleft&amp;space;%7C&amp;space;A&amp;space;%5Cright&amp;space;%7C_F=%5Csqrt%7B%5Csum_%7Bi,j%7DA_%7Bi,j%7D%5E2%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213428.png" alt=""></a></p>
                <p><strong>范数的应用</strong>：正则化——权重衰减/参数范数惩罚</p>
                <p><strong>权重衰减的目的</strong></p>
                <p>限制模型的学习能力，通过限制参数 θ 的规模（主要是权重 w 的规模，偏置 b 不参与惩罚），使模型偏好于权值较小的目标函数，防止过拟合。</p>
                <blockquote>
                  <p>《深度学习》 7.1 参数范数惩罚</p>
                </blockquote>
                <h1>L1 和 L2 范数的异同***</h1>
                <blockquote>
                  <p>《深度学习》 7.1.1 L2 参数正则化 &amp; 7.1.2 - L1 参数正则化</p>
                </blockquote>
                <p><strong>相同点</strong></p>
                <ul>
                  <li>限制模型的学习能力，通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。</li>
                </ul>
                <p><strong>不同点</strong></p>
                <ul>
                  <li>L1 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择；一定程度上防止过拟合</li>
                  <li>L2 正则化主要用于防止模型过拟合</li>
                  <li>L1 适用于特征之间有关联的情况；L2 适用于特征之间没有关联的情况</li>
                </ul>
                <blockquote>
                  <p><a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">机器学习中正则化项L1和L2的直观理解</a> - CSDN博客</p>
                </blockquote>
                <h1>为什么 L1 正则化可以产生稀疏权值，L2 正则化可以防止过拟合？**</h1>
                <h2 id="为什么-l1-正则化可以产生稀疏权值-而-l2-不会？">为什么 L1 正则化可以产生稀疏权值，而 L2 不会？</h2>
                <p>添加 L1 正则化，相当于在 L1范数的约束下求目标函数 J 的最小值，下图展示了二维的情况：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608171710.png" alt=""></p>
                <p>图中 J 与 L 首次相交的点就是最优解。L1 在和每个坐标轴相交的地方都会有“角”出现（多维的情况下，这些角会更多），在角的位置就会产生稀疏的解。而 J 与这些“角”相交的机会远大于其他点，因此 L1 正则化会产生稀疏的权值。</p>
                <p>类似的，可以得到带有 L2正则化的目标函数在二维平面上的图形，如下：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608172312.png" alt=""></p>
                <p>相比 L1，L2 不会产生“角”，因此 J 与 L2 相交的点具有稀疏性的概率就会变得非常小。</p>
                <blockquote>
                  <p><a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">机器学习中正则化项L1和L2的直观理解</a> - CSDN博客</p>
                </blockquote>
                <h2 id="为什么-l1-和-l2-正则化可以防止过拟合？">为什么 L1 和 L2 正则化可以防止过拟合？</h2>
                <p>L1 &amp; L2 正则化会使模型偏好于更小的权值。</p>
                <p>简单来说，更小的权值意味着更低的模型复杂度，也就是对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据（比如异常点，噪声），以提高模型的泛化能力。</p>
                <p>此外，添加正则化相当于为模型添加了某种<strong>先验</strong>（限制），规定了参数的分布，从而降低了模型的复杂度。模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。</p>
                <blockquote>
                  <p><a href="https://blog.csdn.net/heyongluoyao8/article/details/49429629" target="_blank" rel="noopener">机器学习中防止过拟合的处理方法</a> - CSDN博客</p>
                </blockquote>
                <h1>简单介绍常用的激活函数，如 sigmoid、relu、softplus、tanh、RBF 及其应用场景***</h1>
                <h2 id="整流线性单元-relu">整流线性单元（ReLU）</h2>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=g(z)=%5Cmax(0,z)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213451.png" alt=""></a></p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608212808.png" alt=""></p>
                <p>整流线性单元（ReLU）通常是激活函数较好的默认选择。</p>
                <p>整流线性单元易于优化，因为它们和线性单元非常类似。线性单元和整流线性单元的唯一区别在于整流线性单元在其一半的定义域上输出为零。这使得只要整流线性单元处于激活状态，它的导数都能保持较大。它的梯度不仅大而且一致。整流操作的二阶导数几乎处处为 0，并且在整流线性单元处于激活状态时，它的一阶导数处处为 1。这意味着相比于引入二阶效应的激活函数来说，它的梯度方向对于学习来说更加有用。</p>
                <p><strong>ReLU 的拓展</strong></p>
                <p>ReLU 的三种拓展都是基于以下变型：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=g(z,%5Calpha)&amp;space;=%5Cmax(0,z)+%5Calpha%5Cmin(0,z)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214123.png" alt=""></a></p>
                <p>ReLU 及其扩展都是基于一个原则，那就是如果它们的行为更接近线性，那么模型更容易优化。</p>
                <ul>
                  <li>
                    <p>绝对值整流（absolute value rectification）</p>
                    <p>固定 α == -1，此时整流函数即一个绝对值函数</p>
                    <p><a href="http://www.codecogs.com/eqnedit.php?latex=g(z)&amp;space;=%5Cleft&amp;space;%7C&amp;space;z&amp;space;%5Cright&amp;space;%7C" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214502.png" alt=""></a></p>
                    <p>绝对值整流被用于图像中的对象识别 (Jarrett et al., 2009a)，其中寻找在输入照明极性反转下不变的特征是有意义的。</p>
                  </li>
                  <li>
                    <p>渗漏整流线性单元（Leaky ReLU, Maas et al., 2013）</p>
                    <p>固定 α 为一个类似于 0.01 的小值</p>
                  </li>
                  <li>
                    <p>参数化整流线性单元（parametric ReLU, PReLU, He et al., 2015）</p>
                    <p>将 α 作为一个参数学习</p>
                  </li>
                  <li>
                    <p>maxout 单元 (Goodfellow et al., 2013a)</p>
                    <p>maxout 单元 进一步扩展了 ReLU，它是一个可学习的多达 k 段的分段函数</p>
                    <p>关于 maxout 网络的分析可以参考论文或网上的众多分析，下面是 Keras 中的实现：</p>
                    <figure class="highlight plain">
                      <table>
                        <tr>
                          <td class="gutter">
                            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                          </td>
                          <td class="code">
                            <pre><span class="line"># input shape:  [n, input_dim]</span><br><span class="line"># output shape: [n, output_dim]</span><br><span class="line">W = init(shape=[k, input_dim, output_dim])</span><br><span class="line">b = zeros(shape=[k, output_dim])</span><br><span class="line">output = K.max(K.dot(x, W) + b, axis=1)</span><br></pre>
                          </td>
                        </tr>
                      </table>
                    </figure>
                    <blockquote>
                      <p><a href="https://blog.csdn.net/hjimce/article/details/50414467" target="_blank" rel="noopener">深度学习（二十三）Maxout网络学习</a> - CSDN博客</p>
                    </blockquote>
                  </li>
                </ul>
                <h2 id="sigmoid-与-tanh-双曲正切函数">sigmoid 与 tanh（双曲正切函数）</h2>
                <p>在引入 ReLU 之前，大多数神经网络使用 sigmoid 激活函数：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=g(z)=%5Csigma(z)=%5Cfrac%7B1%7D%7B1+%5Cexp(-z)%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214846.png" alt=""></a></p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608195851.png" alt=""></p>
                <p>或者 tanh（双曲正切函数）：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=g(z)&amp;space;=&amp;space;%5Ctanh(z)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214926.png" alt=""></a></p>
                <p>tanh 的图像类似于 sigmoid，区别在其值域为 (-1, 1).</p>
                <p>这两个函数有如下关系：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Ctanh(z)=2%5Csigma&amp;space;(2z)-1" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215029.png" alt=""></a></p>
                <p><strong>sigmoid 函数要点</strong>：</p>
                <ul>
                  <li>sigmoid 常作为输出单元用来预测二值型变量取值为 1 的概率
                    <blockquote>
                      <p>换言之，sigmoid 函数可以用来产生<strong>伯努利分布</strong>中的参数 ϕ，因为它的值域为 (0, 1).</p>
                    </blockquote>
                  </li>
                  <li>sigmoid 函数在输入取绝对值非常大的正值或负值时会出现<strong>饱和</strong>（saturate）现象，在图像上表现为开始变得很平，此时函数会对输入的微小改变会变得不敏感。仅当输入接近 0 时才会变得敏感。
                    <blockquote>
                      <p>饱和现象会导致基于梯度的学习变得困难，并在传播过程中丢失信息。——<a href="#8.-%E5%88%86%E6%AE%B5%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%EF%BC%88%E5%A6%82-ReLU%EF%BC%89%E4%BB%A3%E6%9B%BF-sigmoid-%E7%9A%84%E5%88%A9%E5%BC%8A">为什么用ReLU代替sigmoid？</a></p>
                    </blockquote>
                  </li>
                  <li>如果要使用 sigmoid 作为激活函数时（浅层网络），tanh 通常要比 sigmoid 函数表现更好。
                    <blockquote>
                      <p>tanh 在 0 附近与单位函数类似，这使得训练 tanh 网络更容易些。</p>
                    </blockquote>
                  </li>
                </ul>
                <h2 id="其他激活函数-隐藏单元">其他激活函数（隐藏单元）</h2>
                <p>很多未发布的非线性激活函数也能表现的很好，但没有比流行的激活函数表现的更好。比如使用 cos 也能在 MNIST 任务上得到小于 1% 的误差。通常新的隐藏单元类型只有在被明确证明能够提供显著改进时才会被发布。</p>
                <p><strong>线性激活函数</strong>：</p>
                <p>如果神经网络的每一层都由线性变换组成，那么网络作为一个整体也将是线性的，这会导致失去万能近似的性质。但是，仅<strong>部分层是纯线性</strong>是可以接受的，这可以帮助<strong>减少网络中的参数</strong>。</p>
                <p><strong>softmax</strong>：</p>
                <p>softmax 单元常作为网络的输出层，它很自然地表示了具有 k 个可能值的离散型随机变量的概率分布。</p>
                <p><strong>径向基函数（radial basis function, RBF）</strong>：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=h_i=%5Cexp(-%5Cfrac%7B1%7D%7B%5Csigma_i%5E2%7D%5Cleft&amp;space;%7C&amp;space;W_%7B:,i%7D-x&amp;space;%5Cright&amp;space;%7C%5E2)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215150.png" alt=""></a></p>
                <p>在神经网络中很少使用 RBF 作为激活函数，因为它对大部分 x 都饱和到 0，所以很难优化。</p>
                <p><strong>softplus</strong>：</p>
                <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215222.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=g(z)=%5Czeta(z)=%5Clog(1+%5Cexp(z)" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=g(z)=\zeta(z)=\log(1+\exp(z)</a>))</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608204913.png" alt=""></p>
                <p>softplus 是 ReLU 的平滑版本。通常不鼓励使用 softplus 函数，大家可能希望它具有优于整流线性单元的点，但根据经验来看，它并没有。</p>
                <blockquote>
                  <p>(Glorot et al., 2011a) 比较了这两者，发现 ReLU 的结果更好。</p>
                </blockquote>
                <p><strong>硬双曲正切函数（hard tanh）</strong>：</p>
                <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215308.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=g(z)=%5Cmax(-1,%5Cmin(1,a)" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=g(z)=\max(-1,\min(1,a)</a>))</p>
                <p>它的形状和 tanh 以及整流线性单元类似，但是不同于后者，它是有界的。</p>
                <blockquote>
                  <p>Collobert, 2004</p>
                </blockquote>
                <h2 id="sigmoid-和-softplus-的一些性质">sigmoid 和 softplus 的一些性质</h2>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608205223.png" alt=""></p>
                <blockquote>
                  <p>《深度学习》 3.10 常用函数的有用性质</p>
                </blockquote>
                <h1>Jacobian 和 Hessian 矩阵及其在深度学习中的重要性*</h1>
                <blockquote>
                  <p>《深度学习》 4.3.1 梯度之上：Jacobian 和 Hessian 矩阵</p>
                </blockquote>
                <h1>信息熵、KL 散度（相对熵）与交叉熵**</h1>
                <blockquote>
                  <p>《深度学习》 3.13 信息论</p>
                </blockquote>
                <p>信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事件发生，能提供更多的信息。</p>
                <p>该想法可描述为以下性质：</p>
                <ol>
                  <li>非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量。</li>
                  <li>比较不可能发生的事件具有更高的信息量。</li>
                  <li>独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。</li>
                </ol>
                <h2 id="自信息与信息熵">自信息与信息熵</h2>
                <p>自信息（self-information）是一种量化以上性质的函数，定义一个事件 x 的自信息为：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=I(x)=-%5Clog&amp;space;P(x)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215339.png" alt=""></a></p>
                <blockquote>
                  <p>当该对数的底数为 e 时，单位为奈特（nats，本书标准）；当以 2 为底数时，单位为比特（bit）或香农（shannons）</p>
                </blockquote>
                <p>自信息只处理单个的输出。此时，用信息熵（Information-entropy）来对整个概率分布中的不确定性总量进行量化：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=H(%5Cmathrm%7BX%7D)=%5Cmathbb%7BE%7D_%7B%5Cmathrm%7BX%7D&amp;space;%5Csim&amp;space;P%7D%5BI(x)%5D=-%5Csum_%7Bx&amp;space;%5Cin&amp;space;%5Cmathrm%7BX%7D%7DP(x)%5Clog&amp;space;P(x)"
                    target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215417.png" alt=""></a></p>
                <blockquote>
                  <p>信息熵也称香农熵（Shannon entropy）</p>
                  <p>信息论中，记 <code>0log0 = 0</code></p>
                </blockquote>
                <h2 id="相对熵-kl-散度-与交叉熵">相对熵（KL 散度）与交叉熵</h2>
                <p>P 对 Q 的 <strong>KL散度</strong>（Kullback-Leibler divergence）：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=D_P(Q)=%5Cmathbb%7BE%7D_%7B%5Cmathrm%7BX%7D%5Csim&amp;space;P%7D%5Cleft&amp;space;%5B&amp;space;%5Clog&amp;space;%5Cfrac%7BP(x)%7D%7BQ(x)%7D&amp;space;%5Cright&amp;space;%5D=%5Csum_%7Bx&amp;space;%5Cin&amp;space;%5Cmathrm%7BX%7D%7DP(x)%5Cleft&amp;space;%5B&amp;space;%5Clog&amp;space;P(x)-%5Clog&amp;space;Q(x)&amp;space;%5Cright&amp;space;%5D"
                    target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215445.png" alt=""></a></p>
                <p><strong>KL 散度在信息论中度量的是那个直观量</strong>：</p>
                <p>在离散型变量的情况下， KL 散度衡量的是，当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号的消息时，所需要的额外信息量。</p>
                <p><strong>KL 散度的性质</strong>：</p>
                <ul>
                  <li>非负；KL 散度为 0 当且仅当P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的</li>
                  <li>不对称；D_p(q) != D_q§</li>
                </ul>
                <p><strong>交叉熵</strong>（cross-entropy）：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=H_P(Q)=-%5Cmathbb%7BE%7D_%7B%5Cmathrm%7BX%7D%5Csim&amp;space;P%7D%5Clog&amp;space;Q(x)=-%5Csum_%7Bx&amp;space;%5Cin&amp;space;%5Cmathrm%7BX%7D%7DP(x)%5Clog&amp;space;Q(x)" target="_blank"
                    rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215522.png" alt=""></a></p>
                <blockquote>
                  <p><a href="https://blog.csdn.net/haolexiao/article/details/70142571" target="_blank" rel="noopener">信息量，信息熵，交叉熵，KL散度和互信息（信息增益）</a> - CSDN博客</p>
                </blockquote>
                <p><strong>交叉熵与 KL 散度的关系</strong>：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=H_P(Q)=H(P)+D_P(Q)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215554.png" alt=""></a></p>
                <p><strong>针对 Q 最小化交叉熵等价于最小化 P 对 Q 的 KL 散度</strong>，因为 Q 并不参与被省略的那一项。</p>
                <p>最大似然估计中，最小化 KL 散度其实就是在最小化分布之间的交叉熵。</p>
                <blockquote>
                  <p>《深度学习》 ch5.5 - 最大似然估计</p>
                </blockquote>
                <h1>如何避免数值计算中的上溢和下溢问题，以 softmax 为例*</h1>
                <blockquote>
                  <p>《深度学习》 4.1 上溢与下溢</p>
                </blockquote>
                <ul>
                  <li><strong>上溢</strong>：一个很大的数被近似为 ∞ 或 -∞；</li>
                  <li><strong>下溢</strong>：一个很小的数被近似为 0</li>
                </ul>
                <p>必须对上溢和下溢进行<strong>数值稳定</strong>的一个例子是 <strong>softmax 函数</strong>：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cmathrm%7Bsoftmax%7D(x)=%5Cfrac%7B%5Cexp(x_i)%7D%7B%5Csum_%7Bj=1%7D%5En&amp;space;%5Cexp(x_j)%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215623.png" alt=""></a></p>
                <p>因为 softmax 解析上的函数值不会因为从输入向量减去或加上<strong>标量</strong>而改变， 于是一个简单的解决办法是对 x：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=x=x-%5Cmax_ix_i" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215656.png" alt=""></a></p>
                <p>减去 <code>max(x_i)</code> 导致 <code>exp</code> 的最大参数为 <code>0</code>，这排除了上溢的可能性。同样地，分母中至少有一个值为 <code>1=exp(0)</code> 的项，这就排除了因分母下溢而导致被零除的可能性。</p>
                <p><strong>注意</strong>：虽然解决了分母中的上溢与下溢问题，但是分子中的下溢仍可以导致整体表达式被计算为零。此时如果计算 log softmax(x) 时，依然要注意可能造成的上溢或下溢问题，处理方法同上。</p>
                <p>当然，大多数情况下，这是底层库开发人员才需要注意的问题。</p>
                <h1>训练误差、泛化误差；过拟合、欠拟合；模型容量，表示容量，有效容量，最优容量的概念； 奥卡姆剃刀原则*</h1>
                <blockquote>
                  <p>《深度学习》 5.2 容量、过拟合和欠拟合</p>
                </blockquote>
                <h2 id="过拟合的一些解决方案">过拟合的一些解决方案***</h2>
                <ul>
                  <li>参数范数惩罚（Parameter Norm Penalties）</li>
                  <li>数据增强（Dataset Augmentation）</li>
                  <li>提前终止（Early Stopping）</li>
                  <li>参数绑定与参数共享（Parameter Tying and Parameter Sharing）</li>
                  <li>Bagging 和其他集成方法</li>
                  <li>Dropout</li>
                  <li>批标准化（Batch Normalization）</li>
                </ul>
                <h1>高斯分布的广泛应用的原因**</h1>
                <blockquote>
                  <p>《深度学习》 3.9.3 高斯分布</p>
                </blockquote>
                <h2 id="高斯分布-gaussian-distribution">高斯分布（Gaussian distribution）</h2>
                <p>高斯分布，即正态分布（normal distribution）：</p>
                <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215732.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=N" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=N</a>(x;\mu,\sigma<sup>2)=\sqrt\frac{1}{2\pi\sigma</sup>2}\exp\left&amp;space;(&amp;space;-\frac{1}{2\sigma<sup>2}(x-\mu)</sup>2&amp;space;\right&amp;space;))</p>
                <p>概率密度函数图像：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610131620.png" alt=""></p>
                <p>其中峰的 <code>x</code> 坐标由 <code>µ</code> 给出，峰的宽度受 <code>σ</code> 控制；特别的，当 <code>µ = 0, σ = 1</code>时，称为标准正态分布</p>
                <p>正态分布的均值 <code>E = µ</code>；标准差 <code>std = σ</code>，方差为其平方</p>
                <h2 id="为什么推荐使用高斯分布？">为什么推荐使用高斯分布？</h2>
                <p>当我们由于缺乏关于某个实数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选择，其中有两个原因：</p>
                <ol>
                  <li>我们想要建模的很多分布的真实情况是比较接近正态分布的。<strong>中心极限定理</strong>（central limit theorem）说明很多独立随机变量的和近似服从正态分布。这意味着在实际中，很多复杂系统都可以被成功地建模成正态分布的噪声，即使系统可以被分解成一些更结构化的部分。</li>
                  <li>第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。
                    <blockquote>
                      <p>关于这一点的证明：《深度学习》 ch19.4.2 - 变分推断和变分学习</p>
                    </blockquote>
                  </li>
                </ol>
                <p><strong>多维正态分布</strong></p>
                <p>正态分布可以推广到 n 维空间，这种情况下被称为<strong>多维正态分布</strong>。</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610132602.png" alt=""></p>
                <p>参数 <code>µ</code> 仍然表示分布的均值，只不过现在是一个向量。参数 Σ 给出了分布的协方差矩阵（一个正定对称矩阵）。</p>
                <h1>表示学习、自编码器与深度学习**</h1>
                <p><strong>表示学习</strong>：</p>
                <p>对于许多任务来说，我们很难知道应该提取哪些特征。解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为<strong>表示学习</strong>（representation learning）。学习到的表示往往比手动设计的表示表现得更好。并且它们只需最少的人工干预，就能让AI系统迅速适应新的任务。</p>
                <p><strong>自编码器</strong>：</p>
                <p>表示学习算法的典型例子是 自编码器（autoencoder）。自编码器由一个<strong>编码器</strong>（encoder）函数和一个<strong>解码器</strong>（decoder）函数组合而成。</p>
                <ul>
                  <li>编码器函数将输入数据转换为一种不同的表示;</li>
                  <li>解码器函数则将这个新的表示转换到原来的形式。</li>
                </ul>
                <p>我们期望当输入数据经过编码器和解码器之后<strong>尽可能多地保留信息</strong>，同时希望新的表示有一些好的特性，这也是自编码器的训练目标。</p>
                <p><strong>深度学习</strong>：</p>
                <p>深度学习（deep learning）通过简单的表示来表达复杂的表示，以解决表示学习中的核心问题。</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610194353.png" alt=""></p>
                <p>深度学习模型的示意图</p>
                <p>计算机难以理解原始感观输入数据的含义，如表示为像素值集合的图像，将一组像素映射到对象标识的函数非常复杂。深度学习将所需的复杂映射分解为一系列嵌套的简单映射（每个由模型的不同层描述）来解决这一难题。</p>
                <p>输入展示在<strong>可见层</strong>（visible layer），这样命名的原因是因为它包含我们能观察到的变量。然后是一系列从图像中提取越来越多抽象特征的<strong>隐藏层</strong>（hidden layer），称为“隐藏”的原因是因为它们的值不在数据中给出。</p>
                <p>模型必须确定哪些概念有利于解释观察数据中的关系。这里的图像是每个隐藏单元表示的特征的可视化。给定像素，<strong>第一隐藏层</strong>可以轻易地通过比较相邻像素的亮度来<strong>识别边缘</strong>。有了第一隐藏层描述的边缘，<strong>第二隐藏层</strong>可以容易地<strong>搜索轮廓和角</strong>。给定第二隐藏层中关于角和轮廓的图像描述，<strong>第三隐藏层</strong>可以找到轮廓和角的特定集合来<strong>检测整个特定对象</strong>。最后，根据图像描述中包含的对象部分，可以识别图像中存在的对象。</p>
                <blockquote>
                  <p>实际任务中并不一定具有这么清晰的可解释性，很多时候你并不知道每个隐藏层到底识别出了哪些特征。</p>
                </blockquote>
                <p>学习数据的正确表示的想法是<strong>解释深度学习</strong>的一个视角。</p>
                <blockquote>
                  <p>另一个视角是深度促使计算机学习一个多步骤的计算机程序。——《深度学习》 ch1 - 引言</p>
                  <p>早期的深度学习称为神经网络，因为其主要指导思想来源于生物神经学。从神经网络向深度学习的术语转变也是因为指导思想的改变。</p>
                </blockquote>
                <h1>L1、L2 正则化与 MAP 贝叶斯推断的关系*</h1>
                <blockquote>
                  <p>《深度学习》 5.6.1 最大后验 (MAP) 估计</p>
                </blockquote>
                <p>许多正则化策略可以被解释为 MAP 贝叶斯推断：</p>
                <ul>
                  <li>L2 正则化相当于权重是高斯先验的 MAP 贝叶斯推断</li>
                  <li>对于 L1正则化，用于正则化代价函数的惩罚项与通过 MAP 贝叶斯推断最大化的<strong>对数先验项</strong>是等价的</li>
                </ul>
                <h1>什么是欠约束，为什么大多数的正则化可以使欠约束下的欠定问题在迭代过程中收敛*</h1>
                <h1>为什么考虑在模型训练时对输入 (隐藏单元或权重) 添加方差较小的噪声？*</h1>
                <p>对于某些模型而言，向输入添加方差极小的噪声等价于对权重施加范数惩罚 (Bishop, 1995a,b)。</p>
                <p>在一般情况下，<strong>注入噪声比简单地收缩参数强大</strong>。特别是噪声被添加到<strong>隐藏单元</strong>时会更加强大，<strong>Dropout</strong> 方法正是这种做法的主要发展方向。</p>
                <p>另一种正则化模型的噪声使用方式是将其加到<strong>权重</strong>。这项技术主要用于循环神经网络 (Jim et al., 1996; Graves, 2011)。这可以被解释为关于权重的贝叶斯推断的随机实现。贝叶斯学习过程将权重视为不确定的，并且可以<strong>通过概率分布表示这种不确定性</strong>。向权重添加噪声是反映这种不确定性的一种实用的随机方法。</p>
                <h1>多任务学习、参数绑定和参数共享***</h1>
                <h2 id="多任务学习">多任务学习</h2>
                <blockquote>
                  <p>《深度学习》 7.7 多任务学习</p>
                </blockquote>
                <p>多任务学习 (Caruana, 1993) 是通过<strong>合并多个任务中的样例</strong>（可以视为对参数施加软约束）来提高泛化的一种方式。</p>
                <p>正如额外的训练样本能够将模型参数推向具有更好泛化能力的值一样，当<strong>模型的一部分被多个额外的任务共享</strong>时，这部分将被约束为良好的值（如果共享合理），通常会带来更好的泛化能力。</p>
                <p><strong>多任务学习中一种普遍形式</strong>：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610203703.png" alt=""></p>
                <p>多任务学习在深度学习框架中可以以多种方式进行，该图展示了一种普遍形式：任务共享相同输入但涉及不同语义的输出。</p>
                <p>在该示例中，额外假设顶层隐藏单元 h(1) 和 h(2) 专用于不同的任务——分别预测 y(1) 和 y(2)，而一些<strong>中间层表示</strong> h(shared) 在所有任务之间共享；h(3) 表示无监督学习的情况。</p>
                <p>这里的基本假设是存在解释输入 x 变化的<strong>共同因素池</strong>，而每个任务与这些因素的<strong>子集</strong>相关联。</p>
                <p>该模型通常可以分为两类相关的参数：</p>
                <ol>
                  <li>具体任务的参数（只能从各自任务的样本中实现良好的泛化）</li>
                  <li>所有任务共享的通用参数（从所有任务的汇集数据中获益）——参数共享</li>
                </ol>
                <p>因为<strong>共享参数</strong>，其统计强度可大大提高（共享参数的样本数量相对于单任务模式增加的比例），并能改善泛化和泛化误差的范围 (Baxter, 1995)。</p>
                <p>参数共享仅当不同的任务之间存在某些统计关系的假设是合理（意味着某些参数能通过不同任务共享）时才会发生这种情况</p>
                <h2 id="参数绑定和参数共享">参数绑定和参数共享</h2>
                <blockquote>
                  <p>《深度学习》 7.9 参数绑定和参数共享</p>
                </blockquote>
                <p><strong>参数绑定</strong>：</p>
                <p>有时，我们可能无法准确地知道应该使用什么样的参数，但我们根据相关领域和模型结构方面的知识得知模型参数之间应该存在一些相关性。</p>
                <p>考虑以下情形：我们有两个模型执行相同的分类任务（具有相同类别），但输入分布稍有不同。</p>
                <p>形式地，我们有参数为 w(A) 的模型 A 和参数为 w(B) 的模型 B。这两种模型将输入映射到两个不同但相关的输出： y(A) = f(x;w(A)) 和 y(B) = f(x;w(B))</p>
                <p>可以想象，这些任务会足够相似（或许具有相似的输入和输出分布），因此我们认为模型参数 w(A) 和 w(B) 应彼此靠近。具体来说，我们可以使用以下形式的参数范数惩罚（这里使用的是 L2 惩罚，也可以使用其他选择）：</p>
                <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215812.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=%5COmega&amp;space;" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=\Omega&amp;space;</a>(w<sup>{(A)},w</sup>{(B)})=\left&amp;space;|&amp;space;w<sup>{(A)}-w</sup>{(B)}&amp;space;\right&amp;space;|^2_2)</p>
                <p><strong>参数共享</strong>是这个思路下更流行的做法——强迫部分参数相等</p>
                <p>和正则化参数使其接近（通过范数惩罚）相比，参数共享的一个显著优点是能够“减少内存”——只有参数（唯一一个集合）的子集需要被存储在内存中，特别是在 CNN 中。</p>
                <h1>Dropout 与 Bagging 集成方法的关系，Dropout 带来的意义与其强大的原因***</h1>
                <h2 id="bagging-集成方法">Bagging 集成方法</h2>
                <blockquote>
                  <p>《深度学习》 7.11 Bagging 和其他集成方法</p>
                </blockquote>
                <p><strong>集成方法</strong>：</p>
                <p>其主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为<strong>模型平均</strong>（model averaging）。采用这种策略的技术被称为<strong>集成方法</strong>。</p>
                <p>模型平均（model averaging）<strong>奏效的原因</strong>是不同的模型通常不会在测试集上产生完全相同的误差。平均上， 集成至少与它的任何成员表现得一样好，并且<strong>如果成员的误差是独立的</strong>，集成将显著地比其成员表现得更好。</p>
                <p><strong>Bagging</strong>：</p>
                <p>Bagging（bootstrap aggregating）是通过结合几个模型降低泛化误差的技术 (Breiman, 1994)。</p>
                <p>具体来说，Bagging 涉及构造 k 个<strong>不同的数据集</strong>。每个数据集从原始数据集中<strong>重复采样</strong>构成，和原始数据集具有<strong>相同数量</strong>的样例。这意味着，每个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的例子（更具体的，如果采样所得的训练集与原始数据集大小相同，那所得数据集中大概有原始数据集 <strong>2/3</strong> 的实例）</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611150734.png" alt=""></p>
                <p><strong>图像说明</strong>：该图描述了 Bagging 如何工作。假设我们在上述数据集（包含一个 8、一个 6 和一个 9）上<strong>训练数字 8</strong> 的检测器。假设我们制作了两个不同的重采样数据集。 Bagging 训练程序通过有放回采样构建这些数据集。第一个数据集忽略 9 并重复 8。在这个数据集上，检测器得知数字<strong>顶部有一个环就对应于一个 8</strong>。第二个数据集中，我们忽略 6 并重复 9。在这种情况下，检测器得知数字<strong>底部有一个环就对应于一个 8</strong>。这些单独的分类规则中的每一个都是不可靠的，但如果我们平均它们的输出，就能得到鲁棒的检测器，<strong>只有当 8 的两个环都存在时才能实现最大置信度</strong>。</p>
                <h2 id="dropout">Dropout</h2>
                <p><strong>Dropout 的意义与强大的原因</strong>：</p>
                <p>简单来说，Dropout (Srivastava et al., 2014) 通过<strong>参数共享</strong>提供了一种廉价的 <strong>Bagging</strong> 集成近似，能够训练和评估<strong>指数级数量</strong>的神经网络。</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611152559.png" alt=""></p>
                <p>Dropout 训练的集成包括所有从基础网络除去部分单元后形成的子网络。具体而言，只需将一些单元的<strong>输出乘零</strong>就能有效地删除一个单元。</p>
                <p>通常，<strong>隐藏层</strong>的采样概率为 0.5，<strong>输入</strong>的采样概率为 0.8；超参数也可以采样，但其采样概率一般为 1</p>
                <p><strong>Dropout与Bagging的不同点</strong>：</p>
                <ul>
                  <li>在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型<strong>共享参数</strong>，其中每个模型继承父神经网络参数的不同子集。</li>
                  <li>在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。</li>
                </ul>
                <p><strong>权重比例推断规则</strong>：</p>
                <p>简单来说，如果我们使用 0.5 的包含概率（keep prob），权重比例规则相当于在训练结束后<strong>将权重除 2</strong>，然后像平常一样使用模型；等价的，另一种方法是在训练期间将单元的状态乘 2。</p>
                <p>无论哪种方式，我们的目标是确保在测试时一个单元的期望总输入与在训练时该单元的期望总输入是大致相同的（即使近半单位在训练时丢失）。</p>
                <h1>批梯度下降法（Batch SGD）更新过程中，批的大小会带来怎样的影响**</h1>
                <p>特别说明：本书中，“<strong>批量</strong>”指使用使用全部训练集；“<strong>小批量</strong>”才用来描述小批量随机梯度下降算法中用到的小批量样本；而<strong>随机梯度下降</strong>（SGD）通常指每次只使用单个样本</p>
                <p><strong>批的大小</strong>通常由以下几个因素决定：</p>
                <ul>
                  <li><strong>较大的批能得到更精确的梯度估计</strong>，但回报是小于线性的。</li>
                  <li><strong>较小的批能带来更好的泛化误差</strong>，泛化误差通常在批大小为 1 时最好。但是，因为梯度估计的高方差，小批量训练需要<strong>较小的学习率</strong>以保持稳定性，这意味着<strong>更长的训练时间</strong>。
                    <blockquote>
                      <p>可能是由于小批量在学习过程中加入了噪声，它们会有一些正则化效果 (Wilson and Martinez, 2003)</p>
                    </blockquote>
                  </li>
                  <li><strong>内存消耗和批的大小成正比</strong>，如果批量处理中的所有样本可以并行地处理（通常确是如此）。</li>
                  <li>在某些硬件上使用特定大小可以减少运行时间。尤其是在使用 GPU 时，通常使用 <strong>2 的幂数</strong>作为批量大小可以获得更少的运行时间。一般，2 的幂数的<strong>取值范围是 32 到 256</strong>，16 有时在尝试大模型时使用。</li>
                  <li>小批量更容易利用<strong>多核架构</strong>，但是太小的批并不会减少计算时间，这促使我们使用一些<strong>绝对最小批量</strong></li>
                </ul>
                <p>很多机器学习上的优化问题都可以分解成并行地计算不同样本上单独的更新。换言之，我们在计算小批量样本 X 上最小化 J(X) 的更新时，同时可以计算其他小批量样本上的更新。</p>
                <blockquote>
                  <p>异步并行分布式方法 -&gt; 《深度学习》 12.1.3 大规模的分布式实现</p>
                </blockquote>
                <h1>如何避免深度学习中的病态，鞍点，梯度爆炸，梯度弥散？***</h1>
                <h2 id="病态-ill-conditioning">病态（ill-conditioning）</h2>
                <p><strong>什么是病态？</strong></p>
                <blockquote>
                  <p><a href="https://blog.csdn.net/foolsnowman/article/details/51614862" target="_blank" rel="noopener">神经网络优化中的病态问题</a> - CSDN博客</p>
                  <p><a href="https://www.zhihu.com/question/56977045" target="_blank" rel="noopener">什么是 ill-conditioning 对SGD有什么影响？</a> - 知乎</p>
                </blockquote>
                <p>简单来说，深度学习中的病态问题指的就是学习/优化变的困难，需要更多的迭代次数才能达到相同的精度。</p>
                <blockquote>
                  <p>病态问题普遍存在于数值优化、凸优化或其他形式的优化中 -&gt; ch4.3.1 - 梯度之上：Jacobian 和 Hessian 矩阵</p>
                </blockquote>
                <p>更具体的，导致病态的原因是问题的<strong>条件数</strong>（condition number）非常大，其中<code>条件数 = 函数梯度最大变化速度 / 梯度最小变化速度</code>（对于二阶可导函数，条件数的严格定义是：Hessian矩阵最大特征值的上界 / 最小特征值的下界）。</p>
                <p><strong>条件数大</strong>意味着目标函数在有的地方（或有的方向）变化很快、有的地方很慢，比较不规律，从而很难用当前的局部信息（梯度）去比较准确地预测最优点所在的位置，只能一步步缓慢的逼近最优点，从而优化时需要更多的迭代次数。</p>
                <p><strong>如何避免病态？</strong></p>
                <p>知道了什么是病态，那么所有有利于<strong>加速训练</strong>的方法都属于在避免病态，其中最主要的还是优化算法。</p>
                <p>深度学习主要使用的优化算法是<strong>梯度下降</strong>，所以避免病态问题的关键是改进梯度下降算法：</p>
                <ul>
                  <li>随机梯度下降（SGD）、批量随机梯度下降</li>
                  <li>动态的学习率</li>
                  <li><strong>带动量的 SGD</strong></li>
                </ul>
                <blockquote>
                  <p><a href="#28-sgd-%E4%BB%A5%E5%8F%8A%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E5%B8%A6%E5%8A%A8%E9%87%8F%E7%9A%84-sgd-%E5%AF%B9%E4%BA%8E-hessian-%E7%9F%A9%E9%98%B5%E7%97%85%E6%80%81%E6%9D%A1%E4%BB%B6%E5%8F%8A%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%B7%AE%E7%9A%84%E5%BD%B1%E5%93%8D">28. SGD 以及学习率的选择方法，带动量的 SGD 对于 Hessian 矩阵病态条件及随机梯度方差的影响***</a></p>
                </blockquote>
                <h2 id="鞍点-saddle-point">鞍点（saddle point）</h2>
                <p>对于很多高维非凸函数（神经网络）而言，局部极小值/极大值事实上都<strong>远少于</strong>另一类梯度为零的点：鞍点</p>
                <p><strong>什么是鞍点？</strong></p>
                <p>二维和三维中的鞍点：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611171103.png" alt="">
                  <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611171213.png" alt=""></p>
                <p><strong>鞍点激增对于训练算法来说有哪些影响？</strong></p>
                <p>对于只使用梯度信息的一阶优化算法（随机梯度下降）而言，目前情况还不清楚。不过，虽然鞍点附近的梯度通常会非常小，但是 Goodfellow et al. (2015) 认为连续的梯度下降<strong>会逃离而不是吸引到鞍点</strong>。</p>
                <p>对于牛顿法（二阶梯度）而言，鞍点问题会比较明显。不过神经网络中很少使用二阶梯度进行优化。</p>
                <h2 id="长期依赖与梯度爆炸-消失">长期依赖与梯度爆炸、消失</h2>
                <p>当计算图变得很深时（循环神经网络），神经网络优化算法会面临的另外一个难题就是<strong>长期依赖</strong>，由于变深的结构使模型丧失了学习到先前信息的能力，让优化变得极其困难；具体来说，就是会出现<strong>梯度消失</strong>和<strong>梯度爆炸</strong>问题。</p>
                <p><strong>如何避免梯度爆炸？</strong></p>
                <p><strong>如何缓解梯度消失？</strong></p>
                <p>梯度截断有助于处理爆炸的梯度，但它无助于梯度消失。</p>
                <p>一个想法是：在展开循环架构的计算图中，沿着与弧边相关联的梯度乘积接近 1 的部分创建路径——LSTM, GRU 等<strong>门控机制</strong>正是该想法的实现。</p>
                <blockquote>
                  <p>《深度学习》 10.10 长短期记忆和其他门控 RNN</p>
                </blockquote>
                <p>另一个想法是：正则化或约束参数，以引导“信息流”；或者说，希望<strong>梯度向量</strong>在反向传播时能维持其幅度。形式上，我们要使</p>
                <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180611194550.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=</a>(\nabla_{h<sup>{(t)}}L)\frac{\partial&amp;space;h</sup>{(t)}}{\partial&amp;space;h^{(t-1)}})</p>
                <p>与梯度向量</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cnabla_%7Bh%5E%7B(t)%7D%7DL" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180611194704.png" alt=""></a></p>
                <p>一样大。</p>
                <p><strong>一些具体措施</strong>：</p>
                <ol>
                  <li>
                    <p>批标准化（Batch Normalization）</p>
                    <blockquote>
                      <p><a href="#31-%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96batch-normalization%E7%9A%84%E6%84%8F%E4%B9%89">31. 批标准化（Batch Normalization）的意义**</a></p>
                    </blockquote>
                  </li>
                  <li>
                    <p>在这个目标下， Pascanu et al. (2013a) 提出了以下正则项：</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611194754.png" alt=""></p>
                    <p>这种方法的一个主要弱点是，在处理数据冗余的任务时如语言模型，它并不像 LSTM 一样有效。</p>
                  </li>
                </ol>
                <h1>SGD 以及学习率的选择方法、带动量的 SGD***</h1>
                <h2 id="批-随机梯度下降-sgd-与学习率">（批）随机梯度下降（SGD）与学习率</h2>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611195027.png" alt=""></p>
                <p>SGD 及相关的小批量亦或更广义的基于梯度优化的在线学习算法，一个重要的性质是每一步更新的计算时间不依赖训练样本数目的多寡。因为它每个 step 的样本数是固定的。</p>
                <p>所以即使训练样本数目非常大时，它们也能收敛。对于足够大的数据集， SGD 可能会在处理整个训练集之前就收敛到最终测试集误差的某个固定容差范围内。</p>
                <p><strong>SGD 与学习率</strong></p>
                <p>SGD 算法中的一个关键参数是学习率。在实践中，有必要<strong>随着时间的推移逐渐降低学习率</strong>。</p>
                <p>实践中，一般会线性衰减学习率直到第 τ 次迭代：</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cepsilon_k=(1-%5Calpha)%5Cepsilon_0+%5Calpha%5Cepsilon_%5Ctau" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180611195622.png" alt=""></a></p>
                <p>其中 α=k/τ。在 τ 步迭代之后，一般使 ϵ 保持常数。</p>
                <p>使用线性策略时，需要选择的参数有 ϵ_0, ϵ_τ 和 τ</p>
                <ul>
                  <li>通常 τ 被设为需要反复遍历训练集几百次的迭代次数（？）</li>
                  <li>通常 ϵ_τ 应设为大约 ϵ_0 的 1%</li>
                </ul>
                <p><strong>如何设置 ϵ_0？</strong></p>
                <p>若 ϵ_0 太大，学习曲线将会剧烈振荡，代价函数值通常会明显增加。温和的振荡是良好的，容易在训练随机代价函数（例如使用 Dropout 的代价函数）时出现。<strong>如果学习率太小</strong>，那么学习过程会很缓慢。<strong>如果初始学习率太低</strong>，那么学习可能会卡在一个相当高的代价值。通常，就总训练时间和最终代价值而言，最优初始学习率会高于大约迭代 100 次左右后达到最佳效果的学习率。<strong>因此，通常最好是检测最早的几轮迭代，选择一个比在效果上表现最佳的学习率更大的学习率，但又不能太大导致严重的震荡。</strong></p>
                <p>学习率可通过试验和误差来选取，通常最好的选择方法是<strong>监测目标函数值随时间变化的学习曲线</strong>——与<strong>其说是科学，这更像是一门艺术</strong>。</p>
                <h2 id="带动量的-sgd">带动量的 SGD</h2>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611203427.png" alt=""></p>
                <p>从形式上看， 动量算法引入了变量 v 充当速度角色——它代表参数在参数空间移动的方向和速率。速度被设为负梯度的指数衰减平均。</p>
                <p>之前，步长只是梯度范数乘以学习率。现在，步长取决于梯度序列的大小和排列。当许多连续的梯度指向相同的方向时，步长最大。如果动量算法总是观测到梯度 g，那么它会在方向 −g 上不停加速，直到达到最终速度，其中步长大小为</p>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cfrac%7B%5Cepsilon%5Cleft%7Cg%5Cright%7C%7D%7B1-%5Calpha%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180611210928.png" alt=""></a></p>
                <p>在实践中，α 的一般取值为 0.5, 0.9 和 0.99，分别对应<strong>最大速度</strong> 2倍，10倍和100倍于普通的 SGD 算法。和学习率一样， α 也应该随着时间不断调整（变大），但没有收缩 ϵ 重要。</p>
                <p><strong>为什么要加入动量？</strong></p>
                <p>加入的动量主要目的是解决两个问题： Hessian 矩阵的<strong>病态</strong>条件和<strong>随机梯度的方差</strong>。简单来说，就是为了加速学习。</p>
                <p>虽然动量的加入有助于缓解这些问题，但其代价是引入了另一个超参数。</p>
                <p>带有动量的 SGD（左/上） 和不带动量的 SGD（右/下）：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611204215.png" alt="">
                  <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611204503.png" alt=""></p>
                <p>此图说明动量如何克服病态的问题：等高线描绘了一个二次损失函数（具有病态条件的 Hessian 矩阵）。一个病态条件的二次目标函数看起来像一个长而窄的山谷或具有陡峭边的峡谷。带动量的 SGD 能比较正确地纵向穿过峡谷；而普通的梯度步骤则会浪费时间在峡谷的窄轴上来回移动，因为梯度下降无法利用包含在 Hessian 矩阵中的曲率信息。</p>
                <p><strong>Nesterov 动量</strong></p>
                <p>受 Nesterov 加速梯度算法 (Nesterov, 1983, 2004) 启发， Sutskever et al. (2013) 提出了动量算法的一个变种。其更新规则如下：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611211649.png" alt=""></p>
                <p>其中参数 α 和 ϵ 发挥了和标准动量方法中类似的作用。Nesterov 动量和标准动量之间的<strong>区别体现在梯度计算</strong>上。下面是完整的 Nesterov 动量算法：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611211753.png" alt=""></p>
                <p>Nesterov 动量中，梯度计算在施加当前速度之后。因此，Nesterov 动量可以解释为往标准动量方法中添加了一个校正因子。</p>
                <p>在凸批量梯度的情况下， Nesterov 动量将额外误差收敛率从 O(1/k) 改进到 O(1/k^2)。可惜，在随机梯度的情况下， Nesterov 动量没有改进收敛率。</p>
                <h1>自适应学习率算法：AdaGrad、RMSProp、Adam 等***</h1>
                <p>Delta-bar-delta (Jacobs, 1988) 是一个早期的自适应学习率算法。该方法基于一个很简单的想法，如果损失对于某个给定模型参数的偏导保持相同的符号，那么学习率应该增加。如果对于该参数的偏导变化了符号，那么学习率应减小。当然，这种方法只能应用于全批量优化中（？）。</p>
                <p>最近，提出了一些增量（或者基于小批量）的算法来自适应模型参数的学习率。</p>
                <h2 id="adagrad">AdaGrad</h2>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611214508.png" alt=""></p>
                <p>AdaGrad 会独立地适应所有模型参数的学习率。具体来说，就是缩放每个参数反比于其所有梯度历史平方值总和的平方根 (Duchi et al., 2011)。效果上具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。</p>
                <p><strong>不过</strong>，对于训练深度神经网络模型而言，<strong>从训练开始时</strong>就积累梯度平方会导致有效学习率过早和过量的减小。AdaGrad 在某些深度学习模型上效果不错，但不是全部。</p>
                <h2 id="rmsprop">RMSProp</h2>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611215422.png" alt=""></p>
                <p>RMSProp 修改自 AdaGrad。AdaGrad 旨在应用于<strong>凸问题</strong>时快速收敛，而 RMSProp 在<strong>非凸</strong>设定下效果更好，改变梯度积累为指数加权的移动平均。</p>
                <p>RMSProp 使用指数衰减平均以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛，它就像一个初始化于该碗状结构的 AdaGrad 算法实例。</p>
                <p>相比于 AdaGrad，使用移动平均引入了一个<strong>新的超参数 ρ</strong>，用来控制移动平均的长度范围。</p>
                <p>经验上， RMSProp 已被证明是一种有效且实用的深度神经网络优化算法。目前它是深度学习从业者经常采用的优化方法之一。</p>
                <p><strong>结合 Nesterov 动量的 RMSProp</strong></p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611215923.png" alt=""></p>
                <h2 id="adam">Adam</h2>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611220109.png" alt=""></p>
                <p>Adam (Kingma and Ba, 2014) 是另一种学习率自适应的优化算法。</p>
                <p>首先，在 Adam 中，动量直接并入了梯度一阶矩（指数加权）的估计。将动量加入 RMSProp 最直观的方法是将动量应用于缩放后的梯度。但是结合缩放的动量使用没有明确的理论动机。其次， Adam 包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩的估计。RMSProp 也采用了（非中心的）二阶矩估计，然而缺失了修正因子。因此，不像 Adam， RMSProp 二阶矩估计可能在训练初期有很高的偏置。Adam 通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。</p>
                <p><strong>如何选择自适应学习率算法？</strong></p>
                <p>目前在这一点上没有明确的共识。选择哪一个算法似乎主要取决于使用者对算法的熟悉程度（以便调节超参数）。</p>
                <p>如果不知道选哪个，就用 AdamSGD 吧。</p>
                <h1>基于二阶梯度的优化方法：牛顿法、共轭梯度、BFGS 等的做法*</h1>
                <h1>批标准化（Batch Normalization）的意义**</h1>
                <p>批标准化（Batch Normalization, BN, Ioffe and Szegedy, 2015）是为了克服神经网络<strong>层数加深导致难以训练</strong>而出现的一个算法。</p>
                <p>说到底，BN 还是为了解决<strong>梯度消失/梯度爆炸</strong>问题，特别是梯度消失。</p>
                <p><strong>BN 算法</strong>：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612104740.png" alt=""></p>
                <p>BN 算法需要学习两个参数 γ 和 β.</p>
                <blockquote>
                  <p>Ioffe and Szegedy, 2015, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</p>
                </blockquote>
                <p><strong>批标准化为什么有用？</strong></p>
                <blockquote>
                  <p><a href="https://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="noopener">深度学习（二十九）Batch Normalization 学习笔记</a> - CSDN博客</p>
                  <p><a href="https://www.zhihu.com/question/38102762/answer/85238569" target="_blank" rel="noopener">深度学习中 Batch Normalization为什么效果好？</a> - 知乎</p>
                </blockquote>
                <h1>神经网络中的卷积，以及卷积的动机：稀疏连接、参数共享、等变表示（平移不变性）***</h1>
                <p><strong>神经网络中的卷积</strong>：</p>
                <p>当我们在神经网络中提到卷积时，通常是指由<strong>多个并行卷积</strong>组成的运算。一般而言，每个核只用于提取一种类型的特征，尽管它作用在多个空间位置上。而我们通常希望网络的每一层能够在多个位置提取多种类型的特征。</p>
                <p>卷积的一些基本概念：通道（channel）、卷积核（kernel、filter）、步幅（stride，下采样）、填充（padding）</p>
                <p><strong>为什么使用卷积？（卷积的动机）</strong></p>
                <p>卷积运算通过三个重要的思想来帮助改进机器学习系统：<strong>稀疏交互</strong>（sparseinteractions）、<strong>参数共享</strong>（parameter sharing）、<strong>等变表示</strong>（equivariant representations）。</p>
                <h2 id="稀疏连接-sparse-connectivity">稀疏连接（sparse connectivity）</h2>
                <p>稀疏连接，也称稀疏交互、稀疏权重。</p>
                <p>传统的神经网络中每一个输出单元会与每一个输入单元都产生交互。卷积网络改进了这一点，使具有稀疏交互的特征。CNN 通过使核（kernel、filter）的大小远小于输入的大小来达到的这个目的。</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612120851.png" alt=""></p>
                <p>举个例子，当处理一张图像时，输入的图像可能包含成千上万个像素点，但是我们可以通过只占用几十到上百个像素点的核来<strong>检测一些小的、有意义的特征</strong>，例如图像的边缘。</p>
                <p><strong>稀疏交互的好处</strong>：</p>
                <ul>
                  <li>提高了模型的统计效率：原本一幅图像只能提供少量特征，现在每一块像素区域都可以提供一部分特征</li>
                  <li>减少了模型的存储需求和计算量，因为参数更少</li>
                </ul>
                <p>如果有 m 个输入和 n 个输出，那么矩阵乘法需要 <code>m × n</code> 个参数并且相应算法的时间复杂度为 <code>O(m × n)</code>；如果限制每一个输出拥有的连接数为 k，那么稀疏的连接方法只需要 <code>k × n</code> 个参数以及 <code>O(k × n)</code> 的运行时间。而在实际应用中，<strong>k 要比 m 小几个数量级</strong>。</p>
                <p>虽然看似减少了隐藏单元之间的交互，但实际上处在深层的单元可以间接地连接到全部或者大部分输入。</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612121301.png" alt=""></p>
                <h2 id="参数共享-parameter-sharing">参数共享（parameter sharing）</h2>
                <p>参数共享是指在一个模型的多个函数中使用相同的参数。作为参数共享的同义词，我们可以说 一个网络含有
                  <strong>绑定的权重</strong>（tied weights）</p>
                <p>在传统的神经网络中，当计算一层的输出时，权重矩阵的每一个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。</p>
                <p>考虑一个具体的例子——<strong>边缘检测</strong>——来体会稀疏连接+参数共享带来的效率提升：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612154333.png" alt=""></p>
                <p>两个图像的高度均为 280 个像素。输入图像的宽度为 320 个像素，而输出图像的宽度为 319 个像素（padding=‘VALID’）。对于边缘检测任务而言，只需要一个包含<strong>两个元素的卷积核</strong>就能完成；而为了用矩阵乘法描述相同的变换，需要一个包含 320 × 280 × 319 × 280 ≈ 80亿个元素的矩阵（40亿倍）。</p>
                <p>同样，使用卷积只需要 319 × 280 × 3 = 267,960 次浮点运算（每个输出像素需要两次乘法和一次加法）；而直接运行矩阵乘法的算法将执行超过 160 亿次浮点运算（60000倍）</p>
                <h2 id="平移等变-不变性-translation-invariant">平移等变|不变性（translation invariant）</h2>
                <p>（局部）平移不变性是一个很有用的性质，尤其是当我们关心某个特征<strong>是否出现</strong>而不关心它出现的具体位置时。</p>
                <p><strong>参数共享</strong>（和池化）使卷积神经网络具有一定的<strong>平移不变性</strong>。这就意味着即使图像经历了一个小的平移，依然会产生相同的特征。例如，分类一个 MNIST 数据集的数字，对它进行任意方向的<strong>平移</strong>（不是旋转），无论最终的位置在哪里，都能正确分类。</p>
                <blockquote>
                  <p>池化操作也能够帮助加强网络的平移不变性</p>
                </blockquote>
                <p><strong>什么是等变性？</strong></p>
                <ul>
                  <li>如果一个函数满足<strong>输入改变，输出也以同样的方式改变</strong>这一性质，我们就说它是等变 (equivariant) 的。</li>
                  <li>对于卷积来说，如果令 g 是输入的任意平移函数，那么卷积函数对于 g 具有等变性。</li>
                </ul>
                <p>当处理<strong>时间序列数据</strong>时，这意味着通过卷积可以得到一个由输入中出现不同特征的时刻所组成的时间轴。如果我们把输入中的一个事件向后延时，在输出中仍然会有完全相同的表示，只是时间延后了。</p>
                <p>图像与之类似，卷积产生了一个 2 维映射来表明某些特征在输入中出现的位置。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。</p>
                <p>卷积对其他的一些变换并不是天然等变的，例如对于图像的<strong>放缩</strong>或者<strong>旋转</strong>变换，需要其他的一些机制来处理这些变换。</p>
                <blockquote>
                  <p><a href="http://ufldl.stanford.edu/wiki/index.php/%E6%B1%A0%E5%8C%96#.E6.B1.A0.E5.8C.96.E7.9A.84.E4.B8.8D.E5.8F.98.E6.80.A7" target="_blank" rel="noopener">池化的不变性</a> - Ufldl</p>
                </blockquote>
                <h1>卷积中不同零填充的影响**</h1>
                <p>在任何卷积网络的实现中都有一个重要性质，那就是能够隐含地<strong>对输入用零进行填充</strong>使得它加宽。如果没有这个性质，会极大得限制网络的表示能力。</p>
                <p>三种零填充设定，其中 <code>m</code> 和 <code>k</code> 分别为图像的宽度和卷积核的宽度（高度类似）：</p>
                <ol>
                  <li>有效（valid）卷积——不使用零填充，卷积核只允许访问那些图像中能够<strong>完全包含整个核</strong>的位置，输出的宽度为 <code>m − k + 1</code>.
                    <ul>
                      <li>在这种情况下，输出的所有像素都是输入中相同数量像素的函数，这使得输出像素的表示更加规范。</li>
                      <li>然而，输出的大小在每一层都会缩减，这限制了网络中能够包含的卷积层的层数。（一般情况下，影响不大，除非是上百层的网络）</li>
                    </ul>
                  </li>
                  <li>相同（same）卷积——只进行足够的零填充来<strong>保持输出和输入具有相同的大小</strong>，即输出的宽度为 <code>m</code>.
                    <ul>
                      <li>在这种情况下，只要硬件支持，网络就能包含任意多的卷积层。</li>
                      <li>然而，输入像素中靠近边界的部分相比于中间部分对于输出像素的影响更小。这可能会导致边界像素存在一定程度的欠表示。</li>
                    </ul>
                  </li>
                  <li>全（full）卷积——进行足够多的零填充使得每个像素都能被访问 k 次（非全卷积只有中间的像素能被访问 k 次），最终输出图像的宽度为 <code>m + k − 1</code>.
                    <ul>
                      <li>因为 same 卷积可能导致边界像素欠表示，从而出现了 Full 卷积；</li>
                      <li>但是在这种情况下，输出像素中靠近边界的部分相比于中间部分是更少像素的函数。这将导致学得的卷积核不能再所有所有位置表现一致。</li>
                      <li>事实上，很少使用 Full 卷积</li>
                    </ul>
                    <blockquote>
                      <p>注意：如果以“全卷积”作为关键词搜索，返回的是一个称为 FCN（Fully Convolutional Networks）的卷积结构，而不是这里描述的填充方式。</p>
                    </blockquote>
                  </li>
                </ol>
                <p>通常<strong>零填充的最优数量</strong>（对于测试集的分类正确率）处于 “有效卷积”和 “相同卷积” 之间。</p>
                <h1>基本卷积的变体：反卷积、空洞卷积***</h1>
                <p>原书中也描述一些基本卷积的变体：局部卷积、平铺卷积；</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612213144.png" alt=""></p>
                <blockquote>
                  <p>从上到下一次为局部卷积、平铺卷积和标准卷积；</p>
                  <p>《深度学习》 9.5 基本卷积函数的变体</p>
                </blockquote>
                <p>不过这跟我想的“变体”不太一样（百度都搜不到这两种卷积），下面介绍的是一些我认识中比较流行的卷积变体：</p>
                <h2 id="转置卷积-反卷积-transposed-convolution">转置卷积|反卷积（Transposed convolution）</h2>
                <p><img src="http://www.meng.uno/images/assets/conv_no_padding_no_strides_transposed.gif" alt=""></p>
                <blockquote>
                  <p>No padding, no strides, transposed</p>
                </blockquote>
                <blockquote>
                  <p><a href="https://www.zhihu.com/question/43609045" target="_blank" rel="noopener">如何理解深度学习中的deconvolution networks？</a> - 知乎</p>
                </blockquote>
                <h2 id="空洞卷积-扩张卷积-dilated-convolution">空洞卷积|扩张卷积（Dilated convolution）</h2>
                <p><img src="http://www.meng.uno/images/assets/conv_dilation.gif" alt=""></p>
                <blockquote>
                  <p>No padding, no stride, dilation</p>
                </blockquote>
                <blockquote>
                  <p><a href="https://www.zhihu.com/question/54149221" target="_blank" rel="noopener">如何理解空洞卷积（dilated convolution）？</a> - 知乎</p>
                </blockquote>
                <blockquote>
                  <p>卷积、转置卷积、空洞卷积动图演示：vdumoulin/<a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="noopener">conv_arithmetic</a>: A technical report on convolution arithmetic in the context of deep learning</p>
                </blockquote>
                <h1>池化、池化（Pooling）的作用***</h1>
                <blockquote>
                  <p>《深度学习》 9.3 池化</p>
                </blockquote>
                <p>一次典型的卷积包含三层：第一层并行地计算多个卷积产生一组线性激活响应；第二层中每一个线性激活响应将会通过一个非线性的激活函数；第三层使用<strong>池化函数</strong>（pooling function）来进一步调整这一层的输出。</p>
                <figure class="highlight plain">
                  <table>
                    <tr>
                      <td class="gutter">
                        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                      </td>
                      <td class="code">
                        <pre><span class="line"># Keras</span><br><span class="line">from keras.layers import Input, Conv2D, Activation, MaxPooling2D</span><br><span class="line"></span><br><span class="line">net = Input([in_w, in_h, input_dim])</span><br><span class="line">net = Conv2D(output_dim, kernel_size=(3, 3))(net)</span><br><span class="line">net = Activation(&apos;relu&apos;)(net)</span><br><span class="line">net = MaxPooling2D(pool_size=(2, 2))(net)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">卷积层中，一般 strides=1, padding=&apos;valid&apos;</span><br><span class="line">池化层中，一般 strides=pool_size, padding=&apos;valid&apos;</span><br><span class="line">&quot;&quot;&quot;</span><br></pre>
                      </td>
                    </tr>
                  </table>
                </figure>
                <p>池化函数使用某一位置的相邻输出的<strong>总体统计特征</strong>来代替网络在该位置的输出。</p>
                <p>常见的池化函数：</p>
                <ul>
                  <li>*最大池化（Max pooling）</li>
                  <li>*平均值池化（Mean pooling）</li>
                  <li>L2 范数</li>
                  <li>基于中心像素距离的加权平均</li>
                </ul>
                <p>池化操作有助于卷积网络的平移不变性</p>
                <p><strong>使用池化可以看作是增加了一个无限强的先验</strong>：这一层学得的函数必须具有对少量平移的不变性。当这个假设成立时，池化可以极大地提高网络的统计效率。</p>
                <p>（最大）池化对平移是天然不变的，但池化也能用于学习其他不变性：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613101802.png" alt=""></p>
                <p>这三个过滤器都旨在检测手写的数字 5。每个过滤器尝试匹配稍微<strong>不同方向</strong>的 5。当输入中出现 5 时，无论哪个探测单元被激活，最大池化单元都将产生较大的响应。</p>
                <p>这种多通道方法只在学习其他变换时是必要的。这个原则在 maxout 网络 (Goodfellow et al., 2013b) 和其他卷积网络中更有影响。</p>
                <p>池化综合了区域内的 k 个像素的统计特征而不是单个像素，这种方法提高了网络的计算效率，因为下一层少了约 k 倍的输入。</p>
                <p>在很多任务中，池化还有助于对于处理不同大小的输入：例如我们想对不同大小的图像进行分类时，分类层的输入必须是固定的大小，而这通常通过调整池化区域的偏置大小来实现。</p>
                <p>其他参考：</p>
                <ul>
                  <li>一些理论工作对于在不同情况下应当使用哪种池化函数给出了一些指导 (Boureau et al., 2010)</li>
                  <li>将特征一起动态地池化：对于感兴趣特征的位置运行聚类算法 (Boureau et al., 2011)、先学习一个单独的池化结构，再应用到全部的图像中 (Jia et al., 2012)</li>
                  <li>《深度学习》 20.6 卷积玻尔兹曼机、20.10.6 卷积生成网络</li>
                </ul>
                <h1>卷积与池化的意义、影响（作为一种无限强的先验）**</h1>
                <p>一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值，无论数据对于这些参数的值给出了多大的支持。</p>
                <p>如果把卷积网络类比成全连接网络，那么对于这个全连接网络的权重有一个无限强的先验：隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动；同时要求除了那些处在“感受野”内的权重以外，其余的权重都为零。</p>
                <p>类似的，使用<strong>池化</strong>也是一个无限强的先验：每一个单元都具有对少量平移的不变性。</p>
                <p><strong>卷积与池化作为一种无限强先验的影响</strong>：</p>
                <ol>
                  <li>卷积和池化可能导致<strong>欠拟合</strong>
                    <ul>
                      <li>与任何其他先验类似，卷积和池化只有当先验的假设合理且正确时才有用。</li>
                      <li>如果一项任务涉及到要<strong>对输入中相隔较远的信息进行合并</strong>时，那么卷积所利用的先验可能就不正确了。</li>
                      <li>如果一项任务依赖于保存<strong>精确的空间信息</strong>，那么在所有的特征上使用池化将会增大训练误差。</li>
                    </ul>
                    <blockquote>
                      <p>因此，一些卷积网络结构 (Szegedy et al., 2014a) 为了既获得具有较高不变性的特征又获得当平移不变性不合理时不会导致欠拟合的特征，被设计成在一些通道上使用池化而在另一些通道上不使用。</p>
                    </blockquote>
                  </li>
                  <li>当我们比较卷积模型的统计学习表现时，只能以基准中的其他卷积模型作为比较的对象</li>
                </ol>
                <h1>RNN 的几种基本设计模式</h1>
                <p>循环神经网络中一些重要的设计模式包括以下几种：</p>
                <ol>
                  <li>
                    <p>（*）每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613150111.png" alt=""></p>
                    <ul>
                      <li>将 x 值的输入序列映射到输出值 o 的对应序列</li>
                      <li>损失 L 衡量每个 o 与相应的训练目标 y 的距离</li>
                      <li>损失 L 内部计算 y^ = softmax(o)，并将其与目标 y 比较</li>
                      <li>输入 x 到隐藏 h 的连接由权重矩阵 U 参数化</li>
                      <li>隐藏 h(t-1) 到隐藏 h(t) 的循环连接由权重矩阵 W 参数化</li>
                      <li>隐藏到输出的连接由权重矩阵 V 参数化</li>
                    </ul>
                  </li>
                  <li>
                    <p>每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613150711.png" alt=""></p>
                    <ul>
                      <li>此类 RNN 的唯一循环是从输出 o 到隐藏层 h 的反馈连接</li>
                      <li>表示能力弱于 RNN_1，单更容易训练</li>
                    </ul>
                  </li>
                  <li>
                    <p>隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613151014.png" alt=""></p>
                    <p>这样的网络可以用于概括序列并产生用于进一步处理的固定大小的表示</p>
                  </li>
                </ol>
                <p>一般所说的 RNN（循环神经网络）指的是<strong>第一种</strong>设计模式</p>
                <p>这些循环网络都将一个输入序列映射到<strong>相同长度</strong>的输出序列</p>
                <h1>RNN 更新方程（前向传播公式），包括 LSTM、GRU 等***</h1>
                <blockquote>
                  <p><a href="https://blog.csdn.net/zhangxb35/article/details/70060295" target="_blank" rel="noopener">RNN, LSTM, GRU 公式总结</a> - CSDN博客</p>
                </blockquote>
                <p><strong>基本 RNN</strong></p>
                <blockquote>
                  <p><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks" target="_blank" rel="noopener">Recurrent neural network</a> - Wikipedia</p>
                </blockquote>
                <p>根据隐层 h(t) 接受的是上时刻的隐层 h(t−1) 还是上时刻的输出 y(t−1)，分为两种 RNN：</p>
                <ul>
                  <li>
                    <p>Elman RNN</p>
                    <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613164438.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=%5Cbegin%7Baligned%7D&amp;space;h" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&amp;space;h</a><sup>{(t)}&amp;=\tanh\left(&amp;space;W_hx</sup>{(t)}+U_hh<sup>{(t-1)}&amp;plus;b_h&amp;space;\right)\&amp;space;y</sup>{(t)}&amp;={\rm&amp;space;softmax}\left(&amp;space;W_yh^{(t)}+b_y&amp;space;\right)\&amp;space;\end{aligned})</p>
                  </li>
                  <li>
                    <p>Jordan RNN</p>
                    <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613164637.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=%7B%5Cdisplaystyle&amp;space;%7B%5Cbegin%7Baligned%7D&amp;space;h" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex={\displaystyle&amp;space;{\begin{aligned}&amp;space;h</a><sup>{(t)}&amp;=\tanh\left(&amp;space;W_hx</sup>{(t)}+U_hy<sup>{(t-1)}&amp;plus;b_h&amp;space;\right)\&amp;space;y</sup>{(t)}&amp;={\rm&amp;space;softmax}\left(&amp;space;W_yh^{(t)}+b_y&amp;space;\right)&amp;space;\end{aligned}}})</p>
                  </li>
                </ul>
                <blockquote>
                  <p>《深度学习》 默认的 RNN 是 Elman RNN &gt; <a href="#37-rnn%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9A%84%E5%87%A0%E7%A7%8D%E5%9F%BA%E6%9C%AC%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F">37. RNN（循环神经网络） 的几种基本设计模式**</a></p>
                </blockquote>
                <p><strong>门限 RNN</strong>（LSTM、GRU）与基本 RNN 的主要区别在于 Cell 部分</p>
                <p><strong>LSTM</strong></p>
                <blockquote>
                  <p><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#Variants" target="_blank" rel="noopener">Long short-term memory</a> - Wikipedia</p>
                </blockquote>
                <p><a href="http://www.codecogs.com/eqnedit.php?latex=%7B%5Cdisplaystyle&amp;space;%7B%5Cbegin%7Baligned%7D&amp;space;f_%7Bt%7D&amp;=%5Csigma(W_%7Bf%7Dx_%7Bt%7D+U_%7Bf%7Dh_%7Bt-1%7D+b_%7Bf%7D)%5C&amp;space;i_%7Bt%7D&amp;=%5Csigma(W_%7Bi%7Dx_%7Bt%7D+U_%7Bi%7Dh_%7Bt-1%7D+b_%7Bi%7D)%5C&amp;space;%5Ctilde%7Bc%7D_%7Bt%7D&amp;=%5Ctanh(W_%7Bc%7Dx_%7Bt%7D+U_%7Bc%7Dh_%7Bt-1%7D+b_%7Bc%7D)%5C&amp;space;c_%7Bt%7D&amp;=f_%7Bt%7D%5Ccirc&amp;space;c_%7Bt-1%7D+i_%7Bt%7D%5Ccirc&amp;space;%5Ctilde%7Bc%7D_%7Bt%7D%5C&amp;space;o_%7Bt%7D&amp;=%5Csigma(W_%7Bo%7Dx_%7Bt%7D+U_%7Bo%7Dh_%7Bt-1%7D+b_%7Bo%7D)%5C&amp;space;h_%7Bt%7D&amp;=o_%7Bt%7D%5Ccirc&amp;space;%5Ctanh(c_%7Bt%7D)&amp;space;%5Cend%7Baligned%7D%7D%7D"
                    target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613170734.png" alt=""></a></p>
                <ul>
                  <li>其中 f 为遗忘门（forget），i 为输入门（input），o 为输出门（output）。</li>
                  <li>每个门的输入都是 x 和 h，但是参数都是独立的（参数数量是基本 RNN 的 4 倍）</li>
                  <li>c 表示 cell state（如果用过 tensorflow 中的 RNN，会比较熟悉）</li>
                  <li>如果遗忘门 f 取 0 的话，那么上一时刻的状态就会全部被清空，只关注此时刻的输入</li>
                  <li>输入门 i 决定是否接收此时刻的输入</li>
                  <li>输出门 o 决定是否输出 cell state</li>
                </ul>
                <p>类似基本 RNN，LSTM 也有另一个版本，将公式中所有 h(t-1) 替换为 c(t-1)，但不常见</p>
                <p><strong>GRU</strong></p>
                <blockquote>
                  <p><a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit#Architecture" target="_blank" rel="noopener">Gated recurrent unit</a> - Wikipedia</p>
                </blockquote>
                <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613171757.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=%7B%5Cdisplaystyle&amp;space;%7B%5Cbegin%7Baligned%7D&amp;space;z_%7Bt%7D&amp;=%5Csigma(W_%7Bz%7Dx_%7Bt%7D+U_%7Bz%7Dh_%7Bt-1%7D+b_%7Bz%7D)%5C&amp;space;r_%7Bt%7D&amp;=%5Csigma(W_%7Br%7Dx_%7Bt%7D+U_%7Br%7Dh_%7Bt-1%7D+b_%7Br%7D)%5C&amp;space;%5Ctilde%7Bh%7D"
                    target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex={\displaystyle&amp;space;{\begin{aligned}&amp;space;z_{t}&amp;=\sigma(W_{z}x_{t}+U_{z}h_{t-1}+b_{z})\&amp;space;r_{t}&amp;=\sigma(W_{r}x_{t}+U_{r}h_{t-1}+b_{r})\&amp;space;\tilde{h}</a><em>t&amp;=\tanh(W</em>{h}x_{t}+U_{h}(r_{t}\circ&amp;space;h_{t-1})+b_{h})\&amp;space;h_{t}&amp;=(1-z_{t})\circ&amp;space;h_{t-1}+z_{t}\circ&amp;space;\tilde{h}_t&amp;space;\end{aligned}}})</p>
                <ul>
                  <li>其中 z 为更新门（update），r 为重置门（reset）</li>
                  <li>GRU 可以看作是将 LSTM 中的遗忘门和输入门合二为一了</li>
                </ul>
                <h1>BPTT（back-propagation through time，通过时间反向传播）**</h1>
                <h1>自编码器在深度学习中的意义*</h1>
                <p><strong>自编码器的意义</strong>：</p>
                <ul>
                  <li>传统自编码器被用于降维或特征学习</li>
                  <li>近年来，自编码器与潜变量模型理论的联系将自编码器带到了生成式建模的前沿
                    <ul>
                      <li>几乎任何带有潜变量并配有一个推断过程（计算给定输入的潜在表示）的生成模型，都可以看作是自编码器的一种特殊形式。</li>
                    </ul>
                  </li>
                </ul>
                <p><strong>自编码器的一般结构</strong></p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613200023.png" alt=""></p>
                <ul>
                  <li>自编码器有两个组件：<strong>编码器</strong> f（将 x 映射到 h）和<strong>解码器</strong> g（将 h 映射到 r）</li>
                  <li>一个简单的自编码器试图学习 <code>g(f(x)) = x</code>；换言之，自编码器尝试将输入复制到输出</li>
                  <li>单纯将输入复制到输出没什么用，相反，训练自编码器的目标是获得有用的特征 h。</li>
                </ul>
                <p>自编码器的学习过程就是最小化一个损失函数：</p>
                <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613201829.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=L(%5Cboldsymbol%7Bx%7D,g(f(%5Cboldsymbol%7Bx%7D)" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=L(\boldsymbol{x},g(f(\boldsymbol{x})</a>)))</p>
                <h1>自编码器一些常见的变形与应用：正则自编码器、稀疏自编码器、去噪自编码器*</h1>
                <blockquote>
                  <p><a href="#40-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E5%9C%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%84%8F%E4%B9%89">40. 自编码器在深度学习中的意义</a></p>
                </blockquote>
                <p><strong>欠完备自编码器</strong></p>
                <ul>
                  <li>从自编码器获得有用特征的一种方法是<strong>限制 h 的维度比 x 小</strong>，这种编码维度小于输入维度的自编码器称为<strong>欠完备</strong>（undercomplete）自编码器；</li>
                  <li>相反，如果 h 的维度大于 x，此时称为过完备自编码器。</li>
                  <li><strong>学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征</strong></li>
                  <li>当解码器是线性的且 L 是均方误差，欠完备的自编码器会学习出与 PCA 相同的生成子空间</li>
                  <li>而拥有<strong>非线性</strong>编码器函数 f 和<strong>非线性</strong>解码器函数 g 的自编码器能够学习出更强大的 PCA 非线性推广</li>
                  <li>但如果编码器和解码器被赋予<strong>过大的容量</strong>，自编码器会执行复制任务而捕捉不到任何有关<strong>数据分布</strong>的有用信息。
                    <ul>
                      <li>过完备自编码器就可以看作是被赋予过大容量的情况</li>
                    </ul>
                  </li>
                </ul>
                <p><strong>正则自编码器</strong></p>
                <ul>
                  <li>通过加入正则项到损失函数可以限制模型的容量，同时鼓励模型学习除了复制外的其他特性。</li>
                  <li>这些特性包括稀疏表示、表示的小导数、以及对噪声或输入缺失的鲁棒性。</li>
                  <li>即使模型的容量依然大到足以学习一个无意义的恒等函数，正则自编码器仍然能够从数据中学到一些关于数据分布的信息。</li>
                </ul>
                <p><strong>稀疏自编码器</strong></p>
                <ul>
                  <li>
                    <p>稀疏自编码器一般用来学习特征</p>
                  </li>
                  <li>
                    <p>稀疏自编码器简单地在训练时结合编码层的<strong>稀疏惩罚</strong> Ω(h) 和重构误差：</p>
                    <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613211004.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=L(%5Cboldsymbol%7Bx%7D,g(f(%5Cboldsymbol%7Bx%7D)" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=L(\boldsymbol{x},g(f(\boldsymbol{x})</a>))+\Omega(\boldsymbol{h})&amp;space;=&amp;space;L(\boldsymbol{x},g(f(\boldsymbol{x})))+\lambda\sum_i\left|h_i\right|)</p>
                  </li>
                  <li>
                    <p>稀疏惩罚不算是一个正则项。这仅仅影响模型关于潜变量的分布。这个观点提供了训练自编码器的另一个动机：这是近似训练生成模型的一种途径。这也给出了为什么自编码器学到的特征是有用的另一个解释：它们描述的潜变量可以解释输入。</p>
                    <blockquote>
                      <p>《深度学习》 14.2.1 稀疏自编码器</p>
                    </blockquote>
                  </li>
                </ul>
                <p><strong>去噪自编码器（DAE）</strong></p>
                <ul>
                  <li>
                    <p>去噪自编码器试图学习<strong>更具鲁棒性的</strong>特征</p>
                  </li>
                  <li>
                    <p>与传统自编码器不同，去噪自编码器（denoising autoencoder, DAE）最小化：</p>
                    <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613211437.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=L(%5Cboldsymbol%7Bx%7D,g(f(%5Cboldsymbol%7B%5Ctilde%7Bx%7D%7D)" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=L(\boldsymbol{x},g(f(\boldsymbol{\tilde{x}})</a>)))</p>
                  </li>
                  <li>
                    <p>这里的 x~ 是<strong>被某种噪声损坏</strong>的 x 的副本，去噪自编码器需要预测原始未被损坏数据</p>
                  </li>
                  <li>
                    <p>破坏的过程一般是以某种概率分布（通常是二项分布）将一些值置 0.</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613211935.png" alt=""></p>
                    <blockquote>
                      <p>《深度学习》 14.2.2 去噪自编码器，14.5 去噪自编码器</p>
                    </blockquote>
                  </li>
                </ul>
                <p><strong>为什么DAE有用？</strong></p>
                <ul>
                  <li>对比使用非破损数据进行训练，破损数据训练出来的权重噪声比较小——在破坏数据的过程中去除了真正的噪声</li>
                  <li>破损数据一定程度上减轻了训练数据与测试数据的代沟——使训练数据更接近测试数据
                    <blockquote>
                      <p><a href="https://www.cnblogs.com/neopenx/p/4370350.html" target="_blank" rel="noopener">降噪自动编码器（Denoising Autoencoder)</a> - Physcal - 博客园</p>
                      <blockquote>
                        <p>感觉这两个理由很牵强，但是从数据分布的角度讲太难了</p>
                      </blockquote>
                    </blockquote>
                  </li>
                </ul>
                <h1>半监督的思想以及在深度学习中的应用*</h1>
                <h1>分布式表示的概念、应用，与符号表示（one-hot 表示）的区别***</h1>
                <h2 id="什么是分布式表示？">什么是分布式表示？</h2>
                <ul>
                  <li>
                    <p>所谓分布式表示就是用不同的特征，通过<strong>组合</strong>来表示不同的概念</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614103628.png" alt="">
                      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614104032.png" alt=""></p>
                    <p>（左）是 one-hot 表示（一种稀疏表示），（右）为分布式表示</p>
                    <blockquote>
                      <p><a href="http://baijiahao.baidu.com/s?id=1593632865778730392" target="_blank" rel="noopener">神经网络如何学习分布式表示</a> - 百家号</p>
                    </blockquote>
                  </li>
                </ul>
                <h2 id="分布式表示为什么强大？-分布式表示与符号表示">分布式表示为什么强大？——分布式表示与符号表示</h2>
                <p>分布式表示之所以强大，是因为它能用具有 <code>d</code> 个值的 <code>n</code> 个<strong>线性阀值特征</strong>去描述 <code>d^n</code> 个不同的概念——换言之，在输入维度是 d 的一般情况下，具有 n 个特征的分布式表示可以给 O(n^d) 个不同区域分配唯一的编码</p>
                <blockquote>
                  <p><strong>线性阀值特征</strong>：本身是一个<strong>连续值</strong>，通过划分阈值空间来获得对应的离散特征</p>
                </blockquote>
                <p><strong>符号表示</strong></p>
                <ul>
                  <li>
                    <p>如果我们没有对数据做任何假设，并且每个区域使用唯一的符号来表示，每个符号使用单独的参数去识别空间中的对应区域，那么指定 <code>O(n^d)</code> 个区域需要 <code>O(n^d)</code> 个样本/参数。</p>
                  </li>
                  <li>
                    <p>举个例子：作为纯符号，“猫”和“狗”之间的距离和任意其他两种符号的距离是一样。</p>
                    <p>然而，如果将它们与<strong>有意义的分布式表示</strong>相关联，那么关于猫的很多特点可以推广到狗，反之亦然——比如，某个分布式表示可能会包含诸如“具有皮毛”或“腿的数目”这类在猫和的<strong>嵌入/向量</strong>上具有相同值的项</p>
                  </li>
                </ul>
                <p><strong>分布式表示与符号表示</strong>（最近邻）：</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614111241.png" alt="">
                  <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614111712.png" alt=""></p>
                <ul>
                  <li>两者都在学习如何将输入空间分割成多个区域</li>
                  <li>在输入维度相同的情况下，分布式表示能够比非分布式表示多分配指数级的区域——这一特性可用于解决<strong>维度灾难</strong>问题
                    <blockquote>
                      <p><a href="#44-%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE">44. 如何理解维数灾难？</a></p>
                    </blockquote>
                  </li>
                </ul>
                <p><strong>非线性特征</strong>：</p>
                <p>上面假设了<strong>线性阀值特征</strong>，然而更一般的，分布式表示的优势还体现在其中的每个特征可以用非线性算法——<strong>神经网络</strong>——来提取。简单来说，表示能力又提升了一个级别。</p>
                <p>一般来说，无论我们使用什么算法，需要学习的空间区域数量是固定的；但是使用分布式表示有效的减少了参数的数量——从 <code>O(n^d)</code> 到 <code>O(nd)</code>——这意味着我们需要拟合参数更少，因此只需要更少的训练样本就能获得良好的泛化。</p>
                <p><strong>一些非分布式表示算法</strong>：</p>
                <ul>
                  <li>聚类算法，比如 k-means 算法</li>
                  <li>k-最近邻算法</li>
                  <li>决策树</li>
                  <li>支持向量机</li>
                  <li>基于 n-gram 的语言模型</li>
                </ul>
                <p><strong>什么时候应该使用分布式表示能带来统计优势？</strong> 当一个明显复杂的结构可以
                  <strong>用较少参数紧致地表示</strong>时，使用分布式表示就会具有统计上的优势（避免维数灾难）。</p>
                <blockquote>
                  <p><a href="#44-%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE">44. 如何理解维数灾难？</a></p>
                </blockquote>
                <p>一些传统的非分布式学习算法仅仅在<strong>平滑先验</strong>的情况下能够泛化。</p>
                <h1>如何理解维数灾难？***</h1>
                <blockquote>
                  <p>《深度学习》 5.11.1 维数灾难</p>
                </blockquote>
                <p>概括来说，就是当数据维数很高时，会导致学习变得困难。</p>
                <p>这里的“困难”体现在两方面：</p>
                <ol>
                  <li>当数据较多时，会使训练的周期变得更长</li>
                  <li>当数据较少时，对新数据的泛化能力会更弱，甚至失去泛化能力</li>
                </ol>
                <blockquote>
                  <p>这两点对于任何机器学习算法都是成立的；但在维数灾难的背景下，会加剧这两个影响</p>
                </blockquote>
                <p>对于第二点，书中使用了另一种描述：“由维数灾难带来的一个问题是统计挑战，所谓统计挑战指的是 <strong>x 的可能配置数目远大于训练样本的数目</strong>”。</p>
                <p>为了充分理解这个问题，我们假设输入空间如图所示被分成单元格。</p>
                <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614203447.png" alt=""></p>
                <ul>
                  <li>当数据的维度增大时（从左向右），我们感兴趣的配置数目会随指数级增长。</li>
                  <li>当空间是低维时，我们可以用由少量单元格去描述这个空间。泛化到新数据点时，通过检测和单元格中的训练样本的相似度，我们可以判断如何处理新数据点。</li>
                  <li>当空间的维数很大时，很可能发生大量单元格中没有训练样本的情况。此时，基于“<strong>平滑先验</strong>”的简单算法将无力处理这些新的数据。</li>
                  <li>更一般的，O(nd) 个参数（d 个特征，每个特征有 n 种表示）能够明确表示输入空间中 O(n^d) 个不同区域。如果我们没有对数据做任何假设，并且<strong>每个区域使用唯一的符号来表示</strong>，每个符号使用单独的参数去识别空间中的对应区域，那么指定 O(n^d) 个区域将需要 O(n^d) 个样本。</li>
                </ul>
                <p><strong>如何解决维数灾难？</strong></p>
                <h1>迁移学习相关概念：多任务学习、一次学习、零次学习、多模态学习**</h1>
                <p><strong>什么是迁移学习？</strong></p>
                <ul>
                  <li>迁移学习和领域自适应指的是利用一个任务（例如，分布 P1）中已经学到的内容去改善另一个任务（比如分布 P2）中的泛化情况。
                    <ul>
                      <li>例如，我们可能在第一个任务中学习了一组视觉类别，比如猫和狗，然后在第二种情景中学习一组不同的视觉类别，比如蚂蚁和黄蜂。</li>
                    </ul>
                  </li>
                  <li>除了共享<strong>输出语义</strong>（上面这个例子），有时也会共享<strong>输出语义</strong>
                    <ul>
                      <li>例如，语音识别系统需要<strong>在输出层产生有效的句子</strong>，但是输入附近的较低层可能需要识别相同音素或子音素发音的不同版本（这取决于说话人）</li>
                    </ul>
                  </li>
                </ul>
                <p><strong>迁移学习与多任务学习</strong></p>
                <ul>
                  <li>
                    <p>因为目前迁移学习更流行，因此不少博客简介上，会将多任务学习归属于迁移学习的子类或者迁移学习的相关领域。</p>
                  </li>
                  <li>
                    <p>迁移学习与多任务学习的一些结构：</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180615111903.png" alt="">
                      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610203703.png" alt=""></p>
                    <blockquote>
                      <p>（左）&gt; 《深度学习》 15.2 迁移学习和领域自适应；（右）&gt; 《深度学习》 7.7 多任务学习</p>
                    </blockquote>
                  </li>
                  <li>
                    <p>这两种都可能是迁移学习或者多任务学习的结构。<s>迁移学习的输入在每个任务上具有不同的意义（甚至不同的维度），但是输出在所有的任务上具有<strong>相同的语义</strong>；多任务学习则相反</s></p>
                  </li>
                </ul>
                <p><strong>迁移学习与领域自适应</strong></p>
                <ul>
                  <li>相比于迁移学习和多任务学习，领域自适应的提法比较少，也更简单一些，其在每个情景之间任务（和最优的输入到输出的映射）都是相同的，但是<strong>输入分布</strong>稍有不同。</li>
                  <li>例如，考虑情感分析的任务：网上的评论有许多类别。在书、视频和音乐等媒体内容上训练的顾客评论情感预测器，被用于分析诸如电视机或智能电话的消费电子产品的评论时，领域自适应情景可能会出现。可以想象，存在一个潜在的函数可以判断任何语句是正面的、中性的还是负面的，但是词汇和风格可能会因领域而有差异</li>
                </ul>
                <p><strong>one-shot learning 和 zero-shot learning</strong></p>
                <ul>
                  <li>迁移学习的两种极端形式是<strong>一次学习</strong>（one-shot learning）和<strong>零次学习</strong>（zero-shot learning）
                    <ul>
                      <li>只有少量标注样本的迁移任务被称为 one-shot learning；没有标注样本的迁移任务被称为 zero-shot learning.</li>
                    </ul>
                  </li>
                  <li><strong>one-shot learning</strong>
                    <ul>
                      <li>one-shot learning 稍微简单一点：在大数据上学习 general knowledge，然后在特定任务的小数据上有技巧的 fine tuning。</li>
                    </ul>
                  </li>
                  <li><strong>zero-shot learning</strong>
                    <ul>
                      <li>相比 one-shot，zero-shot learning 要更复杂。</li>
                      <li>先来看一个 zero-shot 的例子：假设学习器已经学会了关于动物、腿和耳朵的概念。如果已知猫有四条腿和尖尖的耳朵，那么学习器可以在没有见过猫的情况下猜测该图像中的动物是猫。</li>
                      <li>(TODO)</li>
                    </ul>
                  </li>
                </ul>
                <p><strong>多模态学习（multi-modal learning）</strong></p>
                <ul>
                  <li>
                    <p>与 zero-shot learning 相同的原理可以解释如何能执行<strong>多模态学习</strong>（multimodal learning）</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180615150955.png" alt=""></p>
                  </li>
                </ul>
                <h1>图模型|结构化概率模型相关概念*</h1>
                <p><strong>有向图模型</strong></p>
                <ul>
                  <li>
                    <p>有向图模型（directed graphical model）是一种结构化概率模型，也被称为<strong>信念网络</strong>（belief network）或者<strong>贝叶斯网络</strong>（Bayesian network）</p>
                  </li>
                  <li>
                    <p>描述接力赛例子的有向图模型</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180617140439.png" alt=""></p>
                    <ul>
                      <li>Alice 在 Bob 之前开始，所以 Alice 的完成时间 t0 影响了 Bob 的完成时间 t1。</li>
                      <li>Carol 只会在 Bob 完成之后才开始，所以 Bob 的完成时间 t1 直接影响了 Carol 的完成时间 t2。</li>
                    </ul>
                  </li>
                  <li>
                    <p>正式地说，变量 x 的有向概率模型是通过有向无环图 G（每个结点都是模型中的随机变量）和一系列<strong>局部条件概率分布</strong>（local conditional probability distribution）来定义的，x 的概率分布可以表示为：</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180617141056.png" alt=""></p>
                    <ul>
                      <li>其中 大P 表示结点 xi 的所有父结点</li>
                    </ul>
                  </li>
                  <li>
                    <p>上述接力赛例子的概率分布可表示为：</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180617141502.png" alt=""></p>
                  </li>
                </ul>
                <p><strong>无向图模型</strong></p>
                <ul>
                  <li>无向图模型（undirected graphical Model），也被称为<strong>马尔可夫随机场</strong>（Markov random field, MRF）或者是<strong>马尔可夫网络</strong>（Markov network）</li>
                  <li>当相互的作用并没有本质性的指向，或者是明确的双向相互作用时，使用无向模型更加合适。</li>
                </ul>
                <p><strong>图模型的优点</strong></p>
                <ol>
                  <li>减少参数的规模
                    <ul>
                      <li>通常意义上说，对每个变量都能取 k 个值的 n 个变量建模，<strong>基于查表的</strong>方法需要的复杂度是 <code>O(k^n)</code>，如果 m 代表图模型的单个条件概率分布中最大的变量数目，那么对这个有向模型建表的复杂度大致为 <code>O(k^m)</code>。只要我们在设计模型时使其满足 <code>m ≪ n</code>，那么复杂度就会被大大地减小；换一句话说，只要图中的每个变量都只有少量的父结点，那么这个分布就可以用较少的参数来表示。</li>
                    </ul>
                  </li>
                  <li>统计的高效性
                    <ul>
                      <li>相比图模型，<strong>基于查表的</strong>模型拥有天文数字级别的参数，为了准确地拟合，相应的训练集的大小也是相同级别的。</li>
                    </ul>
                  </li>
                  <li>减少运行时间
                    <ul>
                      <li>推断的开销：计算分布时，避免对整个表的操作，比如求和</li>
                      <li>采样的开销：类似推断，避免读取整个表格</li>
                    </ul>
                    <blockquote>
                      <p>《深度学习》 16.3 从图模型中采样</p>
                    </blockquote>
                  </li>
                </ol>
                <p><strong>图模型如何用于深度学习</strong></p>
                <ul>
                  <li>受限玻尔兹曼机（RBM）
                    <ul>
                      <li>RBM 是图模型如何用于深度学习的典型例子</li>
                      <li>RBM 本身不是一个深层模型，它有一层潜变量，可用于学习输入的表示。但是它可以被用来构建许多的深层模型。</li>
                    </ul>
                  </li>
                </ul>
                <p><strong>其他相关名词</strong>：</p>
                <ul>
                  <li>信念网络（有向图模型）</li>
                  <li>马尔可夫网络（无向图模型）</li>
                  <li>配分函数</li>
                  <li>能量模型（无向图模型）</li>
                  <li>分离（separation）/d-分离</li>
                  <li>道德图（moralized graph）、弦图（chordal graph）</li>
                  <li>因子图（factor graph）</li>
                  <li>Gibbs 采样</li>
                  <li>结构学习（structure learning）</li>
                </ul>
                <h1>深度生成模型、受限玻尔兹曼机（RBM）相关概念*</h1>
                <h1>深度学习在图像、语音、NLP等领域的常见作法与基本模型**</h1>
                <h2 id="计算机视觉-cv">计算机视觉（CV）</h2>
                <p><strong>预处理</strong> 许多应用领域需要复杂精细的预处理，但是 CV 通常只需要相对少的预处理。</p>
                <p>通常，<strong>标准化</strong>是图像唯一必要的预处理——将图像格式化为具有相同的比例，比如 [0,1] 或者 [-1,1].</p>
                <p>许多框架需要图像缩放到标准的尺寸。但这不是必须的，一些卷积模型接受可变大小的输入并动态地调整它们的池化区域大小以保持输出大小恒定。</p>
                <p>其他预处理操作：</p>
                <p><strong>对比度归一化</strong></p>
                <ul>
                  <li>
                    <p>在许多任务中，对比度是能够安全移除的最为明显的变化源之一。简单地说，对比度指的是图像中亮像素和暗像素之间差异的大小。</p>
                  </li>
                  <li>
                    <p>整个图像的对比度可以表示为：</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619104551.png" alt="">，其中</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619104649.png" alt="">，整个图片的平均强度</p>
                  </li>
                </ul>
                <p><strong>全局对比度归一化</strong>（Global contrast normalization, GCN）</p>
                <ul>
                  <li>
                    <p>GCN 旨在通过从每个图像中减去其平均值，然后重新缩放其使得其像素上的标准差等于某个常数 s 来防止图像具有变化的对比度。定义为：</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619104914.png" alt=""></p>
                    <ul>
                      <li>从大图像中剪切感兴趣的对象所组成的数据集不可能包含任何强度几乎恒定的图像。此时，设置 λ = 0 来忽略小分母问题是安全的。(Goodfellow et al. 2013c)</li>
                      <li>随机剪裁的小图像更可能具有几乎恒定的强度，使得激进的正则化更有用。此时可以加大 λ (ϵ = 0, λ = 10; Coates et al. 2011)</li>
                      <li>尺度参数 s 通常可以设置为 1 (Coates et al. 2011)，或选择使所有样本上每个像素的标准差接近 1 (Goodfellow et al. 2013c)</li>
                    </ul>
                  </li>
                  <li>
                    <p><strong>GCN 的意义</strong></p>
                    <ul>
                      <li>
                        <p>式中的标准差可以看作是对图片 L2 范数的重新缩放（假设移除了均值），但我们倾向于标准差而不是 L2 范数来定义 GCN，是因为标准差包括除以像素数量这一步，从而基于标准差的 GCN 能够使用与图像大小无关的固定的 s.</p>
                      </li>
                      <li>
                        <p>而将标准差视为 L2 范数的缩放，可以将 GCN 理解成到球壳的一种映射。这可能是一个有用的属性，因为神经网络往往更好地响应空间方向，而不是精确的位置。</p>
                        <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619113501.png" alt=""></p>
                        <ul>
                          <li>(左) 原始的输入数据可能拥有任意的范数。</li>
                          <li>(中) λ = 0 时候的 GCN 可以完美地将所有的非零样本投影到球上。这里我们令 s = 1， ϵ = 10−8。由于我们使用的 GCN 是基于归一化标准差而不是 L2 范数，所得到的球并不是单位球。</li>
                          <li>(右) λ &gt; 0 的正则化 GCN 将样本投影到球上，但是并没有完全地丢弃其范数中变化。 s 和 ϵ 的取值与之前一样。</li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                  <li>
                    <p>GCN 的问题：</p>
                    <ul>
                      <li>全局对比度归一化常常不能突出我们想要突出的图像特征，例如边缘和角。</li>
                      <li>例子：如果我们有一个场景，包含了一个大的黑暗区域和一个大的明亮的区域（例如一个城市广场有一半的区域处于建筑物的阴影之中），则全局对比度归一化将确保暗区域的亮度与亮区域的亮度之间存在大的差异。然而，它不能确保暗区内的边缘突出。</li>
                    </ul>
                  </li>
                </ul>
                <p><strong>局部对比度归一化</strong>（local contrast normalization, LCN）</p>
                <ul>
                  <li>
                    <p>GCN 存在的问题催生了 LCN</p>
                  </li>
                  <li>
                    <p>LCN 确保对比度在每个小窗口上被归一化，而不是作为整体在图像上被归一化。</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619114844.png" alt=""></p>
                  </li>
                  <li>
                    <p>LCN 通常可以通过使用可分离卷积来计算特征映射的局部平均值和局部标准差，然后在不同的特征映射上使用逐元素的减法和除法。</p>
                  </li>
                  <li>
                    <p>LCN 是可微分的操作，并且还可以作为一种非线性作用应用于网 络隐藏层，以及应用于输入的预处理操作。
                    </p>
                  </li>
                </ul>
                <p><strong>数据集增强</strong> 数据集增强可以被看作是一种
                  <strong>只对训练集</strong>做预处理的方式。</p>
                <h2 id="语音识别">语音识别</h2>
                <p>自动语音识别（Automatic Speech Recognition, ASR）任务指的是构造一个函数 f*，使得它能够在给定声学序列 X 的情况下计算最有可能的语言序列 y.</p>
                <p>令 X = (x(1), x(2), …, x(T)) 表示语音的输入向量，传统做法以 20ms 左右为一帧分割信号；y = (y1, y2, …, yN) 表示目标的输出序列（通常是一个词或者字符的序列。</p>
                <p>许多语音识别的系统通过特殊的手工设计方法预处理输入信号，从而提取声学特征；也有一些深度学习系统 (Jaitly and Hinton, 2011) 直接从原始输入中学习特征。</p>
                <h2 id="自然语言处理">自然语言处理</h2>
                <p>相关术语：</p>
                <ul>
                  <li>n-gram 语言模型</li>
                  <li>神经语言模型（Neural Language Model, NLM）
                    <ul>
                      <li>结合 n-gram 和神经语言模型</li>
                      <li>分层 Softmax</li>
                    </ul>
                  </li>
                  <li>神经机器翻译
                    <ul>
                      <li>注意力机制</li>
                    </ul>
                  </li>
                </ul>
                <p><strong>n-gram 语言模型</strong></p>
                <ul>
                  <li>语言模型（language model）定义了自然语言中<strong>标记序列</strong>的概率分布。根据模型的设计，标记可以是词、字符、甚至是字节。标记总是离散的实体。</li>
                  <li>n-gram 是最早成功的语言模型
                    <ul>
                      <li>
                        <p>一个 n-gram 是一个包含 n 个<strong>标记</strong>的序列。</p>
                      </li>
                      <li>
                        <p>基于 n-gram 的模型定义一个条件概率——给定前 n − 1 个标记后的第 n 个标记的条件概率：</p>
                        <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619193332.png" alt=""></p>
                      </li>
                      <li>
                        <p>训练 n-gram 模型很简单，因为最大似然估计可以通过简单地统计每个可能的 n-gram 在训练集中<strong>出现的频数</strong>来获得。</p>
                      </li>
                      <li>
                        <p>通常我们同时训练 n-gram 模型和 n − 1 gram 模型。这使得下式可以简单地通过查找两个存储的概率来计算。</p>
                        <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619193540.png" alt=""></p>
                      </li>
                    </ul>
                  </li>
                  <li>n-gram 模型的缺点：
                    <ul>
                      <li><strong>稀疏问题</strong>——n-gram 模型最大似然的基本限制是，在许多情况下从训练集计数估计得到的 Pn 很可能为零。由此产生了各种<strong>平滑算法</strong>。</li>
                      <li><strong>维数灾难</strong>——经典的 n-gram 模型特别容易引起维数灾难。因为存在 |V|^n 可能的 n-gram，而且 |V| 通常很大。</li>
                    </ul>
                  </li>
                </ul>
                <p><strong>神经语言模型</strong>（Neural Language Model, NLM）</p>
                <ul>
                  <li>
                    <p>NLM 是一类用来克服维数灾难的语言模型，它使用词的<strong>分布式表示</strong>对自然语言序列建模</p>
                  </li>
                  <li>
                    <p>NLM 能在识别两个相似词的同时，不丧失将词编码为不同的能力。神经语言模型共享一个词（及其上下文）和其他类似词的统计强度。模型为每个词学习的分布式表示，允许模型处理具有类似共同特征的词来实现这种共享。因为这样的属性很多，所以存在许多泛化的方式，可以将信息从每个训练语句传递到指数数量的语义相关语句。</p>
                    <ul>
                      <li>例如，如果词 dog 和词 cat 映射到具有许多属性的表示，则包含词 cat 的句子可以告知模型对包含词 dog 的句子做出预测，反之亦然。</li>
                    </ul>
                  </li>
                  <li>
                    <p>使用分布式表示来改进自然语言处理模型的基本思想不必局限于神经网络。它还可以用于图模型，其中分布式表示是多个潜变量的形式 (Mnih and Hinton, 2007)。</p>
                  </li>
                </ul>
                <p><strong>词嵌入</strong>（word embedding）</p>
                <ul>
                  <li>
                    <p>词的分布时表示称为词嵌入，在这个解释下，我们将原始符号视为维度等于词表大小的空间中的点。词表示将这些点<strong>嵌入到较低维的特征空间</strong>中。</p>
                  </li>
                  <li>
                    <p>在原始空间中，每个词由一个one-hot向量表示，因此每对词彼此之间的欧氏距离都是 √2。</p>
                  </li>
                  <li>
                    <p>在<strong>嵌入空间</strong>中，经常出现在类似上下文中的词彼此接近。这通常导致具有相似含义的词变得邻近。</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619205103.png" alt=""></p>
                    <ul>
                      <li>这些嵌入是为了可视化才表示为 2 维。在实际应用中，嵌入通常具有更高的维度并且可以同时捕获词之间多种相似性。</li>
                    </ul>
                  </li>
                </ul>
                <p><strong>高维输出</strong></p>
                <ul>
                  <li>对于大词汇表，由于词汇量很大，在词的选择上表示输出分布的计算和存储成本可能非常高。</li>
                  <li>表示这种分布的朴素方法是应用一个仿射变换，将隐藏表示转换到输出空间，然后应用 softmax 函数。因为 softmax 要在所有输出之间归一化，所以需要执行全矩阵乘法，这是高计算成本的原因。因此，输出层的高计算成本在训练期间（计算似然性及其梯度）和测试期间（计算所有或所选词的概率）都有出现。</li>
                  <li><strong>一些解决方案</strong>
                    <ul>
                      <li>使用<strong>短列表</strong>——简单来说，就是限制词表的大小</li>
                      <li><strong>分层 Softmax</strong></li>
                      <li><strong>重要采样</strong>——负采样就是一种简单的重要采样方式</li>
                    </ul>
                  </li>
                </ul>
                <p><strong>分层 Softmax</strong></p>
                <ul>
                  <li>减少大词汇表 V 上高维输出层计算负担的经典方法 (Goodman, 2001) 是<strong>分层地分解概率</strong>。|V| 因子可以降低到 log|V| 一样低，而无需执行与 |V| 成比例数量（并且也与隐藏单元数量成比例）的计算。</li>
                  <li>我们可以认为这种层次结构是先建立词的类别，然后是词类别的类别，然后是词类别的类别的类别等等。这些嵌套类别构成一棵树，其叶子为词。</li>
                  <li>选择一个词的概率是由路径（从树根到包含该词叶子的路径）上的每个节点通向该词分支概率的乘积给出。</li>
                </ul>
                <p><strong>重要采样</strong>/负采样</p>
                <ul>
                  <li>加速神经语言模型训练的一种方式是，避免明确地计算所有未出现在下一位置的词对梯度的贡献。</li>
                  <li>每个不正确的词在此模型下具有低概率。枚举所有这些词的计算成本可能会很高。相反，我们可以仅采样词的子集。</li>
                </ul>
                <p><strong>结合 n-gram 和神经语言模型</strong></p>
                <ul>
                  <li>n-gram 模型相对神经网络的主要优点是具有更高的模型容量（通过存储非常多的元组的频率），并且处理样本只需非常少的计算量。</li>
                  <li>相比之下，将神经网络的参数数目加倍通常也大致加倍计算时间。</li>
                  <li>增加神经语言模型容量的一种简单方法是将之与 n-gram 方法结合，集成两个模型</li>
                </ul>
                <p><strong>神经机器翻译</strong>（NMT）</p>
                <ul>
                  <li>
                    <p>编码器和解码器的想法 (Allen 1987; Chrisman 1991; Forcada and Ñeco 1997)很早就应用到了 NMT 中。</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619214629.png" alt=""></p>
                  </li>
                  <li>
                    <p>基于 MLP 方法的缺点是需要将序列预处理为固定长度。为了使翻译更加灵活，我们希望模型允许可变的输入长度和输出长度。所以大量的 NMT 模型使用 RNN 作为基本单元。</p>
                  </li>
                </ul>
                <p><strong>注意力（Attention）机制</strong></p>
                <ul>
                  <li>
                    <p>使用固定大小的表示概括非常长的句子（例如 60 个词）的所有语义细节是非常困难的。这需要使用足够大的 RNN。这会带来一些列训练问题。</p>
                  </li>
                  <li>
                    <p>更高效的方法是先读取整个句子或段落（以获得正在表达的上下文和焦点），然后一次翻译一个词，每次聚焦于输入句子的不同部分来收集产生下一个输出词所需的语义细节 (Bahdanau et al., 2015)——<strong>Attention</strong> 机制</p>
                    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619214735.png" alt=""></p>
                    <ul>
                      <li>由 Bahdanau et al. (2015) 引入的现代注意力机制，本质上是<strong>加权平均</strong>。</li>
                    </ul>
                  </li>
                </ul>
                <p><strong>其他应用</strong>：</p>
                <ul>
                  <li>推荐系统</li>
                  <li>知识表示、推理和回答</li>
                </ul>
                <p><br><br>本文链接： <a href="http://www.meng.uno/articles/f42d8431/">http://www.meng.uno/articles/f42d8431/</a> 欢迎转载！</p>

              </div>
              <footer class="article-footer">

                <!-- Go to www.addthis.com/dashboard to customize your tools -->

                <div id="wpac-rating" style="margin: 10px auto; text-align:center;"></div>
                <script type="text/javascript">
                  wpac_init = window.wpac_init || [];
                  wpac_init.push({
                    widget: 'Rating',
                    id: 9986
                  });
                  (function() {
                    if ('WIDGETPACK_LOADED' in window) return;
                    WIDGETPACK_LOADED = true;
                    var mc = document.createElement('script');
                    mc.type = 'text/javascript';
                    mc.async = true;
                    mc.src = 'https://embed.widgetpack.com/widget.js';
                    var s = document.getElementsByTagName('script')[0];
                    s.parentNode.insertBefore(mc, s.nextSibling);
                  })();
                </script>

                <div id="donation_div"></div>

                <script src="/js/vdonate.js"></script>
                <script>
                  var a = new Donate({
                    title: '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作!', // 可选参数，打赏标题
                    btnText: '打赏支持', // 可选参数，打赏按钮文字
                    el: document.getElementById('donation_div'),
                    wechatImage: 'http://www.meng.uno/money/wechat.JPG',
                    alipayImage: 'http://www.meng.uno/money/alipay.JPG'
                  });
                </script>


                <div id="comment">
                  <!-- 来必力City版安装代码 -->
                  <div id="lv-container" data-id="city" data-uid="MTAyMC8zNDA2OC8xMDYwNg==">
                    <script type="text/javascript">
                      (function(d, s) {
                        var j, e = d.getElementsByTagName(s)[0];

                        if (typeof LivereTower === 'function') {
                          return;
                        }

                        j = d.createElement(s);
                        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                        j.async = true;

                        e.parentNode.insertBefore(j, e);
                      })(document, 'script');
                    </script>
                    <noscript>为正常使用评论功能请激活JavaScript</noscript>
                  </div>
                  <!-- City版安装代码已完成 -->
                </div>

                <ul class="article-tag-list">
                  <li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li>
                  <li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li>
                </ul>
              </footer>
            </div>

            <nav id="article-nav">

              <a href="/articles/7203e497/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          CNN
        
      </div>
    </a>


              <a href="/articles/33c755f8/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">Oracle教程</div>
    </a>

            </nav>

          </article>

          <!-- Table of Contents -->

          <aside id="toc-sidebar">
            <div id="toc" class="toc-article">
              <strong class="toc-title">目录导航</strong>

              <ol class="nav">
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">1.</span> <span class="nav-text">如何设置网络的初始值？*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">2.</span> <span class="nav-text">梯度爆炸的解决办法</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">3.</span> <span class="nav-text">神经网络（MLP）的万能近似定理*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">4.</span> <span class="nav-text">神经网络中，深度与宽度的关系，及其表示能力的差异**</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">5.</span> <span class="nav-text">在深度神经网络中，引入了隐藏层（非线性单元），放弃了训练问题的凸性，其意义何在？**</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">6.</span> <span class="nav-text">稀疏表示，低维表示，独立表示*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">7.</span> <span class="nav-text">局部不变性（平滑先验）及其在基于梯度的学习上的局限性*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">8.</span> <span class="nav-text">为什么交叉熵损失相比均方误差损失能提高以 sigmoid 和 softmax 作为激活函数的层的性能？**</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">9.</span> <span class="nav-text">分段线性单元（如 ReLU）代替 sigmoid 的利弊</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">10.</span> <span class="nav-text">在做正则化过程中，为什么只对权重做正则惩罚，而不对偏置做权重惩罚*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">11.</span> <span class="nav-text">列举常见的一些范数及其应用场景，如 L0、L1、L2、L∞、Frobenius等范数**</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">12.</span> <span class="nav-text">L1 和 L2 范数的异同***</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">13.</span> <span class="nav-text">为什么 L1 正则化可以产生稀疏权值，L2 正则化可以防止过拟合？**</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#为什么-l1-正则化可以产生稀疏权值-而-l2-不会？"><span class="nav-number">13.1.</span> <span class="nav-text">为什么 L1 正则化可以产生稀疏权值，而 L2 不会？</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#为什么-l1-和-l2-正则化可以防止过拟合？"><span class="nav-number">13.2.</span> <span class="nav-text">为什么 L1 和 L2 正则化可以防止过拟合？</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">14.</span> <span class="nav-text">简单介绍常用的激活函数，如 sigmoid、relu、softplus、tanh、RBF 及其应用场景***</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#整流线性单元-relu"><span class="nav-number">14.1.</span> <span class="nav-text">整流线性单元（ReLU）</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#sigmoid-与-tanh-双曲正切函数"><span class="nav-number">14.2.</span> <span class="nav-text">sigmoid 与 tanh（双曲正切函数）</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#其他激活函数-隐藏单元"><span class="nav-number">14.3.</span> <span class="nav-text">其他激活函数（隐藏单元）</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#sigmoid-和-softplus-的一些性质"><span class="nav-number">14.4.</span> <span class="nav-text">sigmoid 和 softplus 的一些性质</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">15.</span> <span class="nav-text">Jacobian 和 Hessian 矩阵及其在深度学习中的重要性*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">16.</span> <span class="nav-text">信息熵、KL 散度（相对熵）与交叉熵**</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#自信息与信息熵"><span class="nav-number">16.1.</span> <span class="nav-text">自信息与信息熵</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#相对熵-kl-散度-与交叉熵"><span class="nav-number">16.2.</span> <span class="nav-text">相对熵（KL 散度）与交叉熵</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">17.</span> <span class="nav-text">如何避免数值计算中的上溢和下溢问题，以 softmax 为例*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">18.</span> <span class="nav-text">训练误差、泛化误差；过拟合、欠拟合；模型容量，表示容量，有效容量，最优容量的概念； 奥卡姆剃刀原则*</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#过拟合的一些解决方案"><span class="nav-number">18.1.</span> <span class="nav-text">过拟合的一些解决方案***</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">19.</span> <span class="nav-text">高斯分布的广泛应用的原因**</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#高斯分布-gaussian-distribution"><span class="nav-number">19.1.</span> <span class="nav-text">高斯分布（Gaussian distribution）</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#为什么推荐使用高斯分布？"><span class="nav-number">19.2.</span> <span class="nav-text">为什么推荐使用高斯分布？</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">20.</span> <span class="nav-text">表示学习、自编码器与深度学习**</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">21.</span> <span class="nav-text">L1、L2 正则化与 MAP 贝叶斯推断的关系*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">22.</span> <span class="nav-text">什么是欠约束，为什么大多数的正则化可以使欠约束下的欠定问题在迭代过程中收敛*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">23.</span> <span class="nav-text">为什么考虑在模型训练时对输入 (隐藏单元或权重) 添加方差较小的噪声？*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">24.</span> <span class="nav-text">多任务学习、参数绑定和参数共享***</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#多任务学习"><span class="nav-number">24.1.</span> <span class="nav-text">多任务学习</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#参数绑定和参数共享"><span class="nav-number">24.2.</span> <span class="nav-text">参数绑定和参数共享</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">25.</span> <span class="nav-text">Dropout 与 Bagging 集成方法的关系，Dropout 带来的意义与其强大的原因***</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#bagging-集成方法"><span class="nav-number">25.1.</span> <span class="nav-text">Bagging 集成方法</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#dropout"><span class="nav-number">25.2.</span> <span class="nav-text">Dropout</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">26.</span> <span class="nav-text">批梯度下降法（Batch SGD）更新过程中，批的大小会带来怎样的影响**</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">27.</span> <span class="nav-text">如何避免深度学习中的病态，鞍点，梯度爆炸，梯度弥散？***</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#病态-ill-conditioning"><span class="nav-number">27.1.</span> <span class="nav-text">病态（ill-conditioning）</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#鞍点-saddle-point"><span class="nav-number">27.2.</span> <span class="nav-text">鞍点（saddle point）</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#长期依赖与梯度爆炸-消失"><span class="nav-number">27.3.</span> <span class="nav-text">长期依赖与梯度爆炸、消失</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">28.</span> <span class="nav-text">SGD 以及学习率的选择方法、带动量的 SGD***</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#批-随机梯度下降-sgd-与学习率"><span class="nav-number">28.1.</span> <span class="nav-text">（批）随机梯度下降（SGD）与学习率</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#带动量的-sgd"><span class="nav-number">28.2.</span> <span class="nav-text">带动量的 SGD</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">29.</span> <span class="nav-text">自适应学习率算法：AdaGrad、RMSProp、Adam 等***</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#adagrad"><span class="nav-number">29.1.</span> <span class="nav-text">AdaGrad</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#rmsprop"><span class="nav-number">29.2.</span> <span class="nav-text">RMSProp</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#adam"><span class="nav-number">29.3.</span> <span class="nav-text">Adam</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">30.</span> <span class="nav-text">基于二阶梯度的优化方法：牛顿法、共轭梯度、BFGS 等的做法*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">31.</span> <span class="nav-text">批标准化（Batch Normalization）的意义**</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">32.</span> <span class="nav-text">神经网络中的卷积，以及卷积的动机：稀疏连接、参数共享、等变表示（平移不变性）***</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#稀疏连接-sparse-connectivity"><span class="nav-number">32.1.</span> <span class="nav-text">稀疏连接（sparse connectivity）</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#参数共享-parameter-sharing"><span class="nav-number">32.2.</span> <span class="nav-text">参数共享（parameter sharing）</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#平移等变-不变性-translation-invariant"><span class="nav-number">32.3.</span> <span class="nav-text">平移等变|不变性（translation invariant）</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">33.</span> <span class="nav-text">卷积中不同零填充的影响**</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">34.</span> <span class="nav-text">基本卷积的变体：反卷积、空洞卷积***</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#转置卷积-反卷积-transposed-convolution"><span class="nav-number">34.1.</span> <span class="nav-text">转置卷积|反卷积（Transposed convolution）</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#空洞卷积-扩张卷积-dilated-convolution"><span class="nav-number">34.2.</span> <span class="nav-text">空洞卷积|扩张卷积（Dilated convolution）</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">35.</span> <span class="nav-text">池化、池化（Pooling）的作用***</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">36.</span> <span class="nav-text">卷积与池化的意义、影响（作为一种无限强的先验）**</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">37.</span> <span class="nav-text">RNN 的几种基本设计模式</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">38.</span> <span class="nav-text">RNN 更新方程（前向传播公式），包括 LSTM、GRU 等***</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">39.</span> <span class="nav-text">BPTT（back-propagation through time，通过时间反向传播）**</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">40.</span> <span class="nav-text">自编码器在深度学习中的意义*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">41.</span> <span class="nav-text">自编码器一些常见的变形与应用：正则自编码器、稀疏自编码器、去噪自编码器*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">42.</span> <span class="nav-text">半监督的思想以及在深度学习中的应用*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">43.</span> <span class="nav-text">分布式表示的概念、应用，与符号表示（one-hot 表示）的区别***</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#什么是分布式表示？"><span class="nav-number">43.1.</span> <span class="nav-text">什么是分布式表示？</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#分布式表示为什么强大？-分布式表示与符号表示"><span class="nav-number">43.2.</span> <span class="nav-text">分布式表示为什么强大？——分布式表示与符号表示</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">44.</span> <span class="nav-text">如何理解维数灾难？***</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">45.</span> <span class="nav-text">迁移学习相关概念：多任务学习、一次学习、零次学习、多模态学习**</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">46.</span> <span class="nav-text">图模型|结构化概率模型相关概念*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">47.</span> <span class="nav-text">深度生成模型、受限玻尔兹曼机（RBM）相关概念*</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">48.</span> <span class="nav-text">深度学习在图像、语音、NLP等领域的常见作法与基本模型**</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#计算机视觉-cv"><span class="nav-number">48.1.</span> <span class="nav-text">计算机视觉（CV）</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#语音识别"><span class="nav-number">48.2.</span> <span class="nav-text">语音识别</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#自然语言处理"><span class="nav-number">48.3.</span> <span class="nav-text">自然语言处理</span></a></li>
                  </ol>
                </li>
              </ol>

            </div>
          </aside>
        </section>

      </div>


      <footer id="footer">


        <div class="container">
          <div class="row">
            <p id="copyRightEn">&copy; 2018.02.08 - 2018.08.16 <a href="http://www.meng.uno/">匡盟盟</a>&nbsp;<i class="fas fa-cogs"></i> 保留所有权利！</p>

            <p class="busuanzi_uv">

              访客数 : <span id="busuanzi_value_site_uv"></span> | 访问量 : <span id="busuanzi_value_site_pv"></span>

            </p>


            <p id="hitokoto">:D 获取中...</p>
            <!-- 以下写法，选取一种即可 -->

            <!-- 现代写法，推荐 -->
            <!-- 兼容低版本浏览器 (包括 IE)，可移除 -->
            <script src="https://cdn.bootcss.com/bluebird/3.5.1/bluebird.core.min.js"></script>
            <script src="https://cdn.bootcss.com/fetch/2.0.3/fetch.min.js"></script>
            <!--End-->
            <script>
              fetch('https://v1.hitokoto.cn')
                .then(function(res) {
                  return res.json();
                })
                .then(function(data) {
                  var hitokoto = document.getElementById('hitokoto');
                  hitokoto.innerText = data.hitokoto;
                })
                .catch(function(err) {
                  console.error(err);
                })
            </script>

            <a href="http://webscan.360.cn/index/checkwebsite/url/www.meng.uno"><img border="0" height=27px width=74px src="/css/images/webscan.png"/></a>
            <!-- <img border="0" height=27px width=109px style="background-color:white;"src="/css/images/kaba.png"/> -->
            <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" height=27px style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a>
            <script type="text/javascript">
              var locationUrl = escape(document.location.href);
              document.write(unescape("%3Cscript") + " height='27px' width='74px' charset='utf-8' src='http://union.rising.com.cn//InfoManage/TrojanInspect.aspx?p1=XNk3xHG5v8uxFHYb4KaGpnyWjJlbHp7K&p2=RqCQt7iMKRw=&p3=XNk3xHG5v8vv3Z1xqd/V8w==&url=" + locationUrl + "' type='text/javascript'" + unescape("%3E%3C/script%3E"));
            </script>
          </div>

        </div>
      </footer>


      <!-- min height -->

      <script>
        var wrapdiv = document.getElementById("wrap");
        var contentdiv = document.getElementById("content");
        var allheader = document.getElementById("allheader");

        wrapdiv.style.minHeight = document.body.offsetHeight + "px";
        if (allheader != null) {
          contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
        } else {
          contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
        }
      </script>

      <script>
        (function() {
          var src = (document.location.protocol == "http:") ? "http://js.passport.qihucdn.com/11.0.1.js?5d3bca9f7d6a95532f3ebb56e3c6bf11" : "https://jspassport.ssl.qhimg.com/11.0.1.js?5d3bca9f7d6a95532f3ebb56e3c6bf11";
          document.write('<script src="' + src + '" id="sozz"><\/script>');
        })();
      </script>
    </div>
    <!-- <nav id="mobile-nav">

  <a href="/" class="mobile-nav-link">Home</a>

  <a href="/archives" class="mobile-nav-link">Archives</a>

  <a href="/categories" class="mobile-nav-link">Categories</a>

  <a href="/tags" class="mobile-nav-link">Tags</a>

  <a href="/about" class="mobile-nav-link">About</a>

  <a href="/comments" class="mobile-nav-link">Comments</a>

</nav> -->
    <!-- mathjax config similar to math.stackexchange -->

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(), i; for(i=0; i
      < all.length; i +=1 ) { all[i].SourceElement().parentNode.className +=' has-jax' ; } }); </script>

        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>


        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
        <script src="/fancybox/jquery.fancybox.pack.js"></script>


        <script src="/js/scripts.js"></script>




        <script src="/js/dialog.js"></script>


        <!-- Google Analytics -->
        <script type="text/javascript">
          (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
              (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
              m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
          })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

          ga('create', 'UA-113947925-1', 'auto');
          ga('send', 'pageview');
        </script>
        <!-- End Google Analytics -->


        <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
        </script>
  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <h2 class="modal-title" id="myModalLabel">设置</h2>
        </div>
        <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
        <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
        <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
            <div class="panel-body">
              您已调整页面字体大小
            </div>
          </div>
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
          </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
            <div class="panel-body">
              夜间模式已经开启，再次单击按钮即可关闭
            </div>
          </div>

          <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
          </div>
          <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
            <div class="panel-body">
              欢迎来到匡盟盟的博客！
            </div>
            <div class="panel-body">
              一个不满平凡的大龄码农
            </div>
            <div class="panel-body">
              © 2018 匡盟盟 All Rights Reserved.
            </div>
          </div>
        </div>


        <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
        <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
        <div class="modal-footer">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>

  <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>




  <!-- Go to www.addthis.com/dashboard to customize your tools -->
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5a9d5b43ece25231"></script>



</body>
<style>
  .test-div {
    width: 300px;
    height: 300px;
    margin: 20px auto;
    border: 1px solid #aaa;
    position: relative;
  }
</style>

<script src="/js/loading.js"></script>
<script>
  function loading7() {
    $('body').loading({
      loadingWidth: 240,
      title: '请稍等!',
      name: 'test',
      discription: '精彩马上就来...',
      direction: 'row',
      type: 'origin',
      originBg: '#71EA71',
      originDivWidth: 30,
      originDivHeight: 30,
      originWidth: 4,
      originHeight: 4,
      smallLoading: false,
      titleColor: '#388E7A',
      loadingBg: '#312923',
      loadingMaskBg: 'rgba(22,22,22,0.2)'
    });
    setTimeout(function() {
      removeLoading('test');
    }, 1000);
  }
</script>

</html>