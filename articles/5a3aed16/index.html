<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">

  <meta http-equiv="X-Frame-Options" content="DENY">
  <title>自然语言处理（NLP）之词向量 | 欢迎来到匡盟盟的博客！</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta name="keywords" content="匡盟盟, Mengmeng Kuang, Blog, 博客" />


  <meta name="google-site-verification" content="3YclHsmiu1_poywYScAg4jt4RGqoHUoIXQJFV5vEZ1I" />


  <meta name="baidu-site-verification" content="d6tIQA0tgL" />


  <meta name="description" content="背景知识 什么是词向量/词嵌入  * 词向量（word embedding）是一个固定长度的实值向量  * 词向量是神经语言模型的副产品。  * 词向量是针对“词”提出的。事实上，也可以针对更细或更粗的粒度来进行推广——比如字向量、句向量、文档向量等  词向量的理解 TODO word2vec 中的数学原理详解（三）背景知识 - CSDN博客   * 在 NLP 任务中，因为机器无法直接理解自然语">
  <meta name="keywords" content="AI,NLP,词向量,自然语言处理">
  <meta property="og:type" content="article">
  <meta property="og:title" content="自然语言处理（NLP）之词向量">
  <meta property="og:url" content="http://www.meng.uno/articles/5a3aed16/index.html">
  <meta property="og:site_name" content="欢迎来到匡盟盟的博客！">
  <meta property="og:description" content="背景知识 什么是词向量/词嵌入  * 词向量（word embedding）是一个固定长度的实值向量  * 词向量是神经语言模型的副产品。  * 词向量是针对“词”提出的。事实上，也可以针对更细或更粗的粒度来进行推广——比如字向量、句向量、文档向量等  词向量的理解 TODO word2vec 中的数学原理详解（三）背景知识 - CSDN博客   * 在 NLP 任务中，因为机器无法直接理解自然语">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180806115820.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180806120054.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180806120247.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180806141616.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180806144052.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180806150816.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180806150727.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180806151406.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180806161221.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180806165417.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180806165750.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180806170618.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180806162119.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180806172158.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180806173111.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180806175337.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180806193111.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180806200601.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/共现矩阵.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180807144435.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180807150747.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180807151212.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/公式_20180807151738.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180807152158.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180807152241.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180807152306.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180807152431.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180807152459.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180807152530.png">
  <meta property="og:image" content="http://www.meng.uno/images/assets/TIM截图20180807094759.png">
  <meta property="og:updated_time" content="2018-08-16T05:49:08.000Z">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="自然语言处理（NLP）之词向量">
  <meta name="twitter:description" content="背景知识 什么是词向量/词嵌入  * 词向量（word embedding）是一个固定长度的实值向量  * 词向量是神经语言模型的副产品。  * 词向量是针对“词”提出的。事实上，也可以针对更细或更粗的粒度来进行推广——比如字向量、句向量、文档向量等  词向量的理解 TODO word2vec 中的数学原理详解（三）背景知识 - CSDN博客   * 在 NLP 任务中，因为机器无法直接理解自然语">
  <meta name="twitter:image" content="http://www.meng.uno/images/assets/公式_20180806115820.png">
  <meta name="twitter:creator" content="@kuangmengmeng">
  <link rel="publisher" href="mengmengkuang">
  <meta property="fb:admins" content="kuangmengmeng">
  <meta property="fb:app_id" content="1559086807462632">

  <link rel="alternate" href="/atom.xml" title="欢迎来到匡盟盟的博客！" type="application/atom+xml">




  <link rel="icon" id="myid" href="/css/images/logo.png">
  <link rel="apple-touch-icon" id="myid" href="/css/images/logo.png">

  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">

  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face {
      font-family: futura-pt;
      src: url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");
      font-weight: 500;
      font-style: normal;
    }

    @font-face {
      font-family: futura-pt;
      src: url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");
      font-weight: 500;
      font-style: normal;
    }

    @font-face {
      font-family: futura-pt;
      src: url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");
      font-weight: lighter;
      font-style: normal;
    }

    @font-face {
      font-family: futura-pt;
      src: url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");
      font-weight: 400;
      font-style: italic;
    }
  </style>
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>
  <script src="/js/bootstrap.js"></script>
  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css">
  <link rel="stylesheet" href="/css/prism-duotone-sea.css">


  <link rel="stylesheet" href="/css/dialog.css">





  <link rel="stylesheet" href="/css/header-post.css">





  <link rel="stylesheet" href="/css/vdonate.css">




  <!-- 
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":["copy","weixin","linkedin","sqq","tsina","twi","fbook","mail"],"bdPic":"","bdStyle":"0","bdSize":"16"},"slide":{"type":"slide","bdImg":"0","bdPos":"left","bdTop":"100"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["qzone","tsina","tqq","renren","weixin"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

 -->





  <link rel="canonical" href="http://www.meng.uno/articles/5a3aed16/" />

  <link rel="stylesheet" href="/css/animate.css">
  <link rel="stylesheet" href="/css/loading.css">

  <script>
    var linkEle = document.getElementById("myid");
    var tmplink = linkEle.href;

    var tmptitle = document.title;
    document.addEventListener('visibilitychange', function() {
      var isHidden = document.hidden;
      if (isHidden) {
        document.title = '喔唷，崩溃啦！';
        linkEle.href = '/css/images/avatar.png';
      } else {
        document.title = tmptitle;
        linkEle.href = tmplink;

      }
    });


    function Hide() {
      var mychar = document.getElementById("homelogoback").style.display = "none";
    }
  </script>

  <script defer src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"></script>




</head>


<body data-spy="scroll" data-target="#toc" data-offset="50">



  <div id="container">
    <div id="wrap">

      <header>
        <div id="allheader" class="navbar navbar-default navbar-static-top" role="navigation">
          <div class="navbar-inner">
            <div class="container">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>


              <a class="brand" style="
                 border-width: 0px;  margin-top: 0px;" href="#" data-toggle="modal" data-target="#myModal">
                  <img class="img-rotate" style="box-shadow:1px 1px 10px 3px #e5e5e5; border-radius: 50%;" width="124px" height="124px" alt="匡盟盟的博客" src="/css/images/logo.png">
              </a>


              <div class="navbar-collapse collapse">
                <ul class="hnav navbar-nav">

                  <li> <a class="main-nav-link" href="/">首页</a> </li>

                  <li> <a class="main-nav-link" href="/archives">所有文章</a> </li>

                  <li> <a class="main-nav-link" href="/categories">分类</a> </li>

                  <li> <a class="main-nav-link" href="/tags">标签</a> </li>

                  <li> <a class="main-nav-link" href="/about">关于我</a> </li>

                  <li> <a class="main-nav-link" href="/comments">留言板</a> </li>

                  <li>
                    <div id="search-form-wrap">

                      <form class="search-form">
                        <input type="text" style="width=0;" class="ins-search-input search-form-input" placeholder="" />
                        <button type="submit" class="search-form-submit"></button>
                      </form>
                      <div class="ins-search">
                        <div class="ins-search-mask"></div>
                        <div class="ins-search-container">
                          <div class="ins-input-wrapper">
                            <input type="text" class="ins-search-input" placeholder="请输入关键词..." />
                            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
                          </div>
                          <div class="ins-section-wrapper">
                            <div class="ins-section-container"></div>
                          </div>
                        </div>
                      </div>
                      <script>
                        (function(window) {
                          var INSIGHT_CONFIG = {
                            TRANSLATION: {
                              POSTS: '文章',
                              PAGES: '页面',
                              CATEGORIES: '分类',
                              TAGS: '标签',
                              UNTITLED: '空标题',
                            },
                            ROOT_URL: '/',
                            CONTENT_URL: '/content.json',
                          };
                          window.INSIGHT_CONFIG = INSIGHT_CONFIG;
                        })(window);
                      </script>
                      <script src="/js/insight.js"></script>

                    </div>
                  </li>

                </ul>
              </div>
            </div>

          </div>
        </div>

      </header>

      <script>
        (function(w, i, d, g, e, t, s) {
          w[d] = w[d] || [];
          t = i.createElement(g);
          t.async = 1;
          t.src = e;
          s = i.getElementsByTagName(g)[0];
          s.parentNode.insertBefore(t, s);
        })(window, document, '_gscq', 'script', '//widgets.getsitecontrol.com/125646/script.js');
      </script>


      <div id="content" class="outer">

        <section id="main" style="float:none;">
          <article id="post-NLP-词向量" style="width: 75%; float:left;" class="article article-type-post" itemscope itemprop="blogPost">
            <div id="articleInner" class="article-inner">


              <header class="article-header">
                <h1 class="thumb" class="article-title" itemprop="name">
                  自然语言处理（NLP）之词向量
                </h1>
              </header>

              <div class="article-meta">
                <a href="/articles/5a3aed16/" class="article-date">
	  <time datetime="2017-07-05T12:09:21.000Z" itemprop="datePublished">2017-07-05</time>
	</a>
                <a class="article-category-link" href="/categories/NLP/">NLP</a>
                <a class="article-views">
	<span id="busuanzi_container_page_pv">
		阅读量<span id="busuanzi_value_page_pv"></span>
	</span>
	<span class="post-count"> | 字数3,858</span>
	<span class="post-count"> | 预计时间16分钟</span>
	</a>
              </div>
              <div class="article-entry" itemprop="articleBody">

                <h1>背景知识</h1>
                <h2 id="什么是词向量-词嵌入">什么是词向量/词嵌入</h2>
                <ul>
                  <li>词向量（word embedding）是一个固定长度的实值向量</li>
                  <li>词向量是神经语言模型的<strong>副产品</strong>。</li>
                  <li>词向量是针对“词”提出的。事实上，也可以针对更细或更粗的粒度来进行推广——比如字向量、句向量、文档向量等</li>
                </ul>
                <h2 id="词向量的理解-todo">词向量的理解 TODO</h2>
                <blockquote>
                  <p><a href="https://blog.csdn.net/itplus/article/details/37969817" target="_blank" rel="noopener">word2vec 中的数学原理详解（三）背景知识</a> - CSDN博客</p>
                </blockquote>
                <ul>
                  <li>在 NLP 任务中，因为机器无法直接理解自然语言，所以首先要做的事情就是将语言<strong>数学化</strong>——词向量就是一种数学化的方式。</li>
                </ul>
                <p><strong>分布式表示</strong> (distributed representation)</p>
                <ul>
                  <li>分布式假设</li>
                  <li>TODO</li>
                  <li>常见的分布式表示方法
                    <ul>
                      <li>潜在语义分析 (Latent Semantic Analysis, LSA)
                        <ul>
                          <li>SVD 分解</li>
                        </ul>
                      </li>
                      <li>隐含狄利克雷分布 (Latent Dirichlet Allocation, LDA)，主题模型</li>
                      <li>神经网络、深度学习</li>
                    </ul>
                  </li>
                </ul>
                <h1>Word2Vec</h1>
                <ul>
                  <li>Word2Vec 本质上也是一个神经语言模型，但是它的目标并不是语言模型本身，而是词向量；因此，其所作的一系列优化，都是为了更快更好的得到词向量</li>
                  <li>Word2Vec 提供了两套模型：<strong>CBOW</strong> 和 <strong>Skip-Gram</strong>(SG)
                    <ul>
                      <li>CBOW 在已知 <code>context(w)</code> 的情况下，预测 <code>w</code></li>
                      <li>SG 在已知 <code>w</code> 的情况下预测 <code>context(w)</code></li>
                    </ul>
                  </li>
                  <li>从训练集的构建方式可以更好的理解和区别 <strong>CBOW</strong> 和 <strong>SG</strong> 模型
                    <ul>
                      <li>
                        <p>每个训练样本为一个二元组 <code>(x, y)</code>，其中 <code>x</code>为特征，<code>y</code>为标签</p>
                        <p>假设上下文窗口的大小 <code>context_window =5</code>，即</p>
                        <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180806115820.png" height=""></div>
                        <p>或者说 <code>skip_window = 2</code>，有 <code>context_window = skip_window*2 + 1</code></p>
                      </li>
                      <li>
                        <p>CBOW 的训练样本为：</p>
                        <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180806120054.png" height=""></div>
                      </li>
                      <li>
                        <p>SG 的训练样本为：</p>
                        <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180806120247.png" height=""></div>
                      </li>
                      <li>
                        <p>一般来说，<code>skip_window &lt;= 10</code></p>
                      </li>
                    </ul>
                  </li>
                  <li>除了两套模型，Word2Vec 还提供了两套优化方案，分别基于 Hierarchical Softmax (层次SoftMax) 和 Negative Sampling (负采样)</li>
                </ul>
                <h2 id="基于层次-softmax-的-cbow-模型">基于层次 SoftMax 的 CBOW 模型</h2>
                <ul>
                  <li>
                    <p>【<strong>输入层</strong>】将 <code>context(w)</code> 中的词映射为 <code>m</code> 维词向量，共 <code>2c</code> 个</p>
                  </li>
                  <li>
                    <p>【<strong>投影层</strong>】将输入层的 <code>2c</code> 个词向量累加求和，得到新的 <code>m</code> 维词向量</p>
                  </li>
                  <li>
                    <p>【<strong>输出层</strong>】输出层对应一棵<strong>哈夫曼树</strong>，以词表中词作为叶子节点，各词的出现频率作为权重——共 <code>N</code> 个叶子节点，<code>N-1</code> 个非叶子节点</p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180806141616.png" height="400"></div>
                    <ul>
                      <li>【输入层】前者使用的是 <code>w</code> 的前 <code>n-1</code> 个词，后者使用 <code>w</code> 两边的词
                        <blockquote>
                          <p>这是后者词向量的性能优于前者的主要原因</p>
                        </blockquote>
                      </li>
                      <li>【投影层】前者通过拼接，后者通过<strong>累加求和</strong></li>
                      <li>【隐藏层】后者无隐藏层</li>
                      <li>【输出层】前者为线性结构，后者为树形结构</li>
                    </ul>
                  </li>
                  <li>
                    <p>模型改进</p>
                    <ul>
                      <li>从对比中可以看出，CBOW 模型的主要改进都是为了<strong>减少计算量</strong>——取消隐藏层、使用<strong>层Softmax</strong>代替基本Softmax</li>
                    </ul>
                  </li>
                </ul>
                <h3 id="层次-softmax-的正向传播">层次 SoftMax 的正向传播</h3>
                <ul>
                  <li>
                    <p>层 Softmax 实际上是把一个超大的多分类问题转化成一系列二分类问题</p>
                  </li>
                  <li>
                    <p>示例：求 <code>P(&quot;足球&quot;|context(&quot;足球&quot;))</code></p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180806144052.png" height="400"></div>
                    <ul>
                      <li>
                        <p>从根节点到“足球”所在的叶子节点，需要经过 4 个分支，每次分支相当于一次<strong>二分类</strong>（逻辑斯蒂回归，二元Softmax）</p>
                        <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180806150816.png" height=""></div>
                        <blockquote>
                          <p>这里遵从原文，将 0 作为正类，1 作为负类</p>
                        </blockquote>
                      </li>
                      <li>
                        <p>而 <code>P(&quot;足球&quot;|context(&quot;足球&quot;))</code> 就是每次分类正确的概率之积，即</p>
                        <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180806150727.png" height=""></div>
                        <blockquote>
                          <p>这里每个非叶子都对应一个参数 <code>θ_i</code></p>
                        </blockquote>
                      </li>
                    </ul>
                  </li>
                </ul>
                <h4 id="为什么层次-softmax-能加速">为什么层次 SoftMax 能加速</h4>
                <ul>
                  <li>Softmax 大部分的计算量在于分母部分，它需要求出所有分量的和</li>
                  <li>而层次 SoftMax 每次只需要计算两个分量，因此极大的提升了速度</li>
                </ul>
                <h3 id="层次-softmax-的反向传播-todo">层次 Softmax 的反向传播 TODO</h3>
                <blockquote>
                  <p><a href="https://blog.csdn.net/itplus/article/details/37969979" target="_blank" rel="noopener">word2vec 中的数学原理详解（四）基于 Hierarchical Softmax 的模型</a> - CSDN博客</p>
                </blockquote>
                <h2 id="基于层次-softmax-的-skip-gram-模型">基于层次 Softmax 的 Skip-gram 模型</h2>
                <ul>
                  <li>
                    <p>这里保留了【投影层】，但实际上只是一个恒等变换</p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180806151406.png" height="400"></div>
                  </li>
                  <li>
                    <p>从模型的角度看：CBOW 与 SG 模型的区别仅在于 <code>x_w</code> 的构造方式不同，前者是 <code>context(w)</code> 的词向量累加；后者就是 <code>w</code> 的词向量</p>
                  </li>
                  <li>
                    <p>虽然 SG 模型用中心词做特征，上下文词做类标，但实际上两者的地位是等价的</p>
                  </li>
                </ul>
                <h2 id="基于负采样的-cbow-和-skip-gram">基于负采样的 CBOW 和 Skip-gram</h2>
                <ul>
                  <li>层次 Softmax 还不够简单，于是提出了基于负采样的方法进一步提升性能</li>
                  <li>负采样（Negative Sampling）是 NCE(Noise Contrastive Estimation) 的简化版本 &gt; <a href="https://blog.csdn.net/littlely_ll/article/details/79252064" target="_blank" rel="noopener">噪音对比估计（NCE）</a> - CSDN博客</li>
                  <li>CBOW 的训练样本是一个 <code>(context(w), w)</code> 二元对；对于给定的 <code>context(w)</code>，<code>w</code> 就是它的正样本，而其他所有词都是负样本。</li>
                  <li>如果不使用<strong>负采样</strong>，即 N-gram 神经语言模型中的做法，就是对整个词表 Softmax 和交叉熵</li>
                  <li>负采样相当于选取所有负例中的一部分作为负样本，从而减少计算量</li>
                  <li>Skip-gram 模型同理</li>
                </ul>
                <h2 id="负采样算法">负采样算法</h2>
                <ul>
                  <li>负采样算法，即对给定的 <code>w</code> ，生成相应负样本的方法</li>
                  <li>最简单的方法是随机采样，但这会产生一点问题，词表中的词出现频率并不相同
                    <ul>
                      <li>如果不是从词表中采样，而是从语料中采样；显然，那些高频词被选为负样本的概率要大于低频词</li>
                      <li>在词表中采样时也应该遵循这个</li>
                    </ul>
                  </li>
                  <li>因此，负采样算法实际上就是一个<strong>带权采样</strong>过程
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180806161221.png" height="200"></div>
                  </li>
                </ul>
                <h3 id="word2vec-中的做法">Word2Vec 中的做法</h3>
                <ul>
                  <li>
                    <p>记</p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180806165417.png" height=""></div>
                  </li>
                  <li>
                    <p>以这 <code>N+1</code> 个点对区间 <code>[0,1]</code> 做非等距切分</p>
                  </li>
                  <li>
                    <p>引入的一个在区间 <code>[0,1]</code> 上的 <code>M</code> 等距切分，其中 <code>M &gt;&gt; N</code></p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180806165750.png" height=""></div>
                    <blockquote>
                      <p>源码中取 <code>M = 10^8</code></p>
                    </blockquote>
                  </li>
                  <li>
                    <p>然后对两个切分做投影，得到映射关系</p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180806170618.png" height=""></div>
                  </li>
                  <li>
                    <p>采样时，每次生成一个 <code>[1, M-1]</code> 之间的整数 <code>i</code>，则 <code>Table(i)</code> 就对应一个样本；<br> 当采样到正例时，跳过
                    </p>
                  </li>
                  <li>
                    <p>特别的，Word2Vec 在计算 <code>len(w)</code> 时做了一些改动——为 <code>count(·)</code> 加了一个<strong>指数</strong></p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180806162119.png" height=""></div>
                  </li>
                </ul>
                <h2 id="一些源码细节">一些源码细节</h2>
                <h3 id="σ-x-的近似计算"><code>σ(x)</code> 的近似计算</h3>
                <ul>
                  <li>
                    <p>类似带权采样的策略，用<strong>查表</strong>来代替计算</p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180806172158.png" height=""></div>
                  </li>
                  <li>
                    <p>具体计算公式如下</p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180806173111.png" height=""></div>
                    <blockquote>
                      <p>因为 <code>σ(x)</code> 函数的饱和性，当 <code>x &lt; -6 || x &gt; 6</code> 时，函数值基本不变了</p>
                    </blockquote>
                  </li>
                </ul>
                <h3 id="低频词的处理">低频词的处理</h3>
                <ul>
                  <li>对于低频词，会设置阈值（默认 5），对于出现频次低于该阈值的词会直接舍弃，同时训练集中也会被删除</li>
                </ul>
                <h3 id="高频词的处理">高频词的处理</h3>
                <ul>
                  <li>高频词提供的信息相对较少，为了提高低频词的词向量质量，有必要对高频词进行限制</li>
                  <li>高频词对应的词向量在训练时，不会发生明显的变化，因此在训练是可以减少对这些词的训练，从而提升速度</li>
                </ul>
                <p><strong>Sub-sampling 技巧</strong></p>
                <ul>
                  <li>源码中使用 Sub-sampling 技巧来解决高频词的问题，能带来 2~10 倍的训练速度提升，同时提高低频词的词向量精度</li>
                  <li>给定一个词频阈值 <code>t</code>，将 <code>w</code> 以 <code>p(w)</code> 的概率舍弃，<code>p(w)</code> 的计算如下
                    <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180806175337.png" height=""></div>
                  </li>
                </ul>
                <p><strong>Word2Vec 中的Sub-sampling</strong></p>
                <ul>
                  <li>显然，Sub-Sampling 只会针对 出现频次大于 <code>t</code> 的词</li>
                  <li>特别的，Word2Vec 使用如下公式计算 <code>p(w)</code>，效果是类似的
                    <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180806193111.png" height=""></div>
                  </li>
                </ul>
                <h3 id="自适应学习率">自适应学习率</h3>
                <ul>
                  <li>
                    <p>预先设置一个初始的学习率 <code>η_0</code>（默认 0.025），每处理完 <code>M</code>（默认 10000）个词，就根据以下公式调整学习率</p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180806200601.png" height=""></div>
                  </li>
                  <li>
                    <p>随着训练的进行，学习率会主键减小，并趋向于 0</p>
                  </li>
                  <li>
                    <p>为了方式学习率过小，Word2Vec 设置了一个阈值 <code>η_min</code>（默认 <code>0.0001 * η_0</code>）；当学习率小于 <code>η_min</code>，则固定为 <code>η_min</code>。</p>
                  </li>
                </ul>
                <h3 id="参数初始化">参数初始化</h3>
                <ul>
                  <li>词向量服从均匀分布 <code>[-0.5/m, 0.5/m]</code>，其中 <code>m</code> 为词向量的维度</li>
                  <li>所有网络参数初始化为 <code>0</code></li>
                </ul>
                <h1>GloVe</h1>
                <h2 id="共现矩阵">共现矩阵</h2>
                <ul>
                  <li>共现矩阵的实现方式
                    <ul>
                      <li>
                        <p>基于文档 - LSA 模型（SVD分解）</p>
                      </li>
                      <li>
                        <p>基于窗口 - 类似 skip-gram 模型中的方法</p>
                        <div align="center"><img src="http://www.meng.uno/images/assets/共现矩阵.png" height="300"></div>
                        <blockquote>
                          <p><code>skip_window = 1</code> 的共现矩阵</p>
                        </blockquote>
                      </li>
                    </ul>
                  </li>
                </ul>
                <h3 id="构架共现矩阵的细节">构架共现矩阵的细节</h3>
                <ul>
                  <li><strong>功能词的处理</strong>
                    <ul>
                      <li>功能词：如 “the”, “he”, “has”, …</li>
                      <li><strong>法1</strong>）直接忽略
                        <ul>
                          <li>在一些分类问题上可以这么做；如果目标是词向量，则不建议使用这种方法</li>
                        </ul>
                      </li>
                      <li><strong>法2</strong>）设置阈值 <code>min(x, t)</code>
                        <ul>
                          <li>其中 <code>x</code> 为功能词语其他词的共现次数，<code>t</code> 为设置的阈值</li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                  <li>可以尝试使用一些方法代替单纯的计数，如<strong>皮尔逊相关系数</strong>，负数记为 0
                    <blockquote>
                      <p>但是似乎没有人这么做</p>
                    </blockquote>
                  </li>
                </ul>
                <h2 id="glove-的基本思想">GloVe 的基本思想</h2>
                <ul>
                  <li>
                    <p>GloVe 模型的是基于<strong>共现矩阵</strong>构建的</p>
                  </li>
                  <li>
                    <p>GloVe 认为共现矩阵可以通过一些统计信息得到词之间的关系，这些关系可以一定程度上表达词的含义</p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180807144435.png" height=""></div>
                    <ul>
                      <li><strong>solid</strong> related to <strong>ice</strong> but not <strong>steam</strong></li>
                      <li><strong>gas</strong> related to <strong>stream</strong> but not <strong>ice</strong></li>
                      <li><strong>water</strong> related to both</li>
                      <li><strong>fashion</strong> relate not to both</li>
                    </ul>
                    <blockquote>
                      <p>说明 TODO</p>
                    </blockquote>
                  </li>
                  <li>
                    <p>GloVe 的基本思想：</p>
                    <ul>
                      <li>假设词向量已知，如果这些词向量通过<strong>某个函数</strong>（目标函数）可以<strong>拟合</strong>共现矩阵中的统计信息，那么可以认为这些词向量也拥有了共现矩阵中蕴含的语义</li>
                      <li>模型的训练过程就是拟合词向量的过程</li>
                    </ul>
                  </li>
                </ul>
                <h2 id="glove-的目标函数">GloVe 的目标函数</h2>
                <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180807150747.png" height=""></div>
                <p>其中</p>
                <ul>
                  <li>
                    <p><code>w_i</code> 和 <code>w_j</code> 为词向量</p>
                  </li>
                  <li>
                    <p><code>x_ij</code> 为 <code>w_i</code> 和 <code>w_j</code> 的共现次数</p>
                  </li>
                  <li>
                    <p><code>f(x)</code> 是一个权重函数，为了限制高频词和防止 <code>x_ij = 0</code></p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180807151212.png" height=""></div>
                    <ul>
                      <li>当 <code>x_ij = 0</code> 时，有
                        <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180807151738.png" height=""></div>
                      </li>
                    </ul>
                  </li>
                </ul>
                <h3 id="glove-目标函数的推导过程">GloVe 目标函数的推导过程</h3>
                <blockquote>
                  <p>以前整理在 OneNote 上的，有时间在整理</p>
                </blockquote>
                <ul>
                  <li>
                    <p>目标函数</p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180807152158.png" height=""></div>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180807152241.png" height=""></div>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180807152306.png" height=""></div>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180807152431.png" height=""></div>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180807152459.png" height=""></div>
                  </li>
                  <li>
                    <p><code>w_i</code> 的权重函数</p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180807152530.png" height=""></div>
                  </li>
                </ul>
                <h2 id="glove-与-word2vec-的区别">GloVe 与 Word2Vec 的区别</h2>
                <ul>
                  <li>
                    <p>Word2Vec 本质上是一个神经网络；<br> Glove 也利用了<strong>反向传播</strong>来更新词向量，但是结构要更简单，所以 GloVe 的速度更快</p>
                  </li>
                  <li>
                    <p>Glove 认为 Word2Vec 对高频词的处理还不够，导致速度慢；GloVe 认为共现矩阵可以解决这个问题</p>
                    <blockquote>
                      <p>实际 Word2Vec 已结有了一些对高频词的措施</p>
                    </blockquote>
                    <ul>
                      <li>从效果上看，虽然 GloVe 的训练速度更快，但是<strong>词向量的性能</strong>在通用性上要弱一些：<br> 在一些任务上表现优于 Word2Vec，但是在更多的任务上要比 Word2Vec 差</li>
                    </ul>
                  </li>
                </ul>
                <h1>FastText</h1>
                <ul>
                  <li>
                    <p>FastText 是从 Word2Vec 的 CBOW 模型演化而来的；<br> 从网络的角度来看，两者的模型基本一致；区别仅在于两者的分类目标不同；
                      <br> 具体来说，FastText 的分类目标是文档的标签，CBOW 则是中心词的标签</p>
                    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180807094759.png" height=""></div>
                  </li>
                  <li>
                    <p>FastText 与 CBOW 的相同点：</p>
                    <ul>
                      <li>包含三层：输入层、隐含层、输出层（Hierarchical Softmax）</li>
                      <li>输入都是多个单词的词向量</li>
                      <li>隐藏层（投影层）都是对多个词向量的叠加平均</li>
                      <li>输出都是一个特定的 target</li>
                      <li>从网络的角度看，两者基本一致</li>
                    </ul>
                  </li>
                  <li>
                    <p>不同点：</p>
                    <ul>
                      <li>CBOW 的输入是中心词两侧<code>skip_window</code>内的上下文词；FastText 除了上下文词外，还包括这些词的字符级 <strong>N-gram 特征</strong></li>
                      <li>CBOW 的输出是中心词的类标，fastText 的输出是句子对应的类标</li>
                    </ul>
                  </li>
                  <li>
                    <p><strong>注意</strong>，字符级 N-gram 只限制在单个词内，以英文为例</p>
                    <figure class="highlight cpp">
                      <table>
                        <tr>
                          <td class="gutter">
                            <pre><span class="line">1</span><br><span class="line">2</span><br></pre>
                          </td>
                          <td class="code">
                            <pre><span class="line"><span class="comment">// 源码中计算 n-grams 的声明，只计算单个词的字符级 n-gram</span></span><br><span class="line">compute_ngrams(word, <span class="keyword">unsigned</span> <span class="keyword">int</span> min_n, <span class="keyword">unsigned</span> <span class="keyword">int</span> max_n);</span><br></pre>
                          </td>
                        </tr>
                      </table>
                    </figure>
                    <figure class="highlight python">
                      <table>
                        <tr>
                          <td class="gutter">
                            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                          </td>
                          <td class="code">
                            <pre><span class="line"><span class="comment"># &gt; https://github.com/vrasneur/pyfasttext#get-the-subwords</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model.args.get(<span class="string">'minn'</span>), model.args.get(<span class="string">'maxn'</span>)</span><br><span class="line">(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># 调用源码的 Python 接口，源码上也会添加 '&lt;' 和 '&gt;'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model.get_all_subwords(<span class="string">'hello'</span>) <span class="comment"># word + subwords from 2 to 4 characters</span></span><br><span class="line">[<span class="string">'hello'</span>, <span class="string">'&lt;h'</span>, <span class="string">'&lt;he'</span>, <span class="string">'&lt;hel'</span>, <span class="string">'he'</span>, <span class="string">'hel'</span>, <span class="string">'hell'</span>, <span class="string">'el'</span>, <span class="string">'ell'</span>, <span class="string">'ello'</span>, <span class="string">'ll'</span>, <span class="string">'llo'</span>, <span class="string">'llo&gt;'</span>, <span class="string">'lo'</span>, <span class="string">'lo&gt;'</span>, <span class="string">'o&gt;'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># model.get_all_subwords('hello world')  # warning</span></span><br></pre>
                          </td>
                        </tr>
                      </table>
                    </figure>
                  </li>
                  <li>
                    <p><strong>值得一提的是</strong>，因为 FastText 使用了字符级的 N-gram 向量作为额外的特征，使其能够对<strong>未登录词</strong>也能输出相应的词向量；<br></p>
                    <p><strong>具体来说</strong>，<strong>未登录词</strong>的词向量等于其 N-gram 向量的叠加</p>
                  </li>
                </ul>
                <h2 id="gensim-models-fasttext-使用示例"><code>gensim.models.FastText</code> 使用示例</h2>
                <ul>
                  <li>
                    <p>构建 FastText 以及获取词向量</p>
                    <figure class="highlight python">
                      <table>
                        <tr>
                          <td class="gutter">
                            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre>
                          </td>
                          <td class="code">
                            <pre><span class="line"><span class="comment"># gensim 示例</span></span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> gensim.test.utils <span class="keyword">import</span> common_texts</span><br><span class="line"><span class="keyword">from</span> gensim.models.keyedvectors <span class="keyword">import</span> FastTextKeyedVectors</span><br><span class="line"><span class="keyword">from</span> gensim.models._utils_any2vec <span class="keyword">import</span> compute_ngrams, ft_hash</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> FastText</span><br><span class="line"><span class="comment"># 构建 FastText 模型</span></span><br><span class="line">sentences = [[<span class="string">"Hello"</span>, <span class="string">"World"</span>, <span class="string">"!"</span>], [<span class="string">"I"</span>, <span class="string">"am"</span>, <span class="string">"huay"</span>, <span class="string">"."</span>]]</span><br><span class="line">min_ngrams, max_ngrams = <span class="number">2</span>, <span class="number">4</span>  <span class="comment"># ngrams 范围</span></span><br><span class="line">model = FastText(sentences, size=<span class="number">5</span>, min_count=<span class="number">1</span>, min_n=min_ngrams, max_n=max_ngrams)</span><br><span class="line"><span class="comment"># 可以通过相同的方式获取每个单词以及任一个 n-gram 的向量</span></span><br><span class="line">print(model.wv[<span class="string">'hello'</span>])</span><br><span class="line">print(model.wv[<span class="string">'&lt;h'</span>])</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[-0.03481839  0.00606661  0.02581969  0.00188777  0.0325358 ]</span></span><br><span class="line"><span class="string">[ 0.04481247 -0.1784363  -0.03192253  0.07162753  0.16744071]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print()</span><br><span class="line"><span class="comment"># 词向量和 n-gram 向量是分开存储的</span></span><br><span class="line">print(len(model.wv.vectors))  <span class="comment"># 7</span></span><br><span class="line">print(len(model.wv.vectors_ngrams))  <span class="comment"># 57</span></span><br><span class="line"><span class="comment"># gensim 好像没有提供直接获取所有 ngrams tokens 的方法</span></span><br><span class="line">print(model.wv.vocab.keys())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">['Hello', 'World', '!', 'I', 'am', 'huay', '.']</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print()</span><br></pre>
                          </td>
                        </tr>
                      </table>
                    </figure>
                  </li>
                </ul>
                <h3 id="获取单个词的-ngrams-表示">获取单个词的 ngrams 表示</h3>
                <ul>
                  <li>
                    <p>利用源码中 <code>compute_ngrams</code> 方法，gensim 提供了该方法的 Python 接口</p>
                    <figure class="highlight python">
                      <table>
                        <tr>
                          <td class="gutter">
                            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre>
                          </td>
                          <td class="code">
                            <pre><span class="line">sum_ngrams = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> sentences:</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> s:</span><br><span class="line">        w = w.lower()</span><br><span class="line">        <span class="comment"># from gensim.models._utils_any2vec import compute_ngrams</span></span><br><span class="line">        ret = compute_ngrams(w, min_ngrams, max_ngrams)  </span><br><span class="line">        print(ret)</span><br><span class="line">        sum_ngrams += len(ret)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">['&lt;h', 'he', 'el', 'll', 'lo', 'o&gt;', '&lt;he', 'hel', 'ell', 'llo', 'lo&gt;', '&lt;hel', 'hell', 'ello', 'llo&gt;']</span></span><br><span class="line"><span class="string">['&lt;w', 'wo', 'or', 'rl', 'ld', 'd&gt;', '&lt;wo', 'wor', 'orl', 'rld', 'ld&gt;', '&lt;wor', 'worl', 'orld', 'rld&gt;']</span></span><br><span class="line"><span class="string">['&lt;!', '!&gt;', '&lt;!&gt;']</span></span><br><span class="line"><span class="string">['&lt;i', 'i&gt;', '&lt;i&gt;']</span></span><br><span class="line"><span class="string">['&lt;a', 'am', 'm&gt;', '&lt;am', 'am&gt;', '&lt;am&gt;']</span></span><br><span class="line"><span class="string">['&lt;h', 'hu', 'ua', 'ay', 'y&gt;', '&lt;hu', 'hua', 'uay', 'ay&gt;', '&lt;hua', 'huay', 'uay&gt;']</span></span><br><span class="line"><span class="string">['&lt;.', '.&gt;', '&lt;.&gt;']</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">assert</span> sum_ngrams == len(model.wv.vectors_ngrams)</span><br><span class="line">print(sum_ngrams)  <span class="comment"># 57</span></span><br><span class="line">print()</span><br></pre>
                          </td>
                        </tr>
                      </table>
                    </figure>
                  </li>
                </ul>
                <h3 id="计算一个未登录词的词向量">计算一个未登录词的词向量</h3>
                <ul>
                  <li>
                    <p>未登录词实际上是已知 n-grams 向量的叠加平均</p>
                    <figure class="highlight python">
                      <table>
                        <tr>
                          <td class="gutter">
                            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre>
                          </td>
                          <td class="code">
                            <pre><span class="line"><span class="comment"># 因为 "a", "aa", "aaa" 中都只含有 "&lt;a" ，所以它们实际上都是 "&lt;a"</span></span><br><span class="line">print(model.wv[<span class="string">"a"</span>])</span><br><span class="line">print(model.wv[<span class="string">"aa"</span>])</span><br><span class="line">print(model.wv[<span class="string">"aaa"</span>])</span><br><span class="line">print(model.wv[<span class="string">"&lt;a"</span>])  </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[ 0.00226487 -0.19139008  0.17918809  0.13084619 -0.1939924 ]</span></span><br><span class="line"><span class="string">[ 0.00226487 -0.19139008  0.17918809  0.13084619 -0.1939924 ]</span></span><br><span class="line"><span class="string">[ 0.00226487 -0.19139008  0.17918809  0.13084619 -0.1939924 ]</span></span><br><span class="line"><span class="string">[ 0.00226487 -0.19139008  0.17918809  0.13084619 -0.1939924 ]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print()</span><br></pre>
                          </td>
                        </tr>
                      </table>
                    </figure>
                  </li>
                </ul>
                <ul>
                  <li>
                    <p>只要未登录词能被已知的 n-grams 组合，就能得到该词的词向量</p>
                    <blockquote>
                      <p><code>gensim.models.keyedvectors.FastTextKeyedVectors.word_vec(token)</code> 的内部实现</p>
                    </blockquote>
                    <figure class="highlight python">
                      <table>
                        <tr>
                          <td class="gutter">
                            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre>
                          </td>
                          <td class="code">
                            <pre><span class="line">word_unk = <span class="string">"aam"</span></span><br><span class="line">ngrams = compute_ngrams(word_unk, min_ngrams, max_ngrams)  <span class="comment"># min_ngrams, max_ngrams = 2, 4</span></span><br><span class="line">word_vec = np.zeros(model.vector_size, dtype=np.float32)</span><br><span class="line">ngrams_found = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> ngram <span class="keyword">in</span> ngrams:</span><br><span class="line">    ngram_hash = ft_hash(ngram) % model.bucket</span><br><span class="line">    <span class="keyword">if</span> ngram_hash <span class="keyword">in</span> model.wv.hash2index:</span><br><span class="line">        word_vec += model.wv.vectors_ngrams[model.wv.hash2index[ngram_hash]]</span><br><span class="line">        ngrams_found += <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> word_vec.any():  <span class="comment">#</span></span><br><span class="line">    word_vec = word_vec / max(<span class="number">1</span>, ngrams_found)</span><br><span class="line"><span class="keyword">else</span>:  <span class="comment"># 如果一个 ngram 都没找到，gensim 会报错；个人认为把 0 向量传出来也可以</span></span><br><span class="line">    <span class="keyword">raise</span> KeyError(<span class="string">'all ngrams for word %s absent from model'</span> % word_unk)</span><br><span class="line">print(word_vec)</span><br><span class="line">print(model.wv[<span class="string">"aam"</span>])</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[ 0.02210762 -0.10488641  0.05512805  0.09150169  0.00725085]</span></span><br><span class="line"><span class="string">[ 0.02210762 -0.10488641  0.05512805  0.09150169  0.00725085]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 如果一个 ngram 都没找到，gensim 会报错</span></span><br><span class="line"><span class="comment">#   其实可以返回一个 0 向量的，它内部实际上是从一个 0 向量开始累加的；</span></span><br><span class="line"><span class="comment">#   但返回时做了一个判断——如果依然是 0 向量，则报错</span></span><br><span class="line"><span class="comment"># print(model.wv['z'])</span></span><br><span class="line"><span class="string">r"""</span></span><br><span class="line"><span class="string">Traceback (most recent call last):</span></span><br><span class="line"><span class="string">  File "D:/gensim/FastText.py", line 53, in &lt;module&gt;</span></span><br><span class="line"><span class="string">    print(model.wv['z'])</span></span><br><span class="line"><span class="string">  File "D:\program\work\Python\Anaconda3\envs\tf\lib\site-packages\gensim\models\keyedvectors.py", line 336, in __getitem__</span></span><br><span class="line"><span class="string">    return self.get_vector(entities)</span></span><br><span class="line"><span class="string">  File "D:\program\work\Python\Anaconda3\envs\tf\lib\site-packages\gensim\models\keyedvectors.py", line 454, in get_vector</span></span><br><span class="line"><span class="string">    return self.word_vec(word)</span></span><br><span class="line"><span class="string">  File "D:\program\work\Python\Anaconda3\envs\tf\lib\site-packages\gensim\models\keyedvectors.py", line 1989, in word_vec</span></span><br><span class="line"><span class="string">    raise KeyError('all ngrams for word %s absent from model' % word)</span></span><br><span class="line"><span class="string">KeyError: 'all ngrams for word z absent from model'</span></span><br><span class="line"><span class="string">"""</span></span><br></pre>
                          </td>
                        </tr>
                      </table>
                    </figure>
                  </li>
                </ul>
                <h1>CharCNN 字向量</h1>
                <ul>
                  <li>CharCNN 的思想是通过字符向量得到词向量</li>
                </ul>
                <blockquote>
                  <p>[1509] <a href="https://arxiv.org/abs/1509.01626" target="_blank" rel="noopener">Character-level Convolutional Networks for Text Classification</a></p>
                </blockquote>
                <h1>其他实践</h1>
                <h2 id="一般-embedding-维度的选择">一般 embedding 维度的选择</h2>
                <blockquote>
                  <p><a href="https://www.tensorflow.org/versions/master/guide/feature_columns#indicator_and_embedding_columns" target="_blank" rel="noopener">Feature Columns</a>  |  TensorFlow</p>
                </blockquote>
                <ul>
                  <li>
                    <p>经验公式 <code>embedding_size = n_categories ** 0.25</code></p>
                  </li>
                  <li>
                    <p>在大型语料上训练的词向量维度通常会设置的更大一些，比如 <code>100~300</code></p>
                    <blockquote>
                      <p>如果根据经验公式，是不需要这么大的，比如 200W 词表的词向量维度只需要 <code>200W ** 0.25 ≈ 37</code></p>
                    </blockquote>
                  </li>
                </ul>
                <p><br><br>本文链接： <a href="http://www.meng.uno/articles/5a3aed16/">http://www.meng.uno/articles/5a3aed16/</a> 欢迎转载！</p>

              </div>
              <footer class="article-footer">

                <!-- Go to www.addthis.com/dashboard to customize your tools -->

                <div id="wpac-rating" style="margin: 10px auto; text-align:center;"></div>
                <script type="text/javascript">
                  wpac_init = window.wpac_init || [];
                  wpac_init.push({
                    widget: 'Rating',
                    id: 9986
                  });
                  (function() {
                    if ('WIDGETPACK_LOADED' in window) return;
                    WIDGETPACK_LOADED = true;
                    var mc = document.createElement('script');
                    mc.type = 'text/javascript';
                    mc.async = true;
                    mc.src = 'https://embed.widgetpack.com/widget.js';
                    var s = document.getElementsByTagName('script')[0];
                    s.parentNode.insertBefore(mc, s.nextSibling);
                  })();
                </script>

                <div id="donation_div"></div>

                <script src="/js/vdonate.js"></script>
                <script>
                  var a = new Donate({
                    title: '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作!', // 可选参数，打赏标题
                    btnText: '打赏支持', // 可选参数，打赏按钮文字
                    el: document.getElementById('donation_div'),
                    wechatImage: 'http://www.meng.uno/money/wechat.JPG',
                    alipayImage: 'http://www.meng.uno/money/alipay.JPG'
                  });
                </script>


                <div id="comment">
                  <!-- 来必力City版安装代码 -->
                  <div id="lv-container" data-id="city" data-uid="MTAyMC8zNDA2OC8xMDYwNg==">
                    <script type="text/javascript">
                      (function(d, s) {
                        var j, e = d.getElementsByTagName(s)[0];

                        if (typeof LivereTower === 'function') {
                          return;
                        }

                        j = d.createElement(s);
                        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                        j.async = true;

                        e.parentNode.insertBefore(j, e);
                      })(document, 'script');
                    </script>
                    <noscript>为正常使用评论功能请激活JavaScript</noscript>
                  </div>
                  <!-- City版安装代码已完成 -->
                </div>

                <ul class="article-tag-list">
                  <li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li>
                  <li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li>
                  <li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/自然语言处理/">自然语言处理</a></li>
                  <li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/词向量/">词向量</a></li>
                </ul>
              </footer>
            </div>

            <nav id="article-nav">

              <a href="/articles/81b224e1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          中断
        
      </div>
    </a>


              <a href="/articles/41311c08/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">静态链接</div>
    </a>

            </nav>

          </article>

          <!-- Table of Contents -->

          <aside id="toc-sidebar">
            <div id="toc" class="toc-article">
              <strong class="toc-title">目录导航</strong>

              <ol class="nav">
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">1.</span> <span class="nav-text">背景知识</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#什么是词向量-词嵌入"><span class="nav-number">1.1.</span> <span class="nav-text">什么是词向量/词嵌入</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#词向量的理解-todo"><span class="nav-number">1.2.</span> <span class="nav-text">词向量的理解 TODO</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">2.</span> <span class="nav-text">Word2Vec</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#基于层次-softmax-的-cbow-模型"><span class="nav-number">2.1.</span> <span class="nav-text">基于层次 SoftMax 的 CBOW 模型</span></a>
                      <ol class="nav-child">
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#层次-softmax-的正向传播"><span class="nav-number">2.1.1.</span> <span class="nav-text">层次 SoftMax 的正向传播</span></a>
                          <ol class="nav-child">
                            <li class="nav-item nav-level-4"><a class="nav-link" href="#为什么层次-softmax-能加速"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">为什么层次 SoftMax 能加速</span></a></li>
                          </ol>
                        </li>
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#层次-softmax-的反向传播-todo"><span class="nav-number">2.1.2.</span> <span class="nav-text">层次 Softmax 的反向传播 TODO</span></a></li>
                      </ol>
                    </li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#基于层次-softmax-的-skip-gram-模型"><span class="nav-number">2.2.</span> <span class="nav-text">基于层次 Softmax 的 Skip-gram 模型</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#基于负采样的-cbow-和-skip-gram"><span class="nav-number">2.3.</span> <span class="nav-text">基于负采样的 CBOW 和 Skip-gram</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#负采样算法"><span class="nav-number">2.4.</span> <span class="nav-text">负采样算法</span></a>
                      <ol class="nav-child">
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#word2vec-中的做法"><span class="nav-number">2.4.1.</span> <span class="nav-text">Word2Vec 中的做法</span></a></li>
                      </ol>
                    </li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#一些源码细节"><span class="nav-number">2.5.</span> <span class="nav-text">一些源码细节</span></a>
                      <ol class="nav-child">
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#σ-x-的近似计算"><span class="nav-number">2.5.1.</span> <span class="nav-text">σ(x) 的近似计算</span></a></li>
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#低频词的处理"><span class="nav-number">2.5.2.</span> <span class="nav-text">低频词的处理</span></a></li>
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#高频词的处理"><span class="nav-number">2.5.3.</span> <span class="nav-text">高频词的处理</span></a></li>
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#自适应学习率"><span class="nav-number">2.5.4.</span> <span class="nav-text">自适应学习率</span></a></li>
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#参数初始化"><span class="nav-number">2.5.5.</span> <span class="nav-text">参数初始化</span></a></li>
                      </ol>
                    </li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">3.</span> <span class="nav-text">GloVe</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#共现矩阵"><span class="nav-number">3.1.</span> <span class="nav-text">共现矩阵</span></a>
                      <ol class="nav-child">
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#构架共现矩阵的细节"><span class="nav-number">3.1.1.</span> <span class="nav-text">构架共现矩阵的细节</span></a></li>
                      </ol>
                    </li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#glove-的基本思想"><span class="nav-number">3.2.</span> <span class="nav-text">GloVe 的基本思想</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#glove-的目标函数"><span class="nav-number">3.3.</span> <span class="nav-text">GloVe 的目标函数</span></a>
                      <ol class="nav-child">
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#glove-目标函数的推导过程"><span class="nav-number">3.3.1.</span> <span class="nav-text">GloVe 目标函数的推导过程</span></a></li>
                      </ol>
                    </li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#glove-与-word2vec-的区别"><span class="nav-number">3.4.</span> <span class="nav-text">GloVe 与 Word2Vec 的区别</span></a></li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">4.</span> <span class="nav-text">FastText</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#gensim-models-fasttext-使用示例"><span class="nav-number">4.1.</span> <span class="nav-text">gensim.models.FastText 使用示例</span></a>
                      <ol class="nav-child">
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#获取单个词的-ngrams-表示"><span class="nav-number">4.1.1.</span> <span class="nav-text">获取单个词的 ngrams 表示</span></a></li>
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#计算一个未登录词的词向量"><span class="nav-number">4.1.2.</span> <span class="nav-text">计算一个未登录词的词向量</span></a></li>
                      </ol>
                    </li>
                  </ol>
                </li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">5.</span> <span class="nav-text">CharCNN 字向量</span></a></li>
                <li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">6.</span> <span class="nav-text">其他实践</span></a>
                  <ol class="nav-child">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#一般-embedding-维度的选择"><span class="nav-number">6.1.</span> <span class="nav-text">一般 embedding 维度的选择</span></a></li>
                  </ol>
                </li>
              </ol>

            </div>
          </aside>
        </section>

      </div>


      <footer id="footer">


        <div class="container">
          <div class="row">
            <p id="copyRightEn">&copy; 2018.02.08 - 2019.03.24 <a href="http://www.meng.uno/">Mengmeng Kuang</a>&nbsp;<i class="fas fa-cogs"></i> 保留所有权利！</p>

            <p class="busuanzi_uv">

              访客数 : <span id="busuanzi_value_site_uv"></span> | 访问量 : <span id="busuanzi_value_site_pv"></span>

            </p>


            <p id="hitokoto">:D 获取中...</p>
            <!-- 以下写法，选取一种即可 -->

            <!-- 现代写法，推荐 -->
            <!-- 兼容低版本浏览器 (包括 IE)，可移除 -->
            <script src="https://cdn.bootcss.com/bluebird/3.5.1/bluebird.core.min.js"></script>
            <script src="https://cdn.bootcss.com/fetch/2.0.3/fetch.min.js"></script>
            <!--End-->
            <script>
              fetch('https://v1.hitokoto.cn')
                .then(function(res) {
                  return res.json();
                })
                .then(function(data) {
                  var hitokoto = document.getElementById('hitokoto');
                  hitokoto.innerText = data.hitokoto;
                })
                .catch(function(err) {
                  console.error(err);
                })
            </script>

            <a href="http://webscan.360.cn/index/checkwebsite/url/www.meng.uno"><img border="0" height=27px width=74px src="/css/images/webscan.png"/></a>
            <!-- <img border="0" height=27px width=109px style="background-color:white;"src="/css/images/kaba.png"/> -->
            <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" height=27px style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a>
            <script type="text/javascript">
              var locationUrl = escape(document.location.href);
              document.write(unescape("%3Cscript") + " height='27px' width='74px' charset='utf-8' src='http://union.rising.com.cn//InfoManage/TrojanInspect.aspx?p1=XNk3xHG5v8uxFHYb4KaGpnyWjJlbHp7K&p2=RqCQt7iMKRw=&p3=XNk3xHG5v8vv3Z1xqd/V8w==&url=" + locationUrl + "' type='text/javascript'" + unescape("%3E%3C/script%3E"));
            </script>
          </div>

        </div>
      </footer>


      <!-- min height -->

      <script>
        var wrapdiv = document.getElementById("wrap");
        var contentdiv = document.getElementById("content");
        var allheader = document.getElementById("allheader");

        wrapdiv.style.minHeight = document.body.offsetHeight + "px";
        if (allheader != null) {
          contentdiv.style.minHeight = document.body.offsetHeight - allheader.offsetHeight - document.getElementById("footer").offsetHeight + "px";
        } else {
          contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("footer").offsetHeight + "px";
        }
      </script>

      <script>
        (function() {
          var src = (document.location.protocol == "http:") ? "http://js.passport.qihucdn.com/11.0.1.js?5d3bca9f7d6a95532f3ebb56e3c6bf11" : "https://jspassport.ssl.qhimg.com/11.0.1.js?5d3bca9f7d6a95532f3ebb56e3c6bf11";
          document.write('<script src="' + src + '" id="sozz"><\/script>');
        })();
      </script>
    </div>
    <!-- <nav id="mobile-nav">

  <a href="/" class="mobile-nav-link">Home</a>

  <a href="/archives" class="mobile-nav-link">Archives</a>

  <a href="/categories" class="mobile-nav-link">Categories</a>

  <a href="/tags" class="mobile-nav-link">Tags</a>

  <a href="/about" class="mobile-nav-link">About</a>

  <a href="/comments" class="mobile-nav-link">Comments</a>

</nav> -->
    <!-- mathjax config similar to math.stackexchange -->

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(), i; for(i=0; i
      < all.length; i +=1 ) { all[i].SourceElement().parentNode.className +=' has-jax' ; } }); </script>

        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>


        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
        <script src="/fancybox/jquery.fancybox.pack.js"></script>


        <script src="/js/scripts.js"></script>




        <script src="/js/dialog.js"></script>


        <!-- Google Analytics -->
        <script type="text/javascript">
          (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
              (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
              m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
          })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

          ga('create', 'UA-113947925-1', 'auto');
          ga('send', 'pageview');
        </script>
        <!-- End Google Analytics -->


        <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
        </script>
  </div>

  <div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <h2 class="modal-title" id="myModalLabel">设置</h2>
        </div>
        <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
        <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
        <div class="modal-body">
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" onclick="javascript:setFontSize();" aria-expanded="true" aria-controls="collapseOne">
              正文字号大小
            </a>
          </div>
          <div id="collapseOne" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingOne">
            <div class="panel-body">
              您已调整页面字体大小
            </div>
          </div>
          <div style="margin:6px;">
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" onclick="javascript:setBackground();" aria-expanded="true" aria-controls="collapseTwo">
              夜间护眼模式
            </a>
          </div>
          <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
            <div class="panel-body">
              夜间模式已经开启，再次单击按钮即可关闭
            </div>
          </div>

          <div>
            <a data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="true" aria-controls="collapseThree">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关 于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a>
          </div>
          <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
            <div class="panel-body">
              欢迎来到匡盟盟的博客！
            </div>
            <div class="panel-body">
              A first year MPhil student in CS at HKU
            </div>
            <div class="panel-body">
              © 2019 Mengmeng Kuang All Rights Reserved.
            </div>
          </div>
        </div>


        <hr style="margin-top:0px; margin-bottom:0px; width:80%; border-top: 1px solid #000;">
        <hr style="margin-top:2px; margin-bottom:0px; width:80%; border-top: 3px solid #000;">
        <div class="modal-footer">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>

  <a id="menu-switch"><i class="fa fa-bars fa-lg"></i></a>



</body>
<style>
  .test-div {
    width: 300px;
    height: 300px;
    margin: 20px auto;
    border: 1px solid #aaa;
    position: relative;
  }
</style>

<script src="/js/loading.js"></script>
<script>
  function loading7() {
    $('body').loading({
      loadingWidth: 240,
      title: '请稍等!',
      name: 'test',
      discription: '精彩马上就来...',
      direction: 'row',
      type: 'origin',
      originBg: '#71EA71',
      originDivWidth: 30,
      originDivHeight: 30,
      originWidth: 4,
      originHeight: 4,
      smallLoading: false,
      titleColor: '#388E7A',
      loadingBg: '#312923',
      loadingMaskBg: 'rgba(22,22,22,0.2)'
    });
    setTimeout(function() {
      removeLoading('test');
    }, 1000);
  }
</script>

</html>