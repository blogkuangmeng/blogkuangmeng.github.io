<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>欢迎来到匡盟盟的博客！</title>
  
  <subtitle>Colyn 崛起正当时！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.meng.uno/"/>
  <updated>2018-10-27T15:11:04.000Z</updated>
  <id>http://www.meng.uno/</id>
  
  <author>
    <name>匡盟盟</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>FastSP: linear time calculation of alignment accuracy</title>
    <link href="http://www.meng.uno/articles/bd4539a2/"/>
    <id>http://www.meng.uno/articles/bd4539a2/</id>
    <published>2018-10-27T12:31:08.000Z</published>
    <updated>2018-10-27T15:11:04.000Z</updated>
    
    <content type="html"><![CDATA[<h1>A General Observation</h1><p><img src="http://www.meng.uno/images/fastsp/1.jpg" alt="Observation"></p><p>From the result, we could see that:</p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>      </td>      <td class="code">        <pre><span class="line">SP-Score: 19/26 = 0.7307…</span><br><span class="line">Modeler: 19/27 = 0.7037…</span><br><span class="line">SP-FN: (26 - 19)/26 = 0.2692….</span><br><span class="line">SP-FP: (27 - 19)/27 = 0.2962….</span><br><span class="line">TC: 3/8 = 0.375</span><br></pre>      </td>    </tr>  </table></figure><h1>Approach</h1><p>From the last section, we know our propose is to calculate:</p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>      </td>      <td class="code">        <pre><span class="line">1. Number of shared homologies.</span><br><span class="line">2. Number of homologies in the reference alignment.</span><br><span class="line">3. Number of homologies in the estimated alignment.</span><br><span class="line">4. Number of correctly aligned columns.</span><br><span class="line">5. Number of aligned columns in the reference alignment.</span><br></pre>      </td>    </tr>  </table></figure><p>At the same time, we have a general thinking which is:</p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>      </td>      <td class="code">        <pre><span class="line">2,3 —&gt; 1</span><br><span class="line">5 is the easiest</span><br><span class="line">1 — &gt; 4</span><br></pre>      </td>    </tr>  </table></figure><p>First of all, it gives these definitions:</p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>      </td>      <td class="code">        <pre><span class="line">Si represent the i-th sequence in the alignment.</span><br><span class="line">Ai represent the i-th alignment.</span><br><span class="line">Ni,j represent the j-th site in Si.</span><br></pre>      </td>    </tr>  </table></figure><p>I will give you an example:</p><p><img src="http://www.meng.uno/images/fastsp/2.jpg" alt="Example"></p><p>Then, we could know these (Explaining an example):</p><p><img src="http://www.meng.uno/images/fastsp/3.jpg" alt="Approach"></p><h1>Algorithm</h1><p>This section is for programming.</p><p>Let’s look at some difinitions first:</p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>      </td>      <td class="code">        <pre><span class="line">n represent the number of sequences in the alignment.</span><br><span class="line">k represent the biggest length of sequences in the alignment.</span><br><span class="line">S[i,j] represent a n•k matrix which equals (a, b) means Ni,j appears in site a for the reference alignment and in site b for the estimated alignment.</span><br><span class="line">bx represent the number of non-gapped entries in the x-th site.</span><br><span class="line">mi represent the number of elements in the i-th equivalence class.</span><br><span class="line">Nx represent the number of homologies in the estimated alignment that are shared with the x-th site in the reference alignment.   </span><br><span class="line">hi represent the number of homologous pairs in alignment Ai.</span><br></pre>      </td>    </tr>  </table></figure><p>Explaining them by this example:</p><p><img src="http://www.meng.uno/images/fastsp/4.jpg" alt="Example"></p><p>The pseudo code is like this:</p><p><img src="http://www.meng.uno/images/fastsp/5.jpg" alt="Pseudo Code"></p><p>Make a mapping:</p><p><img src="http://www.meng.uno/images/fastsp/6.jpg" alt="Mapping"></p><p>The result could be:</p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>      </td>      <td class="code">        <pre><span class="line">SP-Score: N/h1</span><br><span class="line">Modeler: N/h2</span><br><span class="line">TC: cor_num/k</span><br></pre>      </td>    </tr>  </table></figure><h1>Evaluation</h1><p>Calculating the matrix S: <code>O(nk)</code></p><p>Calculating combination number: <code>O(1)</code></p><p>Calculating each Nx: <code>O(n)</code></p><p>As for the FOR loop: <code>O(nk)</code></p><p><strong>So:</strong></p><p>The time complexity is <code>O(n•k)</code>.</p><p>The space complexity is <code>O(n•k)</code>.</p><p>Comparing to the other programs:</p><p><img src="http://www.meng.uno/images/fastsp/7.jpg" alt="evaluation"></p><p><br><br>本文链接： <a href="http://www.meng.uno/articles/bd4539a2/">http://www.meng.uno/articles/bd4539a2/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      A General Observation


From the result, we could see that:

1
2
3
4
5


SP-Score: 19/26 = 0.7307…
Modeler: 19/27 = 0.7037…
SP-FN: (26 - 19)/26 = 0.2692….
SP-FP: (27 - 19)/27 = 0.2962….
TC: 3/8 = 0.375


Approach
From the last section, we know our propose is to calculate:

1
2
3
4
5


1. Number of s
    
    </summary>
    
      <category term="Bioinformatics" scheme="http://www.meng.uno/categories/Bioinformatics/"/>
    
    
      <category term="MSA" scheme="http://www.meng.uno/tags/MSA/"/>
    
      <category term="Bioinformatics" scheme="http://www.meng.uno/tags/Bioinformatics/"/>
    
      <category term="Accuracy" scheme="http://www.meng.uno/tags/Accuracy/"/>
    
  </entry>
  
  <entry>
    <title>Implementation details of TensorFlow</title>
    <link href="http://www.meng.uno/articles/717ad116/"/>
    <id>http://www.meng.uno/articles/717ad116/</id>
    <published>2018-10-02T02:45:47.000Z</published>
    <updated>2018-10-02T03:04:39.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Brief introduction to TensorFlow</h1><ul>  <li>Open source</li>  <li>A second-generation machine learning system</li>  <li>Developed by Google</li>  <li>Improving flexibility and portability, speed and scalability</li>  <li>A framework for implementing and executing machine learning algorithms</li>  <li>In the form of a tensor flowing over a Graph</li></ul><blockquote>  <p>Stars of open source Deep Learning platforms in GitHub</p></blockquote><p><img src="http://www.meng.uno/images/tensorflow/stars.png" alt="Stars on GitHub"></p><h1>The architecture of TensorFlow</h1><p><img src="http://www.meng.uno/images/tensorflow/arch.png" alt="architecture"></p><ul>  <li>    <p><strong>Front-end:</strong> Provide programming model, responsible for the construction of computational graphs, Python, C++ and other language support.</p>  </li>  <li>    <p><strong>Back-end:</strong> Provide the runtime environment, responsible for executing the calculation diagram, and using C++.</p>  </li></ul><h1>Code directory organization structure</h1><p><img src="http://www.meng.uno/images/tensorflow/code.png" alt="Code"></p><ul>  <li><strong>graph:</strong> Calculate flow graph related operations, such as construct, partition, optimize, execute, etc.</li>  <li><strong>kernels:</strong> Opkernels, such as matmul, conv2d, argmax, batch_norm, etc.</li>  <li><strong>ops:</strong> basic operations, gradient operation, IO related ops, control flow and data flow operation.</li>  <li><strong>eigen3:</strong> eigen matrix operation library, TensorFlow foundation operations’ call.</li></ul><h1>TensorFlow programming mode</h1><ul>  <li>TensorFlow uses <strong>symbolic programming</strong>.</li>  <li>Symbolic programming abstracts the calculation process into a graph, and all input nodes, operation nodes and output nodes are symbolized.</li>  <li>Symbolic programming is more efficient in memory and computation.</li>  <li>Symbolic programming programs either explicitly or implicitly contain compilation steps, wrapping previously defined computational diagrams into callable functions, whereas the actual calculation occurs after compilation.</li></ul><h1>Basic concepts of TensorFlow</h1><ul>  <li>Use Graph to represent the calculation process.</li>  <li>Execution diagram in Session.</li>  <li>Using Tensor to represent data.</li>  <li>Using Variable to maintain state.</li>  <li>Use Feed and Fetch to assign or extract data from any operation.</li></ul><blockquote>  <p>Graph is a description of the computation process and needs to be run in Session.</p>  <p>TensorFlow provides a Feed mechanism to import data from outside, in addition to using Variable and Constant to import data.</p>  <p>Tensor, that is, any dimension of data, one-dimensional, two-dimensional, three-dimensional, four-dimensional data collectively known as tensor. TensorFlow refers to keeping data nodes unchanged and allowing data to flow.</p></blockquote><p><strong>An Example:</strong></p><p><img src="http://www.meng.uno/images/tensorflow/ex.png" alt="Example"></p><h1>TensorFlow implementation process</h1><h2 id="the-construction-of-a-graph">The construction of a graph</h2><p>Creating a graph to represent and train the neural network in this phase.</p><p><img src="http://www.meng.uno/images/tensorflow/graph.png" alt="Graph"></p><h2 id="the-execution-of-the-graph">The execution of the graph</h2><p>The training operations in the diagram is executed repeatedly at this stage.</p><h1>Brief summary of TensorFlow</h1><p>TensorFlow is a programming system that represents computation as a graph. The nodes in the graph are called ops (operation). A ops uses 0 or more Tensors to generate 0 or more Tensors by performing some operations. A Tensor is a multidimensional array.  For example, you can represent a batch of images as a four-dimensional array [batch, height, width, channels], with floating-point values.</p><p>TensorFlow uses the tensor data structure (which is actually a multidimensional data) to represent all the data and pass it between the nodes in the graph calculation. A tensor has a fixed type, level, and size, and you can refer to Rank, Shape, and Type  for a deeper understanding of these concepts.</p><p><br><br>本文链接： <a href="http://www.meng.uno/articles/717ad116/">http://www.meng.uno/articles/717ad116/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      Brief introduction to TensorFlow
 * Open source
 * A second-generation machine learning system
 * Developed by Google
 * Improving flexibility and portability, speed and scalability
 * A framework for implementing and executing machine learning algorithms
 * In the form of a tensor flowing over a Gr
    
    </summary>
    
      <category term="DeepLearning" scheme="http://www.meng.uno/categories/DeepLearning/"/>
    
    
      <category term="AI" scheme="http://www.meng.uno/tags/AI/"/>
    
      <category term="Deep Learning" scheme="http://www.meng.uno/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://www.meng.uno/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>静态前端页面向静态前端页面跳转并执行AJAX操作将数据写入跳入界面</title>
    <link href="http://www.meng.uno/articles/2b207973/"/>
    <id>http://www.meng.uno/articles/2b207973/</id>
    <published>2018-08-15T14:09:21.000Z</published>
    <updated>2018-08-15T14:27:45.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>  <p>在暑假进行的项目 <a href="https://github.com/kuangmeng/MedicalTextInfo" target="_blank" rel="noopener">医疗文本处理平台</a> 中，我需要将队友的Ajax写好的功能整合到我的界面上，因为他之前是一个界面，而我想做成像搜索引擎那种，在一个页面上输入搜索词，跳转到另一个页面显示结果（后跳转界面还可以继续通过Ajax获得新的搜索结果）</p></blockquote><p>在这里将前一个页面命名为A，后一个为B（什么文件格式不重要，只要是静态页面就成）。</p><h1>A中的JavaScript代码</h1><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre>      </td>      <td class="code">        <pre><span class="line">&lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">function jumpOnClick(flag) &#123;</span><br><span class="line">url = &quot;section3_2.jsp?text=&quot; + encodeURIComponent(document.getElementById(&apos;search&apos;).value) + &quot;&amp;flag=&quot; + flag;</span><br><span class="line">if(document.getElementById(&apos;search&apos;).value.match(&quot;\\s+&quot;) || document.getElementById(&apos;search&apos;).value == null || document.getElementById(&apos;search&apos;).value == &quot;&quot;)&#123;</span><br><span class="line">alert(&quot;请输入症状或问题后点击相应查询按钮！&quot;);</span><br><span class="line">return;</span><br><span class="line">&#125;</span><br><span class="line">//网页跳转</span><br><span class="line">location.href = url;</span><br><span class="line"></span><br><span class="line">window.event.returnValue=false;</span><br><span class="line">&#125;</span><br><span class="line">&lt;/script&gt;</span><br></pre>      </td>    </tr>  </table></figure><p>在A中，我将调用放到了按钮的onclick中。</p><h1>B中的JavaScript代码</h1><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre>      </td>      <td class="code">        <pre><span class="line">&lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">　　function GetUrlParam() &#123;</span><br><span class="line">　　　　var url = document.location.toString();</span><br><span class="line">　　　　var arrObj = url.split(&quot;?&quot;);</span><br><span class="line">var text,flag;</span><br><span class="line">　　　　if (arrObj.length &gt; 1) &#123;</span><br><span class="line">　　　　　　var arrPara = arrObj[1].split(&quot;&amp;&quot;);</span><br><span class="line">　　　　　　var arr;</span><br><span class="line">　　　　　　for (var i = 0; i &lt; arrPara.length; i++) &#123;</span><br><span class="line">　　　　　　　　arr = arrPara[i].split(&quot;=&quot;);</span><br><span class="line">　　　　　　　　if (arr != null &amp;&amp; arr[0] == &quot;text&quot;) &#123;</span><br><span class="line">　　　　　　　　　　text = decodeURIComponent(arr[1]);</span><br><span class="line">   flag = 0;</span><br><span class="line">   if(text == &quot;&quot; || text == null)&#123;</span><br><span class="line">   return;</span><br><span class="line">   &#125;</span><br><span class="line">    var psel = document.getElementById(&quot;kw&quot;);</span><br><span class="line">                    psel.value = text; //设置</span><br><span class="line">　　　　　　　　&#125;else if(arr != null &amp;&amp; arr[0] == &quot;flag&quot;)&#123;</span><br><span class="line">  flag = arr[1];</span><br><span class="line">  &#125;</span><br><span class="line">　　　　　　&#125;</span><br><span class="line">if(flag == 1)&#123;</span><br><span class="line">searchOnClick(text);</span><br><span class="line">&#125;else if(flag == 2)&#123;</span><br><span class="line">search2OnClick(text);</span><br><span class="line">&#125;</span><br><span class="line">　　　　&#125;</span><br><span class="line">　　&#125;</span><br><span class="line">&lt;/script&gt;</span><br></pre>      </td>    </tr>  </table></figure><p>在B中，我将调用放到了body的onload中。</p><blockquote>  <p>至此完成上述功能，并且保证了在后跳入页面上刷新时，不会因为保留了跳入内容而无法刷新的情况。</p></blockquote><p><br><br>本文链接： <a href="http://www.meng.uno/articles/2b207973/">http://www.meng.uno/articles/2b207973/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      在暑假进行的项目 医疗文本处理平台 中，我需要将队友的Ajax写好的功能整合到我的界面上，因为他之前是一个界面，而我想做成像搜索引擎那种，在一个页面上输入搜索词，跳转到另一个页面显示结果（后跳转界面还可以继续通过Ajax获得新的搜索结果）

在这里将前一个页面命名为A，后一个为B（什么文件格式不重要，只要是静态页面就成）。

A中的JavaScript代码
1
2
3
4
5
6
7
8
9
10
11
12
13


&lt;script type=&quot;text/javascript&quot;&gt;
			function jumpOnClick(flag) {
					url = &quot;section3_2.j
    
    </summary>
    
      <category term="Java开发Tips" scheme="http://www.meng.uno/categories/Java%E5%BC%80%E5%8F%91Tips/"/>
    
    
      <category term="Java" scheme="http://www.meng.uno/tags/Java/"/>
    
      <category term="前端" scheme="http://www.meng.uno/tags/%E5%89%8D%E7%AB%AF/"/>
    
      <category term="Web" scheme="http://www.meng.uno/tags/Web/"/>
    
      <category term="Ajax" scheme="http://www.meng.uno/tags/Ajax/"/>
    
  </entry>
  
  <entry>
    <title>知识图谱（Knowledge Graph）</title>
    <link href="http://www.meng.uno/articles/349dc05d/"/>
    <id>http://www.meng.uno/articles/349dc05d/</id>
    <published>2018-08-07T07:05:54.000Z</published>
    <updated>2018-08-07T07:14:42.000Z</updated>
    
    <content type="html"><![CDATA[<h1>简介</h1><p>近两年来，随着Linking Open Data等项目的全面展开，语义Web数据源的数量激增，大量RDF数据被发布。互联网正从仅包含网页和网页之间超链接的文档万维网(Document Web)转变成包含大量描述各种实体和实体之间丰富关系的数据万维网(Data Web)。在这个背景下，Google、百度和搜狗等搜索引擎公司纷纷以此为基础构建知识图谱，分别为Knowledge Graph、知心和知立方，来改进搜索质量，从而拉开了语义搜索的序幕。下面我将从以下几个方面来介绍知识图谱：知识图谱的表示和在搜索中的展现形式，知识图谱的构建和知识图谱在搜索中的应用等，从而让大家有机会了解其内部的技术实现和各种挑战。</p><h2 id="知识图谱的表示和在搜索中的展现形式">知识图谱的表示和在搜索中的展现形式</h2><p>正如Google的辛格博士在介绍知识图谱时提到的：“The world is not made of strings , but is made of things.”，知识图谱旨在描述真实世界中存在的各种实体或概念。其中，每个实体或概念用一个全局唯一确定的ID来标识，称为它们的标识符(identifier)。每个属性-值对(attribute-value pair，又称AVP)用来刻画实体的内在特性，而关系(relation)用来连接两个实体，刻画它们之间的关联。知识图谱亦可被看作是一张巨大的图，图中的节点表示实体或概念，而图中的边则由属性或关系构成。上述图模型可用W3C提出的资源描述框架RDF[2]  或属性图(property graph)[3] 来表示。知识图谱率先由Google提出，以提高其搜索的质量。</p><p>为了更好地理解知识图谱，我们先来看一下其在搜索中的展现形式，即知识卡片(又称Knowledge Card)。知识卡片旨在为用户提供更多与搜索内容相关的信息。更具体地说，知识卡片为用户查询中所包含的实体或返回的答案提供详细的结构化摘要。从某种意义来说，它是特定于查询(query specific)的知识图谱。例如，当在搜索引擎中输入“姚明”作为关键词时，我们发现搜索结果页面的右侧原先用于置放广告的地方被知识卡片所取代。广告被移至左上角，而广告下面则显示的是传统的搜索结果，即匹配关键词的文档列表。这个布局上的微调也预示着各大搜索引擎在提高用户体验和直接返回答案方面的决心。</p><h2 id="相关名词解释">相关名词解释</h2><ol>  <li>    <p>Knowledge Base：通常翻译为“知识库”。知识库是人工智能的经典概念之一。最早是作为专家系统（Expert System）的组成部分，用于支持推理。知识库中的知识有很多种不同的形式，例如本体知识、关联性知识、规则库、案例知识等。相比于知识库的概念，知识图谱更加侧重关联性知识的构建，如三元组。</p>  </li>  <li>    <p>The Semantic Web ：通常翻译为“语义网”或“语义互联网”，是Web之父Tim Berners Lee于1998年提出的【1】。语义互联网的核心内涵是：Web不仅仅要通过超链接把文本页面链接起来，还应该把事物链接起来，使得搜索引擎可以直接对事物进行搜索，而不仅仅是对网页进行搜索。谷歌知识图谱是语义互联网这一理念的商业化实现。也可以把语义互联网看做是一个基于互联网共同构建的全球知识库。</p>  </li>  <li>    <p>Linked Data：通常翻译为“链接数据”。是Tim Berners Lee于2006年提出，是为了强调语义互联网的目的是要建立数据之间的链接，而非仅仅是把结构化的数据发布到网上。他为建立数据之间的链接制定了四个原则。从理念上讲，链接数据最接近于知识图谱的概念。但很多商业知识图谱的具体实现并不一定完全遵循Tim所提出的那四个原则。</p>  </li>  <li>    <p>Semantic Net/ Semantic Network：通常翻译为“语义网络”或“语义网”，这个翻译通常被与Semantic Web的翻译混淆起来，为了以示区别，这里采用“语义网络”的翻译。语义网络最早是1960年由认知科学家Allan M. Collins作为知识表示的一种方法提出。WordNet是最典型的语义网络。相比起知识图谱，早期的语义网络更加侧重描述概念以及概念之间的关系，而知识图谱更加强调数据或事物之间的链接。</p>  </li>  <li>    <p>Ontology：通常翻译为“本体”。本体本身是个哲学名词。在上个世纪80年代，人工智能研究人员将这一概念引入了计算机领域。Tom Gruber把本体定义为“概念和关系的形式化描述”。通俗点讲，本体相似于数据库中的Schema，主要用来定义类和关系，以及类层次和关系层次等。OWL是最常用的本体描述语言。本体通常被用来为知识图谱定义Schema。</p>  </li></ol><p>通过上述的介绍，大家应该对知识图谱的表示以及其在搜索中的展现形式有了更深的了解。接着，我将介绍知识图谱的构建以及如何在搜索中应用知识图谱返回相应的知识卡片以及答案。</p><h1>知识图谱的构建</h1><h2 id="知识图谱的规模">知识图谱的规模</h2><p>据不完全统计，Google知识图谱到目前为止包含了5亿个实体和35亿条事实(形如实体-属性-值，和实体-关系-实体)。其知识图谱是面向全球的，因此包含了实体和相关事实的多语言描述。不过相比占主导的英语外，仅包含其他语言(如中文)的知识图谱的规模则小了很多。与此不同的是，百度和搜狗主要针对中文搜索推出知识图谱，其知识库中的知识也主要以中文来描述，其规模略小于Google的。</p><h2 id="知识图谱的数据来源">知识图谱的数据来源</h2><p>为了提高搜索质量，特别是提供如对话搜索和复杂问答等新的搜索体验，我们不仅要求知识图谱包含大量高质量的常识性知识，还要能及时发现并添加新的知识。在这种背景下，知识图谱通过收集来自百科类站点和各种垂直站点的结构化数据来覆盖大部分常识性知识。这些数据普遍质量较高，更新比较慢。而另一方面，知识图谱通过从各种半结构化数据(形如HTML表格)抽取相关实体的属性-值对来丰富实体的描述。此外，通过搜索日志(query log)发现新的实体或新的实体属性从而不断扩展知识图谱的覆盖率。相比高质量的常识性知识，通过数据挖掘抽取得到的知识数据更大，更能反映当前用户的查询需求并能及时发现最新的实体或事实，但其质量相对较差，存在一定的错误。这些知识利用互联网的冗余性在后续的挖掘中通过投票或其他聚合算法来评估其置信度，并通过人工审核加入到知识图谱中。</p><h3 id="百科类数据">百科类数据</h3><p>维基百科，通过协同编辑，已经成为最大的在线百科全书，其质量与大英百科媲美。可以通过以下方式来从维基百科中获取所需的内容：通过文章页面(Article Page)抽取各种实体;通过重定向页面(Redirect Page)获得这些实体的同义词(又称Synonym);通过去歧义页面(Disambiguation Page)和内链锚文本(Internal Link Anchor Text)获得它们的同音异义词(又称Homonym);通过概念页面(Category Page)获得各种概念以及其上下位(subclass)关系;通过文章页面关联的开放分类抽取实体所对应的类别;通过信息框(Infobox)抽取实体所对应的属性-值对和关系-实体对。类似地，从百度百科和互动百科抽取各种中文知识来弥补维基百科中文数据不足的缺陷。此外，Freebase[5]  是另一个重要的百科类的数据源，其包含超过3900万个实体(其称为Topics)和18亿条事实，规模远大于维基百科。对比之前提及的知识图谱的规模，我们发现仅Freebase一个数据源就构成了Google知识图谱的半壁江山。更为重要的是，维基百科所编辑的是各种词条，这些词条以文章的形式来展现，包含各种半结构化信息，需要通过事先制定的规则来抽取知识;而Freebase则直接编辑知识，包括实体及其包含的属性和关系，以及实体所属的类型等结构化信息。因此，不需要通过任何抽取规则即可获得高质量的知识。虽然开发Freebase的母公司MetaWeb于2010年被Google收购，Freebase还是作为开放的知识管理平台独立运行。所以百度和搜狗也将Freebase加入到其知识图谱中。</p><h3 id="结构化数据">结构化数据</h3><p>除了百科类的数据，各大搜索引擎公司在构建知识图谱时，还考虑其他结构化数据。其中，LOD项目在发布各种语义数据的同时，通过owl:sameAs将新发布的语义数据中涉及的实体和LOD中已有数据源所包含的潜在同一实体进行关联，从而实现了手工的实体对齐(entity alignment)。LOD不仅包括如DBpedia和YAGO等通用语义数据集，还包括如MusicBrainz和DrugBank等特定领域的知识库。因此，Google等通过整合LOD中的(部分)语义数据提高知识的覆盖率，尤其是垂直领域的各种知识。此外，Web上存在大量高质量的垂直领域站点(如电商网站，点评网站等)，这些站点被称为Deep  Web。它们通过动态网页技术将保存在数据库中的各种领域相关的结构化数据以HTML表格的形式展现给用户。各大搜索引擎公司通过收购这些站点或购买其数据来进一步扩充其知识图谱在特定领域的知识。 这样做出于三方面原因：</p><ol>  <li>大量爬取这些站点的数据会占据大量带宽，导致这些站点无法被正常访问;</li>  <li>爬取全站点数据可能会涉及知识产权纠纷;</li>  <li>相比静态网页的爬取，Deep Web爬虫需要通过表单填充(Form Filling)技术来获取相关内容，且解析这些页面中包含的结构化信息需要额外的自动化抽取算法，具体细节在下一节描述。</li></ol><h3 id="半结构化数据挖掘avp">半结构化数据挖掘AVP</h3><p>虽然从Deep Web爬取数据并解析其中所包含的结构化信息面临很大的挑战，各大搜索引擎公司仍在这方面投入了大量精力。一方面，Web上存在大量长尾的结构化站点，这些站点提供的数据与最主流的相关领域站点所提供的内容具有很强的互补性，因此对这些长尾站点进行大规模的信息抽取(尤其是实体相关的属性-值对的抽取)对于知识图谱所含内容的扩展是非常有价值的。另一方面，中文百科类的站点(如百度百科等)的结构化程度远不如维基百科，能通过信息框获得AVP的实体非常稀少，大量属性-值对隐含在一些列表或表格中。一个切实可行的做法是构建面向站点的包装器(Site-specific  Wrapper)。其背后的基本思想是：**一个Deep Web站点中的各种页面由统一的程序动态生成，具有类似的布局和结构。**利用这一点，我们仅需从当前待抽取站点采样并标注几个典型详细页面(Detailed Pages)，利用这些页面通过模式学习算法(Pattern Learning)自动构建出一个或多个以类Xpath表示的模式，然后将其应用在该站点的其他详细页面中从而实现自动化的AVP抽取。对于百科类站点，我们可以将具有相同类别的页面作为某个“虚拟”站点，并使用类似的方法进行实体AVP的抽取。自动学习获得的模式并非完美，可能会遗漏部分重要的属性，也可能产生错误的抽取结果。为了应对这个问题，搜索引擎公司往往通过构建工具来可视化这些模式，并人工调整或新增合适的模式用于抽取。此外，通过人工评估抽取的结果，将那些抽取结果不令人满意的典型页面进行再标注来更新训练样本，从而达到主动学习(Active  Learning)的目的。</p><h3 id="通过搜索日志进行实体和实体属性等挖掘">通过搜索日志进行实体和实体属性等挖掘</h3><p>搜索日志是搜索引擎公司积累的宝贵财富。一条搜索日志形如**&lt;查询，点击的页面链接，时间戳&gt;**。通过挖掘搜索日志，我们往往可以发现最新出现的各种实体及其属性，从而保证知识图谱的实时性。这里侧重于从查询的关键词短语和点击的页面所对应的标题中抽取实体及其属性。选择查询作为抽取目标的意义在于其反映了用户最新最广泛的需求，从中能挖掘出用户感兴趣的实体以及实体对应的属性。而选择页面的标题作为抽取目标的意义在于标题往往是对整个页面的摘要，包含最重要的信息。据百度研究者的统计，90%以上的实体可以在网页标题中被找到。为了完成上述抽取任务，一个常用的做法是：针对每个类别，挑选出若干属于该类的实体(及相关属性)作为种子(Seeds)，找到包含这些种子的查询和页面标题，形成正则表达式或文法模式。这些模式将被用于抽取查询和页面标题中出现的其他实体及其属性。如果当前抽取所得的实体未被包含在知识图谱中，则该实体成为一个新的候选实体。类似地，如果当前被抽取的属性未出现在知识图谱中，则此属性成为一个新的候选属性。这里，我们仅保留置信度高的实体及其属性，新增的实体和属性将被作为新的种子发现新的模式。此过程不断迭代直到没有新的种子可以加入或所有的模式都已经找到且无法泛化。在决定模式的好坏时，常用的基本原则是尽量多地发现属于当前类别的实体和对应属性，尽量少地抽取出属于其他类别的实体及属性。上述方法被称为基于Bootstrapping的多类别协同模式学习。</p><h2 id="从抽取图谱到知识图谱">从抽取图谱到知识图谱</h2><p>上述所介绍的方法仅仅是从各种类型的数据源抽取构建知识图谱所需的各种候选实体(概念)及其属性关联，形成了一个个孤立的抽取图谱(Extraction Graphs)。为了形成一个真正的知识图谱，我们需要将这些信息孤岛集成在一起。下面我对知识图谱挖掘所涉及的重要技术点逐一进行介绍。</p><h3 id="实体对齐">实体对齐</h3><p>实体对齐(Object Alignment)旨在发现具有不同ID但却代表真实世界中同一对象的那些实体，并将这些实体归并为一个具有全局唯一标识的实体对象添加到知识图谱中。虽然实体对齐在数据库领域被广泛研究，但面对如此多异构数据源上的Web规模的实体对齐，这还是第一次尝试。各大搜索引擎公司普遍采用的方法是聚类。聚类的关键在于定义合适的相似度度量。这些相似度度量遵循如下观察：具有相同描述的实体可能代表同一实体(字符相似);具有相同属性-值的实体可能代表相同对象(属性相似);具有相同邻居的实体可能指向同一个对象(结构相似)。在此基础上，为了解决大规模实体对齐存在的效率问题，各种基于数据划分或分割的算法被提出将实体分成一个个子集，在这些子集上使用基于更复杂的相似度计算的聚类并行地发现潜在相同的对象。另外，利用来自如LOD中已有的对齐标注数据(使用owl:sameAs关联两个实体)作为训练数据，然后结合相似度计算使用如标签传递(Label  Propagation)等基于图的半监督学习算法发现更多相同的实体对。无论何种自动化方法都无法保证100%的准确率，所以这些方法的产出结果将作为候选供人工进一步审核和过滤。</p><h3 id="知识图谱schema构建">知识图谱schema构建</h3><p>在之前的技术点介绍中，大部分篇幅均在介绍知识图谱中数据层(Data Level)的构建，而没有过多涉及模式层(Schema Level)。事实上，模式是对知识的提炼，而且遵循预先给定的schema有助于知识的标准化，更利于查询等后续处理。为知识图谱构建schema相当于为其建立本体(Ontology)。最基本的本体包括概念、概念层次、属性、属性值类型、关系、关系定义域(Domain)概念集以及关系值域(Range)概念集。在此基础上，我们可以额外添加规则(Rules)或公理(Axioms)来表示模式层更复杂的约束关系。面对如此庞大且领域无关的知识库，即使是构建最基本的本体，也是非常有挑战的。Google等公司普遍采用的方法是自顶向下(Top-Down)和自底向上(Bottom-Up)相结合的方式。这里，自顶向下的方式是指通过本体编辑器(Ontology  Editor)预先构建本体。当然这里的本体构建不是从无到有的过程，而是依赖于从百科类和结构化数据得到的高质量知识中所提取的模式信息。更值得一提的是，Google知识图谱的Schema是在其收购的Freebase的schema基础上修改而得。Freebase的模式定义了Domain(领域)，Type(类别)和Topic(主题，即实体)。每个Domain有若干Types，每个Type包含多个Topics且和多个Properties关联，这些Properties规定了属于当前Type的那些Topics需要包含的属性和关系。定义好的模式可被用于抽取属于某个Type或满足某个Property的新实体(或实体对)。另一方面，自底向上的方式则通过上面介绍的各种抽取技术，特别是通过搜索日志和Web  Table抽取发现的类别、属性和关系，并将这些置信度高的模式合并到知识图谱中。合并过程将使用类似实体对齐的对齐算法。对于未能匹配原有知识图谱中模式的类别、属性和关系作为新的模式加入知识图谱供人工过滤。自顶向下的方法有利于抽取新的实例，保证抽取质量，而自底向上的方法则能发现新的模式。两者是互补的。</p><h3 id="不一致性的解决">不一致性的解决</h3><p>当融合来自不同数据源的信息构成知识图谱时，有一些实体会同时属于两个互斥的类别(如男女)或某个实体所对应的一个Property11对应多个值。这样就会出现不一致性。这些互斥的类别对以及Functional Properties可以看作是模式层的知识，通常规模不是很大，可以通过手工指定规则来定义。而由于不一致性的检测要面对大规模的实体及相关事实，纯手工的方法将不再可行。一个简单有效的方法充分考虑数据源的可靠性以及不同信息在各个数据源中出现的频度等因素来决定最终选用哪个类别或哪个属性值。也就是说，我们优先采用那些可靠性高的数据源(如百科类或结构化数据)抽取得到的事实。另外，如果一个实体在多个数据源中都被识别为某个类别的实例，或实体某个functional  property在多个数据源中都对应相同的值，那么我们倾向于最终选择该类别和该值。注：在统计某个类别在数据源中出现的频率前需要完成类别对齐计算。类似地，对于数值型的属性值我们还需要额外统一它们所使用的单位。</p><h2 id="知识图谱上的挖掘">知识图谱上的挖掘</h2><p>通过各种信息抽取和数据集成技术已经可以构建Web规模的知识图谱。为了进一步增加图谱的知识覆盖率，需要进一步在知识图谱上进行挖掘。下面将介绍几项重要的基于知识图谱的挖掘技术。</p><h3 id="推理">推理</h3><p>推理(Reasoning或Inference)被广泛用于发现隐含知识。推理功能一般通过可扩展的规则引擎来完成。知识图谱上的规则一般涉及两大类。一类是针对属性的，即通过数值计算来获取其属性值。例如：知识图谱中包含某人的出生年月，我们可以通过当前日期减去其出生年月获取其年龄。这类规则对于那些属性值随时间或其他因素发生改变的情况特别有用。另一类是针对关系的，即通过(链式)规则发现实体间的隐含关系。例如，我们可以定义规定：岳父是妻子的父亲。利用这条规则，当已知姚明的妻子(叶莉)和叶莉的父亲(叶发)时，可以推出姚明的岳父是叶发。</p><h3 id="实体重要性排序">实体重要性排序</h3><p>搜索引擎识别用户查询中提到的实体，并通过知识卡片展现该实体的结构化摘要。当查询涉及多个实体时，搜索引擎将选择与查询更相关且更重要的实体来展示。实体的相关性度量需在查询时在线计算，而实体重要性与查询无关可离线计算。搜索引擎公司将PageRank算法[12] 应用在知识图谱上来计算实体的重要性。和传统的Web Graph相比，知识图谱中的节点从单一的网页变成了各种类型的实体，而图中的边也由连接网页的超链接(Hyperlink)变成丰富的各种语义关系。由于不同的实体和语义关系的流行程度以及抽取的置信度均不同，而这些因素将影响实体重要性的最终计算结果，因此，各大搜索引擎公司嵌入这些因素来刻画实体和语义关系的初始重要性，从而使用带偏的PageRank算法(Biased  PageRank)。</p><h3 id="相关实体挖掘">相关实体挖掘</h3><p>在相同查询中共现的实体，或在同一个查询会话(Session)中被提到的其他实体称为相关实体。一个常用的做法是将这些查询或会话看作是虚拟文档，将其中出现的实体看作是文档中的词条，使用主题模型(如LDA)发现虚拟文档集中的主题分布。其中每个主题包含1个或多个实体，这些在同一个主题中的实体互为相关实体。当用户输入查询时，搜索引擎分析查询的主题分布并选出最相关的主题。同时，搜索引擎将给出该主题中与知识卡片所展现的实体最相关的那些实体作为“其他人还搜了”的推荐结果。</p><h2 id="知识图谱的更新和维护">知识图谱的更新和维护</h2><h3 id="type和collection的关系">Type和Collection的关系</h3><p>知识图谱的schema为了保证其质量，由专业团队审核和维护。以Google知识图谱为例，目前定义的Type数在103-104的数量级。为了提高知识图谱的覆盖率，搜索引擎公司还通过自动化算法从各种数据源抽取新的类型信息(也包含关联的Property信息)，这些类型信息通过一个称为Collection的数据结构保存。它们不是马上被加入到知识图谱schema中。有些今天生成后第二天就被删除了，有些则能长期的保留在Collection中，如果Collection中的某一种类型能够长期的保留，发展到一定程度后，由专业的人员进行决策和命名并最终成为一种新的Type。</p><h3 id="结构化站点包装器的维护">结构化站点包装器的维护</h3><p>站点的更新常常会导致原有模式失效。搜索引擎会定期检查站点是否存在更新。当检测到现有页面(原先已爬取)发生了变化，搜索引擎会检查这些页面的变化量，同时使用最新的站点包装器进行AVP抽取。如果变化量超过事先设定的阈值且抽取结果与原先标注的答案差别较大，则表明现有的站点包装器失效了。在这种情况下，需要对最新的页面进行重新标注并学习新的模式，从而构建更新的包装器。</p><h3 id="知识图谱的更新频率">知识图谱的更新频率</h3><p>加入到知识图谱中的数据不是一成不变的。Type对应的实例往往是动态变化的。例如，美国总统，随着时间的推移，可能对应不同的人。由于数据层的规模和更新频度都远超schema层，搜索引擎公司利用其强大的计算保证图谱每天的更新都能在3个小时内完成，而实时的热点也能保证在事件发生6个小时内在搜索结果中反映出来。</p><h3 id="众包-crowdsourcing-反馈机制">众包(Crowdsourcing)反馈机制</h3><p>除了搜索引擎公司内部的专业团队对构建的知识图谱进行审核和维护，它们还依赖用户来帮助改善图谱。具体来说，用户可以对搜索结果中展现的知识卡片所列出的实体相关的事实进行纠错。当很多用户都指出某个错误时，搜索引擎将采纳并修正。这种利用群体智慧的协同式知识编辑是对专业团队集中式管理的互补。</p><h1>知识图谱在搜索中的应用</h1><h2 id="查询理解">查询理解</h2><p>搜索引擎借助知识图谱来识别查询中涉及到的实体(概念)及其属性等，并根据实体的重要性展现相应的知识卡片。搜索引擎并非展现实体的全部属性，而是根据当前输入的查询自动选择最相关的属性及属性值来显示。此外，搜索引擎仅当知识卡片所涉及的知识的正确性很高(通常超过95%，甚至达到99%)时，才会展现。当要展现的实体被选中之后，利用相关实体挖掘来推荐其他用户可能感兴趣的实体供进一步浏览。</p><h2 id="问题回答">问题回答</h2><p>除了展现与查询相关的知识卡片，知识图谱对于搜索所带来的另一个革新是：直接返回答案，而不仅仅是排序的文档列表。要实现自动问答系统，搜索引擎不仅要理解查询中涉及到的实体及其属性，更需要理解查询所对应的语义信息。搜索引擎通过高效的图搜索，在知识图谱中查找连接这些实体及属性的子图并转换为相应的图查询(如SPARQL[13] )。这些翻译过的图查询被进一步提交给图数据库进行回答返回相应的答案。</p><h1>总结</h1><p>这篇文章比较系统地介绍了知识图谱的表示、构建、挖掘以及在搜索中的应用。通过上述介绍，大家可以看出：</p><ol>  <li>目前知识图谱还处于初期阶段;</li>  <li>人工干预很重要;</li>  <li>结构化数据在知识图谱的构建中起到决定性作用;</li>  <li>各大搜索引擎公司为了保证知识图谱的质量多半采用成熟的算法;</li>  <li>知识卡片的给出相对比较谨慎;</li>  <li>更复杂的自然语言查询将崭露头角(如Google的蜂鸟算法)。</li></ol><p>此外，知识图谱的构建是多学科的结合，需要知识库、自然语言理解，机器学习和数据挖掘等多方面知识的融合。有很多开放性问题需要学术界和业界一起解决。我们有理由相信学术界在上述方面的突破将会极大地促进知识图谱的发展。</p><p><br><br>本文链接： <a href="http://www.meng.uno/articles/349dc05d/">http://www.meng.uno/articles/349dc05d/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      简介
近两年来，随着Linking Open Data等项目的全面展开，语义Web数据源的数量激增，大量RDF数据被发布。互联网正从仅包含网页和网页之间超链接的文档万维网(Document Web)转变成包含大量描述各种实体和实体之间丰富关系的数据万维网(Data Web)。在这个背景下，Google、百度和搜狗等搜索引擎公司纷纷以此为基础构建知识图谱，分别为Knowledge Graph、知心和知立方，来改进搜索质量，从而拉开了语义搜索的序幕。下面我将从以下几个方面来介绍知识图谱：知识图谱的表示和在搜索中的展现形式，知识图谱的构建和知识图谱在搜索中的应用等，从而让大家有机会了解其内部的技术实
    
    </summary>
    
      <category term="AI" scheme="http://www.meng.uno/categories/AI/"/>
    
      <category term="NLP" scheme="http://www.meng.uno/categories/AI/NLP/"/>
    
    
      <category term="知识图谱" scheme="http://www.meng.uno/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="Knowledge Graph" scheme="http://www.meng.uno/tags/Knowledge-Graph/"/>
    
  </entry>
  
  <entry>
    <title>Java合并List</title>
    <link href="http://www.meng.uno/articles/66999e7d/"/>
    <id>http://www.meng.uno/articles/66999e7d/</id>
    <published>2018-07-15T02:15:54.000Z</published>
    <updated>2018-08-15T14:35:34.000Z</updated>
    
    <content type="html"><![CDATA[<h1>问题</h1><p>在写我的毕业设计时，遇到了这样两个问题：</p><ol>  <li>给定一个分词结果（List    <string>）与一个知道偏置的专有名词（特定领域命名实体）的结果（List      <stirng>），怎么将两者融合成一个统一的分词结果（List        <string>）。</string></stirng></string></li>  <li>给定一个分词结果（List    <string>）与一条规则（人为规定的分词结果（List      <string>）），怎么将规则整合到分词结果中，得到一个统一的分词结果（List        <string>）。</string></string></string></li></ol><p>虽然在算法上没有多少难度，但是在实现上还是挺费时间思考，所以我将我的实现保存起来，以后没准我还会再用到。</p><h1>方案</h1><h2 id="合并专有名词">合并专有名词</h2><figure class="highlight java">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre>      </td>      <td class="code">        <pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">segTemp</span><span class="params">(List&lt;String&gt; tmp, List&lt;Term&gt; area, <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">  List&lt;String&gt; ret = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">  <span class="keyword">int</span> area_len = area.size();</span><br><span class="line">  <span class="keyword">if</span> (area_len == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> tmp;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> tmp_index = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> tmp_i = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; tmp.size(); i++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (index == area.get(tmp_i).getOffe()) &#123;</span><br><span class="line">      ret.add(area.get(tmp_i).getRealName());</span><br><span class="line">      index += area.get(tmp_i).getRealName().length();</span><br><span class="line">      i--;</span><br><span class="line">      <span class="keyword">if</span> (tmp_i &lt; area_len - <span class="number">1</span>) &#123;</span><br><span class="line">        tmp_i++;</span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (index &gt; tmp_index) &#123;</span><br><span class="line">      <span class="keyword">if</span> (tmp_index + tmp.get(i).length() &lt;= index)&#123;</span><br><span class="line">        tmp_index += tmp.get(i).length();</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        ret.add(tmp.get(i).substring(index - tmp_index));</span><br><span class="line">        tmp_index += tmp.get(i).length();</span><br><span class="line">        index = tmp_index;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (index + tmp.get(i).length() &lt;= area.get(tmp_i).getOffe()) &#123;</span><br><span class="line">      ret.add(tmp.get(i));</span><br><span class="line">      index += tmp.get(i).length();</span><br><span class="line">      tmp_index += tmp.get(i).length();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (index + tmp.get(i).length() &gt; area.get(tmp_i).getOffe()</span><br><span class="line">        &amp;&amp; index &lt; area.get(tmp_i).getOffe()) &#123;</span><br><span class="line">      ret.add(tmp.get(i).substring(<span class="number">0</span>, area.get(tmp_i).getOffe() - index));</span><br><span class="line">      index += area.get(tmp_i).getOffe() - index;</span><br><span class="line">      tmp_index += tmp.get(i).length();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 从上一个位置break</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; tmp.size(); j++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (index &gt; tmp_index) &#123;</span><br><span class="line">      <span class="keyword">if</span> (tmp_index + tmp.get(j).length() &lt;= index) &#123;</span><br><span class="line">        tmp_index += tmp.get(j).length();</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        ret.add(tmp.get(j).substring(index - tmp_index));</span><br><span class="line">        tmp_index += tmp.get(j).length();</span><br><span class="line">        index = tmp_index;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      ret.add(tmp.get(j));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre>      </td>    </tr>  </table></figure><h2 id="合并规则">合并规则</h2><figure class="highlight java">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre>      </td>      <td class="code">        <pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;String&gt; <span class="title">match</span><span class="params">(String text, List&lt;String&gt; ori, List&lt;Rule&gt; rule)</span></span>&#123;</span><br><span class="line"><span class="keyword">if</span>(rule.isEmpty())&#123;</span><br><span class="line"><span class="keyword">return</span> ori;</span><br><span class="line">&#125;</span><br><span class="line">List&lt;String&gt; ret = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"><span class="keyword">boolean</span> flag = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">for</span>(Rule ru : rule)&#123;</span><br><span class="line">List&lt;Integer&gt; loc = getLocation(text,ru.toString());</span><br><span class="line"><span class="keyword">if</span>(loc.isEmpty())&#123;</span><br><span class="line"><span class="keyword">continue</span>;</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">flag = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">int</span> num = loc.size();</span><br><span class="line"><span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> idx = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt; num;i++)&#123;</span><br><span class="line"><span class="keyword">int</span> tmp = loc.get(i);</span><br><span class="line"><span class="keyword">while</span>(idx+ori.get(j).length() &lt; tmp)&#123;</span><br><span class="line">ret.add(ori.get(j));</span><br><span class="line">idx += ori.get(j).length(); </span><br><span class="line">j++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(ori.get(j).substring(idx+ori.get(j).length() - tmp) != <span class="keyword">null</span> || !ori.get(j).substring(idx+ori.get(j).length() - tmp).equals(<span class="string">""</span>))&#123;ret.add(ori.get(j).substring(idx+ori.get(j).length() - tmp));</span><br><span class="line">j++;</span><br><span class="line">&#125;</span><br><span class="line">ret.addAll(ru.getRule());</span><br><span class="line"><span class="keyword">while</span>(j&lt;ori.size())&#123;</span><br><span class="line"><span class="keyword">if</span>(idx + ori.get(j).length() &lt;= tmp+ru.toString().length())&#123;</span><br><span class="line">idx += ori.get(j).length();</span><br><span class="line">j++;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>(idx + ori.get(j).length() &gt; tmp+ru.toString().length() &amp;&amp; idx &lt; tmp + ru.toString().length())&#123;</span><br><span class="line">idx += ori.get(j).length();</span><br><span class="line">ret.add(ori.get(j).substring(idx - (tmp + ru.toString().length() )));</span><br><span class="line">j++;</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(j&lt;ori.size())&#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> t = j;t &lt; ori.size();t++)&#123;</span><br><span class="line">ret.add(ori.get(t));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">ori = ret;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(flag == <span class="keyword">false</span>)&#123;</span><br><span class="line"><span class="keyword">return</span> ori;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre>      </td>    </tr>  </table></figure><p><br><br>本文链接： <a href="http://www.meng.uno/articles/66999e7d/">http://www.meng.uno/articles/66999e7d/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      问题
在写我的毕业设计时，遇到了这样两个问题：

 1. 给定一个分词结果（List ）与一个知道偏置的专有名词（特定领域命名实体）的结果（List ），怎么将两者融合成一个统一的分词结果（List ）。
 2. 给定一个分词结果（List ）与一条规则（人为规定的分词结果（List ）），怎么将规则整合到分词结果中，得到一个统一的分词结果（List ）。

虽然在算法上没有多少难度，但是在实现上还是挺费时间思考，所以我将我的实现保存起来，以后没准我还会再用到。

方案
合并专有名词
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
2
    
    </summary>
    
      <category term="Java开发Tips" scheme="http://www.meng.uno/categories/Java%E5%BC%80%E5%8F%91Tips/"/>
    
    
      <category term="Java" scheme="http://www.meng.uno/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理（NLP）基础</title>
    <link href="http://www.meng.uno/articles/d1bd92c6/"/>
    <id>http://www.meng.uno/articles/d1bd92c6/</id>
    <published>2018-07-05T14:09:21.000Z</published>
    <updated>2018-08-16T03:55:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1>NLP 概述</h1><h2 id="解决-nlp-问题的一般思路">解决 NLP 问题的一般思路</h2><figure class="highlight tex">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>      </td>      <td class="code">        <pre><span class="line">这个问题人类可以做好么？</span><br><span class="line">  - 可以 -&gt; 记录自己的思路 -&gt; 设计流程让机器完成你的思路</span><br><span class="line">  - 很难 -&gt; 尝试从计算机的角度来思考问题</span><br></pre>      </td>    </tr>  </table></figure><h2 id="nlp-的历史进程">NLP 的历史进程</h2><ul>  <li>    <p><strong>规则系统</strong></p>    <ul>      <li>正则表达式/自动机</li>      <li>规则是固定的</li>      <li><strong>搜索引擎</strong>        <figure class="highlight tex">          <table>            <tr>              <td class="gutter">                <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>              </td>              <td class="code">                <pre><span class="line">“豆瓣酱用英语怎么说？”</span><br><span class="line">规则：“xx用英语怎么说？” =&gt; translate(XX, English)</span><br><span class="line"></span><br><span class="line">“我饿了”</span><br><span class="line">规则：“我饿（死）了” =&gt; recommend(饭店，地点)</span><br></pre>              </td>            </tr>          </table>        </figure>      </li>    </ul>  </li>  <li>    <p><strong>概率系统</strong></p>    <ul>      <li>        <p>规则从数据中<strong>抽取</strong></p>      </li>      <li>        <p>规则是有<strong>概率</strong>的</p>      </li>      <li>        <p>概率系统的一般<strong>工作方式</strong></p>        <figure class="highlight tex">          <table>            <tr>              <td class="gutter">                <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>              </td>              <td class="code">                <pre><span class="line">流程设计</span><br><span class="line">  收集训练数据</span><br><span class="line">    预处理</span><br><span class="line">      特征工程</span><br><span class="line">        分类器（机器学习算法）</span><br><span class="line">          预测</span><br><span class="line">            评价</span><br></pre>              </td>            </tr>          </table>        </figure>        <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180807203305.png" height="200"></div>        <ul>          <li>最重要的部分：数据收集、预处理、特征工程</li>        </ul>      </li>      <li>        <p>示例</p>        <figure class="highlight tex">          <table>            <tr>              <td class="gutter">                <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre>              </td>              <td class="code">                <pre><span class="line">任务：</span><br><span class="line">  “豆瓣酱用英语怎么说” =&gt; translate(豆瓣酱，Eng)</span><br><span class="line">流程设计（序列标注）：</span><br><span class="line">  子任务1： 找出目标语言 “豆瓣酱用 **英语** 怎么说”</span><br><span class="line">  子任务2： 找出翻译目标 “ **豆瓣酱** 用英语怎么说”</span><br><span class="line">收集训练数据：</span><br><span class="line">  （子任务1）</span><br><span class="line">  “豆瓣酱用英语怎么说”</span><br><span class="line">  “茄子用英语怎么说”</span><br><span class="line">  “黄瓜怎么翻译成英语”</span><br><span class="line">预处理：</span><br><span class="line">  分词：“豆瓣酱 用 英语 怎么说”</span><br><span class="line">抽取特征：</span><br><span class="line">  （前后各一个词）</span><br><span class="line">  0 茄子：    &lt; _ 用</span><br><span class="line">  0 用：      豆瓣酱 _ 英语</span><br><span class="line">  1 英语：    用 _ 怎么说</span><br><span class="line">  0 怎么说：  英语 _ &gt;</span><br><span class="line">分类器：</span><br><span class="line">  SVM/CRF/HMM/RNN</span><br><span class="line">预测：</span><br><span class="line">  0.1 茄子：    &lt; _ 用</span><br><span class="line">  0.1 用：      豆瓣酱 _ 英语</span><br><span class="line">  0.7 英语：    用 _ 怎么说</span><br><span class="line">  0.1 怎么说：  英语 _ &gt;</span><br><span class="line">评价：</span><br><span class="line">  准确率</span><br></pre>              </td>            </tr>          </table>        </figure>      </li>    </ul>  </li></ul><ul>  <li>    <p>概率系统的优/缺点</p>    <ul>      <li><code>+</code> 规则更加贴近于真实事件中的规则，因而效果往往比较好</li>      <li><code>-</code> 特征是由专家/人指定的；</li>      <li><code>-</code> 流程是由专家/人设计的；</li>      <li><code>-</code> 存在独立的<strong>子任务</strong></li>    </ul>  </li>  <li>    <p><strong>深度学习</strong></p>    <ul>      <li>深度学习相对概率模型的优势        <ul>          <li>特征是由专家指定的 <code>-&gt;</code> 特征是由深度学习自己提取的</li>          <li>流程是由专家设计的 <code>-&gt;</code> 模型结构是由专家设计的</li>          <li>存在独立的子任务 <code>-&gt;</code> End-to-End Training</li>        </ul>      </li>    </ul>  </li></ul><h2 id="seq2seq-模型">Seq2Seq 模型</h2><ul>  <li>    <p>大部分自然语言问题都可以使用 Seq2Seq 模型解决</p>    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180807210029.png" height="200"></div>  </li>  <li>    <p><strong>“万物”皆 Seq2Seq</strong></p>    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180807210133.png" height="300"></div>  </li></ul><h2 id="评价机制">评价机制</h2><h3 id="困惑度-perplexity-ppx">困惑度 (Perplexity, PPX)</h3><blockquote>  <p><a href="https://en.wikipedia.org/wiki/Perplexity" target="_blank" rel="noopener">Perplexity</a> - Wikipedia</p></blockquote><ul>  <li>在信息论中，perplexity 用于度量一个<strong>概率分布</strong>或<strong>概率模型</strong>预测样本的好坏程度</li></ul><p>  </p><h3>基本公式</h3><p></p><ul>  <li>    <p><strong>概率分布</strong>（离散）的困惑度</p>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=\dpi{150}&space;{\displaystyle&space;2^{H(p)}=2^{-\sum&space;_{x}p(x)\log&space;_{2}p(x)}}" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_20180728195601.png"></a></div>    <blockquote>      <p>其中 <code>H(p)</code> 即<strong>信息熵</strong></p>    </blockquote>  </li>  <li>    <p><strong>概率模型</strong>的困惑度</p>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=\dpi{150}&space;{\displaystyle&space;b^{-{\frac&space;{1}{N}}\sum&space;_{i=1}^{N}\log&space;_{b}q(x_{i})}}" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_20180728201614.png"></a></div>    <blockquote>      <p>通常 <code>b=2</code></p>    </blockquote>  </li>  <li>    <p><strong>指数部分</strong>也可以是<strong>交叉熵</strong>的形式，此时困惑度相当于交叉熵的指数形式</p>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=\dpi{150}&space;2^{H(\tilde{p},q)}&space;=&space;2^{-\sum_x\tilde{p}(x)\log_{2}q(x)}" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_20180728202629.png"></a></div>    <blockquote>      <p>其中 <code>p~</code> 为<strong>测试集</strong>中的经验分布——<code>p~(x) = n/N</code>，其中 <code>n</code> 为 x 的出现次数，N 为测试集的大小</p>    </blockquote>  </li></ul><p><strong>语言模型中的 PPX</strong></p><ul>  <li>    <p>在 <strong>NLP</strong> 中，困惑度常作为<strong>语言模型</strong>的评价指标</p>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=\dpi{150}&space;\begin{aligned}&space;\mathrm{PPX}(W_{test})&space;&=2^{-\sum_{i=1}^{|V|}\tilde{p}(w_i)\log_{2}q(w_i)}\\&space;&=2^{-\sum_{i=1}^{|V|}\frac{\mathrm{cnt}(w_i)}{N}\log_{2}q(w_i)}&space;\end{aligned}" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_20180728205525.png"></a></div>  </li>  <li>    <p>直观来说，就是下一个<strong>候选词数目</strong>的期望值——</p>    <p>如果不使用任何模型，那么下一个候选词的数量就是整个词表的数量；通过使用 <code>bi-gram</code>语言模型，可以将整个数量限制到 <code>200</code> 左右</p>  </li></ul><h3 id="bleu">BLEU</h3><blockquote>  <p><a href="https://blog.csdn.net/qq_21190081/article/details/53115580" target="_blank" rel="noopener">一种机器翻译的评价准则——BLEU</a> - CSDN博客</p></blockquote><ul>  <li>    <p>机器翻译评价准则</p>  </li>  <li>    <p>计算公式</p>    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180728212554.png" height=""></div>    <p>其中</p>    <div style="position:relative;left:25%"><img src="http://www.meng.uno/images/assets/TIM截图20180728215749.png" height=""></div>    <div style="position:relative;left:25%"><img src="http://www.meng.uno/images/assets/TIM截图20180728212444.png" height=""></div>    <!-- <div style="position:relative;left:25%"><img src="http://www.meng.uno/images/assets/TIM截图20180728212444.png" height="" /></div> -->    <blockquote>      <p><code>c</code> 为生成句子的长度；<code>r</code> 为参考句子的长度——目的是<strong>惩罚</strong>长度过短的候选句子</p>    </blockquote>  </li>  <li>    <p>为了计算方便，会加一层 <code>log</code></p>    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180728213300.png" height=""></div>    <blockquote>      <p>通常 <code>N=4, w_n=1/4</code></p>    </blockquote>  </li></ul><h3 id="rouge">ROUGE</h3><blockquote>  <p><a href="https://blog.csdn.net/qq_25222361/article/details/78694617" target="_blank" rel="noopener">自动文摘评测方法：Rouge-1、Rouge-2、Rouge-L、Rouge-S</a> - CSDN博客</p></blockquote><ul>  <li>一种机器翻译/自动摘要的评价准则</li></ul><blockquote>  <p><a href="https://blog.csdn.net/joshuaxx316/article/details/58696552" target="_blank" rel="noopener">BLEU，ROUGE，METEOR，ROUGE-浅述自然语言处理机器翻译常用评价度量</a> - CSDN博客</p></blockquote><h1>语言模型</h1><h2 id="xx-模型的含义">XX 模型的含义</h2><ul>  <li>如果能使用某个方法对 XX <strong>打分</strong>（Score），那么就可以把这个方法称为 “<strong>XX 模型</strong>”    <ul>      <li><strong>篮球明星模型</strong>: <code>Score(库里)</code>、<code>Score(詹姆斯)</code></li>      <li><strong>话题模型</strong>——对一段话是否在谈论某一话题的打分        <figure class="highlight plain">          <table>            <tr>              <td class="gutter">                <pre><span class="line">1</span><br><span class="line">2</span><br></pre>              </td>              <td class="code">                <pre><span class="line">Score( NLP | &quot;什么 是 语言 模型？&quot; ) --&gt; 0.8</span><br><span class="line">Score( ACM | &quot;什么 是 语言 模型？&quot; ) --&gt; 0.05</span><br></pre>              </td>            </tr>          </table>        </figure>      </li>    </ul>  </li></ul><h2 id="概率-统计语言模型-plm-slm">概率/统计语言模型 (PLM, SLM)</h2><ul>  <li>    <p><strong>语言模型</strong>是一种对语言打分的方法；而<strong>概率语言模型</strong>把语言的“得分”通过<strong>概率</strong>来体现</p>  </li>  <li>    <p>具体来说，概率语言模型计算的是<strong>一个序列</strong>作为一句话可能的概率</p>    <figure class="highlight plain">      <table>        <tr>          <td class="gutter">            <pre><span class="line">1</span><br><span class="line">2</span><br></pre>          </td>          <td class="code">            <pre><span class="line">Score(&quot;什么 是 语言 模型&quot;) --&gt; 0.05   # 比较常见的说法，得分比较高</span><br><span class="line">Score(&quot;什么 有 语言 模型&quot;) --&gt; 0.01   # 不太常见的说法，得分比较低</span><br></pre>          </td>        </tr>      </table>    </figure>  </li></ul><ul>  <li>    <p>以上过程可以形式化为：</p>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=p(W)=p(w_1^T)=p(w_1,w_2,...,w_T" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_20180805204149.png" height=""></a></div>    <p>根据贝叶斯公式，有</p>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=p(w_1^T)=p(w_1)\cdot&space;p(w_2|w_1)\cdot&space;p(w_3|w_1^2)\cdots&space;p(w_T|w_1^{T-1})" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_20180805211530.png" height=""></a></div>  </li>  <li>    <p>其中每个条件概率就是<strong>模型的参数</strong>；如果这个参数都是已知的，那么就能得到整个序列的概率了</p>  </li></ul><h3 id="参数的规模">参数的规模</h3><ul>  <li>设词表的大小为 <code>N</code>，考虑长度为 <code>T</code> 的句子，理论上有 <code>N^T</code> 种可能的句子，每个句子中有 <code>T</code> 个参数，那么参数的数量将达到 <code>O(T*N^T)</code></li></ul><h3 id="可用的概率模型">可用的概率模型</h3><ul>  <li>统计语言模型实际上是一个概率模型，所以常见的概率模型都可以用于求解这些参数</li>  <li>常见的概率模型有：N-gram 模型、决策树、最大熵模型、隐马尔可夫模型、条件随机场、神经网络等</li>  <li>目前常用于语言模型的是 N-gram 模型和神经语言模型（下面介绍）</li></ul><h2 id="n-gram-语言模型">N-gram 语言模型</h2><ul>  <li>    <p>马尔可夫(Markov)假设——未来的事件，只取决于有限的历史</p>  </li>  <li>    <p>基于马尔可夫假设，N-gram 语言模型认为一个词出现的概率只与它前面的 n-1 个词相关</p>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=p(w_k|w_1,..,w_{k-1})\approx&space;p(w_k|w_{k-n&plus;1},..,w_{k-1})" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_20180805211644.png" height=""></a></div>  </li>  <li>    <p>根据<strong>条件概率公式</strong>与<strong>大数定律</strong>，当语料的规模足够大时，有</p>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=p(w_k|w_{k-n&plus;1}^{k-1})=\frac{p(w_{k-n&plus;1}^k)}{p(w_{k-n&plus;1}^{k-1})}\approx&space;\frac{\mathrm{count}(w_{k-n&plus;1}^k)}{\mathrm{count}(w_{k-n&plus;1}^{k-1})}" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_20180805211936.png" height=""></a></div>  </li>  <li>    <p>以 <code>n=2</code> 即 bi-gram 为例，有</p>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=p(w_k|w_{k-1})=\frac{p(w_{k-1},w_k)}{p(w_{k-1})}\approx&space;\frac{\mathrm{count}(w_{k-1},w_k)}{\mathrm{count}(w_{k-1})}" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_20180805212222.png" height=""></a></div>  </li>  <li>    <p>假设词表的规模 <code>N=200000</code>（汉语的词汇量），模型参数与 `n· 的关系表</p>    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180805212441.png" height=""></div>  </li></ul><h3 id="可靠性与可区别性">可靠性与可区别性</h3><ul>  <li>假设没有计算和存储限制，<code>n</code> 是不是越大越好？</li>  <li>早期因为计算性能的限制，一般最大取到 <code>n=4</code>；如今，即使 <code>n&gt;10</code> 也没有问题，</li>  <li>但是，随着 <code>n</code> 的增大，模型的性能增大却不显著，这里涉及了<strong>可靠性与可区别性</strong>的问题</li>  <li>参数越多，模型的可区别性越好，但是可靠性却在下降——因为语料的规模是有限的，导致 <code>count(W)</code> 的实例数量不够，从而降低了可靠性</li></ul><h3 id="oov-问题">OOV 问题</h3><ul>  <li>OOV 即 Out Of Vocabulary，也就是序列中出现了词表外词，或称为<strong>未登录词</strong></li>  <li>或者说在测试集和验证集上出现了训练集中没有过的词</li>  <li>一般<strong>解决方案</strong>：    <ul>      <li>设置一个词频阈值，只有高于该阈值的词才会加入词表</li>      <li>所有低于阈值的词替换为 UNK（一个特殊符号）</li>    </ul>  </li>  <li>无论是统计语言模型还是神经语言模型都是类似的处理方式    <blockquote>      <p><a href="#nplm-%E4%B8%AD%E7%9A%84-oov-%E9%97%AE%E9%A2%98">NPLM 中的 OOV 问题</a></p>    </blockquote>  </li></ul><h3 id="平滑处理-todo">平滑处理 TODO</h3><ul>  <li><code>count(W) = 0</code> 是怎么办？</li>  <li>平滑方法（层层递进）：    <ul>      <li>Add-one Smoothing (Laplace)</li>      <li>Add-k Smoothing (k&lt;1)</li>      <li>Back-off （回退）</li>      <li>Interpolation （插值法）</li>      <li>Absolute Discounting （绝对折扣法）</li>      <li>Kneser-Ney Smoothing （KN）</li>      <li>Modified Kneser-Ney</li>    </ul>    <blockquote>      <p><a href="https://blog.csdn.net/baimafujinji/article/details/51297802" target="_blank" rel="noopener">自然语言处理中N-Gram模型的Smoothing算法</a> - CSDN博客</p>    </blockquote>  </li></ul><h2 id="神经概率语言模型-nplm">神经概率语言模型 (NPLM)</h2><ul>  <li>    <p>神经概率语言模型依然是一个概率语言模型，它通过<strong>神经网络</strong>来计算概率语言模型中每个参数</p>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=p(w|{\color{Red}\text{context}(w)})=g(i_w,{\color{Red}V_{context}})" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_20180806100950.png" height=""></a></div>    <ul>      <li>其中 <code>g</code> 表示神经网络，<code>i_w</code> 为 <code>w</code> 在词表中的序号，<code>context(w)</code> 为 <code>w</code> 的上下文，<code>V_context</code> 为上下文构成的特征向量。</li>      <li><code>V_context</code> 由上下文的<strong>词向量</strong>进一步组合而成</li>    </ul>  </li></ul><h3 id="n-gram-神经语言模型">N-gram 神经语言模型</h3><ul>  <li>    <p>这是一个经典的神经概率语言模型，它沿用了 N-gram 模型中的思路，将 <code>w</code> 的前 <code>n-1</code> 个词作为 <code>w</code> 的上下文 <code>context(w)</code>，而 <code>V_context</code> 由这 <code>n-1</code> 个词的词向量拼接而成，即</p>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=p(w_k|{\color{Red}w_{k-n&plus;1}^{k-1}})=g(i_{w_k},{\color{Red}[c(w_{k-n&plus;1});...;c(w_{k-1})]})" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_20180806102047.png" height=""></a></div>    <ul>      <li>其中 <code>c(w)</code> 表示 <code>w</code> 的词向量</li>      <li>不同的神经语言模型中 <code>context(w)</code> 可能不同，比如 Word2Vec 中的 CBOW 模型</li>    </ul>  </li>  <li>    <p>每个训练样本是形如 <code>(context(w), w)</code> 的二元对，其中 <code>context(w)</code> 取 w 的前 <code>n-1</code> 个词；当不足 <code>n-1</code>，用特殊符号填充</p>    <ul>      <li>同一个网络只能训练特定的 <code>n</code>，不同的 <code>n</code> 需要训练不同的神经网络</li>    </ul>  </li></ul><h4 id="n-gram-神经语言模型的网络结构">N-gram 神经语言模型的网络结构</h4><ul>  <li>    <p>【<strong>输入层</strong>】首先，将 <code>context(w)</code> 中的每个词映射为一个长为 <code>m</code> 的词向量，<strong>词向量在训练开始时是随机的</strong>，并<strong>参与训练</strong>；</p>  </li>  <li>    <p>【<strong>投影层</strong>】将所有上下文词向量<strong>拼接</strong>为一个长向量，作为 <code>w</code> 的特征向量，该向量的维度为 <code>m(n-1)</code></p>  </li>  <li>    <p>【<strong>隐藏层</strong>】拼接后的向量会经过一个规模为 <code>h</code> 隐藏层，该隐层使用的激活函数为 <code>tanh</code></p>  </li>  <li>    <p>【<strong>输出层</strong>】最后会经过一个规模为 <code>N</code> 的 Softmax 输出层，从而得到词表中每个词作为下一个词的概率分布</p>    <blockquote>      <p>其中 <code>m, n, h</code> 为超参数，<code>N</code> 为词表大小，视训练集规模而定，也可以人为设置阈值</p>    </blockquote>  </li>  <li>    <p>训练时，使用<strong>交叉熵</strong>作为损失函数</p>  </li>  <li>    <p><strong>当训练完成时</strong>，就得到了 N-gram 神经语言模型，以及副产品<strong>词向量</strong></p>  </li>  <li>    <p>整个模型可以概括为如下公式：</p>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=y=U\cdot\tanh(Wx&plus;p)&plus;q" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_2018080695721.png" height=""></a></div>    <br>    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180805234123.png" height="200"></div>  </li></ul><blockquote>  <p>原文的模型还考虑了投影层与输出层有有边相连的情形，因而会多一个权重矩阵，但本质上是一致的：</p></blockquote><blockquote>  <blockquote>    <div align="center"><a href="http://www.codecogs.com/eqnedit.php?latex=y=U\cdot\tanh(W_1x&plus;p)&plus;W_2x&plus;q" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/公式_2018080695819.png" height=""></a></div><br>  </blockquote></blockquote><blockquote>  <blockquote>    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180805231056.png" height=""></div>  </blockquote></blockquote><h3 id="模型参数的规模与运算量">模型参数的规模与运算量</h3><ul>  <li>    <p>模型的超参数：<code>m, n, h, N</code></p>    <ul>      <li><code>m</code> 为词向量的维度，通常在 <code>10^1 ~ 10^2</code></li>      <li><code>n</code> 为 n-gram 的规模，一般小于 5</li>      <li><code>h</code> 为隐藏的单元数，一般在 <code>10^2</code></li>      <li><code>N</code> 位词表的数量，一般在 <code>10^4 ~ 10^5</code>，甚至 <code>10^6</code></li>    </ul>  </li>  <li>    <p>网络参数包括两部分</p>    <ul>      <li>词向量 <code>C</code>: 一个 <code>N * m</code> 的矩阵——其中 <code>N</code> 为词表大小，<code>m</code> 为词向量的维度</li>      <li>网络参数 <code>W, U, p, q</code>：        <figure class="highlight plain">          <table>            <tr>              <td class="gutter">                <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>              </td>              <td class="code">                <pre><span class="line">- W: h * m(n-1) 的矩阵</span><br><span class="line">- p: h * 1      的矩阵</span><br><span class="line">- U: N * h    的矩阵</span><br><span class="line">- q: N * 1    的矩阵</span><br></pre>              </td>            </tr>          </table>        </figure>      </li>    </ul>  </li>  <li>    <p>模型的运算量</p>    <ul>      <li>主要集中在隐藏层和输出层的矩阵运算以及 SoftMax 的归一化计算</li>      <li>此后的相关研究中，主要是针对这一部分进行优化，其中就包括 <strong>Word2Vec</strong> 的工作</li>    </ul>  </li></ul><h3 id="相比-n-gram-模型-nplm-的优势">相比 N-gram 模型，NPLM 的优势</h3><ul>  <li>单词之间的相似性可以通过词向量来体现    <blockquote>      <p>相比神经语言模型本身，作为其副产品的词向量反而是更大的惊喜</p>    </blockquote>  </li>  <li>自带平滑处理</li></ul><h3 id="nplm-中的-oov-问题">NPLM 中的 OOV 问题</h3><ul>  <li>    <p>在处理语料阶段，与 N-gram 中的处理方式是一样的——将不满阈值的词全部替换为 UNK      <strong>神经网络</strong>中，一般有如下几种处理 UNK 的思路</p>  </li>  <li>    <p>为 UNK 分配一个随机初始化的 embedding，并<strong>参与训练</strong></p>    <blockquote>      <p>最终得到的 embedding 会有一定的语义信息，但具体好坏未知</p>    </blockquote>  </li>  <li>    <p>把 UNK 都初始化成 0 向量，<strong>不参与训练</strong></p>    <blockquote>      <p>UNK 共享相同的语义信息</p>    </blockquote>  </li>  <li>    <p>每次都把 UNK 初始化成一个新的随机向量，<strong>不参与训练</strong></p>    <blockquote>      <p>常用的方法——因为本身每个 UNK 都不同，随机更符合对 UNK 基于最大熵的估计</p>    </blockquote>    <blockquote>      <blockquote>        <p><a href="https://stackoverflow.com/questions/45113130/how-to-add-new-embeddings-for-unknown-words-in-tensorflow-training-pre-set-fo" target="_blank" rel="noopener">How to add new embeddings for unknown words in Tensorflow (training &amp; pre-set for testing)</a> - Stack Overflow</p>      </blockquote>    </blockquote>    <blockquote>      <blockquote>        <p><a href="https://stackoverflow.com/questions/45495190/initializing-out-of-vocabulary-oov-tokens" target="_blank" rel="noopener">Initializing Out of Vocabulary (OOV) tokens</a> - Stack Overflow</p>      </blockquote>    </blockquote>  </li>  <li>    <p>基于 Char-Level 的方法</p>  </li></ul><blockquote>  <p>PaperWeekly 第七期 – <a href="https://zhuanlan.zhihu.com/p/22700538?refer=paperweekly" target="_blank" rel="noopener">基于Char-level的NMT OOV解决方案</a></p></blockquote><p><br><br>本文链接： <a href="http://www.meng.uno/articles/d1bd92c6/">http://www.meng.uno/articles/d1bd92c6/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      NLP 概述
解决 NLP 问题的一般思路
1
2
3


这个问题人类可以做好么？
  - 可以 -&gt; 记录自己的思路 -&gt; 设计流程让机器完成你的思路
  - 很难 -&gt; 尝试从计算机的角度来思考问题


NLP 的历史进程
 *  规则系统
   
    * 正则表达式/自动机
    * 规则是固定的
    * 搜索引擎 1
      2
      3
      4
      5
      
      
      “豆瓣酱用英语怎么说？”
      规则：“xx用英语怎么说？” =&gt; translate(XX, English)
      
      “我饿了”
    
    </summary>
    
      <category term="NLP" scheme="http://www.meng.uno/categories/NLP/"/>
    
    
      <category term="AI" scheme="http://www.meng.uno/tags/AI/"/>
    
      <category term="NLP" scheme="http://www.meng.uno/tags/NLP/"/>
    
      <category term="自然语言处理" scheme="http://www.meng.uno/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>CNN</title>
    <link href="http://www.meng.uno/articles/7203e497/"/>
    <id>http://www.meng.uno/articles/7203e497/</id>
    <published>2018-07-04T04:09:21.000Z</published>
    <updated>2018-08-16T04:23:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1>为什么使用 CNN 代替 RNN</h1><blockquote>  <p><a href="https://www.jiqizhixin.com/articles/041503" target="_blank" rel="noopener">关于序列建模，是时候抛弃RNN和LSTM了</a> | 机器之心 <a href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0" target="_blank" rel="noopener">[英文原文]</a></p></blockquote><p><strong>RNN/LSTM 本身的问题(3)</strong></p><ol>  <li>RNN 需要更多的资源来训练，它和 硬件加速不匹配    <blockquote>      <p>训练 RNN 和 LSTM 非常困难，因为计算能力受到内存和带宽等的约束。简单来说，每个 LSTM 单元需要四个仿射变换，且每一个时间步都需要运行一次，这样的仿射变换会要求非常多的内存带宽。<strong>添加更多的计算单元很容易，但添加更多的内存带宽却很难</strong>——这与目前的硬件加速技术不匹配，一个可能的解决方案就是让计算在存储器设备中完成。</p>    </blockquote>  </li>  <li>RNN 容易发生<strong>梯度消失</strong>，即使是 LSTM    <blockquote>      <p>在长期信息访问当前处理单元之前，需要按顺序地通过所有之前的单元。这意味着它很容易遭遇梯度消失问题；LSTM 一定程度上解决了这个问题，但 LSTM 网络中依然存在顺序访问的序列路径——直观来说，LSTM 能跳过一段信息中不太重要的部分，但如果整段信息都很重要，它依然需要完整的顺序访问，此时就跟 RNN 没有区别了。</p>    </blockquote>  </li>  <li><strong>注意力机制模块</strong>（记忆模块）的应用    <ul>      <li>注意力机制模块可以同时<strong>前向预测</strong>和<strong>后向回顾</strong>。</li>      <li><strong>分层注意力编码器</strong>（Hierarchical attention encoder）</li>    </ul>    <div align="center"><img src="http://www.meng.uno/images/assets/TIM截图20180720101423.png" height="250"></div>    - 分层注意力模块通过一个**层次结构**将过去编码向量**汇总**到一个**上下文向量**`C_t` ——这是一种更好的**观察过去信息**的方式（观点） - **分层结构**可以看做是一棵**树**，其路径长度为 `logN`，而 RNN/LSTM 则相当于一个**链表**，其路径长度为 `N`，如果序列足够长，那么可能 `N >> logN` > [放弃 RNN/LSTM 吧，因为真的不好用！望周知~](https://blog.csdn.net/heyc861221/article/details/80174475)    - CSDN博客  </li></ol><p><strong>任务角度</strong></p><ol>  <li>从任务本身考虑，我认为也是 CNN 更有利，LSTM 因为能记忆比较长的信息，所以在推断方面有不错的表现（直觉）；但是在事实类问答中，并不需要复杂的推断，答案往往藏在一个 <strong>n-gram 短语</strong>中，而 CNN 能很好的对 n-gram 建模。</li></ol><h1>常见的卷积结构</h1><blockquote>  <p><a href="https://zhuanlan.zhihu.com/p/28186857" target="_blank" rel="noopener">一文了解各种卷积结构原理及优劣</a> - 知乎 &amp; vdumoulin/    <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="noopener">conv_arithmetic</a> - GitHUub</p></blockquote><h2 id="基本卷积">基本卷积</h2><table style="width:100%; table-layout:fixed;">  <tr>    <td>No padding, no strides</td>    <td>Arbitrary padding, no strides</td>    <td>Half padding, no strides</td>    <td>Full padding, no strides</td>  </tr>  <tr>    <td><img width="150px" src="http://www.meng.uno/images/assets/no_padding_no_strides.gif"></td>    <td><img width="150px" src="http://www.meng.uno/images/assets/arbitrary_padding_no_strides.gif"></td>    <td><img width="150px" src="http://www.meng.uno/images/assets/same_padding_no_strides.gif"></td>    <td><img width="150px" src="http://www.meng.uno/images/assets/full_padding_no_strides.gif"></td>  </tr>  <tr>    <td>No padding, strides</td>    <td>Padding, strides</td>    <td>Padding, strides (odd)</td>  </tr>  <tr>    <td><img width="150px" src="http://www.meng.uno/images/assets/no_padding_strides.gif"></td>    <td><img width="150px" src="http://www.meng.uno/images/assets/padding_strides.gif"></td>    <td><img width="150px" src="http://www.meng.uno/images/assets/padding_strides_odd.gif"></td>  </tr></table><!-- TODO: 更细的分类 --><h2 id="转置卷积">转置卷积</h2><ul>  <li>转置卷积（Transposed Convolution），又称反卷积（Deconvolution）、Fractionally Strided Convolution    <blockquote>      <p>反卷积的说法不够准确，数学上有定义真正的反卷积，两者的操作是不同的</p>    </blockquote>  </li>  <li>转置卷积是卷积的<strong>逆过程</strong>，如果把基本的卷积（+池化）看做“缩小分辨率”的过程，那么转置卷积就是“<strong>扩充分辨率</strong>”的过程。    <ul>      <li>为了实现扩充的目的，需要对输入以某种方式进行<strong>填充</strong>。</li>    </ul>  </li>  <li>转置卷积与数学上定义的反卷积不同——在数值上，它不能实现卷积操作的逆过程。其内部实际上执行的是常规的卷积操作。    <ul>      <li>转置卷积只是为了<strong>重建</strong>先前的空间分辨率，执行了卷积操作。</li>    </ul>  </li>  <li>虽然转置卷积并不能还原数值，但是用于<strong>编码器-解码器结构</strong>中，效果仍然很好。——这样，转置卷积可以同时实现图像的<strong>粗粒化</strong>和卷积操作，而不是通过两个单独过程来完成。</li></ul><table style="width:100%; table-layout:fixed;">  <tr>    <td>No padding, no strides, transposed</td>    <td>Arbitrary padding, no strides, transposed</td>    <td>Half padding, no strides, transposed</td>    <td>Full padding, no strides, transposed</td>  </tr>  <tr>    <td><img width="150px" src="http://www.meng.uno/images/assets/no_padding_no_strides_transposed.gif"></td>    <td><img width="150px" src="http://www.meng.uno/images/assets/arbitrary_padding_no_strides_transposed.gif"></td>    <td><img width="150px" src="http://www.meng.uno/images/assets/same_padding_no_strides_transposed.gif"></td>    <td><img width="150px" src="http://www.meng.uno/images/assets/full_padding_no_strides_transposed.gif"></td>  </tr>  <tr>    <td>No padding, strides, transposed</td>    <td>Padding, strides, transposed</td>    <td>Padding, strides, transposed (odd)</td>  </tr>  <tr>    <td><img width="150px" src="http://www.meng.uno/images/assets/no_padding_strides_transposed.gif"></td>    <td><img width="150px" src="http://www.meng.uno/images/assets/padding_strides_transposed.gif"></td>    <td><img width="150px" src="http://www.meng.uno/images/assets/padding_strides_odd_transposed.gif"></td>  </tr></table><h2 id="空洞卷积">空洞卷积</h2><ul>  <li>空洞卷积（Atrous Convolutions）也称扩张卷积（Dilated Convolutions）、膨胀卷积。    <div align="center"><img src="http://www.meng.uno/images/assets/conv_dilation.gif" height="200"><br>No padding, no strides.</div>  </li></ul><p><strong>空洞卷积的作用</strong></p><ul>  <li>    <p>空洞卷积使 CNN 能够<strong>捕捉更远的信息，获得更大的感受野</strong>；同时不增加参数的数量，也不影响训练的速度。</p>  </li>  <li>    <p>示例：Conv1D + 空洞卷积</p>    <div align="center"><img src="http://www.meng.uno/images/assets/普通卷积与膨胀卷积.png" height="200"></div>    <!-- - 普通卷积在第三层时，每个节点只能捕捉到前后3个输入 -->  </li></ul><h2 id="可分离卷积">可分离卷积</h2><ul>  <li>可分离卷积（separable convolution）</li>  <li>TODO</li></ul><h2 id="keras-实现">Keras 实现</h2><ul>  <li>    <p>Keras 中通过在卷积层中加入参数 <code>dilation_rate</code>实现</p>    <figure class="highlight python">      <table>        <tr>          <td class="gutter">            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>          </td>          <td class="code">            <pre><span class="line">Conv1D(filters=config.filters,</span><br><span class="line">      kernel_size=config.kernel_size,</span><br><span class="line">      dilation_rate=<span class="number">2</span>)</span><br></pre>          </td>        </tr>      </table>    </figure>    <p>TODO: 维度变化</p>  </li></ul><h1>门卷积</h1><blockquote>  <p><a href="https://blog.csdn.net/stdcoutzyx/article/details/55004458" target="_blank" rel="noopener">卷积新用之语言模型</a> - CSDN博客</p></blockquote><ul>  <li>    <p>类似 LSTM 的过滤机制，实际上是卷积网络与<strong>门限单元</strong>（Gated Linear Unit）的组合</p>  </li>  <li>    <p>核心公式</p>    <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180720110804.png"></div>    <!-- \boldsymbol{Y}=\text{Conv1D}_{(1)}(\boldsymbol{X}) \otimes \sigma\Big(\text{Conv1D}_{(2)}(\boldsymbol{X})\Big)dsymbol{X})\Big) -->    <blockquote>      <p>中间的运算符表示<strong>逐位相乘</strong>—— Tensorflow 中由 <code>tf.multiply(a, b)</code> 实现，其中 a 和 b 的 shape 要相同；后一个卷积使用<code>sigmoid</code>激活函数</p>    </blockquote>  </li>  <li>    <p>一个门卷积 Block</p>    <div align="center"><img src="http://www.meng.uno/images/assets/门卷积.jpg" height=""></div>    <blockquote>      <p><code>W</code> 和 <code>V</code> 表明参数不共享</p>    </blockquote>  </li>  <li>    <p>实践中，为了防止梯度消失，还会在每个 Block 中加入残差</p>  </li></ul><h2 id="门卷积的作用">门卷积的作用</h2><ul>  <li>减缓梯度消失</li>  <li>解决语言顺序依存问题（？ TODO）</li>  <li></li></ul><h2 id="门卷积是如何防止梯度消失的">门卷积是如何防止梯度消失的</h2><ul>  <li>    <p>因为公式中有一个卷积没有经过激活函数，所以对这部分求导是个常数，所以梯度消失的概率很小。</p>  </li>  <li>    <p>如果还是担心梯度消失，还可以加入<strong>残差</strong>——要求输入输出的 shape 一致</p>    <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180720113735.png"></div>    <!-- \boldsymbol{Y}={\color{Red} \boldsymbol{X} \,+\;} \text{Conv1D}_{(1)}(\boldsymbol{X}) \otimes \sigma\Big(\text{Conv1D}_{(2)}(\boldsymbol{X})\Big) -->    <p>更直观的理解：</p>    <div align="center"><img src="http://www.meng.uno/images/assets/公式_20180720120400.png"></div>    <!-- \begin{aligned}\boldsymbol{Y}=&\,\boldsymbol{X} + {\color{Red}\text{Conv1D}_{(1)}(\boldsymbol{X})}\otimes \sigma\Big(\text{Conv1D}_{(2)}(\boldsymbol{X})\Big)\\=&\,\boldsymbol{X} + {\color{Red}\Big(\text{Conv1D}_{(1)}(\boldsymbol{X}) - \boldsymbol{X}\Big)}\otimes \sigma\Big(\text{Conv1D}_{(2)}(\boldsymbol{X})\Big)\\ =&\,\boldsymbol{X}\otimes \Big[1-\sigma\Big(\text{Conv1D}_{(2)}(\boldsymbol{X})\Big)\Big] + \text{Conv1D}_{(1)}(\boldsymbol{X}) \otimes \sigma\Big(\text{Conv1D}_{(2)}(\boldsymbol{X})\Big)\\ =&\,\boldsymbol{X}\otimes \Big(1-\boldsymbol{\sigma}\Big) + \text{Conv1D}_{(1)}(\boldsymbol{X}) \otimes \boldsymbol{\sigma} \end{aligned} -->    <p>即信息以 <code>1-σ</code> 的概率直接通过，以 <code>σ</code> 的概率经过变换后通过——类似 GRU</p>    <blockquote>      <p>因为<code>Conv1D(X)</code>没有经过激活函数，所以实际上它只是一个线性变化；因此与 <code>Conv1D(X) - X</code> 是等价的</p>      <p><a href="https://kexue.fm/archives/5409#%E9%97%A8%E6%9C%BA%E5%88%B6" target="_blank" rel="noopener">基于CNN的阅读理解式问答模型：DGCNN</a> - 科学空间|Scientific Spaces</p>    </blockquote>  </li></ul><p><br><br>本文链接： <a href="http://www.meng.uno/articles/7203e497/">http://www.meng.uno/articles/7203e497/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      为什么使用 CNN 代替 RNN
关于序列建模，是时候抛弃RNN和LSTM了 | 机器之心 [英文原文]

RNN/LSTM 本身的问题(3)

 1. RNN 需要更多的资源来训练，它和 硬件加速不匹配 训练 RNN 和 LSTM 非常困难，因为计算能力受到内存和带宽等的约束。简单来说，每个 LSTM 单元需要四个仿射变换，且每一个时间步都需要运行一次，这样的仿射变换会要求非常多的内存带宽。添加更多的计算单元很容易，但添加更多的内存带宽却很难——这与目前的硬件加速技术不匹配，一个可能的解决方案就是让计算在存储器设备中完成。
    
    
 2. RNN 容易发生梯度消失，即使是 LST
    
    </summary>
    
      <category term="DeepLearning" scheme="http://www.meng.uno/categories/DeepLearning/"/>
    
    
      <category term="CNN" scheme="http://www.meng.uno/tags/CNN/"/>
    
      <category term="深度学习" scheme="http://www.meng.uno/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://www.meng.uno/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>《深度学习》问题解答</title>
    <link href="http://www.meng.uno/articles/f42d8431/"/>
    <id>http://www.meng.uno/articles/f42d8431/</id>
    <published>2018-05-03T18:09:21.000Z</published>
    <updated>2018-08-16T04:44:14.000Z</updated>
    
    <content type="html"><![CDATA[<h1>如何设置网络的初始值？*</h1><blockquote>  <p>《深度学习》 8.4 参数初始化策略</p></blockquote><p>一般总是使用服从（截断）高斯或均匀分布的随机值，具体是高斯还是均匀分布影响不大，但是也没有详细的研究。</p><p>但是，<strong>初始值的大小</strong>会对优化结果和网络的泛化能力产生较大的影响。</p><p>更大的初始值有助于避免冗余的单元；但如果初始值太大，又会造成梯度爆炸。</p><p>一些启发式初始化策略通常是根据输入与输出的单元数来决定初始权重的大小，比如 Glorot and Bengio (2010) 中建议建议使用的标准初始化，其中 m 为输入数，n 为输出数</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=W_%7Bi,j%7D&amp;space;%5Csim&amp;space;U(-%5Csqrt%7B%5Cfrac%7B6%7D%7Bm+n%7D%7D,%5Csqrt%7B%5Cfrac%7B6%7D%7Bm+n%7D%7D)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610212719.png" alt=""></a></p><p>还有一些方法推荐使用随机正交矩阵来初始化权重 (Saxe et al., 2013)。</p><blockquote>  <p>常用的初始化策略可以参考 Keras 中文文档：<a href="http://keras-cn.readthedocs.io/en/latest/other/initializations/" target="_blank" rel="noopener">初始化方法Initializers</a></p></blockquote><h1>梯度爆炸的解决办法</h1><p><strong>梯度爆炸</strong>：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611172657.png" alt=""></p><ol>  <li>    <p><strong>梯度截断</strong>（gradient clipping）——如果梯度超过某个阈值，就对其进行限制</p>    <blockquote>      <p>《深度学习》 10.11.1 截断梯度</p>    </blockquote>    <p>下面是 Tensorflow 提供的几种方法：</p>    <ul>      <li><code>tf.clip_by_value(t, clip_value_min, clip_value_max)</code></li>      <li><code>tf.clip_by_norm(t, clip_norm)</code></li>      <li><code>tf.clip_by_average_norm(t, clip_norm)</code></li>      <li><code>tf.clip_by_global_norm(t_list, clip_norm)</code></li>    </ul>    <p>这里以<code>tf.clip_by_global_norm</code>为例：</p>    <figure class="highlight plain">      <table>        <tr>          <td class="gutter">            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>          </td>          <td class="code">            <pre><span class="line">To perform the clipping, the values `t_list[i]` are set to:</span><br><span class="line"></span><br><span class="line">    t_list[i] * clip_norm / max(global_norm, clip_norm)</span><br><span class="line"></span><br><span class="line">where:</span><br><span class="line"></span><br><span class="line">    global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))</span><br></pre>          </td>        </tr>      </table>    </figure>    <p>用法：</p>    <figure class="highlight plain">      <table>        <tr>          <td class="gutter">            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>          </td>          <td class="code">            <pre><span class="line">train_op = tf.train.AdamOptimizer()</span><br><span class="line">params = tf.trainable_variables()</span><br><span class="line">gradients = tf.gradients(loss, params)</span><br><span class="line"></span><br><span class="line">clip_norm = 100</span><br><span class="line">clipped_gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm)</span><br><span class="line"></span><br><span class="line">optimizer_op = train_op.apply_gradients(zip(clipped_gradients, params))</span><br></pre>          </td>        </tr>      </table>    </figure>    <blockquote>      <p>clip_norm 的设置视 loss 的大小而定，如果比较大，那么可以设为 100 或以上，如果比较小，可以设为 10 或以下。</p>    </blockquote>  </li>  <li>    <p>良好的参数初始化策略也能缓解梯度爆炸问题（权重正则化）</p>  </li>  <li>    <p>使用线性整流激活函数，如 ReLU 等</p>  </li></ol><h1>神经网络（MLP）的万能近似定理*</h1><blockquote>  <p>《深度学习》 6.4.1 万能近似性质和深度</p></blockquote><p>一个前馈神经网络如果具有至少一个非线性输出层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。</p><h1>神经网络中，深度与宽度的关系，及其表示能力的差异**</h1><blockquote>  <p>《深度学习》 6.4 - 架构设计；这一节的内容比较分散，想要更好的回答这个问题，需要理解深度学习的本质——学习多层次组合（ch1.2），这才是现代深度学习的基本原理。</p></blockquote><p>隐藏层的数量称为模型的<strong>深度</strong>，隐藏层的维数（单元数）称为该层的<strong>宽度</strong>。</p><p><strong>万能近似定理</strong>表明一个单层的网络就足以表达任意函数，但是该层的维数可能非常大，且几乎没有泛化能力；此时，使用更深的模型能够减少所需的单元数，同时增强泛化能力（减少泛化误差）。参数数量相同的情况下，浅层网络比深层网络更容易过拟合。</p><h1>在深度神经网络中，引入了隐藏层（非线性单元），放弃了训练问题的凸性，其意义何在？**</h1><blockquote>  <p>《深度学习》 6 深度前馈网络（引言） &amp; 6.3 隐藏单元</p></blockquote><p>放弃训练问题的凸性，简单来说，就是放弃寻求问题的最优解。</p><p><strong>非线性单元</strong>的加入，使训练问题不再是一个<strong>凸优化</strong>问题。这意味着神经网络很难得到最优解，即使一个只有两层和三个节点的简单神经网络，其训练优化问题仍然是 NP-hard 问题 (Blum &amp; Rivest, 1993).</p><blockquote>  <p><a href="http://baijiahao.baidu.com/s?id=1561255903377484&amp;wfr=spider&amp;for=pc%EF%BC%89" target="_blank" rel="noopener">深度学习的核心问题——NP-hard问题</a> - 百家号</p></blockquote><p>但即使如此，使用神经网络也是利大于弊的：</p><ul>  <li>人类设计者只需要寻找正确的<strong>函数族</strong>即可，而不需要去寻找精确的函数。</li>  <li>使用简单的梯度下降优化方法就可以高效地找到足够好的局部最小值</li>  <li>增强了模型的学习/拟合能力，如原书中所说“ maxout 单元可以以任意精度近似任何凸函数”。至于放弃凸性后的优化问题可以在结合工程实践来不断改进。 “似乎传统的优化理论结果是残酷的，但我们可以通过<strong>工程方法</strong>和<strong>数学技巧</strong>来尽量规避这些问题，例如启发式方法、增加更多的机器和使用新的硬件（如GPU）。”</li></ul><blockquote>  <p><a href="https://github.com/elviswf/DeepLearningBookQA_cn/issues/1#issuecomment-396061806" target="_blank" rel="noopener">Issue #1</a> · elviswf/DeepLearningBookQA_cn</p></blockquote><h1>稀疏表示，低维表示，独立表示*</h1><blockquote>  <p>《深度学习》 5.8 无监督学习算法</p></blockquote><p>无监督学习任务的目的是找到数据的“最佳”表示。“最佳”可以有不同的表示，但是一般来说，是指该表示在比本身表示的信息更简单的情况下，尽可能地保存关于 x 更多的信息。</p><p>低维表示、稀疏表示和独立表示是最常见的三种“简单”表示：1）低维表示尝试将 x 中的信息尽可能压缩在一个较小的表示中；2）稀疏表示将数据集嵌入到输入项大多数为零的表示中；3）独立表示试图分开数据分布中变化的来源，使得表示的维度是统计独立的。</p><p>这三种表示不是互斥的，比如主成分分析（PCA）就试图同时学习低维表示和独立表示。</p><p>表示的概念是深度学习的核心主题之一。</p><h1>局部不变性（平滑先验）及其在基于梯度的学习上的局限性*</h1><blockquote>  <p>《深度学习》 5.11.2 局部不变性与平滑正则化</p></blockquote><p>局部不变性：函数在局部小区域内不会发生较大的变化。</p><p>为了更好地<strong>泛化</strong>，机器学习算法需要由一些先验来引导应该学习什么类型的函数。</p><p>其中最广泛使用的“隐式先验”是平滑先验（smoothness prior），也称局部不变性先验（local constancy prior）。许多简单算法完全依赖于此先验达到良好的（局部）泛化，一个极端例子是 k-最近邻系列的学习算法。</p><p>但是仅依靠平滑先验<strong>不足以</strong>应对人工智能级别的任务。简单来说，区分输入空间中 O(k) 个区间，需要 O(k) 个样本，通常也会有 O(k) 个参数。最近邻算法中，每个训练样本至多用于定义一个区间。类似的，决策树也有平滑学习的局限性。</p><p>以上问题可以总结为：是否可以有效地表示复杂的函数，以及所估计的函数是否可以很好地泛化到新的输入。该问题的一个关键观点是，只要我们通过额外假设生成数据的分布来建立区域间的依赖关系，那么 O(k) 个样本足以描述多如 O(2^k) 的大量区间。通过这种方式，能够做到<strong>非局部的泛化</strong>。</p><blockquote>  <p>一些其他的机器学习方法往往会提出更强的，针对特定问题的假设，例如周期性。通常，神经网络不会包含这些很强的针对性假设——深度学习的核心思想是假设数据由因素或特征组合产生，这些因素或特征可能来自一个层次结构的多个层级。许多其他类似的通用假设进一步提高了深度学习算法。这些很温和的假设允许了样本数目和可区分区间数目之间的<strong>指数增益</strong>。深度的分布式表示带来的指数增益有效地解决了维数灾难带来的挑战</p>  <blockquote>    <p>指数增益：《深度学习》 ch6.4.1、ch15.4、ch15.5</p>  </blockquote></blockquote><h1>为什么交叉熵损失相比均方误差损失能提高以 sigmoid 和 softmax 作为激活函数的层的性能？**</h1><blockquote>  <p>《深度学习》 6.6 小结 中提到了这个结论，但是没有给出具体原因（可能在前文）。</p></blockquote><p>简单来说，就是使用均方误差（MSE）作为损失函数时，会导致大部分情况下<strong>梯度偏小</strong>，其结果就是权重的更新很慢，且容易造成“梯度消失”现象。而交叉熵损失克服了这个缺点，当误差大的时候，权重更新就快，当误差小的时候，权重的更新才慢。</p><p>具体推导过程如下：</p><blockquote>  <p><a href="https://blog.csdn.net/guoyunfei20/article/details/78247263" target="_blank" rel="noopener">https://blog.csdn.net/guoyunfei20/article/details/78247263</a> - CSDN 博客</p>  <p>这里给出了一个具体的<a href="https://blog.csdn.net/shmily_skx/article/details/53053870" target="_blank" rel="noopener">例子</a></p></blockquote><h1>分段线性单元（如 ReLU）代替 sigmoid 的利弊</h1><ul>  <li>    <p>当神经网络比较小时，sigmoid 表现更好；</p>  </li>  <li>    <p>在深度学习早期，人们认为应该避免具有不可导点的激活函数，而 ReLU 不是全程可导/可微的</p>  </li>  <li>    <p>sigmoid 和 tanh 的输出是有界的，适合作为下一层的输入，以及整个网络的输出。实际上，目前大多数网络的输出层依然使用的 sigmoid（单输出） 或 softmax（多输出）。</p>    <blockquote>      <p>为什么 ReLU 不是全程可微也能用于基于梯度的学习？——虽然 ReLU 在 0 点不可导，但是它依然存在左导数和右导数，只是它们不相等（相等的话就可导了），于是在实现时通常会返回左导数或右导数的其中一个，而不是报告一个导数不存在的错误。</p>      <blockquote>        <p>一阶函数：可微==可导</p>      </blockquote>    </blockquote>  </li>  <li>    <p>对于小数据集，使用整流非线性甚至比学习隐藏层的权重值更加重要 (Jarrett et al., 2009b)</p>  </li>  <li>    <p>当数据增多时，在深度整流网络中的学习比在激活函数具有曲率或两侧<strong>饱和</strong>的深度网络中的学习更容易 (Glorot et al., 2011a)：传统的 sigmoid 函数，由于两端饱和，在传播过程中容易丢弃信息</p>  </li>  <li>    <p>ReLU 的过程更接近生物神经元的作用过程</p>    <blockquote>      <p>饱和（saturate）现象：在函数图像上表现为变得很平，对输入的微小改变会变得不敏感。</p>    </blockquote>  </li></ul><blockquote>  <p><a href="https://blog.csdn.net/code_lr/article/details/51836153" target="_blank" rel="noopener">https://blog.csdn.net/code_lr/article/details/51836153</a> - CSDN博客</p>  <blockquote>    <p>答案总结自该知乎问题：<a href="https://www.zhihu.com/question/29021768" target="_blank" rel="noopener">https://www.zhihu.com/question/29021768</a></p>  </blockquote></blockquote><h1>在做正则化过程中，为什么只对权重做正则惩罚，而不对偏置做权重惩罚*</h1><blockquote>  <p>《深度学习》 7.1 参数范数惩罚</p></blockquote><p>在神经网络中，参数包括每一层仿射变换的<strong>权重</strong>和<strong>偏置</strong>，我们通常只对权重做惩罚而不对偏置做正则惩罚。</p><p>精确拟合偏置所需的数据通常比拟合权重少得多。每个权重会指定两个变量如何相互作用。我们需要在各种条件下观察这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。另外，正则化偏置参数可能会导致明显的欠拟合。</p><h1>列举常见的一些范数及其应用场景，如 L0、L1、L2、L∞、Frobenius等范数**</h1><blockquote>  <p>《深度学习》 2.5 范数（介绍）</p></blockquote><p>L0: 向量中非零元素的个数</p><p>L1: 向量中所有元素的绝对值之和</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cleft&amp;space;%7C&amp;space;x&amp;space;%5Cright&amp;space;%7C_1=%5Csum_i%7B%5Cleft&amp;space;%7C&amp;space;x_i&amp;space;%5Cright&amp;space;%7C%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213145.png" alt=""></a></p><p>L2: 向量中所有元素平方和的开放</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cleft&amp;space;%7C&amp;space;x&amp;space;%5Cright&amp;space;%7C_2=%5Csqrt%7B%5Csum_i%7B%5Cleft&amp;space;%7C&amp;space;x_i&amp;space;%5Cright&amp;space;%7C%5E2%7D%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213218.png" alt=""></a></p><p>其中 L1 和 L2 范数分别是 Lp (p&gt;=1) 范数的特例：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cleft&amp;space;%7C&amp;space;x&amp;space;%5Cright&amp;space;%7C_p=(%5Csum_i%7B%5Cleft&amp;space;%7C&amp;space;x_i&amp;space;%5Cright&amp;space;%7C%5E2%7D)%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213257.png" alt=""></a></p><p>L∞: 向量中最大元素的绝对值，也称最大范数</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cleft&amp;space;%7C&amp;space;x&amp;space;%5Cright&amp;space;%7C_%5Cinfty=%5Cmax_i%5Cleft&amp;space;%7C&amp;space;x&amp;space;%5Cright&amp;space;%7C" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213349.png" alt=""></a></p><p>Frobenius 范数：相当于作用于矩阵的 L2 范数</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cleft&amp;space;%7C&amp;space;A&amp;space;%5Cright&amp;space;%7C_F=%5Csqrt%7B%5Csum_%7Bi,j%7DA_%7Bi,j%7D%5E2%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213428.png" alt=""></a></p><p><strong>范数的应用</strong>：正则化——权重衰减/参数范数惩罚</p><p><strong>权重衰减的目的</strong></p><p>限制模型的学习能力，通过限制参数 θ 的规模（主要是权重 w 的规模，偏置 b 不参与惩罚），使模型偏好于权值较小的目标函数，防止过拟合。</p><blockquote>  <p>《深度学习》 7.1 参数范数惩罚</p></blockquote><h1>L1 和 L2 范数的异同***</h1><blockquote>  <p>《深度学习》 7.1.1 L2 参数正则化 &amp; 7.1.2 - L1 参数正则化</p></blockquote><p><strong>相同点</strong></p><ul>  <li>限制模型的学习能力，通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。</li></ul><p><strong>不同点</strong></p><ul>  <li>L1 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择；一定程度上防止过拟合</li>  <li>L2 正则化主要用于防止模型过拟合</li>  <li>L1 适用于特征之间有关联的情况；L2 适用于特征之间没有关联的情况</li></ul><blockquote>  <p><a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">机器学习中正则化项L1和L2的直观理解</a> - CSDN博客</p></blockquote><h1>为什么 L1 正则化可以产生稀疏权值，L2 正则化可以防止过拟合？**</h1><h2 id="为什么-l1-正则化可以产生稀疏权值-而-l2-不会？">为什么 L1 正则化可以产生稀疏权值，而 L2 不会？</h2><p>添加 L1 正则化，相当于在 L1范数的约束下求目标函数 J 的最小值，下图展示了二维的情况：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608171710.png" alt=""></p><p>图中 J 与 L 首次相交的点就是最优解。L1 在和每个坐标轴相交的地方都会有“角”出现（多维的情况下，这些角会更多），在角的位置就会产生稀疏的解。而 J 与这些“角”相交的机会远大于其他点，因此 L1 正则化会产生稀疏的权值。</p><p>类似的，可以得到带有 L2正则化的目标函数在二维平面上的图形，如下：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608172312.png" alt=""></p><p>相比 L1，L2 不会产生“角”，因此 J 与 L2 相交的点具有稀疏性的概率就会变得非常小。</p><blockquote>  <p><a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">机器学习中正则化项L1和L2的直观理解</a> - CSDN博客</p></blockquote><h2 id="为什么-l1-和-l2-正则化可以防止过拟合？">为什么 L1 和 L2 正则化可以防止过拟合？</h2><p>L1 &amp; L2 正则化会使模型偏好于更小的权值。</p><p>简单来说，更小的权值意味着更低的模型复杂度，也就是对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据（比如异常点，噪声），以提高模型的泛化能力。</p><p>此外，添加正则化相当于为模型添加了某种<strong>先验</strong>（限制），规定了参数的分布，从而降低了模型的复杂度。模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。</p><blockquote>  <p><a href="https://blog.csdn.net/heyongluoyao8/article/details/49429629" target="_blank" rel="noopener">机器学习中防止过拟合的处理方法</a> - CSDN博客</p></blockquote><h1>简单介绍常用的激活函数，如 sigmoid、relu、softplus、tanh、RBF 及其应用场景***</h1><h2 id="整流线性单元-relu">整流线性单元（ReLU）</h2><p><a href="http://www.codecogs.com/eqnedit.php?latex=g(z)=%5Cmax(0,z)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213451.png" alt=""></a></p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608212808.png" alt=""></p><p>整流线性单元（ReLU）通常是激活函数较好的默认选择。</p><p>整流线性单元易于优化，因为它们和线性单元非常类似。线性单元和整流线性单元的唯一区别在于整流线性单元在其一半的定义域上输出为零。这使得只要整流线性单元处于激活状态，它的导数都能保持较大。它的梯度不仅大而且一致。整流操作的二阶导数几乎处处为 0，并且在整流线性单元处于激活状态时，它的一阶导数处处为 1。这意味着相比于引入二阶效应的激活函数来说，它的梯度方向对于学习来说更加有用。</p><p><strong>ReLU 的拓展</strong></p><p>ReLU 的三种拓展都是基于以下变型：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=g(z,%5Calpha)&amp;space;=%5Cmax(0,z)+%5Calpha%5Cmin(0,z)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214123.png" alt=""></a></p><p>ReLU 及其扩展都是基于一个原则，那就是如果它们的行为更接近线性，那么模型更容易优化。</p><ul>  <li>    <p>绝对值整流（absolute value rectification）</p>    <p>固定 α == -1，此时整流函数即一个绝对值函数</p>    <p><a href="http://www.codecogs.com/eqnedit.php?latex=g(z)&amp;space;=%5Cleft&amp;space;%7C&amp;space;z&amp;space;%5Cright&amp;space;%7C" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214502.png" alt=""></a></p>    <p>绝对值整流被用于图像中的对象识别 (Jarrett et al., 2009a)，其中寻找在输入照明极性反转下不变的特征是有意义的。</p>  </li>  <li>    <p>渗漏整流线性单元（Leaky ReLU, Maas et al., 2013）</p>    <p>固定 α 为一个类似于 0.01 的小值</p>  </li>  <li>    <p>参数化整流线性单元（parametric ReLU, PReLU, He et al., 2015）</p>    <p>将 α 作为一个参数学习</p>  </li>  <li>    <p>maxout 单元 (Goodfellow et al., 2013a)</p>    <p>maxout 单元 进一步扩展了 ReLU，它是一个可学习的多达 k 段的分段函数</p>    <p>关于 maxout 网络的分析可以参考论文或网上的众多分析，下面是 Keras 中的实现：</p>    <figure class="highlight plain">      <table>        <tr>          <td class="gutter">            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>          </td>          <td class="code">            <pre><span class="line"># input shape:  [n, input_dim]</span><br><span class="line"># output shape: [n, output_dim]</span><br><span class="line">W = init(shape=[k, input_dim, output_dim])</span><br><span class="line">b = zeros(shape=[k, output_dim])</span><br><span class="line">output = K.max(K.dot(x, W) + b, axis=1)</span><br></pre>          </td>        </tr>      </table>    </figure>    <blockquote>      <p><a href="https://blog.csdn.net/hjimce/article/details/50414467" target="_blank" rel="noopener">深度学习（二十三）Maxout网络学习</a> - CSDN博客</p>    </blockquote>  </li></ul><h2 id="sigmoid-与-tanh-双曲正切函数">sigmoid 与 tanh（双曲正切函数）</h2><p>在引入 ReLU 之前，大多数神经网络使用 sigmoid 激活函数：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=g(z)=%5Csigma(z)=%5Cfrac%7B1%7D%7B1+%5Cexp(-z)%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214846.png" alt=""></a></p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608195851.png" alt=""></p><p>或者 tanh（双曲正切函数）：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=g(z)&amp;space;=&amp;space;%5Ctanh(z)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214926.png" alt=""></a></p><p>tanh 的图像类似于 sigmoid，区别在其值域为 (-1, 1).</p><p>这两个函数有如下关系：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Ctanh(z)=2%5Csigma&amp;space;(2z)-1" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215029.png" alt=""></a></p><p><strong>sigmoid 函数要点</strong>：</p><ul>  <li>sigmoid 常作为输出单元用来预测二值型变量取值为 1 的概率    <blockquote>      <p>换言之，sigmoid 函数可以用来产生<strong>伯努利分布</strong>中的参数 ϕ，因为它的值域为 (0, 1).</p>    </blockquote>  </li>  <li>sigmoid 函数在输入取绝对值非常大的正值或负值时会出现<strong>饱和</strong>（saturate）现象，在图像上表现为开始变得很平，此时函数会对输入的微小改变会变得不敏感。仅当输入接近 0 时才会变得敏感。    <blockquote>      <p>饱和现象会导致基于梯度的学习变得困难，并在传播过程中丢失信息。——<a href="#8.-%E5%88%86%E6%AE%B5%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83%EF%BC%88%E5%A6%82-ReLU%EF%BC%89%E4%BB%A3%E6%9B%BF-sigmoid-%E7%9A%84%E5%88%A9%E5%BC%8A">为什么用ReLU代替sigmoid？</a></p>    </blockquote>  </li>  <li>如果要使用 sigmoid 作为激活函数时（浅层网络），tanh 通常要比 sigmoid 函数表现更好。    <blockquote>      <p>tanh 在 0 附近与单位函数类似，这使得训练 tanh 网络更容易些。</p>    </blockquote>  </li></ul><h2 id="其他激活函数-隐藏单元">其他激活函数（隐藏单元）</h2><p>很多未发布的非线性激活函数也能表现的很好，但没有比流行的激活函数表现的更好。比如使用 cos 也能在 MNIST 任务上得到小于 1% 的误差。通常新的隐藏单元类型只有在被明确证明能够提供显著改进时才会被发布。</p><p><strong>线性激活函数</strong>：</p><p>如果神经网络的每一层都由线性变换组成，那么网络作为一个整体也将是线性的，这会导致失去万能近似的性质。但是，仅<strong>部分层是纯线性</strong>是可以接受的，这可以帮助<strong>减少网络中的参数</strong>。</p><p><strong>softmax</strong>：</p><p>softmax 单元常作为网络的输出层，它很自然地表示了具有 k 个可能值的离散型随机变量的概率分布。</p><p><strong>径向基函数（radial basis function, RBF）</strong>：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=h_i=%5Cexp(-%5Cfrac%7B1%7D%7B%5Csigma_i%5E2%7D%5Cleft&amp;space;%7C&amp;space;W_%7B:,i%7D-x&amp;space;%5Cright&amp;space;%7C%5E2)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215150.png" alt=""></a></p><p>在神经网络中很少使用 RBF 作为激活函数，因为它对大部分 x 都饱和到 0，所以很难优化。</p><p><strong>softplus</strong>：</p><p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215222.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=g(z)=%5Czeta(z)=%5Clog(1+%5Cexp(z)" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=g(z)=\zeta(z)=\log(1+\exp(z)</a>))</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608204913.png" alt=""></p><p>softplus 是 ReLU 的平滑版本。通常不鼓励使用 softplus 函数，大家可能希望它具有优于整流线性单元的点，但根据经验来看，它并没有。</p><blockquote>  <p>(Glorot et al., 2011a) 比较了这两者，发现 ReLU 的结果更好。</p></blockquote><p><strong>硬双曲正切函数（hard tanh）</strong>：</p><p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215308.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=g(z)=%5Cmax(-1,%5Cmin(1,a)" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=g(z)=\max(-1,\min(1,a)</a>))</p><p>它的形状和 tanh 以及整流线性单元类似，但是不同于后者，它是有界的。</p><blockquote>  <p>Collobert, 2004</p></blockquote><h2 id="sigmoid-和-softplus-的一些性质">sigmoid 和 softplus 的一些性质</h2><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608205223.png" alt=""></p><blockquote>  <p>《深度学习》 3.10 常用函数的有用性质</p></blockquote><h1>Jacobian 和 Hessian 矩阵及其在深度学习中的重要性*</h1><blockquote>  <p>《深度学习》 4.3.1 梯度之上：Jacobian 和 Hessian 矩阵</p></blockquote><h1>信息熵、KL 散度（相对熵）与交叉熵**</h1><blockquote>  <p>《深度学习》 3.13 信息论</p></blockquote><p>信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事件发生，能提供更多的信息。</p><p>该想法可描述为以下性质：</p><ol>  <li>非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量。</li>  <li>比较不可能发生的事件具有更高的信息量。</li>  <li>独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。</li></ol><h2 id="自信息与信息熵">自信息与信息熵</h2><p>自信息（self-information）是一种量化以上性质的函数，定义一个事件 x 的自信息为：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=I(x)=-%5Clog&amp;space;P(x)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215339.png" alt=""></a></p><blockquote>  <p>当该对数的底数为 e 时，单位为奈特（nats，本书标准）；当以 2 为底数时，单位为比特（bit）或香农（shannons）</p></blockquote><p>自信息只处理单个的输出。此时，用信息熵（Information-entropy）来对整个概率分布中的不确定性总量进行量化：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=H(%5Cmathrm%7BX%7D)=%5Cmathbb%7BE%7D_%7B%5Cmathrm%7BX%7D&amp;space;%5Csim&amp;space;P%7D%5BI(x)%5D=-%5Csum_%7Bx&amp;space;%5Cin&amp;space;%5Cmathrm%7BX%7D%7DP(x)%5Clog&amp;space;P(x)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215417.png" alt=""></a></p><blockquote>  <p>信息熵也称香农熵（Shannon entropy）</p>  <p>信息论中，记 <code>0log0 = 0</code></p></blockquote><h2 id="相对熵-kl-散度-与交叉熵">相对熵（KL 散度）与交叉熵</h2><p>P 对 Q 的 <strong>KL散度</strong>（Kullback-Leibler divergence）：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=D_P(Q)=%5Cmathbb%7BE%7D_%7B%5Cmathrm%7BX%7D%5Csim&amp;space;P%7D%5Cleft&amp;space;%5B&amp;space;%5Clog&amp;space;%5Cfrac%7BP(x)%7D%7BQ(x)%7D&amp;space;%5Cright&amp;space;%5D=%5Csum_%7Bx&amp;space;%5Cin&amp;space;%5Cmathrm%7BX%7D%7DP(x)%5Cleft&amp;space;%5B&amp;space;%5Clog&amp;space;P(x)-%5Clog&amp;space;Q(x)&amp;space;%5Cright&amp;space;%5D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215445.png" alt=""></a></p><p><strong>KL 散度在信息论中度量的是那个直观量</strong>：</p><p>在离散型变量的情况下， KL 散度衡量的是，当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号的消息时，所需要的额外信息量。</p><p><strong>KL 散度的性质</strong>：</p><ul>  <li>非负；KL 散度为 0 当且仅当P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的</li>  <li>不对称；D_p(q) != D_q§</li></ul><p><strong>交叉熵</strong>（cross-entropy）：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=H_P(Q)=-%5Cmathbb%7BE%7D_%7B%5Cmathrm%7BX%7D%5Csim&amp;space;P%7D%5Clog&amp;space;Q(x)=-%5Csum_%7Bx&amp;space;%5Cin&amp;space;%5Cmathrm%7BX%7D%7DP(x)%5Clog&amp;space;Q(x)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215522.png" alt=""></a></p><blockquote>  <p><a href="https://blog.csdn.net/haolexiao/article/details/70142571" target="_blank" rel="noopener">信息量，信息熵，交叉熵，KL散度和互信息（信息增益）</a> - CSDN博客</p></blockquote><p><strong>交叉熵与 KL 散度的关系</strong>：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=H_P(Q)=H(P)+D_P(Q)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215554.png" alt=""></a></p><p><strong>针对 Q 最小化交叉熵等价于最小化 P 对 Q 的 KL 散度</strong>，因为 Q 并不参与被省略的那一项。</p><p>最大似然估计中，最小化 KL 散度其实就是在最小化分布之间的交叉熵。</p><blockquote>  <p>《深度学习》 ch5.5 - 最大似然估计</p></blockquote><h1>如何避免数值计算中的上溢和下溢问题，以 softmax 为例*</h1><blockquote>  <p>《深度学习》 4.1 上溢与下溢</p></blockquote><ul>  <li><strong>上溢</strong>：一个很大的数被近似为 ∞ 或 -∞；</li>  <li><strong>下溢</strong>：一个很小的数被近似为 0</li></ul><p>必须对上溢和下溢进行<strong>数值稳定</strong>的一个例子是 <strong>softmax 函数</strong>：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cmathrm%7Bsoftmax%7D(x)=%5Cfrac%7B%5Cexp(x_i)%7D%7B%5Csum_%7Bj=1%7D%5En&amp;space;%5Cexp(x_j)%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215623.png" alt=""></a></p><p>因为 softmax 解析上的函数值不会因为从输入向量减去或加上<strong>标量</strong>而改变， 于是一个简单的解决办法是对 x：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=x=x-%5Cmax_ix_i" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215656.png" alt=""></a></p><p>减去 <code>max(x_i)</code> 导致 <code>exp</code> 的最大参数为 <code>0</code>，这排除了上溢的可能性。同样地，分母中至少有一个值为 <code>1=exp(0)</code> 的项，这就排除了因分母下溢而导致被零除的可能性。</p><p><strong>注意</strong>：虽然解决了分母中的上溢与下溢问题，但是分子中的下溢仍可以导致整体表达式被计算为零。此时如果计算 log softmax(x) 时，依然要注意可能造成的上溢或下溢问题，处理方法同上。</p><p>当然，大多数情况下，这是底层库开发人员才需要注意的问题。</p><h1>训练误差、泛化误差；过拟合、欠拟合；模型容量，表示容量，有效容量，最优容量的概念； 奥卡姆剃刀原则*</h1><blockquote>  <p>《深度学习》 5.2 容量、过拟合和欠拟合</p></blockquote><h2 id="过拟合的一些解决方案">过拟合的一些解决方案***</h2><ul>  <li>参数范数惩罚（Parameter Norm Penalties）</li>  <li>数据增强（Dataset Augmentation）</li>  <li>提前终止（Early Stopping）</li>  <li>参数绑定与参数共享（Parameter Tying and Parameter Sharing）</li>  <li>Bagging 和其他集成方法</li>  <li>Dropout</li>  <li>批标准化（Batch Normalization）</li></ul><h1>高斯分布的广泛应用的原因**</h1><blockquote>  <p>《深度学习》 3.9.3 高斯分布</p></blockquote><h2 id="高斯分布-gaussian-distribution">高斯分布（Gaussian distribution）</h2><p>高斯分布，即正态分布（normal distribution）：</p><p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215732.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=N" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=N</a>(x;\mu,\sigma<sup>2)=\sqrt\frac{1}{2\pi\sigma</sup>2}\exp\left&amp;space;(&amp;space;-\frac{1}{2\sigma<sup>2}(x-\mu)</sup>2&amp;space;\right&amp;space;))</p><p>概率密度函数图像：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610131620.png" alt=""></p><p>其中峰的 <code>x</code> 坐标由 <code>µ</code> 给出，峰的宽度受 <code>σ</code> 控制；特别的，当 <code>µ = 0, σ = 1</code>时，称为标准正态分布</p><p>正态分布的均值 <code>E = µ</code>；标准差 <code>std = σ</code>，方差为其平方</p><h2 id="为什么推荐使用高斯分布？">为什么推荐使用高斯分布？</h2><p>当我们由于缺乏关于某个实数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选择，其中有两个原因：</p><ol>  <li>我们想要建模的很多分布的真实情况是比较接近正态分布的。<strong>中心极限定理</strong>（central limit theorem）说明很多独立随机变量的和近似服从正态分布。这意味着在实际中，很多复杂系统都可以被成功地建模成正态分布的噪声，即使系统可以被分解成一些更结构化的部分。</li>  <li>第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。    <blockquote>      <p>关于这一点的证明：《深度学习》 ch19.4.2 - 变分推断和变分学习</p>    </blockquote>  </li></ol><p><strong>多维正态分布</strong></p><p>正态分布可以推广到 n 维空间，这种情况下被称为<strong>多维正态分布</strong>。</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610132602.png" alt=""></p><p>参数 <code>µ</code> 仍然表示分布的均值，只不过现在是一个向量。参数 Σ 给出了分布的协方差矩阵（一个正定对称矩阵）。</p><h1>表示学习、自编码器与深度学习**</h1><p><strong>表示学习</strong>：</p><p>对于许多任务来说，我们很难知道应该提取哪些特征。解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为<strong>表示学习</strong>（representation learning）。学习到的表示往往比手动设计的表示表现得更好。并且它们只需最少的人工干预，就能让AI系统迅速适应新的任务。</p><p><strong>自编码器</strong>：</p><p>表示学习算法的典型例子是 自编码器（autoencoder）。自编码器由一个<strong>编码器</strong>（encoder）函数和一个<strong>解码器</strong>（decoder）函数组合而成。</p><ul>  <li>编码器函数将输入数据转换为一种不同的表示;</li>  <li>解码器函数则将这个新的表示转换到原来的形式。</li></ul><p>我们期望当输入数据经过编码器和解码器之后<strong>尽可能多地保留信息</strong>，同时希望新的表示有一些好的特性，这也是自编码器的训练目标。</p><p><strong>深度学习</strong>：</p><p>深度学习（deep learning）通过简单的表示来表达复杂的表示，以解决表示学习中的核心问题。</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610194353.png" alt=""></p><p>深度学习模型的示意图</p><p>计算机难以理解原始感观输入数据的含义，如表示为像素值集合的图像，将一组像素映射到对象标识的函数非常复杂。深度学习将所需的复杂映射分解为一系列嵌套的简单映射（每个由模型的不同层描述）来解决这一难题。</p><p>输入展示在<strong>可见层</strong>（visible layer），这样命名的原因是因为它包含我们能观察到的变量。然后是一系列从图像中提取越来越多抽象特征的<strong>隐藏层</strong>（hidden layer），称为“隐藏”的原因是因为它们的值不在数据中给出。</p><p>模型必须确定哪些概念有利于解释观察数据中的关系。这里的图像是每个隐藏单元表示的特征的可视化。给定像素，<strong>第一隐藏层</strong>可以轻易地通过比较相邻像素的亮度来<strong>识别边缘</strong>。有了第一隐藏层描述的边缘，<strong>第二隐藏层</strong>可以容易地<strong>搜索轮廓和角</strong>。给定第二隐藏层中关于角和轮廓的图像描述，<strong>第三隐藏层</strong>可以找到轮廓和角的特定集合来<strong>检测整个特定对象</strong>。最后，根据图像描述中包含的对象部分，可以识别图像中存在的对象。</p><blockquote>  <p>实际任务中并不一定具有这么清晰的可解释性，很多时候你并不知道每个隐藏层到底识别出了哪些特征。</p></blockquote><p>学习数据的正确表示的想法是<strong>解释深度学习</strong>的一个视角。</p><blockquote>  <p>另一个视角是深度促使计算机学习一个多步骤的计算机程序。——《深度学习》 ch1 - 引言</p>  <p>早期的深度学习称为神经网络，因为其主要指导思想来源于生物神经学。从神经网络向深度学习的术语转变也是因为指导思想的改变。</p></blockquote><h1>L1、L2 正则化与 MAP 贝叶斯推断的关系*</h1><blockquote>  <p>《深度学习》 5.6.1 最大后验 (MAP) 估计</p></blockquote><p>许多正则化策略可以被解释为 MAP 贝叶斯推断：</p><ul>  <li>L2 正则化相当于权重是高斯先验的 MAP 贝叶斯推断</li>  <li>对于 L1正则化，用于正则化代价函数的惩罚项与通过 MAP 贝叶斯推断最大化的<strong>对数先验项</strong>是等价的</li></ul><h1>什么是欠约束，为什么大多数的正则化可以使欠约束下的欠定问题在迭代过程中收敛*</h1><h1>为什么考虑在模型训练时对输入 (隐藏单元或权重) 添加方差较小的噪声？*</h1><p>对于某些模型而言，向输入添加方差极小的噪声等价于对权重施加范数惩罚 (Bishop, 1995a,b)。</p><p>在一般情况下，<strong>注入噪声比简单地收缩参数强大</strong>。特别是噪声被添加到<strong>隐藏单元</strong>时会更加强大，<strong>Dropout</strong> 方法正是这种做法的主要发展方向。</p><p>另一种正则化模型的噪声使用方式是将其加到<strong>权重</strong>。这项技术主要用于循环神经网络 (Jim et al., 1996; Graves, 2011)。这可以被解释为关于权重的贝叶斯推断的随机实现。贝叶斯学习过程将权重视为不确定的，并且可以<strong>通过概率分布表示这种不确定性</strong>。向权重添加噪声是反映这种不确定性的一种实用的随机方法。</p><h1>多任务学习、参数绑定和参数共享***</h1><h2 id="多任务学习">多任务学习</h2><blockquote>  <p>《深度学习》 7.7 多任务学习</p></blockquote><p>多任务学习 (Caruana, 1993) 是通过<strong>合并多个任务中的样例</strong>（可以视为对参数施加软约束）来提高泛化的一种方式。</p><p>正如额外的训练样本能够将模型参数推向具有更好泛化能力的值一样，当<strong>模型的一部分被多个额外的任务共享</strong>时，这部分将被约束为良好的值（如果共享合理），通常会带来更好的泛化能力。</p><p><strong>多任务学习中一种普遍形式</strong>：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610203703.png" alt=""></p><p>多任务学习在深度学习框架中可以以多种方式进行，该图展示了一种普遍形式：任务共享相同输入但涉及不同语义的输出。</p><p>在该示例中，额外假设顶层隐藏单元 h(1) 和 h(2) 专用于不同的任务——分别预测 y(1) 和 y(2)，而一些<strong>中间层表示</strong> h(shared) 在所有任务之间共享；h(3) 表示无监督学习的情况。</p><p>这里的基本假设是存在解释输入 x 变化的<strong>共同因素池</strong>，而每个任务与这些因素的<strong>子集</strong>相关联。</p><p>该模型通常可以分为两类相关的参数：</p><ol>  <li>具体任务的参数（只能从各自任务的样本中实现良好的泛化）</li>  <li>所有任务共享的通用参数（从所有任务的汇集数据中获益）——参数共享</li></ol><p>因为<strong>共享参数</strong>，其统计强度可大大提高（共享参数的样本数量相对于单任务模式增加的比例），并能改善泛化和泛化误差的范围 (Baxter, 1995)。</p><p>参数共享仅当不同的任务之间存在某些统计关系的假设是合理（意味着某些参数能通过不同任务共享）时才会发生这种情况</p><h2 id="参数绑定和参数共享">参数绑定和参数共享</h2><blockquote>  <p>《深度学习》 7.9 参数绑定和参数共享</p></blockquote><p><strong>参数绑定</strong>：</p><p>有时，我们可能无法准确地知道应该使用什么样的参数，但我们根据相关领域和模型结构方面的知识得知模型参数之间应该存在一些相关性。</p><p>考虑以下情形：我们有两个模型执行相同的分类任务（具有相同类别），但输入分布稍有不同。</p><p>形式地，我们有参数为 w(A) 的模型 A 和参数为 w(B) 的模型 B。这两种模型将输入映射到两个不同但相关的输出： y(A) = f(x;w(A)) 和 y(B) = f(x;w(B))</p><p>可以想象，这些任务会足够相似（或许具有相似的输入和输出分布），因此我们认为模型参数 w(A) 和 w(B) 应彼此靠近。具体来说，我们可以使用以下形式的参数范数惩罚（这里使用的是 L2 惩罚，也可以使用其他选择）：</p><p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215812.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=%5COmega&amp;space;" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=\Omega&amp;space;</a>(w<sup>{(A)},w</sup>{(B)})=\left&amp;space;|&amp;space;w<sup>{(A)}-w</sup>{(B)}&amp;space;\right&amp;space;|^2_2)</p><p><strong>参数共享</strong>是这个思路下更流行的做法——强迫部分参数相等</p><p>和正则化参数使其接近（通过范数惩罚）相比，参数共享的一个显著优点是能够“减少内存”——只有参数（唯一一个集合）的子集需要被存储在内存中，特别是在 CNN 中。</p><h1>Dropout 与 Bagging 集成方法的关系，Dropout 带来的意义与其强大的原因***</h1><h2 id="bagging-集成方法">Bagging 集成方法</h2><blockquote>  <p>《深度学习》 7.11 Bagging 和其他集成方法</p></blockquote><p><strong>集成方法</strong>：</p><p>其主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为<strong>模型平均</strong>（model averaging）。采用这种策略的技术被称为<strong>集成方法</strong>。</p><p>模型平均（model averaging）<strong>奏效的原因</strong>是不同的模型通常不会在测试集上产生完全相同的误差。平均上， 集成至少与它的任何成员表现得一样好，并且<strong>如果成员的误差是独立的</strong>，集成将显著地比其成员表现得更好。</p><p><strong>Bagging</strong>：</p><p>Bagging（bootstrap aggregating）是通过结合几个模型降低泛化误差的技术 (Breiman, 1994)。</p><p>具体来说，Bagging 涉及构造 k 个<strong>不同的数据集</strong>。每个数据集从原始数据集中<strong>重复采样</strong>构成，和原始数据集具有<strong>相同数量</strong>的样例。这意味着，每个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的例子（更具体的，如果采样所得的训练集与原始数据集大小相同，那所得数据集中大概有原始数据集 <strong>2/3</strong> 的实例）</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611150734.png" alt=""></p><p><strong>图像说明</strong>：该图描述了 Bagging 如何工作。假设我们在上述数据集（包含一个 8、一个 6 和一个 9）上<strong>训练数字 8</strong> 的检测器。假设我们制作了两个不同的重采样数据集。 Bagging 训练程序通过有放回采样构建这些数据集。第一个数据集忽略 9 并重复 8。在这个数据集上，检测器得知数字<strong>顶部有一个环就对应于一个 8</strong>。第二个数据集中，我们忽略 6 并重复 9。在这种情况下，检测器得知数字<strong>底部有一个环就对应于一个 8</strong>。这些单独的分类规则中的每一个都是不可靠的，但如果我们平均它们的输出，就能得到鲁棒的检测器，<strong>只有当 8 的两个环都存在时才能实现最大置信度</strong>。</p><h2 id="dropout">Dropout</h2><p><strong>Dropout 的意义与强大的原因</strong>：</p><p>简单来说，Dropout (Srivastava et al., 2014) 通过<strong>参数共享</strong>提供了一种廉价的 <strong>Bagging</strong> 集成近似，能够训练和评估<strong>指数级数量</strong>的神经网络。</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611152559.png" alt=""></p><p>Dropout 训练的集成包括所有从基础网络除去部分单元后形成的子网络。具体而言，只需将一些单元的<strong>输出乘零</strong>就能有效地删除一个单元。</p><p>通常，<strong>隐藏层</strong>的采样概率为 0.5，<strong>输入</strong>的采样概率为 0.8；超参数也可以采样，但其采样概率一般为 1</p><p><strong>Dropout与Bagging的不同点</strong>：</p><ul>  <li>在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型<strong>共享参数</strong>，其中每个模型继承父神经网络参数的不同子集。</li>  <li>在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。</li></ul><p><strong>权重比例推断规则</strong>：</p><p>简单来说，如果我们使用 0.5 的包含概率（keep prob），权重比例规则相当于在训练结束后<strong>将权重除 2</strong>，然后像平常一样使用模型；等价的，另一种方法是在训练期间将单元的状态乘 2。</p><p>无论哪种方式，我们的目标是确保在测试时一个单元的期望总输入与在训练时该单元的期望总输入是大致相同的（即使近半单位在训练时丢失）。</p><h1>批梯度下降法（Batch SGD）更新过程中，批的大小会带来怎样的影响**</h1><p>特别说明：本书中，“<strong>批量</strong>”指使用使用全部训练集；“<strong>小批量</strong>”才用来描述小批量随机梯度下降算法中用到的小批量样本；而<strong>随机梯度下降</strong>（SGD）通常指每次只使用单个样本</p><p><strong>批的大小</strong>通常由以下几个因素决定：</p><ul>  <li><strong>较大的批能得到更精确的梯度估计</strong>，但回报是小于线性的。</li>  <li><strong>较小的批能带来更好的泛化误差</strong>，泛化误差通常在批大小为 1 时最好。但是，因为梯度估计的高方差，小批量训练需要<strong>较小的学习率</strong>以保持稳定性，这意味着<strong>更长的训练时间</strong>。    <blockquote>      <p>可能是由于小批量在学习过程中加入了噪声，它们会有一些正则化效果 (Wilson and Martinez, 2003)</p>    </blockquote>  </li>  <li><strong>内存消耗和批的大小成正比</strong>，如果批量处理中的所有样本可以并行地处理（通常确是如此）。</li>  <li>在某些硬件上使用特定大小可以减少运行时间。尤其是在使用 GPU 时，通常使用 <strong>2 的幂数</strong>作为批量大小可以获得更少的运行时间。一般，2 的幂数的<strong>取值范围是 32 到 256</strong>，16 有时在尝试大模型时使用。</li>  <li>小批量更容易利用<strong>多核架构</strong>，但是太小的批并不会减少计算时间，这促使我们使用一些<strong>绝对最小批量</strong></li></ul><p>很多机器学习上的优化问题都可以分解成并行地计算不同样本上单独的更新。换言之，我们在计算小批量样本 X 上最小化 J(X) 的更新时，同时可以计算其他小批量样本上的更新。</p><blockquote>  <p>异步并行分布式方法 -&gt; 《深度学习》 12.1.3 大规模的分布式实现</p></blockquote><h1>如何避免深度学习中的病态，鞍点，梯度爆炸，梯度弥散？***</h1><h2 id="病态-ill-conditioning">病态（ill-conditioning）</h2><p><strong>什么是病态？</strong></p><blockquote>  <p><a href="https://blog.csdn.net/foolsnowman/article/details/51614862" target="_blank" rel="noopener">神经网络优化中的病态问题</a> - CSDN博客</p>  <p><a href="https://www.zhihu.com/question/56977045" target="_blank" rel="noopener">什么是 ill-conditioning 对SGD有什么影响？</a> - 知乎</p></blockquote><p>简单来说，深度学习中的病态问题指的就是学习/优化变的困难，需要更多的迭代次数才能达到相同的精度。</p><blockquote>  <p>病态问题普遍存在于数值优化、凸优化或其他形式的优化中 -&gt; ch4.3.1 - 梯度之上：Jacobian 和 Hessian 矩阵</p></blockquote><p>更具体的，导致病态的原因是问题的<strong>条件数</strong>（condition number）非常大，其中<code>条件数 = 函数梯度最大变化速度 / 梯度最小变化速度</code>（对于二阶可导函数，条件数的严格定义是：Hessian矩阵最大特征值的上界 / 最小特征值的下界）。</p><p><strong>条件数大</strong>意味着目标函数在有的地方（或有的方向）变化很快、有的地方很慢，比较不规律，从而很难用当前的局部信息（梯度）去比较准确地预测最优点所在的位置，只能一步步缓慢的逼近最优点，从而优化时需要更多的迭代次数。</p><p><strong>如何避免病态？</strong></p><p>知道了什么是病态，那么所有有利于<strong>加速训练</strong>的方法都属于在避免病态，其中最主要的还是优化算法。</p><p>深度学习主要使用的优化算法是<strong>梯度下降</strong>，所以避免病态问题的关键是改进梯度下降算法：</p><ul>  <li>随机梯度下降（SGD）、批量随机梯度下降</li>  <li>动态的学习率</li>  <li><strong>带动量的 SGD</strong></li></ul><blockquote>  <p><a href="#28-sgd-%E4%BB%A5%E5%8F%8A%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E5%B8%A6%E5%8A%A8%E9%87%8F%E7%9A%84-sgd-%E5%AF%B9%E4%BA%8E-hessian-%E7%9F%A9%E9%98%B5%E7%97%85%E6%80%81%E6%9D%A1%E4%BB%B6%E5%8F%8A%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%B7%AE%E7%9A%84%E5%BD%B1%E5%93%8D">28. SGD 以及学习率的选择方法，带动量的 SGD 对于 Hessian 矩阵病态条件及随机梯度方差的影响***</a></p></blockquote><h2 id="鞍点-saddle-point">鞍点（saddle point）</h2><p>对于很多高维非凸函数（神经网络）而言，局部极小值/极大值事实上都<strong>远少于</strong>另一类梯度为零的点：鞍点</p><p><strong>什么是鞍点？</strong></p><p>二维和三维中的鞍点：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611171103.png" alt="">  <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611171213.png" alt=""></p><p><strong>鞍点激增对于训练算法来说有哪些影响？</strong></p><p>对于只使用梯度信息的一阶优化算法（随机梯度下降）而言，目前情况还不清楚。不过，虽然鞍点附近的梯度通常会非常小，但是 Goodfellow et al. (2015) 认为连续的梯度下降<strong>会逃离而不是吸引到鞍点</strong>。</p><p>对于牛顿法（二阶梯度）而言，鞍点问题会比较明显。不过神经网络中很少使用二阶梯度进行优化。</p><h2 id="长期依赖与梯度爆炸-消失">长期依赖与梯度爆炸、消失</h2><p>当计算图变得很深时（循环神经网络），神经网络优化算法会面临的另外一个难题就是<strong>长期依赖</strong>，由于变深的结构使模型丧失了学习到先前信息的能力，让优化变得极其困难；具体来说，就是会出现<strong>梯度消失</strong>和<strong>梯度爆炸</strong>问题。</p><p><strong>如何避免梯度爆炸？</strong></p><p><strong>如何缓解梯度消失？</strong></p><p>梯度截断有助于处理爆炸的梯度，但它无助于梯度消失。</p><p>一个想法是：在展开循环架构的计算图中，沿着与弧边相关联的梯度乘积接近 1 的部分创建路径——LSTM, GRU 等<strong>门控机制</strong>正是该想法的实现。</p><blockquote>  <p>《深度学习》 10.10 长短期记忆和其他门控 RNN</p></blockquote><p>另一个想法是：正则化或约束参数，以引导“信息流”；或者说，希望<strong>梯度向量</strong>在反向传播时能维持其幅度。形式上，我们要使</p><p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180611194550.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=</a>(\nabla_{h<sup>{(t)}}L)\frac{\partial&amp;space;h</sup>{(t)}}{\partial&amp;space;h^{(t-1)}})</p><p>与梯度向量</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cnabla_%7Bh%5E%7B(t)%7D%7DL" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180611194704.png" alt=""></a></p><p>一样大。</p><p><strong>一些具体措施</strong>：</p><ol>  <li>    <p>批标准化（Batch Normalization）</p>    <blockquote>      <p><a href="#31-%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96batch-normalization%E7%9A%84%E6%84%8F%E4%B9%89">31. 批标准化（Batch Normalization）的意义**</a></p>    </blockquote>  </li>  <li>    <p>在这个目标下， Pascanu et al. (2013a) 提出了以下正则项：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611194754.png" alt=""></p>    <p>这种方法的一个主要弱点是，在处理数据冗余的任务时如语言模型，它并不像 LSTM 一样有效。</p>  </li></ol><h1>SGD 以及学习率的选择方法、带动量的 SGD***</h1><h2 id="批-随机梯度下降-sgd-与学习率">（批）随机梯度下降（SGD）与学习率</h2><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611195027.png" alt=""></p><p>SGD 及相关的小批量亦或更广义的基于梯度优化的在线学习算法，一个重要的性质是每一步更新的计算时间不依赖训练样本数目的多寡。因为它每个 step 的样本数是固定的。</p><p>所以即使训练样本数目非常大时，它们也能收敛。对于足够大的数据集， SGD 可能会在处理整个训练集之前就收敛到最终测试集误差的某个固定容差范围内。</p><p><strong>SGD 与学习率</strong></p><p>SGD 算法中的一个关键参数是学习率。在实践中，有必要<strong>随着时间的推移逐渐降低学习率</strong>。</p><p>实践中，一般会线性衰减学习率直到第 τ 次迭代：</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cepsilon_k=(1-%5Calpha)%5Cepsilon_0+%5Calpha%5Cepsilon_%5Ctau" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180611195622.png" alt=""></a></p><p>其中 α=k/τ。在 τ 步迭代之后，一般使 ϵ 保持常数。</p><p>使用线性策略时，需要选择的参数有 ϵ_0, ϵ_τ 和 τ</p><ul>  <li>通常 τ 被设为需要反复遍历训练集几百次的迭代次数（？）</li>  <li>通常 ϵ_τ 应设为大约 ϵ_0 的 1%</li></ul><p><strong>如何设置 ϵ_0？</strong></p><p>若 ϵ_0 太大，学习曲线将会剧烈振荡，代价函数值通常会明显增加。温和的振荡是良好的，容易在训练随机代价函数（例如使用 Dropout 的代价函数）时出现。<strong>如果学习率太小</strong>，那么学习过程会很缓慢。<strong>如果初始学习率太低</strong>，那么学习可能会卡在一个相当高的代价值。通常，就总训练时间和最终代价值而言，最优初始学习率会高于大约迭代 100 次左右后达到最佳效果的学习率。<strong>因此，通常最好是检测最早的几轮迭代，选择一个比在效果上表现最佳的学习率更大的学习率，但又不能太大导致严重的震荡。</strong></p><p>学习率可通过试验和误差来选取，通常最好的选择方法是<strong>监测目标函数值随时间变化的学习曲线</strong>——与<strong>其说是科学，这更像是一门艺术</strong>。</p><h2 id="带动量的-sgd">带动量的 SGD</h2><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611203427.png" alt=""></p><p>从形式上看， 动量算法引入了变量 v 充当速度角色——它代表参数在参数空间移动的方向和速率。速度被设为负梯度的指数衰减平均。</p><p>之前，步长只是梯度范数乘以学习率。现在，步长取决于梯度序列的大小和排列。当许多连续的梯度指向相同的方向时，步长最大。如果动量算法总是观测到梯度 g，那么它会在方向 −g 上不停加速，直到达到最终速度，其中步长大小为</p><p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cfrac%7B%5Cepsilon%5Cleft%7Cg%5Cright%7C%7D%7B1-%5Calpha%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180611210928.png" alt=""></a></p><p>在实践中，α 的一般取值为 0.5, 0.9 和 0.99，分别对应<strong>最大速度</strong> 2倍，10倍和100倍于普通的 SGD 算法。和学习率一样， α 也应该随着时间不断调整（变大），但没有收缩 ϵ 重要。</p><p><strong>为什么要加入动量？</strong></p><p>加入的动量主要目的是解决两个问题： Hessian 矩阵的<strong>病态</strong>条件和<strong>随机梯度的方差</strong>。简单来说，就是为了加速学习。</p><p>虽然动量的加入有助于缓解这些问题，但其代价是引入了另一个超参数。</p><p>带有动量的 SGD（左/上） 和不带动量的 SGD（右/下）：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611204215.png" alt="">  <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611204503.png" alt=""></p><p>此图说明动量如何克服病态的问题：等高线描绘了一个二次损失函数（具有病态条件的 Hessian 矩阵）。一个病态条件的二次目标函数看起来像一个长而窄的山谷或具有陡峭边的峡谷。带动量的 SGD 能比较正确地纵向穿过峡谷；而普通的梯度步骤则会浪费时间在峡谷的窄轴上来回移动，因为梯度下降无法利用包含在 Hessian 矩阵中的曲率信息。</p><p><strong>Nesterov 动量</strong></p><p>受 Nesterov 加速梯度算法 (Nesterov, 1983, 2004) 启发， Sutskever et al. (2013) 提出了动量算法的一个变种。其更新规则如下：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611211649.png" alt=""></p><p>其中参数 α 和 ϵ 发挥了和标准动量方法中类似的作用。Nesterov 动量和标准动量之间的<strong>区别体现在梯度计算</strong>上。下面是完整的 Nesterov 动量算法：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611211753.png" alt=""></p><p>Nesterov 动量中，梯度计算在施加当前速度之后。因此，Nesterov 动量可以解释为往标准动量方法中添加了一个校正因子。</p><p>在凸批量梯度的情况下， Nesterov 动量将额外误差收敛率从 O(1/k) 改进到 O(1/k^2)。可惜，在随机梯度的情况下， Nesterov 动量没有改进收敛率。</p><h1>自适应学习率算法：AdaGrad、RMSProp、Adam 等***</h1><p>Delta-bar-delta (Jacobs, 1988) 是一个早期的自适应学习率算法。该方法基于一个很简单的想法，如果损失对于某个给定模型参数的偏导保持相同的符号，那么学习率应该增加。如果对于该参数的偏导变化了符号，那么学习率应减小。当然，这种方法只能应用于全批量优化中（？）。</p><p>最近，提出了一些增量（或者基于小批量）的算法来自适应模型参数的学习率。</p><h2 id="adagrad">AdaGrad</h2><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611214508.png" alt=""></p><p>AdaGrad 会独立地适应所有模型参数的学习率。具体来说，就是缩放每个参数反比于其所有梯度历史平方值总和的平方根 (Duchi et al., 2011)。效果上具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。</p><p><strong>不过</strong>，对于训练深度神经网络模型而言，<strong>从训练开始时</strong>就积累梯度平方会导致有效学习率过早和过量的减小。AdaGrad 在某些深度学习模型上效果不错，但不是全部。</p><h2 id="rmsprop">RMSProp</h2><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611215422.png" alt=""></p><p>RMSProp 修改自 AdaGrad。AdaGrad 旨在应用于<strong>凸问题</strong>时快速收敛，而 RMSProp 在<strong>非凸</strong>设定下效果更好，改变梯度积累为指数加权的移动平均。</p><p>RMSProp 使用指数衰减平均以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛，它就像一个初始化于该碗状结构的 AdaGrad 算法实例。</p><p>相比于 AdaGrad，使用移动平均引入了一个<strong>新的超参数 ρ</strong>，用来控制移动平均的长度范围。</p><p>经验上， RMSProp 已被证明是一种有效且实用的深度神经网络优化算法。目前它是深度学习从业者经常采用的优化方法之一。</p><p><strong>结合 Nesterov 动量的 RMSProp</strong></p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611215923.png" alt=""></p><h2 id="adam">Adam</h2><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611220109.png" alt=""></p><p>Adam (Kingma and Ba, 2014) 是另一种学习率自适应的优化算法。</p><p>首先，在 Adam 中，动量直接并入了梯度一阶矩（指数加权）的估计。将动量加入 RMSProp 最直观的方法是将动量应用于缩放后的梯度。但是结合缩放的动量使用没有明确的理论动机。其次， Adam 包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩的估计。RMSProp 也采用了（非中心的）二阶矩估计，然而缺失了修正因子。因此，不像 Adam， RMSProp 二阶矩估计可能在训练初期有很高的偏置。Adam 通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。</p><p><strong>如何选择自适应学习率算法？</strong></p><p>目前在这一点上没有明确的共识。选择哪一个算法似乎主要取决于使用者对算法的熟悉程度（以便调节超参数）。</p><p>如果不知道选哪个，就用 AdamSGD 吧。</p><h1>基于二阶梯度的优化方法：牛顿法、共轭梯度、BFGS 等的做法*</h1><h1>批标准化（Batch Normalization）的意义**</h1><p>批标准化（Batch Normalization, BN, Ioffe and Szegedy, 2015）是为了克服神经网络<strong>层数加深导致难以训练</strong>而出现的一个算法。</p><p>说到底，BN 还是为了解决<strong>梯度消失/梯度爆炸</strong>问题，特别是梯度消失。</p><p><strong>BN 算法</strong>：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612104740.png" alt=""></p><p>BN 算法需要学习两个参数 γ 和 β.</p><blockquote>  <p>Ioffe and Szegedy, 2015, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</p></blockquote><p><strong>批标准化为什么有用？</strong></p><blockquote>  <p><a href="https://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="noopener">深度学习（二十九）Batch Normalization 学习笔记</a> - CSDN博客</p>  <p><a href="https://www.zhihu.com/question/38102762/answer/85238569" target="_blank" rel="noopener">深度学习中 Batch Normalization为什么效果好？</a> - 知乎</p></blockquote><h1>神经网络中的卷积，以及卷积的动机：稀疏连接、参数共享、等变表示（平移不变性）***</h1><p><strong>神经网络中的卷积</strong>：</p><p>当我们在神经网络中提到卷积时，通常是指由<strong>多个并行卷积</strong>组成的运算。一般而言，每个核只用于提取一种类型的特征，尽管它作用在多个空间位置上。而我们通常希望网络的每一层能够在多个位置提取多种类型的特征。</p><p>卷积的一些基本概念：通道（channel）、卷积核（kernel、filter）、步幅（stride，下采样）、填充（padding）</p><p><strong>为什么使用卷积？（卷积的动机）</strong></p><p>卷积运算通过三个重要的思想来帮助改进机器学习系统：<strong>稀疏交互</strong>（sparseinteractions）、<strong>参数共享</strong>（parameter sharing）、<strong>等变表示</strong>（equivariant representations）。</p><h2 id="稀疏连接-sparse-connectivity">稀疏连接（sparse connectivity）</h2><p>稀疏连接，也称稀疏交互、稀疏权重。</p><p>传统的神经网络中每一个输出单元会与每一个输入单元都产生交互。卷积网络改进了这一点，使具有稀疏交互的特征。CNN 通过使核（kernel、filter）的大小远小于输入的大小来达到的这个目的。</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612120851.png" alt=""></p><p>举个例子，当处理一张图像时，输入的图像可能包含成千上万个像素点，但是我们可以通过只占用几十到上百个像素点的核来<strong>检测一些小的、有意义的特征</strong>，例如图像的边缘。</p><p><strong>稀疏交互的好处</strong>：</p><ul>  <li>提高了模型的统计效率：原本一幅图像只能提供少量特征，现在每一块像素区域都可以提供一部分特征</li>  <li>减少了模型的存储需求和计算量，因为参数更少</li></ul><p>如果有 m 个输入和 n 个输出，那么矩阵乘法需要 <code>m × n</code> 个参数并且相应算法的时间复杂度为 <code>O(m × n)</code>；如果限制每一个输出拥有的连接数为 k，那么稀疏的连接方法只需要 <code>k × n</code> 个参数以及 <code>O(k × n)</code> 的运行时间。而在实际应用中，<strong>k 要比 m 小几个数量级</strong>。</p><p>虽然看似减少了隐藏单元之间的交互，但实际上处在深层的单元可以间接地连接到全部或者大部分输入。</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612121301.png" alt=""></p><h2 id="参数共享-parameter-sharing">参数共享（parameter sharing）</h2><p>参数共享是指在一个模型的多个函数中使用相同的参数。作为参数共享的同义词，我们可以说 一个网络含有  <strong>绑定的权重</strong>（tied weights）</p><p>在传统的神经网络中，当计算一层的输出时，权重矩阵的每一个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。</p><p>考虑一个具体的例子——<strong>边缘检测</strong>——来体会稀疏连接+参数共享带来的效率提升：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612154333.png" alt=""></p><p>两个图像的高度均为 280 个像素。输入图像的宽度为 320 个像素，而输出图像的宽度为 319 个像素（padding=‘VALID’）。对于边缘检测任务而言，只需要一个包含<strong>两个元素的卷积核</strong>就能完成；而为了用矩阵乘法描述相同的变换，需要一个包含 320 × 280 × 319 × 280 ≈ 80亿个元素的矩阵（40亿倍）。</p><p>同样，使用卷积只需要 319 × 280 × 3 = 267,960 次浮点运算（每个输出像素需要两次乘法和一次加法）；而直接运行矩阵乘法的算法将执行超过 160 亿次浮点运算（60000倍）</p><h2 id="平移等变-不变性-translation-invariant">平移等变|不变性（translation invariant）</h2><p>（局部）平移不变性是一个很有用的性质，尤其是当我们关心某个特征<strong>是否出现</strong>而不关心它出现的具体位置时。</p><p><strong>参数共享</strong>（和池化）使卷积神经网络具有一定的<strong>平移不变性</strong>。这就意味着即使图像经历了一个小的平移，依然会产生相同的特征。例如，分类一个 MNIST 数据集的数字，对它进行任意方向的<strong>平移</strong>（不是旋转），无论最终的位置在哪里，都能正确分类。</p><blockquote>  <p>池化操作也能够帮助加强网络的平移不变性</p></blockquote><p><strong>什么是等变性？</strong></p><ul>  <li>如果一个函数满足<strong>输入改变，输出也以同样的方式改变</strong>这一性质，我们就说它是等变 (equivariant) 的。</li>  <li>对于卷积来说，如果令 g 是输入的任意平移函数，那么卷积函数对于 g 具有等变性。</li></ul><p>当处理<strong>时间序列数据</strong>时，这意味着通过卷积可以得到一个由输入中出现不同特征的时刻所组成的时间轴。如果我们把输入中的一个事件向后延时，在输出中仍然会有完全相同的表示，只是时间延后了。</p><p>图像与之类似，卷积产生了一个 2 维映射来表明某些特征在输入中出现的位置。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。</p><p>卷积对其他的一些变换并不是天然等变的，例如对于图像的<strong>放缩</strong>或者<strong>旋转</strong>变换，需要其他的一些机制来处理这些变换。</p><blockquote>  <p><a href="http://ufldl.stanford.edu/wiki/index.php/%E6%B1%A0%E5%8C%96#.E6.B1.A0.E5.8C.96.E7.9A.84.E4.B8.8D.E5.8F.98.E6.80.A7" target="_blank" rel="noopener">池化的不变性</a> - Ufldl</p></blockquote><h1>卷积中不同零填充的影响**</h1><p>在任何卷积网络的实现中都有一个重要性质，那就是能够隐含地<strong>对输入用零进行填充</strong>使得它加宽。如果没有这个性质，会极大得限制网络的表示能力。</p><p>三种零填充设定，其中 <code>m</code> 和 <code>k</code> 分别为图像的宽度和卷积核的宽度（高度类似）：</p><ol>  <li>有效（valid）卷积——不使用零填充，卷积核只允许访问那些图像中能够<strong>完全包含整个核</strong>的位置，输出的宽度为 <code>m − k + 1</code>.    <ul>      <li>在这种情况下，输出的所有像素都是输入中相同数量像素的函数，这使得输出像素的表示更加规范。</li>      <li>然而，输出的大小在每一层都会缩减，这限制了网络中能够包含的卷积层的层数。（一般情况下，影响不大，除非是上百层的网络）</li>    </ul>  </li>  <li>相同（same）卷积——只进行足够的零填充来<strong>保持输出和输入具有相同的大小</strong>，即输出的宽度为 <code>m</code>.    <ul>      <li>在这种情况下，只要硬件支持，网络就能包含任意多的卷积层。</li>      <li>然而，输入像素中靠近边界的部分相比于中间部分对于输出像素的影响更小。这可能会导致边界像素存在一定程度的欠表示。</li>    </ul>  </li>  <li>全（full）卷积——进行足够多的零填充使得每个像素都能被访问 k 次（非全卷积只有中间的像素能被访问 k 次），最终输出图像的宽度为 <code>m + k − 1</code>.    <ul>      <li>因为 same 卷积可能导致边界像素欠表示，从而出现了 Full 卷积；</li>      <li>但是在这种情况下，输出像素中靠近边界的部分相比于中间部分是更少像素的函数。这将导致学得的卷积核不能再所有所有位置表现一致。</li>      <li>事实上，很少使用 Full 卷积</li>    </ul>    <blockquote>      <p>注意：如果以“全卷积”作为关键词搜索，返回的是一个称为 FCN（Fully Convolutional Networks）的卷积结构，而不是这里描述的填充方式。</p>    </blockquote>  </li></ol><p>通常<strong>零填充的最优数量</strong>（对于测试集的分类正确率）处于 “有效卷积”和 “相同卷积” 之间。</p><h1>基本卷积的变体：反卷积、空洞卷积***</h1><p>原书中也描述一些基本卷积的变体：局部卷积、平铺卷积；</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180612213144.png" alt=""></p><blockquote>  <p>从上到下一次为局部卷积、平铺卷积和标准卷积；</p>  <p>《深度学习》 9.5 基本卷积函数的变体</p></blockquote><p>不过这跟我想的“变体”不太一样（百度都搜不到这两种卷积），下面介绍的是一些我认识中比较流行的卷积变体：</p><h2 id="转置卷积-反卷积-transposed-convolution">转置卷积|反卷积（Transposed convolution）</h2><p><img src="http://www.meng.uno/images/assets/conv_no_padding_no_strides_transposed.gif" alt=""></p><blockquote>  <p>No padding, no strides, transposed</p></blockquote><blockquote>  <p><a href="https://www.zhihu.com/question/43609045" target="_blank" rel="noopener">如何理解深度学习中的deconvolution networks？</a> - 知乎</p></blockquote><h2 id="空洞卷积-扩张卷积-dilated-convolution">空洞卷积|扩张卷积（Dilated convolution）</h2><p><img src="http://www.meng.uno/images/assets/conv_dilation.gif" alt=""></p><blockquote>  <p>No padding, no stride, dilation</p></blockquote><blockquote>  <p><a href="https://www.zhihu.com/question/54149221" target="_blank" rel="noopener">如何理解空洞卷积（dilated convolution）？</a> - 知乎</p></blockquote><blockquote>  <p>卷积、转置卷积、空洞卷积动图演示：vdumoulin/<a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="noopener">conv_arithmetic</a>: A technical report on convolution arithmetic in the context of deep learning</p></blockquote><h1>池化、池化（Pooling）的作用***</h1><blockquote>  <p>《深度学习》 9.3 池化</p></blockquote><p>一次典型的卷积包含三层：第一层并行地计算多个卷积产生一组线性激活响应；第二层中每一个线性激活响应将会通过一个非线性的激活函数；第三层使用<strong>池化函数</strong>（pooling function）来进一步调整这一层的输出。</p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>      </td>      <td class="code">        <pre><span class="line"># Keras</span><br><span class="line">from keras.layers import Input, Conv2D, Activation, MaxPooling2D</span><br><span class="line"></span><br><span class="line">net = Input([in_w, in_h, input_dim])</span><br><span class="line">net = Conv2D(output_dim, kernel_size=(3, 3))(net)</span><br><span class="line">net = Activation(&apos;relu&apos;)(net)</span><br><span class="line">net = MaxPooling2D(pool_size=(2, 2))(net)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">卷积层中，一般 strides=1, padding=&apos;valid&apos;</span><br><span class="line">池化层中，一般 strides=pool_size, padding=&apos;valid&apos;</span><br><span class="line">&quot;&quot;&quot;</span><br></pre>      </td>    </tr>  </table></figure><p>池化函数使用某一位置的相邻输出的<strong>总体统计特征</strong>来代替网络在该位置的输出。</p><p>常见的池化函数：</p><ul>  <li>*最大池化（Max pooling）</li>  <li>*平均值池化（Mean pooling）</li>  <li>L2 范数</li>  <li>基于中心像素距离的加权平均</li></ul><p>池化操作有助于卷积网络的平移不变性</p><p><strong>使用池化可以看作是增加了一个无限强的先验</strong>：这一层学得的函数必须具有对少量平移的不变性。当这个假设成立时，池化可以极大地提高网络的统计效率。</p><p>（最大）池化对平移是天然不变的，但池化也能用于学习其他不变性：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613101802.png" alt=""></p><p>这三个过滤器都旨在检测手写的数字 5。每个过滤器尝试匹配稍微<strong>不同方向</strong>的 5。当输入中出现 5 时，无论哪个探测单元被激活，最大池化单元都将产生较大的响应。</p><p>这种多通道方法只在学习其他变换时是必要的。这个原则在 maxout 网络 (Goodfellow et al., 2013b) 和其他卷积网络中更有影响。</p><p>池化综合了区域内的 k 个像素的统计特征而不是单个像素，这种方法提高了网络的计算效率，因为下一层少了约 k 倍的输入。</p><p>在很多任务中，池化还有助于对于处理不同大小的输入：例如我们想对不同大小的图像进行分类时，分类层的输入必须是固定的大小，而这通常通过调整池化区域的偏置大小来实现。</p><p>其他参考：</p><ul>  <li>一些理论工作对于在不同情况下应当使用哪种池化函数给出了一些指导 (Boureau et al., 2010)</li>  <li>将特征一起动态地池化：对于感兴趣特征的位置运行聚类算法 (Boureau et al., 2011)、先学习一个单独的池化结构，再应用到全部的图像中 (Jia et al., 2012)</li>  <li>《深度学习》 20.6 卷积玻尔兹曼机、20.10.6 卷积生成网络</li></ul><h1>卷积与池化的意义、影响（作为一种无限强的先验）**</h1><p>一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值，无论数据对于这些参数的值给出了多大的支持。</p><p>如果把卷积网络类比成全连接网络，那么对于这个全连接网络的权重有一个无限强的先验：隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动；同时要求除了那些处在“感受野”内的权重以外，其余的权重都为零。</p><p>类似的，使用<strong>池化</strong>也是一个无限强的先验：每一个单元都具有对少量平移的不变性。</p><p><strong>卷积与池化作为一种无限强先验的影响</strong>：</p><ol>  <li>卷积和池化可能导致<strong>欠拟合</strong>    <ul>      <li>与任何其他先验类似，卷积和池化只有当先验的假设合理且正确时才有用。</li>      <li>如果一项任务涉及到要<strong>对输入中相隔较远的信息进行合并</strong>时，那么卷积所利用的先验可能就不正确了。</li>      <li>如果一项任务依赖于保存<strong>精确的空间信息</strong>，那么在所有的特征上使用池化将会增大训练误差。</li>    </ul>    <blockquote>      <p>因此，一些卷积网络结构 (Szegedy et al., 2014a) 为了既获得具有较高不变性的特征又获得当平移不变性不合理时不会导致欠拟合的特征，被设计成在一些通道上使用池化而在另一些通道上不使用。</p>    </blockquote>  </li>  <li>当我们比较卷积模型的统计学习表现时，只能以基准中的其他卷积模型作为比较的对象</li></ol><h1>RNN 的几种基本设计模式</h1><p>循环神经网络中一些重要的设计模式包括以下几种：</p><ol>  <li>    <p>（*）每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613150111.png" alt=""></p>    <ul>      <li>将 x 值的输入序列映射到输出值 o 的对应序列</li>      <li>损失 L 衡量每个 o 与相应的训练目标 y 的距离</li>      <li>损失 L 内部计算 y^ = softmax(o)，并将其与目标 y 比较</li>      <li>输入 x 到隐藏 h 的连接由权重矩阵 U 参数化</li>      <li>隐藏 h(t-1) 到隐藏 h(t) 的循环连接由权重矩阵 W 参数化</li>      <li>隐藏到输出的连接由权重矩阵 V 参数化</li>    </ul>  </li>  <li>    <p>每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613150711.png" alt=""></p>    <ul>      <li>此类 RNN 的唯一循环是从输出 o 到隐藏层 h 的反馈连接</li>      <li>表示能力弱于 RNN_1，单更容易训练</li>    </ul>  </li>  <li>    <p>隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613151014.png" alt=""></p>    <p>这样的网络可以用于概括序列并产生用于进一步处理的固定大小的表示</p>  </li></ol><p>一般所说的 RNN（循环神经网络）指的是<strong>第一种</strong>设计模式</p><p>这些循环网络都将一个输入序列映射到<strong>相同长度</strong>的输出序列</p><h1>RNN 更新方程（前向传播公式），包括 LSTM、GRU 等***</h1><blockquote>  <p><a href="https://blog.csdn.net/zhangxb35/article/details/70060295" target="_blank" rel="noopener">RNN, LSTM, GRU 公式总结</a> - CSDN博客</p></blockquote><p><strong>基本 RNN</strong></p><blockquote>  <p><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks" target="_blank" rel="noopener">Recurrent neural network</a> - Wikipedia</p></blockquote><p>根据隐层 h(t) 接受的是上时刻的隐层 h(t−1) 还是上时刻的输出 y(t−1)，分为两种 RNN：</p><ul>  <li>    <p>Elman RNN</p>    <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613164438.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=%5Cbegin%7Baligned%7D&amp;space;h" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&amp;space;h</a><sup>{(t)}&amp;=\tanh\left(&amp;space;W_hx</sup>{(t)}+U_hh<sup>{(t-1)}&amp;plus;b_h&amp;space;\right)\&amp;space;y</sup>{(t)}&amp;={\rm&amp;space;softmax}\left(&amp;space;W_yh^{(t)}+b_y&amp;space;\right)\&amp;space;\end{aligned})</p>  </li>  <li>    <p>Jordan RNN</p>    <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613164637.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=%7B%5Cdisplaystyle&amp;space;%7B%5Cbegin%7Baligned%7D&amp;space;h" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex={\displaystyle&amp;space;{\begin{aligned}&amp;space;h</a><sup>{(t)}&amp;=\tanh\left(&amp;space;W_hx</sup>{(t)}+U_hy<sup>{(t-1)}&amp;plus;b_h&amp;space;\right)\&amp;space;y</sup>{(t)}&amp;={\rm&amp;space;softmax}\left(&amp;space;W_yh^{(t)}+b_y&amp;space;\right)&amp;space;\end{aligned}}})</p>  </li></ul><blockquote>  <p>《深度学习》 默认的 RNN 是 Elman RNN &gt; <a href="#37-rnn%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%9A%84%E5%87%A0%E7%A7%8D%E5%9F%BA%E6%9C%AC%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F">37. RNN（循环神经网络） 的几种基本设计模式**</a></p></blockquote><p><strong>门限 RNN</strong>（LSTM、GRU）与基本 RNN 的主要区别在于 Cell 部分</p><p><strong>LSTM</strong></p><blockquote>  <p><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#Variants" target="_blank" rel="noopener">Long short-term memory</a> - Wikipedia</p></blockquote><p><a href="http://www.codecogs.com/eqnedit.php?latex=%7B%5Cdisplaystyle&amp;space;%7B%5Cbegin%7Baligned%7D&amp;space;f_%7Bt%7D&amp;=%5Csigma(W_%7Bf%7Dx_%7Bt%7D+U_%7Bf%7Dh_%7Bt-1%7D+b_%7Bf%7D)%5C&amp;space;i_%7Bt%7D&amp;=%5Csigma(W_%7Bi%7Dx_%7Bt%7D+U_%7Bi%7Dh_%7Bt-1%7D+b_%7Bi%7D)%5C&amp;space;%5Ctilde%7Bc%7D_%7Bt%7D&amp;=%5Ctanh(W_%7Bc%7Dx_%7Bt%7D+U_%7Bc%7Dh_%7Bt-1%7D+b_%7Bc%7D)%5C&amp;space;c_%7Bt%7D&amp;=f_%7Bt%7D%5Ccirc&amp;space;c_%7Bt-1%7D+i_%7Bt%7D%5Ccirc&amp;space;%5Ctilde%7Bc%7D_%7Bt%7D%5C&amp;space;o_%7Bt%7D&amp;=%5Csigma(W_%7Bo%7Dx_%7Bt%7D+U_%7Bo%7Dh_%7Bt-1%7D+b_%7Bo%7D)%5C&amp;space;h_%7Bt%7D&amp;=o_%7Bt%7D%5Ccirc&amp;space;%5Ctanh(c_%7Bt%7D)&amp;space;%5Cend%7Baligned%7D%7D%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613170734.png" alt=""></a></p><ul>  <li>其中 f 为遗忘门（forget），i 为输入门（input），o 为输出门（output）。</li>  <li>每个门的输入都是 x 和 h，但是参数都是独立的（参数数量是基本 RNN 的 4 倍）</li>  <li>c 表示 cell state（如果用过 tensorflow 中的 RNN，会比较熟悉）</li>  <li>如果遗忘门 f 取 0 的话，那么上一时刻的状态就会全部被清空，只关注此时刻的输入</li>  <li>输入门 i 决定是否接收此时刻的输入</li>  <li>输出门 o 决定是否输出 cell state</li></ul><p>类似基本 RNN，LSTM 也有另一个版本，将公式中所有 h(t-1) 替换为 c(t-1)，但不常见</p><p><strong>GRU</strong></p><blockquote>  <p><a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit#Architecture" target="_blank" rel="noopener">Gated recurrent unit</a> - Wikipedia</p></blockquote><p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613171757.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=%7B%5Cdisplaystyle&amp;space;%7B%5Cbegin%7Baligned%7D&amp;space;z_%7Bt%7D&amp;=%5Csigma(W_%7Bz%7Dx_%7Bt%7D+U_%7Bz%7Dh_%7Bt-1%7D+b_%7Bz%7D)%5C&amp;space;r_%7Bt%7D&amp;=%5Csigma(W_%7Br%7Dx_%7Bt%7D+U_%7Br%7Dh_%7Bt-1%7D+b_%7Br%7D)%5C&amp;space;%5Ctilde%7Bh%7D" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex={\displaystyle&amp;space;{\begin{aligned}&amp;space;z_{t}&amp;=\sigma(W_{z}x_{t}+U_{z}h_{t-1}+b_{z})\&amp;space;r_{t}&amp;=\sigma(W_{r}x_{t}+U_{r}h_{t-1}+b_{r})\&amp;space;\tilde{h}</a><em>t&amp;=\tanh(W</em>{h}x_{t}+U_{h}(r_{t}\circ&amp;space;h_{t-1})+b_{h})\&amp;space;h_{t}&amp;=(1-z_{t})\circ&amp;space;h_{t-1}+z_{t}\circ&amp;space;\tilde{h}_t&amp;space;\end{aligned}}})</p><ul>  <li>其中 z 为更新门（update），r 为重置门（reset）</li>  <li>GRU 可以看作是将 LSTM 中的遗忘门和输入门合二为一了</li></ul><h1>BPTT（back-propagation through time，通过时间反向传播）**</h1><h1>自编码器在深度学习中的意义*</h1><p><strong>自编码器的意义</strong>：</p><ul>  <li>传统自编码器被用于降维或特征学习</li>  <li>近年来，自编码器与潜变量模型理论的联系将自编码器带到了生成式建模的前沿    <ul>      <li>几乎任何带有潜变量并配有一个推断过程（计算给定输入的潜在表示）的生成模型，都可以看作是自编码器的一种特殊形式。</li>    </ul>  </li></ul><p><strong>自编码器的一般结构</strong></p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613200023.png" alt=""></p><ul>  <li>自编码器有两个组件：<strong>编码器</strong> f（将 x 映射到 h）和<strong>解码器</strong> g（将 h 映射到 r）</li>  <li>一个简单的自编码器试图学习 <code>g(f(x)) = x</code>；换言之，自编码器尝试将输入复制到输出</li>  <li>单纯将输入复制到输出没什么用，相反，训练自编码器的目标是获得有用的特征 h。</li></ul><p>自编码器的学习过程就是最小化一个损失函数：</p><p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613201829.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=L(%5Cboldsymbol%7Bx%7D,g(f(%5Cboldsymbol%7Bx%7D)" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=L(\boldsymbol{x},g(f(\boldsymbol{x})</a>)))</p><h1>自编码器一些常见的变形与应用：正则自编码器、稀疏自编码器、去噪自编码器*</h1><blockquote>  <p><a href="#40-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E5%9C%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%84%8F%E4%B9%89">40. 自编码器在深度学习中的意义</a></p></blockquote><p><strong>欠完备自编码器</strong></p><ul>  <li>从自编码器获得有用特征的一种方法是<strong>限制 h 的维度比 x 小</strong>，这种编码维度小于输入维度的自编码器称为<strong>欠完备</strong>（undercomplete）自编码器；</li>  <li>相反，如果 h 的维度大于 x，此时称为过完备自编码器。</li>  <li><strong>学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征</strong></li>  <li>当解码器是线性的且 L 是均方误差，欠完备的自编码器会学习出与 PCA 相同的生成子空间</li>  <li>而拥有<strong>非线性</strong>编码器函数 f 和<strong>非线性</strong>解码器函数 g 的自编码器能够学习出更强大的 PCA 非线性推广</li>  <li>但如果编码器和解码器被赋予<strong>过大的容量</strong>，自编码器会执行复制任务而捕捉不到任何有关<strong>数据分布</strong>的有用信息。    <ul>      <li>过完备自编码器就可以看作是被赋予过大容量的情况</li>    </ul>  </li></ul><p><strong>正则自编码器</strong></p><ul>  <li>通过加入正则项到损失函数可以限制模型的容量，同时鼓励模型学习除了复制外的其他特性。</li>  <li>这些特性包括稀疏表示、表示的小导数、以及对噪声或输入缺失的鲁棒性。</li>  <li>即使模型的容量依然大到足以学习一个无意义的恒等函数，正则自编码器仍然能够从数据中学到一些关于数据分布的信息。</li></ul><p><strong>稀疏自编码器</strong></p><ul>  <li>    <p>稀疏自编码器一般用来学习特征</p>  </li>  <li>    <p>稀疏自编码器简单地在训练时结合编码层的<strong>稀疏惩罚</strong> Ω(h) 和重构误差：</p>    <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613211004.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=L(%5Cboldsymbol%7Bx%7D,g(f(%5Cboldsymbol%7Bx%7D)" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=L(\boldsymbol{x},g(f(\boldsymbol{x})</a>))+\Omega(\boldsymbol{h})&amp;space;=&amp;space;L(\boldsymbol{x},g(f(\boldsymbol{x})))+\lambda\sum_i\left|h_i\right|)</p>  </li>  <li>    <p>稀疏惩罚不算是一个正则项。这仅仅影响模型关于潜变量的分布。这个观点提供了训练自编码器的另一个动机：这是近似训练生成模型的一种途径。这也给出了为什么自编码器学到的特征是有用的另一个解释：它们描述的潜变量可以解释输入。</p>    <blockquote>      <p>《深度学习》 14.2.1 稀疏自编码器</p>    </blockquote>  </li></ul><p><strong>去噪自编码器（DAE）</strong></p><ul>  <li>    <p>去噪自编码器试图学习<strong>更具鲁棒性的</strong>特征</p>  </li>  <li>    <p>与传统自编码器不同，去噪自编码器（denoising autoencoder, DAE）最小化：</p>    <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180613211437.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=L(%5Cboldsymbol%7Bx%7D,g(f(%5Cboldsymbol%7B%5Ctilde%7Bx%7D%7D)" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=L(\boldsymbol{x},g(f(\boldsymbol{\tilde{x}})</a>)))</p>  </li>  <li>    <p>这里的 x~ 是<strong>被某种噪声损坏</strong>的 x 的副本，去噪自编码器需要预测原始未被损坏数据</p>  </li>  <li>    <p>破坏的过程一般是以某种概率分布（通常是二项分布）将一些值置 0.</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180613211935.png" alt=""></p>    <blockquote>      <p>《深度学习》 14.2.2 去噪自编码器，14.5 去噪自编码器</p>    </blockquote>  </li></ul><p><strong>为什么DAE有用？</strong></p><ul>  <li>对比使用非破损数据进行训练，破损数据训练出来的权重噪声比较小——在破坏数据的过程中去除了真正的噪声</li>  <li>破损数据一定程度上减轻了训练数据与测试数据的代沟——使训练数据更接近测试数据    <blockquote>      <p><a href="https://www.cnblogs.com/neopenx/p/4370350.html" target="_blank" rel="noopener">降噪自动编码器（Denoising Autoencoder)</a> - Physcal - 博客园</p>      <blockquote>        <p>感觉这两个理由很牵强，但是从数据分布的角度讲太难了</p>      </blockquote>    </blockquote>  </li></ul><h1>半监督的思想以及在深度学习中的应用*</h1><h1>分布式表示的概念、应用，与符号表示（one-hot 表示）的区别***</h1><h2 id="什么是分布式表示？">什么是分布式表示？</h2><ul>  <li>    <p>所谓分布式表示就是用不同的特征，通过<strong>组合</strong>来表示不同的概念</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614103628.png" alt="">      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614104032.png" alt=""></p>    <p>（左）是 one-hot 表示（一种稀疏表示），（右）为分布式表示</p>    <blockquote>      <p><a href="http://baijiahao.baidu.com/s?id=1593632865778730392" target="_blank" rel="noopener">神经网络如何学习分布式表示</a> - 百家号</p>    </blockquote>  </li></ul><h2 id="分布式表示为什么强大？-分布式表示与符号表示">分布式表示为什么强大？——分布式表示与符号表示</h2><p>分布式表示之所以强大，是因为它能用具有 <code>d</code> 个值的 <code>n</code> 个<strong>线性阀值特征</strong>去描述 <code>d^n</code> 个不同的概念——换言之，在输入维度是 d 的一般情况下，具有 n 个特征的分布式表示可以给 O(n^d) 个不同区域分配唯一的编码</p><blockquote>  <p><strong>线性阀值特征</strong>：本身是一个<strong>连续值</strong>，通过划分阈值空间来获得对应的离散特征</p></blockquote><p><strong>符号表示</strong></p><ul>  <li>    <p>如果我们没有对数据做任何假设，并且每个区域使用唯一的符号来表示，每个符号使用单独的参数去识别空间中的对应区域，那么指定 <code>O(n^d)</code> 个区域需要 <code>O(n^d)</code> 个样本/参数。</p>  </li>  <li>    <p>举个例子：作为纯符号，“猫”和“狗”之间的距离和任意其他两种符号的距离是一样。</p>    <p>然而，如果将它们与<strong>有意义的分布式表示</strong>相关联，那么关于猫的很多特点可以推广到狗，反之亦然——比如，某个分布式表示可能会包含诸如“具有皮毛”或“腿的数目”这类在猫和的<strong>嵌入/向量</strong>上具有相同值的项</p>  </li></ul><p><strong>分布式表示与符号表示</strong>（最近邻）：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614111241.png" alt="">  <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614111712.png" alt=""></p><ul>  <li>两者都在学习如何将输入空间分割成多个区域</li>  <li>在输入维度相同的情况下，分布式表示能够比非分布式表示多分配指数级的区域——这一特性可用于解决<strong>维度灾难</strong>问题    <blockquote>      <p><a href="#44-%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE">44. 如何理解维数灾难？</a></p>    </blockquote>  </li></ul><p><strong>非线性特征</strong>：</p><p>上面假设了<strong>线性阀值特征</strong>，然而更一般的，分布式表示的优势还体现在其中的每个特征可以用非线性算法——<strong>神经网络</strong>——来提取。简单来说，表示能力又提升了一个级别。</p><p>一般来说，无论我们使用什么算法，需要学习的空间区域数量是固定的；但是使用分布式表示有效的减少了参数的数量——从 <code>O(n^d)</code> 到 <code>O(nd)</code>——这意味着我们需要拟合参数更少，因此只需要更少的训练样本就能获得良好的泛化。</p><p><strong>一些非分布式表示算法</strong>：</p><ul>  <li>聚类算法，比如 k-means 算法</li>  <li>k-最近邻算法</li>  <li>决策树</li>  <li>支持向量机</li>  <li>基于 n-gram 的语言模型</li></ul><p><strong>什么时候应该使用分布式表示能带来统计优势？</strong> 当一个明显复杂的结构可以  <strong>用较少参数紧致地表示</strong>时，使用分布式表示就会具有统计上的优势（避免维数灾难）。</p><blockquote>  <p><a href="#44-%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE">44. 如何理解维数灾难？</a></p></blockquote><p>一些传统的非分布式学习算法仅仅在<strong>平滑先验</strong>的情况下能够泛化。</p><h1>如何理解维数灾难？***</h1><blockquote>  <p>《深度学习》 5.11.1 维数灾难</p></blockquote><p>概括来说，就是当数据维数很高时，会导致学习变得困难。</p><p>这里的“困难”体现在两方面：</p><ol>  <li>当数据较多时，会使训练的周期变得更长</li>  <li>当数据较少时，对新数据的泛化能力会更弱，甚至失去泛化能力</li></ol><blockquote>  <p>这两点对于任何机器学习算法都是成立的；但在维数灾难的背景下，会加剧这两个影响</p></blockquote><p>对于第二点，书中使用了另一种描述：“由维数灾难带来的一个问题是统计挑战，所谓统计挑战指的是 <strong>x 的可能配置数目远大于训练样本的数目</strong>”。</p><p>为了充分理解这个问题，我们假设输入空间如图所示被分成单元格。</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180614203447.png" alt=""></p><ul>  <li>当数据的维度增大时（从左向右），我们感兴趣的配置数目会随指数级增长。</li>  <li>当空间是低维时，我们可以用由少量单元格去描述这个空间。泛化到新数据点时，通过检测和单元格中的训练样本的相似度，我们可以判断如何处理新数据点。</li>  <li>当空间的维数很大时，很可能发生大量单元格中没有训练样本的情况。此时，基于“<strong>平滑先验</strong>”的简单算法将无力处理这些新的数据。</li>  <li>更一般的，O(nd) 个参数（d 个特征，每个特征有 n 种表示）能够明确表示输入空间中 O(n^d) 个不同区域。如果我们没有对数据做任何假设，并且<strong>每个区域使用唯一的符号来表示</strong>，每个符号使用单独的参数去识别空间中的对应区域，那么指定 O(n^d) 个区域将需要 O(n^d) 个样本。</li></ul><p><strong>如何解决维数灾难？</strong></p><h1>迁移学习相关概念：多任务学习、一次学习、零次学习、多模态学习**</h1><p><strong>什么是迁移学习？</strong></p><ul>  <li>迁移学习和领域自适应指的是利用一个任务（例如，分布 P1）中已经学到的内容去改善另一个任务（比如分布 P2）中的泛化情况。    <ul>      <li>例如，我们可能在第一个任务中学习了一组视觉类别，比如猫和狗，然后在第二种情景中学习一组不同的视觉类别，比如蚂蚁和黄蜂。</li>    </ul>  </li>  <li>除了共享<strong>输出语义</strong>（上面这个例子），有时也会共享<strong>输出语义</strong>    <ul>      <li>例如，语音识别系统需要<strong>在输出层产生有效的句子</strong>，但是输入附近的较低层可能需要识别相同音素或子音素发音的不同版本（这取决于说话人）</li>    </ul>  </li></ul><p><strong>迁移学习与多任务学习</strong></p><ul>  <li>    <p>因为目前迁移学习更流行，因此不少博客简介上，会将多任务学习归属于迁移学习的子类或者迁移学习的相关领域。</p>  </li>  <li>    <p>迁移学习与多任务学习的一些结构：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180615111903.png" alt="">      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180610203703.png" alt=""></p>    <blockquote>      <p>（左）&gt; 《深度学习》 15.2 迁移学习和领域自适应；（右）&gt; 《深度学习》 7.7 多任务学习</p>    </blockquote>  </li>  <li>    <p>这两种都可能是迁移学习或者多任务学习的结构。<s>迁移学习的输入在每个任务上具有不同的意义（甚至不同的维度），但是输出在所有的任务上具有<strong>相同的语义</strong>；多任务学习则相反</s></p>  </li></ul><p><strong>迁移学习与领域自适应</strong></p><ul>  <li>相比于迁移学习和多任务学习，领域自适应的提法比较少，也更简单一些，其在每个情景之间任务（和最优的输入到输出的映射）都是相同的，但是<strong>输入分布</strong>稍有不同。</li>  <li>例如，考虑情感分析的任务：网上的评论有许多类别。在书、视频和音乐等媒体内容上训练的顾客评论情感预测器，被用于分析诸如电视机或智能电话的消费电子产品的评论时，领域自适应情景可能会出现。可以想象，存在一个潜在的函数可以判断任何语句是正面的、中性的还是负面的，但是词汇和风格可能会因领域而有差异</li></ul><p><strong>one-shot learning 和 zero-shot learning</strong></p><ul>  <li>迁移学习的两种极端形式是<strong>一次学习</strong>（one-shot learning）和<strong>零次学习</strong>（zero-shot learning）    <ul>      <li>只有少量标注样本的迁移任务被称为 one-shot learning；没有标注样本的迁移任务被称为 zero-shot learning.</li>    </ul>  </li>  <li><strong>one-shot learning</strong>    <ul>      <li>one-shot learning 稍微简单一点：在大数据上学习 general knowledge，然后在特定任务的小数据上有技巧的 fine tuning。</li>    </ul>  </li>  <li><strong>zero-shot learning</strong>    <ul>      <li>相比 one-shot，zero-shot learning 要更复杂。</li>      <li>先来看一个 zero-shot 的例子：假设学习器已经学会了关于动物、腿和耳朵的概念。如果已知猫有四条腿和尖尖的耳朵，那么学习器可以在没有见过猫的情况下猜测该图像中的动物是猫。</li>      <li>(TODO)</li>    </ul>  </li></ul><p><strong>多模态学习（multi-modal learning）</strong></p><ul>  <li>    <p>与 zero-shot learning 相同的原理可以解释如何能执行<strong>多模态学习</strong>（multimodal learning）</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180615150955.png" alt=""></p>  </li></ul><h1>图模型|结构化概率模型相关概念*</h1><p><strong>有向图模型</strong></p><ul>  <li>    <p>有向图模型（directed graphical model）是一种结构化概率模型，也被称为<strong>信念网络</strong>（belief network）或者<strong>贝叶斯网络</strong>（Bayesian network）</p>  </li>  <li>    <p>描述接力赛例子的有向图模型</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180617140439.png" alt=""></p>    <ul>      <li>Alice 在 Bob 之前开始，所以 Alice 的完成时间 t0 影响了 Bob 的完成时间 t1。</li>      <li>Carol 只会在 Bob 完成之后才开始，所以 Bob 的完成时间 t1 直接影响了 Carol 的完成时间 t2。</li>    </ul>  </li>  <li>    <p>正式地说，变量 x 的有向概率模型是通过有向无环图 G（每个结点都是模型中的随机变量）和一系列<strong>局部条件概率分布</strong>（local conditional probability distribution）来定义的，x 的概率分布可以表示为：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180617141056.png" alt=""></p>    <ul>      <li>其中 大P 表示结点 xi 的所有父结点</li>    </ul>  </li>  <li>    <p>上述接力赛例子的概率分布可表示为：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180617141502.png" alt=""></p>  </li></ul><p><strong>无向图模型</strong></p><ul>  <li>无向图模型（undirected graphical Model），也被称为<strong>马尔可夫随机场</strong>（Markov random field, MRF）或者是<strong>马尔可夫网络</strong>（Markov network）</li>  <li>当相互的作用并没有本质性的指向，或者是明确的双向相互作用时，使用无向模型更加合适。</li></ul><p><strong>图模型的优点</strong></p><ol>  <li>减少参数的规模    <ul>      <li>通常意义上说，对每个变量都能取 k 个值的 n 个变量建模，<strong>基于查表的</strong>方法需要的复杂度是 <code>O(k^n)</code>，如果 m 代表图模型的单个条件概率分布中最大的变量数目，那么对这个有向模型建表的复杂度大致为 <code>O(k^m)</code>。只要我们在设计模型时使其满足 <code>m ≪ n</code>，那么复杂度就会被大大地减小；换一句话说，只要图中的每个变量都只有少量的父结点，那么这个分布就可以用较少的参数来表示。</li>    </ul>  </li>  <li>统计的高效性    <ul>      <li>相比图模型，<strong>基于查表的</strong>模型拥有天文数字级别的参数，为了准确地拟合，相应的训练集的大小也是相同级别的。</li>    </ul>  </li>  <li>减少运行时间    <ul>      <li>推断的开销：计算分布时，避免对整个表的操作，比如求和</li>      <li>采样的开销：类似推断，避免读取整个表格</li>    </ul>    <blockquote>      <p>《深度学习》 16.3 从图模型中采样</p>    </blockquote>  </li></ol><p><strong>图模型如何用于深度学习</strong></p><ul>  <li>受限玻尔兹曼机（RBM）    <ul>      <li>RBM 是图模型如何用于深度学习的典型例子</li>      <li>RBM 本身不是一个深层模型，它有一层潜变量，可用于学习输入的表示。但是它可以被用来构建许多的深层模型。</li>    </ul>  </li></ul><p><strong>其他相关名词</strong>：</p><ul>  <li>信念网络（有向图模型）</li>  <li>马尔可夫网络（无向图模型）</li>  <li>配分函数</li>  <li>能量模型（无向图模型）</li>  <li>分离（separation）/d-分离</li>  <li>道德图（moralized graph）、弦图（chordal graph）</li>  <li>因子图（factor graph）</li>  <li>Gibbs 采样</li>  <li>结构学习（structure learning）</li></ul><h1>深度生成模型、受限玻尔兹曼机（RBM）相关概念*</h1><h1>深度学习在图像、语音、NLP等领域的常见作法与基本模型**</h1><h2 id="计算机视觉-cv">计算机视觉（CV）</h2><p><strong>预处理</strong> 许多应用领域需要复杂精细的预处理，但是 CV 通常只需要相对少的预处理。</p><p>通常，<strong>标准化</strong>是图像唯一必要的预处理——将图像格式化为具有相同的比例，比如 [0,1] 或者 [-1,1].</p><p>许多框架需要图像缩放到标准的尺寸。但这不是必须的，一些卷积模型接受可变大小的输入并动态地调整它们的池化区域大小以保持输出大小恒定。</p><p>其他预处理操作：</p><p><strong>对比度归一化</strong></p><ul>  <li>    <p>在许多任务中，对比度是能够安全移除的最为明显的变化源之一。简单地说，对比度指的是图像中亮像素和暗像素之间差异的大小。</p>  </li>  <li>    <p>整个图像的对比度可以表示为：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619104551.png" alt="">，其中</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619104649.png" alt="">，整个图片的平均强度</p>  </li></ul><p><strong>全局对比度归一化</strong>（Global contrast normalization, GCN）</p><ul>  <li>    <p>GCN 旨在通过从每个图像中减去其平均值，然后重新缩放其使得其像素上的标准差等于某个常数 s 来防止图像具有变化的对比度。定义为：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619104914.png" alt=""></p>    <ul>      <li>从大图像中剪切感兴趣的对象所组成的数据集不可能包含任何强度几乎恒定的图像。此时，设置 λ = 0 来忽略小分母问题是安全的。(Goodfellow et al. 2013c)</li>      <li>随机剪裁的小图像更可能具有几乎恒定的强度，使得激进的正则化更有用。此时可以加大 λ (ϵ = 0, λ = 10; Coates et al. 2011)</li>      <li>尺度参数 s 通常可以设置为 1 (Coates et al. 2011)，或选择使所有样本上每个像素的标准差接近 1 (Goodfellow et al. 2013c)</li>    </ul>  </li>  <li>    <p><strong>GCN 的意义</strong></p>    <ul>      <li>        <p>式中的标准差可以看作是对图片 L2 范数的重新缩放（假设移除了均值），但我们倾向于标准差而不是 L2 范数来定义 GCN，是因为标准差包括除以像素数量这一步，从而基于标准差的 GCN 能够使用与图像大小无关的固定的 s.</p>      </li>      <li>        <p>而将标准差视为 L2 范数的缩放，可以将 GCN 理解成到球壳的一种映射。这可能是一个有用的属性，因为神经网络往往更好地响应空间方向，而不是精确的位置。</p>        <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619113501.png" alt=""></p>        <ul>          <li>(左) 原始的输入数据可能拥有任意的范数。</li>          <li>(中) λ = 0 时候的 GCN 可以完美地将所有的非零样本投影到球上。这里我们令 s = 1， ϵ = 10−8。由于我们使用的 GCN 是基于归一化标准差而不是 L2 范数，所得到的球并不是单位球。</li>          <li>(右) λ &gt; 0 的正则化 GCN 将样本投影到球上，但是并没有完全地丢弃其范数中变化。 s 和 ϵ 的取值与之前一样。</li>        </ul>      </li>    </ul>  </li>  <li>    <p>GCN 的问题：</p>    <ul>      <li>全局对比度归一化常常不能突出我们想要突出的图像特征，例如边缘和角。</li>      <li>例子：如果我们有一个场景，包含了一个大的黑暗区域和一个大的明亮的区域（例如一个城市广场有一半的区域处于建筑物的阴影之中），则全局对比度归一化将确保暗区域的亮度与亮区域的亮度之间存在大的差异。然而，它不能确保暗区内的边缘突出。</li>    </ul>  </li></ul><p><strong>局部对比度归一化</strong>（local contrast normalization, LCN）</p><ul>  <li>    <p>GCN 存在的问题催生了 LCN</p>  </li>  <li>    <p>LCN 确保对比度在每个小窗口上被归一化，而不是作为整体在图像上被归一化。</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619114844.png" alt=""></p>  </li>  <li>    <p>LCN 通常可以通过使用可分离卷积来计算特征映射的局部平均值和局部标准差，然后在不同的特征映射上使用逐元素的减法和除法。</p>  </li>  <li>    <p>LCN 是可微分的操作，并且还可以作为一种非线性作用应用于网 络隐藏层，以及应用于输入的预处理操作。    </p>  </li></ul><p><strong>数据集增强</strong> 数据集增强可以被看作是一种  <strong>只对训练集</strong>做预处理的方式。</p><h2 id="语音识别">语音识别</h2><p>自动语音识别（Automatic Speech Recognition, ASR）任务指的是构造一个函数 f*，使得它能够在给定声学序列 X 的情况下计算最有可能的语言序列 y.</p><p>令 X = (x(1), x(2), …, x(T)) 表示语音的输入向量，传统做法以 20ms 左右为一帧分割信号；y = (y1, y2, …, yN) 表示目标的输出序列（通常是一个词或者字符的序列。</p><p>许多语音识别的系统通过特殊的手工设计方法预处理输入信号，从而提取声学特征；也有一些深度学习系统 (Jaitly and Hinton, 2011) 直接从原始输入中学习特征。</p><h2 id="自然语言处理">自然语言处理</h2><p>相关术语：</p><ul>  <li>n-gram 语言模型</li>  <li>神经语言模型（Neural Language Model, NLM）    <ul>      <li>结合 n-gram 和神经语言模型</li>      <li>分层 Softmax</li>    </ul>  </li>  <li>神经机器翻译    <ul>      <li>注意力机制</li>    </ul>  </li></ul><p><strong>n-gram 语言模型</strong></p><ul>  <li>语言模型（language model）定义了自然语言中<strong>标记序列</strong>的概率分布。根据模型的设计，标记可以是词、字符、甚至是字节。标记总是离散的实体。</li>  <li>n-gram 是最早成功的语言模型    <ul>      <li>        <p>一个 n-gram 是一个包含 n 个<strong>标记</strong>的序列。</p>      </li>      <li>        <p>基于 n-gram 的模型定义一个条件概率——给定前 n − 1 个标记后的第 n 个标记的条件概率：</p>        <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619193332.png" alt=""></p>      </li>      <li>        <p>训练 n-gram 模型很简单，因为最大似然估计可以通过简单地统计每个可能的 n-gram 在训练集中<strong>出现的频数</strong>来获得。</p>      </li>      <li>        <p>通常我们同时训练 n-gram 模型和 n − 1 gram 模型。这使得下式可以简单地通过查找两个存储的概率来计算。</p>        <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619193540.png" alt=""></p>      </li>    </ul>  </li>  <li>n-gram 模型的缺点：    <ul>      <li><strong>稀疏问题</strong>——n-gram 模型最大似然的基本限制是，在许多情况下从训练集计数估计得到的 Pn 很可能为零。由此产生了各种<strong>平滑算法</strong>。</li>      <li><strong>维数灾难</strong>——经典的 n-gram 模型特别容易引起维数灾难。因为存在 |V|^n 可能的 n-gram，而且 |V| 通常很大。</li>    </ul>  </li></ul><p><strong>神经语言模型</strong>（Neural Language Model, NLM）</p><ul>  <li>    <p>NLM 是一类用来克服维数灾难的语言模型，它使用词的<strong>分布式表示</strong>对自然语言序列建模</p>  </li>  <li>    <p>NLM 能在识别两个相似词的同时，不丧失将词编码为不同的能力。神经语言模型共享一个词（及其上下文）和其他类似词的统计强度。模型为每个词学习的分布式表示，允许模型处理具有类似共同特征的词来实现这种共享。因为这样的属性很多，所以存在许多泛化的方式，可以将信息从每个训练语句传递到指数数量的语义相关语句。</p>    <ul>      <li>例如，如果词 dog 和词 cat 映射到具有许多属性的表示，则包含词 cat 的句子可以告知模型对包含词 dog 的句子做出预测，反之亦然。</li>    </ul>  </li>  <li>    <p>使用分布式表示来改进自然语言处理模型的基本思想不必局限于神经网络。它还可以用于图模型，其中分布式表示是多个潜变量的形式 (Mnih and Hinton, 2007)。</p>  </li></ul><p><strong>词嵌入</strong>（word embedding）</p><ul>  <li>    <p>词的分布时表示称为词嵌入，在这个解释下，我们将原始符号视为维度等于词表大小的空间中的点。词表示将这些点<strong>嵌入到较低维的特征空间</strong>中。</p>  </li>  <li>    <p>在原始空间中，每个词由一个one-hot向量表示，因此每对词彼此之间的欧氏距离都是 √2。</p>  </li>  <li>    <p>在<strong>嵌入空间</strong>中，经常出现在类似上下文中的词彼此接近。这通常导致具有相似含义的词变得邻近。</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619205103.png" alt=""></p>    <ul>      <li>这些嵌入是为了可视化才表示为 2 维。在实际应用中，嵌入通常具有更高的维度并且可以同时捕获词之间多种相似性。</li>    </ul>  </li></ul><p><strong>高维输出</strong></p><ul>  <li>对于大词汇表，由于词汇量很大，在词的选择上表示输出分布的计算和存储成本可能非常高。</li>  <li>表示这种分布的朴素方法是应用一个仿射变换，将隐藏表示转换到输出空间，然后应用 softmax 函数。因为 softmax 要在所有输出之间归一化，所以需要执行全矩阵乘法，这是高计算成本的原因。因此，输出层的高计算成本在训练期间（计算似然性及其梯度）和测试期间（计算所有或所选词的概率）都有出现。</li>  <li><strong>一些解决方案</strong>    <ul>      <li>使用<strong>短列表</strong>——简单来说，就是限制词表的大小</li>      <li><strong>分层 Softmax</strong></li>      <li><strong>重要采样</strong>——负采样就是一种简单的重要采样方式</li>    </ul>  </li></ul><p><strong>分层 Softmax</strong></p><ul>  <li>减少大词汇表 V 上高维输出层计算负担的经典方法 (Goodman, 2001) 是<strong>分层地分解概率</strong>。|V| 因子可以降低到 log|V| 一样低，而无需执行与 |V| 成比例数量（并且也与隐藏单元数量成比例）的计算。</li>  <li>我们可以认为这种层次结构是先建立词的类别，然后是词类别的类别，然后是词类别的类别的类别等等。这些嵌套类别构成一棵树，其叶子为词。</li>  <li>选择一个词的概率是由路径（从树根到包含该词叶子的路径）上的每个节点通向该词分支概率的乘积给出。</li></ul><p><strong>重要采样</strong>/负采样</p><ul>  <li>加速神经语言模型训练的一种方式是，避免明确地计算所有未出现在下一位置的词对梯度的贡献。</li>  <li>每个不正确的词在此模型下具有低概率。枚举所有这些词的计算成本可能会很高。相反，我们可以仅采样词的子集。</li></ul><p><strong>结合 n-gram 和神经语言模型</strong></p><ul>  <li>n-gram 模型相对神经网络的主要优点是具有更高的模型容量（通过存储非常多的元组的频率），并且处理样本只需非常少的计算量。</li>  <li>相比之下，将神经网络的参数数目加倍通常也大致加倍计算时间。</li>  <li>增加神经语言模型容量的一种简单方法是将之与 n-gram 方法结合，集成两个模型</li></ul><p><strong>神经机器翻译</strong>（NMT）</p><ul>  <li>    <p>编码器和解码器的想法 (Allen 1987; Chrisman 1991; Forcada and Ñeco 1997)很早就应用到了 NMT 中。</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619214629.png" alt=""></p>  </li>  <li>    <p>基于 MLP 方法的缺点是需要将序列预处理为固定长度。为了使翻译更加灵活，我们希望模型允许可变的输入长度和输出长度。所以大量的 NMT 模型使用 RNN 作为基本单元。</p>  </li></ul><p><strong>注意力（Attention）机制</strong></p><ul>  <li>    <p>使用固定大小的表示概括非常长的句子（例如 60 个词）的所有语义细节是非常困难的。这需要使用足够大的 RNN。这会带来一些列训练问题。</p>  </li>  <li>    <p>更高效的方法是先读取整个句子或段落（以获得正在表达的上下文和焦点），然后一次翻译一个词，每次聚焦于输入句子的不同部分来收集产生下一个输出词所需的语义细节 (Bahdanau et al., 2015)——<strong>Attention</strong> 机制</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180619214735.png" alt=""></p>    <ul>      <li>由 Bahdanau et al. (2015) 引入的现代注意力机制，本质上是<strong>加权平均</strong>。</li>    </ul>  </li></ul><p><strong>其他应用</strong>：</p><ul>  <li>推荐系统</li>  <li>知识表示、推理和回答</li></ul><p><br><br>本文链接： <a href="http://www.meng.uno/articles/f42d8431/">http://www.meng.uno/articles/f42d8431/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      如何设置网络的初始值？*
《深度学习》 8.4 参数初始化策略

一般总是使用服从（截断）高斯或均匀分布的随机值，具体是高斯还是均匀分布影响不大，但是也没有详细的研究。

但是，初始值的大小会对优化结果和网络的泛化能力产生较大的影响。

更大的初始值有助于避免冗余的单元；但如果初始值太大，又会造成梯度爆炸。

一些启发式初始化策略通常是根据输入与输出的单元数来决定初始权重的大小，比如 Glorot and Bengio (2010) 中建议建议使用的标准初始化，其中 m 为输入数，n 为输出数



还有一些方法推荐使用随机正交矩阵来初始化权重 (Saxe et al., 2013)。

常用
    
    </summary>
    
      <category term="DeepLearning" scheme="http://www.meng.uno/categories/DeepLearning/"/>
    
    
      <category term="深度学习" scheme="http://www.meng.uno/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://www.meng.uno/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Oracle教程</title>
    <link href="http://www.meng.uno/articles/33c755f8/"/>
    <id>http://www.meng.uno/articles/33c755f8/</id>
    <published>2018-04-04T06:56:38.000Z</published>
    <updated>2018-04-04T08:08:39.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Oracle第一章</h1><ol>  <li>首先打开Oracle服务</li>  <li>配置监听器（这个是因为教室的电脑Oracle安装有问题，没有配置好监听器）开始菜单中找到<em>net configration assistant</em>添加一个监听器</li>  <li>用system用户登录sqlplus</li>  <li>解锁scott用户 :（也是因为教室的Oracle安装问题导致scott账户未解锁）</li></ol><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br></pre>      </td>      <td class="code">        <pre><span class="line">alter user scott account unlock;</span><br></pre>      </td>    </tr>  </table></figure><ol start="5">  <li>修改scott密码:</li></ol><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br></pre>      </td>      <td class="code">        <pre><span class="line">alter user scott identified by tiger;</span><br></pre>      </td>    </tr>  </table></figure><ol start="6">  <li>使用scott登录sqlplus, scott是oracle自带的一个实例账户，它带有四个实例表,其中重要的就是<code>emp</code>员工表与<code>dept</code>部门表</li>  <li>安装PL/SQL第三方工具, 因为Oracle没有自带的图形化界面管理器，所以我们需要安装PLSQL，它是oracle的一个第三方GUI工具。</li></ol><p><strong><em>介绍一下Oracle的命令</em></strong></p><ul>  <li>连接数据库：</li></ul><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br></pre>      </td>      <td class="code">        <pre><span class="line">connect scoott/tiger@orcl;</span><br></pre>      </td>    </tr>  </table></figure><p>用户名为scott，密码为tiger,数据库名为orcl</p><ul>  <li>显示当前用户：</li></ul><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br></pre>      </td>      <td class="code">        <pre><span class="line">show user;</span><br></pre>      </td>    </tr>  </table></figure><p>也可使用查询语句：</p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br></pre>      </td>      <td class="code">        <pre><span class="line">select USER from dual; --dual是oracle的一个虚拟表</span><br></pre>      </td>    </tr>  </table></figure><ul>  <li>显示表结构(以emp表为例)：</li></ul><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br></pre>      </td>      <td class="code">        <pre><span class="line">describe emp;</span><br></pre>      </td>    </tr>  </table></figure><p>可简写为：</p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br></pre>      </td>      <td class="code">        <pre><span class="line">desc emp;</span><br></pre>      </td>    </tr>  </table></figure><h1>Oracle第二章</h1><h2 id="创建表空间">创建表空间</h2><p>（在SqlServer中称为创建一个是数据库，而在Oracle中则称为创建一个表空间）</p><p><em>格式： create tablespace 表空间名 datafile ‘文件路径’ size 文件大小</em></p><p>如：</p><pre><code>cerate tablespace myspace datafile 'D:\myspace.dbf' size 10MB;</code></pre><p>删除表空间：</p><pre><code>drop tablespace myspace incluiding contents and datafile;</code></pre><h2 id="创建用户">创建用户</h2><p><em>格式： create user 用户名 identified by 密码 default tablespace 默认表空间</em></p><p>如：</p><pre><code>create user user1 identified by user1 default tablespace system;</code></pre><p><strong>删除用户：</strong></p><pre><code>drop user user1 cascade;</code></pre><h2 id="给用户授权">给用户授权</h2><p><em>方式一：授予角色</em></p><pre><code>1、connect     //登录2、resource    //普通权限，用于操作3、DBA         //管理员权限（慎用）</code></pre><p>如：</p><pre><code>grant connect to user1;grant connect,resource to user1;</code></pre><p><em>方式二：授予单个权限</em></p><p>如：</p><pre><code>grant create table to user1;           //授予user1建表的权限grant drop table to user1;             //授予user1删表的权限</code></pre><p><em>方式三：将某个对象的权限授予用户</em></p><p>如：</p><pre><code>grant select on scott.emp to user1;      //将scott用户的emp表的查询权限授予user1grant all on scott.emp to user1;       //将scott用户的emp表的所有权限授予user1 </code></pre><p><strong>收回权限：</strong></p><p><em>格式： revoke 权限 from 用户</em></p><p>如：</p><pre><code>revoke connect from user1;   //收回user1的connect权限revoke select on scott.emp from user1;    //收回user1对emp表的查询权限  </code></pre><h1>Oracle第三章</h1><h2 id="基本查询">基本查询</h2><p><em>select格式：</em></p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>      </td>      <td class="code">        <pre><span class="line">select 列名 from 表名 ；</span><br><span class="line">where 查询条件</span><br><span class="line">group by 分组列</span><br><span class="line">having 分组后条件</span><br><span class="line">order by 排序列 asc[desc]</span><br></pre>      </td>    </tr>  </table></figure><p>如：查询部门10的雇员</p><pre><code>select * from emp where deptno=10;</code></pre><h2 id="行号-rownum">行号（rownum）</h2><p><strong>每个表都有一个虚列ROWNUM，它用来显示结果中记录的行号。我们在查询中也可以显示这个列。</strong></p><p>如：显示emp表的行号</p><pre><code>select rownum,ename from emp;</code></pre><p>如：显示前三行</p><pre><code>select * from emp where rownum&lt;=3;</code></pre><h2 id="查询进行计算">查询进行计算</h2><p>如：显示雇员工资上浮20%的结果</p><pre><code>select ename,sal,sal*(1+20%) from emp;</code></pre><p>如：显示每个员工的总工资（工资+奖金）</p><pre><code>update emp set comm = o where comm is null;    //因为null的特殊性，它与任何值运算都等于null，所以先要把它更新为0，后面我们会学到一个函数来处理null值select ename,sal+comm from emp;</code></pre><h2 id="使用别名">使用别名</h2><p>如：在查询中使用列别名</p><pre><code>select ename as 名称，sal as 工资 from emp; //建议省略as</code></pre><p><em>另，在别名为关键字或有特殊符号时需要加双引号</em></p><p>如：</p><pre><code>select ename as &quot;select&quot;,sal*12+5000 as &quot;年度工资（加年终奖）&quot; from emp;</code></pre><h2 id="连接运算符">连接运算符</h2><p><strong>连接运算符是双竖线“||”。通过连接运算可以将两个字符串连接在一起。</strong></p><p>如：在查询中使用连接运算</p><pre><code>select ename||job as &quot;雇员和职务表&quot; from emp;</code></pre><p>*注意：‘5’||5结果为’55’    ‘5’+5结果为 10 *</p><h2 id="六-消除重复行-distinct">六、消除重复行（distinct）</h2><p><strong>如果在显示结果中存在重复行，可以使用关键字<code>distinct</code>消除重复显示</strong></p><p>如：统计职务的数量</p><pre><code>select count(distinct job) from emp;</code></pre><h2 id="排序">排序</h2><p><strong>1、升序（默认为升序<code>asc</code>,所以可以忽略）</strong></p><p>如：查询雇员姓名和工资，并按工资从小到大排序</p><pre><code>select ename,sal from emp order by sal asc;</code></pre><p><strong>2、降序（<code>desc</code>不可忽略）</strong></p><p>如：查询雇员姓名和雇佣日期，并按雇佣日期排序，后雇佣的先显示</p><pre><code>select ename,hiredate from emp order by hiredate desc;</code></pre><p><strong>3、多列排序</strong></p><p><em>可以按多列进行排序，先按第一列，然后按第二列、第三列…。</em></p><p>如：查询雇员信息，先按部门从小到大排序，再按雇佣时间的先后排序</p><pre><code>select ename,deptno,hiredate from emp order by deptno hiredate;</code></pre><h1>Oracle第四章——条件查询、字符型函数</h1><h2 id="条件查询">条件查询</h2><p><strong>1、模糊查询(between、in、like)</strong></p><p>A、between：在某某之间。如,显示工资在1000~2000之间的雇员</p><pre><code>select * from emp where sal beteween 1000 and 2000;</code></pre><p>B、in：在某某之间。如，显示职务为“SALMAN”，“CLEARK”和“MANAGER”的雇员信息</p><pre><code>select * from emp where job in ('SALMAN','CLERK','MANAGER');</code></pre><p>C、like：与通配符使用</p><p><em>通配符：% 代表0个或任意个字符     —_ 代表1个字符</em></p><p>如：显示姓名以“S”开头的雇员信息。</p><pre><code>select * from emp where ename like 'S%';</code></pre><p>显示姓名第二个字符为“A”的雇员信息</p><pre><code>select * from emp ename like '_A%';</code></pre><p><strong>2、空值查询</strong></p><p><em>空：is null      非空： is not null</em></p><p>如：查询奖金为空的雇员信息</p><pre><code>select * from emp where comm is null;</code></pre><h2 id="函数">函数</h2><p><strong>1、数学函数</strong></p><table>  <tr>    <td>函数</td>    <td>功能</td>    <td>实例</td>    <td>结果</td>  </tr>  <tr>    <td>abs</td>    <td>求绝对值函数</td>    <td>abs(-5)</td>    <td>5</td>  </tr>  <tr>    <td>sqrt</td>    <td>求平方根</td>    <td>sqrt(2)</td>    <td>1.414</td>  </tr>  <tr>    <td>power</td>    <td>求幂函数</td>    <td>power(2,3)</td>    <td>8</td>  </tr></table><p>使用求绝对值函数abs</p><pre><code>select abs(-5) from dual;</code></pre><p>使用求平方根函数sqrt。</p><pre><code>select sqrt(2) from dual;</code></pre><p>使用ceil函数。</p><pre><code>select ceil(2.35) from dual;</code></pre><p>使用floor函数。</p><pre><code>select floor(2.35) from dual;</code></pre><p><strong>2、使用四舍五入函数round</strong> <small> 格式：round(数字，保留的位数)</small></p><pre><code>select round(45.923,2), round(45.923,0), round(45.923,-1) from dual;</code></pre><p><strong>3、字符型函数</strong></p><table><tr><td>ascii</td><td>返回与ASCII码相应的字符</td><td>Ascii('A')</td><td>65</td></tr><td>char</td><td>返回与ASCII码相应的字符</td><td>char(65)</td><td>A</td><tr><td>lower</td><td>将字符串转换成小写</td><td>lower ('SQL Course')</td><td>sql course</td></tr><tr><td>upper</td><td>将字符串转换成</td><td>upper('SQL Course')</td><td>SQL COURSE</td></tr><tr><td>initcap</td><td>将字符串转换成每个单词以大写开头</td><td>initcap('SQL course')</td><td>SQL Course</td></tr><tr><td>concat</td><td>连接两个字符串</td><td>concat('SQL', ' Course')</td><td>SQL Course</td></tr><tr><td>substr</td><td>给出起始位置和长度，返回子字符串</td><td>substr('String',1,3)</td><td>Str</td></tr><tr><td>length</td><td>求字符串的长度</td><td>length('Wellcom')</td><td>7</td></tr><tr><td>trim</td><td>在一个字符串中去除另一个字符串</td><td>trim('S' FROM 'SSMITH')</td><td>MITH</td></tr><tr><td>replace</td><td>用一个字符串替换另一个字符串中的子字符串</td><td>replace('ABC', 'B', 'D')</td><td>ADC</td></tr></table><p>如果不知道表的字段内容是大写还是小写，可以转换后比较。</p><pre><code>select empno, ename,deptno from emp where lower(ename)='blake';</code></pre><p>显示名称以“W”开头的雇员，并将名称转换成以大写开头。</p><pre><code>select empno,initcap(ename),job from emp wher substr(ename,1,1)='W';</code></pre><p>显示雇员名称中包含“S”的雇员名称及名称长度。</p><pre><code>select empno,ename,legth(ename) from emp where instr(ename,'S',1,1)&gt;0;</code></pre><h1>Oracle第五章——函数</h1><h2 id="日期型函数">日期型函数</h2><p><strong>Oracle使用内部数字格式来保存时间和日期，包括世纪、年、月、日、小时、分、秒。缺省日期格式为 DD-MON-YY，如“08-05月-03”代表2003年5月8日。</strong></p><ul><li>SYSDATE：返回系统日期和时间的虚列函数。</li></ul><p>如：返回系统的当前日期。</p><pre><code>SELECT sysdate FROM dual;</code></pre><ul><li>对两个日期相减，得到相隔天数。</li></ul><p><em>通过加小时来增加天数，24小时为一天，如12小时可以写成12/24(或0.5)。</em></p><p>如：例1 假定当前的系统日期是2003年2月6日，求再过1000天的日期。</p><pre><code>SELECT sysdate+1000 AS &quot;NEW DATE&quot; FROM dual;</code></pre><p>例2：两个日期相减</p><pre><code> select to_date('1-1月-2000') - to_date('1-8月-1999') from dual;</code></pre><ul><li>其它日期函数</li></ul><table><thead><tr><th>函数</th><th>功能</th><th>实例</th><th>结果</th></tr></thead><tbody><tr><td>months_between</td><td>返回两个日期间的月份</td><td>months_between (‘04-11月-05’,‘11-1月-01’)57.7741935</td><td></td></tr><tr><td>add_months</td><td>返回把月份数加到日期上的新日期</td><td>add_months(‘06-2月-03’,1)，add_months(‘06-2月-03’,-1)</td><td>06-3月-03，06-1月-03</td></tr><tr><td>next_day</td><td>返回指定日期后的星期对应的新日期</td><td>next_day(‘06-2月-03’,‘星期一’)</td><td>10-2月-03</td></tr><tr><td>last_day</td><td>返回指定日期所在的月的最后一天</td><td>last_day(‘06-2月-03’)</td><td>28-2月-03</td></tr><tr><td>round</td><td>按指定格式对日期进行四舍五入</td><td>round(to_date(‘13-2月-03’),‘YEAR’)，round(to_date(‘13-2月-03’),‘MONTH’)，round(to_date(‘13-2月-03’),‘DAY’)</td><td>01-1月-03，01-2月-03，16-2月-03(按周四舍五入)</td></tr></tbody></table><p>如：返回2003年2月的最后一天。</p><pre><code>SELECT last_day('08-2月-03') FROM dual;</code></pre><p>假定当前的系统日期是2003年2月6日，显示部门10雇员的雇佣天数。</p><pre><code>SELECT ename, round(sysdate-hiredate) DAYS FROM emp WHERE  deptno = 10;</code></pre><h2 id="转换函数">转换函数</h2><table><thead><tr><th>函数</th><th>功能</th><th>实例</th><th>结果</th></tr></thead><tbody><tr><td>To_char</td><td>转换成字符串类型</td><td>To_char(1234.5, ‘$9999.9’)</td><td>$1234.5</td></tr><tr><td>To_date</td><td>转换成日期类型</td><td>To_date(‘1980-01-01’, ‘yyyy-mm-dd’)</td><td>01-1月-80</td></tr><tr><td>To_number</td><td>转换成数值类型</td><td>To_number(‘1234.5’)</td><td>1234.5</td></tr></tbody></table><ul><li>自动类型转换</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SELECT &apos;12.5&apos;+11 FROM dual;    //结果为：23.5</span><br><span class="line">Select  ‘12.5’||11 from dual;    //结果为：’12.511’</span><br></pre></td></tr></table></figure><ul><li>日期类型转换</li></ul><table><thead><tr><th>代码</th><th>代表的格式</th><th>例子</th></tr></thead><tbody><tr><td>AM、PM</td><td>上午、下午</td><td>08 AM</td></tr><tr><td>D</td><td>数字表示的星期(1～7)</td><td>1,2,3,4,5,6,7</td></tr><tr><td>DD</td><td>数字表示月中的日期(1～31)</td><td>1,2,3,…,31</td></tr><tr><td>MM</td><td>两位数的月份</td><td>01,02,…,12</td></tr><tr><td>Y、YY、YYY、YYYY</td><td>年份的后几位</td><td>3,03,003,2003</td></tr><tr><td>RR</td><td>解决Y2K问题的年度转换</td><td></td></tr><tr><td>DY</td><td>简写的星期名</td><td>MON,TUE,FRI,…</td></tr><tr><td>DAY</td><td>全拼的星期名</td><td>MONDAY,TUESDAY,…</td></tr><tr><td>MON</td><td>简写的月份名</td><td>JAN,FEB,MAR,…</td></tr><tr><td>MONTH</td><td>全拼的月份名</td><td>JANUARY,FEBRUARY,…</td></tr><tr><td>HH、HH12</td><td>12小时制的小时(1～12)</td><td>1,2,3,…,12</td></tr><tr><td>HH24</td><td>24小时制的小时(0～23)</td><td>0,1,2,…,23</td></tr><tr><td>MI</td><td>分(0～59)</td><td>0,1,2,…,59</td></tr><tr><td>SS</td><td>秒(0～59)</td><td>0,1,2,…,59</td></tr><tr><td>,./-;:</td><td>原样显示的标点符号</td><td></td></tr><tr><td>‘TEXT’</td><td>引号中的文本原样显示</td><td>TEXT</td></tr></tbody></table><p>如：1、日期型转字符型</p><p>将日期转换成带时间和星期的字符串并显示。</p><pre><code>SELECT TO_CHAR(sysdate,'YYYY-MM-DD HH24:MI:SS AM DY') FROM dual;</code></pre><p>将日期显示转换成中文的年月日。</p><pre><code>SELECT TO_CHAR(sysdate,'YYYY&quot;年&quot;MM&quot;月&quot;DD&quot;日&quot;') FROM dual;</code></pre><p>2.字符型转日期型</p><p>往emp表中插入一条记录</p><pre><code>insert into emp values(8888,'张三','CLERK',7369,to_date('1-1月-2000'),1000,10,10);insert into emp values(8889,'李四','CLERK',7369,to_date('2000-01-01','YYYY-MM-DD'),1000,10,10);</code></pre><h2 id="其他常用函数">其他常用函数</h2><table><thead><tr><th>函数</th><th>功能</th><th>实例</th><th>结果</th></tr></thead><tbody><tr><td>nvl</td><td>空值转换函数</td><td>nvl(null, ‘空’)</td><td>空</td></tr><tr><td>decode</td><td>实现分支功能</td><td>decode(1,1, ‘男’, 2, ‘女’)</td><td>男</td></tr><tr><td>userenv</td><td>返回环境信息</td><td>userenv(‘LANGUAGE’)</td><td>SIMPLIFIED CHINESE_CHINA.ZHS16GBK</td></tr><tr><td>greatest</td><td>返回参数的最大值</td><td>greatest(20,35,18,9)</td><td>35</td></tr><tr><td>least</td><td>least返回参数的最小值</td><td>least(20,35,18,9)</td><td>9</td></tr></tbody></table><h3 id="1-空值的转换">1．空值的转换</h3><p><em>如果对空值NULL不能很好的处理，就会在查询中出现一些问题。在一个空值上进行算术运算的结果都是NULL。最典型的例子是，在查询雇员表时，将工资sal字段和津贴字段comm进行相加，如果津贴为空，则相加结果也为空，这样容易引起误解。</em></p><p><strong>使用nvl函数，可以转换NULL为实际值。该函数判断字段的内容，如果不为空，返回原值；为空，则返回给定的值。</strong></p><p>如下3个函数，分别用新内容代替字段的空值：</p><pre><code>nvl(comm, 0)：用0代替空的Comm值。nvl(hiredate, '01-1月-97')：用1997年1月1日代替空的雇佣日期。nvl(job, '无')：用“无”代替空的职务。</code></pre><p>使用nvl函数转换空值。</p><pre><code>SELECTename,nvl(job,'无'),nvl(hiredate,'01-1月-97'),nvl(comm,0) FROM emp;</code></pre><h3 id="2-decode函数">2．decode函数</h3><p><em>decode函数可以通过比较进行内容的转换，完成的功能相当于分支语句。在参数的最后位置上可以存在单独的参数，如果以上比较过程没有找到匹配值，则返回该参数的值，如果不存在该参数，则返回NULL。</em></p><p>将职务转换成中文显示。</p><pre><code>SELECTename,decode(job, 'MANAGER', '经理', 'CLERK','职员', 'SALESMAN','推销员', 'ANALYST','系统分析员','未知') FROM emp;</code></pre><h3 id="3-最大-最小值函数">3．最大、最小值函数</h3><p><em>greatest返回参数列表中的最大值，least返回参数列表中的最小值。</em></p><p><strong>如果表达式中有NULL，则返回NULL。</strong></p><h1>Oracle第六章——相等、外连接</h1><h2 id="相等连接">相等连接</h2><h3 id="1-三个步骤">1、三个步骤</h3><p>A、先列出要显示的列： select ename,job,comm,emp,deptno,dname</p><p>B、列出查询的表： from emp,dept</p><p>C、列出多表相连条件（主外键）：where emp.deptno=dept.deptno</p><p><em>注意：如果两个表有同名列，那么前面必须接表名 如： emp.deptno ,如果不是同名字段则表名可以省略</em></p><h3 id="2-inner-join-的写法">2、inner join 的写法</h3><pre><code>select enaem,job,sal,comm,emp.deptno,dname from emp inner join dept on emp.deptno = dept.deptno;</code></pre><h3 id="3-三表或三表以上的写法">3、三表或三表以上的写法</h3><pre><code>select 字段1，字段2 , 字段3 。。。。from 表1，表2，表3.。。where 表1.外键 = 表2.主键  and 表1.外键 = 表3.主键 and 。。。</code></pre><p><em>注意：两个表有一个条件 ，三个表有两个条件 ，四个表有三个条件 以此类推</em></p><h2 id="外连接-不等连接">外连接（不等连接）</h2><p><em>左外连接即在内连接的基础上，左边表中有但右边表中没有的记录也以null的形式显示出来，右外连接则反之</em></p><h3 id="1-写法1">1、写法1</h3><p><em>(右外连接)</em></p><pre><code>select ename,d.deptno,dname from emp e,dept d where e.deptno(+) = d.deptno</code></pre><p><em>(左外连接)</em></p><pre><code>select ename,d.deptno,dname from emp e,dept d where d.deptno = e.deptno(+)    </code></pre><h3 id="2-写法2">2、写法2</h3><pre><code>select ename,d.deptno,dname from emp e right join dept d on e.deptno = d.deptno  </code></pre><h1>Oracle第七章——连接、分组查询</h1><h2 id="不等连接">不等连接</h2><p><strong>拿一个表作为另一表的查询条件或范围</strong></p><p>如：显示雇员名称，工资和所属工资等级。</p><pre><code>select e.ename,e.sal,s.grade from emp e,salgrade s where e.sal between s.losal and s.hisal;</code></pre><h2 id="自连接">自连接</h2><p><strong>自连接就是一个表，同本身进行连接。对于自连接可以想像存在两个相同的表(表和表的副本)，可以通过不同的别名区别两个相同的表（其它就是内连接)</strong></p><p>如：显示雇员名称和雇员的经理名称</p><pre><code>select worker.ename||'的经理是'||manager.ename as 雇员经理 from emp worker,emp manager where worker.mgr=manager.empno;</code></pre><h2 id="组函数">组函数</h2><ul><li>组函数只能应用于SELECT子句、HAVING子句或ORDER BY子句中。</li><li>组函数也可以称为统计函数。</li><li>组函数忽略列的空值。</li><li>对组可以应用组函数。</li><li>在组函数中可使用DISTINCT或ALL关键字。</li><li>ALL表示对所有非NULL值(可重复)进行运算。</li><li>DISTINCT 表示对每一个非NULL值，如果存在重复值，则组函数只运算一次。如果不指明上述关键字，默认为ALL。</li></ul><table><thead><tr><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>AVG</td><td>求平均值</td></tr><tr><td>COUNT</td><td>求计数值，返回非空行数，*表示返回所有行</td></tr><tr><td>MAX</td><td>求最大值</td></tr><tr><td>MIN</td><td>求最小值</td></tr><tr><td>SUM</td><td>求和</td></tr><tr><td>SIDDEV</td><td>求标准偏差，是根据差的平方根得到的</td></tr><tr><td>VARIANCE</td><td>求统计方差</td></tr></tbody></table><h2 id="分组查询">分组查询</h2><p><strong>1、如：按职务统计工资总和。</strong></p><pre><code>select deptno,job,sum(sal) from emp group by deptno,job;</code></pre><p><strong>2、多列分组</strong></p><p>如：按部门和职务分组统计工资总和:</p><pre><code>select deptno,job,sum(sal) from emp group by deptno,job;</code></pre><p><strong>3、HAVING</strong></p><p><em>HAVING从句过滤分组后的结果，它只能出现在GROUP BY从句之后，而WHERE从句要出现在GROUP BY从句之前。</em></p><p>如：统计各部门的最高工资，排除最高工资小于3000的部门。</p><pre><code>select deptno,max(sal) from emp group by deptno having max(sal)&gt;=3000;</code></pre><p><strong>4、分组统计结果排序</strong></p><p><em>可以使用ORDER BY从句对统计的结果进行排序，ORDER BY从句要出现在语句的最后。</em></p><p>如：按职务统计工资总和并排序。</p><pre><code>select job 职务, sum(sal) 工资总和 from emp group by job order by sum(sal);</code></pre><p><strong>5、组函数的嵌套使用</strong></p><p>如：求各部门平均工资的最高值。</p><pre><code>select max(avg(sal)) from emp group by deptno;</code></pre><h1>Oracle第八章——子查询</h1><h2 id="子查询">子查询</h2><p><strong>通过把一个查询的结果作为另一个查询的一部分,子查询一般出现在SELECT语句的WHERE子句中，Oracle也支持在FROM或HAVING子句中出现子查询。子查询比主查询先执行，结果作为主查询的条件，在书写上要用圆括号扩起来，并放在比较运算符的右侧。</strong></p><h3 id="1-单行子查询">1、单行子查询</h3><p>如：查询比SCOTT工资高的雇员名字和工资。</p><pre><code>select ename,sal from emp where sal&gt;(select sal from emp where empno=7788);</code></pre><h3 id="2-多行子查询">2、多行子查询*</h3><p><strong>如果子查询返回多行的结果，则我们称它为多行子查询。多行子查询要使用不同的比较运算符号，它们是IN、ANY和ALL。</strong></p><p>如:查询工资低于任意一个“CLERK”的工资的雇员信息。</p><pre><code>select empno,ename,job,sal from emp where sal &lt; any (select sal from emp where job = 'CLERK') and job &lt;&gt; 'CLERK';</code></pre><p>如：查询工资比所有的“SALESMAN”都高的雇员的编号、名字和工资。</p><pre><code>select empno,ename,job from emp where job in (select job from emp where deptno = 10) and deptno = 20;</code></pre><h3 id="3-多列子查询">3.多列子查询</h3><p>**如果子查询返回多列，则对应的比较条件中也应该出现多列，这种查询称为多列子查询。以下是多列子查询的训练实例。**如： 查询职务和部门与SCOTT相同的雇员的信息。</p><pre><code>select empno, ename,sal from emp where (job,deptno) = (select job,deptno from emp where empno = 7788);</code></pre><h3 id="4-在from从句中使用子查询">4．在FROM从句中使用子查询</h3><p><strong>在FROM从句中也可以使用子查询，在原理上这与在WHERE条件中使用子查询类似。有的时候我们可能要求从雇员表中按照雇员出现的位置来检索雇员，很容易想到的是使用rownum虚列。比如我们要求显示雇员表中6～9位置上的雇员，可以用以下方法</strong></p><p>如：查询雇员表中排在第6～9位置上的雇员。</p><pre><code>select ename, sal, from (select rownum as num,ename,sal from emp where rownum&lt;=9) where num&gt;=6;</code></pre><h2 id="集合运算">集合运算</h2><table><thead><tr><th>操作</th><th>描述</th></tr></thead><tbody><tr><td>union</td><td>并集，合并两个操作的结果，去掉重复的部分</td></tr><tr><td>union all</td><td>并集，合并两个操作的结果，保留重复的部分</td></tr><tr><td>minus</td><td>差集，从前面的操作结果中去掉与后面操作结果相同的部分</td></tr><tr><td>intersect</td><td>交集，取两个操作结果中相同的部分</td></tr></tbody></table><p>如：查询部门10和部门20的所有职务。</p><pre><code>select job from emp where deptno = 10 unionselect job from emp where deptno = 20;</code></pre><p>如：查询只在部门表中出现，但没有在雇员表中出现的部门编号。</p><pre><code>select deptno from deptminusselect deptno from emp;</code></pre><h1>Oracle第九章——增删改、序列、事务</h1><h2 id="增删改">增删改</h2><p>增： <code>insert into 表名(列名) values (值)；</code></p><p>删： <code>delete from 表名 where 条件；</code></p><p>改： <code>update 表名 set 列名1=值1，列名2=值2... where 条件；</code></p><h2 id="复制数据">复制数据</h2><p><strong>1、通过一条查询语句创建一个新表(要求目标表不存在)</strong></p><pre><code>create table manager as select empno,ename,sal, from emp where job= 'CLERK';</code></pre><p><strong>2、通过一条查询语句复制数据(要求目标表必须已建好)</strong></p><pre><code>insert into manager select empno,ename,sal from emp where job = 'CLERK';</code></pre><h2 id="序列">序列</h2><h3 id="1-创建序列">1、创建序列</h3><p>如：创建从2000起始，增量为1 的序列abc：</p><pre><code>create sequence abc increment by 1 start with 2000maxvalue 99999 cycle nocache;</code></pre><h3 id="2-使用序列">2、使用序列</h3><p>序列名.nextval: 代表下一个值</p><p>序列名.currval: 代表当前值</p><p>如：</p><pre><code>insert into manager values(abc.nextval,'小王',2500);insert into manager values(abc.nextval,'小赵'，2800);</code></pre><h2 id="事务">事务</h2><p>    <strong>两次连续成功的COMMIT或ROLLBACK之间的操作，称为一个事务。在一个事务内，数据的修改一起提交或撤销，如果发生故障或系统错误，整个事务也会自动撤销</strong><br>    <strong>数据库事务处理可分为隐式和显式两种。显式事务操作通过命令实现，隐式事务由系统自动完成提交或撤销(回退)工作，无需用户的干预。</strong></p><p>1、隐式提交的情况包括：</p><p>    当用户正常退出SQL*Plus或执行CREATE、DROP、GRANT、REVOKE等命令时会发生事务的自动提交。</p><p>2、显示事务:</p><pre>COMMIT        数据库事务提交，将变化写入数据库ROLLBACK数据库事务回退，撤销对数据的修改SAVEPOINT创建保存点，用于事务的阶段回退</pre><h1>Oracle第十章————建表</h1><h2 id="建表">建表</h2><p>格式：</p><pre>create table 表名      (  列名1   类型   约束,  列名2   类型   约束,  ......      );</pre><p>如：</p><p>– 创建出版社表</p><pre><code>create table 出版社（编号 varchar2(2),出版社名称 varchar2(30),地址 varchar2(30),联系电话 varchar2(20)）;</code></pre><p>– 创建图书表</p><pre><code>create table 图书 (图书编号 VARCHAR2(5),图书名称 VARCHAR2(30),出版社编号 VARCHAR2(2),作者 VARCHAR2(10),出版日期 DATE,数量 NUMBER(3),单价 NUMBER(7,2));</code></pre><h2 id="通过子查询建表">通过子查询建表</h2><p>步骤1：完全复制图书表到“图书1”</p><pre><code>create table 图书1 as select * from 图书;</code></pre><p>步骤2：创建新的图书表“图书2”，只包含书名和单价</p><pre><code>create table 图书2（书名，单价） as seelct 图书名称，单价 from 图书；</code></pre><p>步骤3：创建新的图书表“图书3”，只包含书名和单价，不复制内容</p><pre><code>create table 图书3（书名，单价） as select 图书名称，单价 from 图书 where 1=2；</code></pre><h2 id="添加表的约束">添加表的约束</h2><pre>主键     primary key      PK唯一     unique           UQ默认值   default          DF检查约束 check            CK外键约束 foreign key      FK</pre><h3 id="方法一：建表的同时添加约束">方法一：建表的同时添加约束</h3><p>如：</p><pre>create table stuinfo( sno int primary key not null,       --主键 sname varchar2(10) unique not null,       --唯一 sex char(2) default '男' check(sex='男' or sex = '女') not null,   --默认及检查 saddress varchar2(50) not null, phone char(11), email varchar2(50));create table stumarks( marksId int, sno int references stuinfo(sno) not null,     --外键 score number(5,1), examDate date default sysdate);</pre><h3 id="方法二：建表完成后-再添加约束">方法二：建表完成后，再添加约束</h3><p>如：（之前已建好了出版社表及图书表）</p><p>–主键约束</p><pre><code>alter table 出版社 add constraint PK_编号 primary key (编号);</code></pre><p>–唯一约束</p><pre><code>alter table 出版社 add constraint UQ_地址 unique (地址);</code></pre><p>–检查约束</p><pre><code>alter table 出版社 add constraint CK_联系电话 check (联系电话 like '1%');</code></pre><p>–默认值</p><pre><code>alter table 出版社 modify 地址 default '湘潭';</code></pre><p>–外键约束</p><pre><code>alter table 图书 add constraint FK_图书编号 foreign key (图书编号) references 出版社(编号);</code></pre><p>–外键约束</p><pre><code>alter table 图书 add constraint FK_图书编号 foreign key (图书编号) references 出版社(编号);</code></pre><h2 id="查看约束条件">查看约束条件</h2><p><strong>数据字典<code>USER_CONSTRAINTS</code>中包含了当前模式用户的约束条件信息。其中，CONSTRAINTS_TYPE 显示的约束类型为：</strong></p><pre>C：CHECK约束。P：PRIMARY KEY约束。U：UNIQUE约束。R：FOREIGN KEY约束。</pre><p><em>其他信息可根据需要进行查询显示，可用DESCRIBE命令查看<code>USER_CONSTRAINTS的</code>结构。</em></p><p>如:检查表的约束信息：</p><pre><code>SELECT CONSTRAINT_NAME,CONSTRAINT_TYPE,SEARCHCONDITONFROM USER_CONSTRAINTSWHERE TABLE_NAME='图书';</code></pre><h2 id="删除约束条件">删除约束条件</h2><pre><code>ALTER TABLE 表名 DROP CONSTRAINT 约束名;</code></pre><h2 id="表的操作">表的操作</h2><pre>1、删除表    drop table 表名2、重命名表  RENAME 表名 TO 新表名;3、查看表</pre><p><em>可以通过对数据字典<code>USER_OBJECTS</code>的查询，显示当前模式用户的所有表。</em></p><p>如： 显示当前用户的所有表。</p><pre><code>SELECT object_name FROM user_objects WHERE object_type='TABLE';</code></pre><h2 id="修改表">修改表</h2><h3 id="1-增加新列">1、增加新列:</h3><p>如： 为“出版社”增加一列“电子邮件”：</p><pre><code>ALTER TABLE 出版社ADD 电子邮件 VARCHAR2(30) CHECK(电子邮件 LIKE '%@%');</code></pre><h3 id="2-修改列">2、修改列</h3><p>修改列定义有以下一些特点：</p><pre>(1) 列的宽度可以增加或减小，在表的列没有数据或数据为NULL时才能减小宽度。(2) 在表的列没有数据或数据为NULL时才能改变数据类型，CHAR和VARCHAR2之间可以随意转换。(3) 只有当列的值非空时，才能增加约束条件NOT NULL。(4) 修改列的默认值，只影响以后插入的数据。如：修改“出版社”表“电子邮件”列的宽度为40。</pre><pre><code>ALTER TABLE 出版社 MODIFY 电子邮件 VARCHAR2(40);</code></pre><h3 id="3-删除列">3、删除列</h3><p>如：删除“出版社”表的“电子邮件”列。</p><pre><code>ALTER TABLE 出版社 DROP COLUMN 电子邮件;</code></pre><h1>Oracle第十一章————视图</h1><h2 id="分区表">分区表</h2><p><strong>在某些场合会使用非常大的表，比如人口信息统计表。如果一个表很大，就会降低查询的速度，并增加管理的难度。一旦发生磁盘损坏，可能整个表的数据就会丢失，恢复比较困难。根据这一情况，可以创建分区表，把一个大表分成几个区(小段)，对数据的操作和管理都可以针对分区进行，这样就可以提高数据库的运行效率。分区可以存在于不同的表空间上，提高了数据的可用性。例：创建和使用分区表。</strong></p><p>如：创建按成绩分区的考生表，共分为3个区：</p><pre>CREATE TABLE 考生 (考号 VARCHAR2(5),姓名 VARCHAR2(30),成绩 NUMBER(3))PARTITION BY RANGE(成绩)(PARTITION A VALUES LESS THAN (300)TABLESPACE USERS,PARTITION B VALUES LESS THAN (500)TABLESPACE USERS,PARTITION C VALUES LESS THAN (MAXVALUE)TABLESPACE USERS);</pre><p>步骤3：检查A区中的考生：</p><pre><code>SELECT *  FROM  考生 PARTITION(A);</code></pre><p>步骤4：检查全部的考生：</p><pre><code>SELECT *  FROM  考生;</code></pre><h2 id="视图">视图</h2><h3 id="1-视图的概念">1、视图的概念</h3><p>视图不同于表，视图本身不包含任何数据。而视图只是一种定义，对应一个查询语句。视图的数据都来自于某些表，这些表被称为基表。</p><p>视图可以在表能够使用的任何地方使用，但在对视图的操作上同表相比有些限制，特别是插入和修改操作。对视图的操作将传递到基表，所以在表上定义的约束条件和触发器在视图上将同样起作用。2、视图的创建</p><h3 id="2-格式：">2、格式：</h3><pre><code>create [or replace] view 视图名 asselect 语句;</code></pre><p>例：创建图书作者视图：</p><pre><code>CREATE VIEW 图书作者(书名,作者) AS SELECT 图书名称,作者 FROM 图书;</code></pre><p>查询视图全部内容</p><pre><code>SELECT * FROM 图书作者;    </code></pre><p>查询部分视图：</p><pre><code>SELECT 作者 FROM 图书作者;</code></pre><p>删除视图：</p><pre><code>DROP VIEW 清华图书;</code></pre><h3 id="3-创建只读视图">3．创建只读视图</h3><p>创建只读视图要用<code>WITH READ ONLY</code>选项。</p><p>例：创建emp表的经理视图：</p><pre><code>CREATE OR REPLACE VIEW manager AS SELECT * FROM emp WHERE job= 'MANAGER'WITH READ ONLY;</code></pre><h3 id="4-使用with-check-option选项">4．使用WITH CHECK OPTION选项</h3><p>使用<code>WITH CHECK OPTION</code>选项。使用该选项，可以对视图的插入或更新进行限制，即该数据必须满足视图定义中的子查询中的WHERE条件，否则不允许插入或更新。</p><p>例：</p><pre><code>CREATE OR REPLACE VIEW 清华图书 AS SELECT * FROM 图书 WHERE 出版社编号= '01'</code></pre><p><em>WITH CHECK OPTION;注：插入数据时，由于带了with check option的选项，则只能插入出版社编为’01’的数据</em></p><h3 id="5-来自基表的限制">5．来自基表的限制</h3><p>除了以上的限制，基表本身的限制和约束也必须要考虑。如果生成子查询的语句是一个分组查询，或查询中出现计算列，这时显然不能对表进行插入。另外，主键和NOT NULL列如果没有出现在视图的子查询中，也不能对视图进行插入。在视图中插入的数据，也必须满足基表的约束条件。</p><h3 id="6-视图的查看">6.视图的查看</h3><p><code>USER_VIEWS</code>字典中包含了视图的定义。</p><p><code>USER_UPDATABLE_COLUMNS</code>字典包含了哪些列可以更新、插入、删除。</p><p><code>USER_OBJECTS</code>字典中包含了用户的对象。</p><p>可以通过<code>DESCRIB</code>E命令查看字典的其他列信息。</p><p>例：查看用户拥有的视图：</p><pre><code>SELECT object_name FROM user_objects WHERE object_type='VIEW';</code></pre><h1>Oracle第十二章——索引、同义词、数据库链接、PL/SQL语句</h1><h2 id="索引">索引</h2><p><strong>索引(INDEX)是为了加快数据的查找而创建的数据库对象，特别是对大表，索引可以有效地提高查找速度，也可以保证数据的惟一性</strong></p><p><strong>创建索引一般要掌握以下原则：只有较大的表才有必要建立索引，表的记录应该大于50条，查询数据小于总行数的2%～4%。虽然可以为表创建多个索引，但是无助于查询的索引不但不会提高效率，还会增加系统开销。因为当执行DML操作时，索引也要跟着更新，这时索引可能会降低系统的性能。</strong></p><p>创建索引：</p><pre><code> CREATE INDEX 索引名 ON 表名(列名);</code></pre><p>删除索引：</p><pre><code>  DROP INDEX 索引名；</code></pre><h2 id="同义词">同义词</h2><p><strong>同义词(SYNONYM)是为模式对象起的别名，可以为表、视图、序列、过程、函数和包等数据库模式对象创建同义词。</strong></p><p>创建私有同义词：</p><pre><code>  CREATE SYNONYM BOOK FOR 图书；</code></pre><p>创建公有同义词(先要获得创建公有同义词的权限)：</p><pre><code>  CREATE PUBLIC SYNONYM BOOK FOR SCOTT.图书；</code></pre><p>删除同义词：</p><pre><code>DROP SYNONYM 同义词名；</code></pre><h2 id="数据库链接">数据库链接</h2><p><strong>数据库链接(DATABASE LINK)是在分布式环境下，为了访问远程数据库而创建的数据通信链路。</strong></p><p>格式：</p><pre><code> CREATE DATABASE LINK 链接名 CONNECT TO 账户 IDENTIFIED BY 口令 USING 服务名;</code></pre><p>数据库链接一旦建立并测试成功，就可以使用以下形式来访问远程用户的表。</p><pre><code>表名@数据库链接名</code></pre><h2 id="pl-sql">PL/sql</h2><h3 id="1-块结构和基本语法要求">1、块结构和基本语法要求</h3><p>块中各部分的作用解释如下：</p><pre><code>(1)  DECLARE：声明部分标志。(2)  BEGIN：可执行部分标志。(3)  EXCEPTION：异常处理部分标志。(4)  END；：程序结束标志。</code></pre><h3 id="2-输出">2、输出</h3><p>第一种形式：</p><pre><code>DBMS_OUTPUT.PUT(字符串表达式)；</code></pre><p>第二种形式：</p><pre><code>DBMS_OUTPUT.PUT_LINE(字符串表达式)；</code></pre><p>第三种形式：</p><pre><code>DBMS_OUTPUT.NEW_LINE；</code></pre><h3 id="3-变量赋值：">3、变量赋值：</h3><p>第一种形式：</p><pre><code>SELECT 列名1，列名2... INTO 变量1，变量2... FROM 表名 WHERE 条件；第二种形式：变量名:=值</code></pre><p>例：查询雇员编号为7788的雇员姓名和工资。</p><pre><code>SET SERVEROUTPUT ON--在命令行界面必须写DECLARE--定义部分标识 v_name  VARCHAR2(10);--定义字符串变量v_name v_sal   NUMBER(5);--定义数值变量v_salBEGIN--可执行部分标识SELECT ename,sal INTO v_name,v_sal  FROM emp  WHERE empno=7788;--在程序中插入的SQL语句DBMS_OUTPUT.PUT_LINE('7788号雇员是：'||v_name||'，工资为：'||to_char(v_sal));--输出雇员名和工资END;</code></pre><h3 id="4-结合变量的定义和使用-即全局变量">4、结合变量的定义和使用（即全局变量）</h3><p><em><em>该变量是在整个SQL</em>Plus环境下有效的变量，在退出SQL</em>Plus之前始终有效，所以可以使用该变量在不同的程序之间传递信息。结合变量不是由程序定义的，而是使用系统命令VARIABLE定义的。**</p><p>例：定义并使用结合变量</p><p>步骤1：输入和执行下列命令，定义结合变量g_ename：</p><pre><code>--SET SERVEROUTPUT ON VARIABLE  g_ename VARCHAR2(100)BEGIN:g_ename:=:g_ename|| 'Hello~ ';--在程序中使用结合变量DBMS_OUTPUT.PUT_LINE(:g_ename);                --输出结合变量的值END;</code></pre><h3 id="5-记录变量的定义">5．记录变量的定义</h3><p><strong>还可以根据表或视图的一个记录中的所有字段定义变量，称为记录变量。记录变量包含若干个字段，在结构上同表的一个记录相同，定义方法是在表名后跟%ROWTYPE。记录变量的字段名就是表的字段名，数据类型也一致。</strong></p><p>如：</p><pre><code> v_name emp.ename%TYPE;</code></pre><h1>Oracle第十三章——PL/SQL空值语句、游标</h1><h2 id="if语句">IF语句</h2><h3 id="1-if-then-end-if形式">1、IF-THEN-END IF形式</h3><pre>   IF 条件 then        语句集;   END IF;</pre><h3 id="2-if-then-else-end-if形式">2、IF-THEN-ELSE-END IF形式</h3><pre>   IF 条件 then        语句集1;   ElSE       语句集2   END IF;</pre><h3 id="3-if-then-elsif-else-end-if形式">3．IF-THEN-ELSIF-ELSE-END IF形式</h3><pre>   IF 条件1 THEN语句集1;   ELSIF 条件2 THEN语句集2;   ELSIF 条件3 THEN语句集3;   ...   ELSE语句集n;   END IF;</pre><h2 id="case语句">CASE语句</h2><h3 id="1-基本case结构">1．基本CASE结构</h3><pre>   CASE 变量或表达式   When 值1 then 结果1;   When 值2 then 结果2;   When 值3 then 结果3;   ...   ELSE 结果n;   END CASE;</pre><h3 id="2-搜索case结构">2.搜索CASE结构</h3><pre>  CASE    When 条件1 then 结果1;   When 条件2 then 结果2;   When 条件3 then 结果3;   ...   ELSE 结果n;   END CASE;</pre><h2 id="循环">循环</h2><h3 id="1-基本loop循环">1．基本LOOP循环</h3><pre>   loop      语句集;   exit when 条件      语句集;   end loop;</pre><h3 id="2-for-loop循环">2.FOR LOOP循环</h3><p>FOR循环是固定次数循环，格式如下：</p><pre>FOR 控制变量 in [REVERSE] 下限..上限 LOOP   语句集;  END LOOP;</pre><p><em>注：循环控制变量是隐含定义的，不需要声明。</em></p><p><strong>下限和上限用于指明循环次数。正常情况下循环控制变量的取值由下限到上限递增，REVERSE关键字表示循环控制变量的取值由上限到下限递减。</strong></p><h2 id="3-while-loop循环">3．WHILE LOOP循环</h2><pre>   while 条件 loop       语句集;   end loop;</pre><h2 id="游标">游标</h2><h3 id="1-概念">1、概念</h3><p><strong>游标是SQL的一个内存工作区，由系统或用户以变量的形式定义。游标的作用就是用于临时存储从数据库中提取的数据块。在某些情况下，需要把数据从存放在磁盘的表中调到计算机内存中进行处理，最后将处理结果显示出来或最终写回数据库。这样数据处理的速度才会提高，否则频繁的磁盘数据交换会降低效率。</strong></p><p>游标有两种类型：显式游标和隐式游标。</p><p>在前述程序中用到的SELECT…INTO…查询语句，一次只能从数据库中提取一行数据，系统都会使用一个隐式游标。</p><p>显式游标对应一个返回结果为多行多列的SELECT语句。</p><p>游标一旦打开，数据就从数据库中传送到游标变量中，然后应用程序再从游标变量中分解出需要的数据，并进行处理。</p><h3 id="2-隐式游标属性">2、隐式游标属性</h3><table><thead><tr><th>隐式游标的属性</th><th>返回值类型</th><th>意义</th></tr></thead><tbody><tr><td>SQL%ROWCOUNT</td><td>整型</td><td>代表DML语句成功执行的数据行数</td></tr><tr><td>SQL%FOUND</td><td>布尔型</td><td>值为TRUE代表插入、删除、更新或单行查询操作成功</td></tr><tr><td>SQL%NOTFOUND</td><td>布尔型</td><td>与SQL%FOUND属性返回值相反</td></tr><tr><td>SQL%ISOPEN</td><td>布尔型</td><td>DML执行过程中为真，结束后为假</td></tr></tbody></table><p>如：使用隐式游标的属性，判断对雇员工资的修改是否成功。</p><pre><code>SET SERVEROUTPUT ON BEGIN  UPDATE emp SET sal=sal+100 WHERE empno=1234; IF SQL%FOUND THEN  DBMS_OUTPUT.PUT_LINE('成功修改雇员工资！');  COMMIT;  ELSEDBMS_OUTPUT.PUT_LINE('修改雇员工资失败！'); END IF; END;</code></pre><h3 id="3-显式游标">3、显式游标</h3><p>游标的使用分成以下4个步骤。</p><h4 id="a-声明游标">a．声明游标</h4><p>在DECLEAR部分按以下格式声明游标：</p><pre><code>CURSOR 游标名[(参数1 数据类型[，参数2 数据类型...])] IS SELECT语句;</code></pre><p>参数是可选部分，所定义的参数可以出现在SELECT语句的WHERE子句中。如果定义了参数，则必须在打开游标时传递相应的实际参数。</p><h4 id="b-打开游标">b.打开游标</h4><p>在可执行部分，按以下格式打开游标：</p><pre><code>OPEN 游标名[(实际参数1[，实际参数2...])];</code></pre><p>打开游标时，SELECT语句的查询结果就被传送到了游标工作区。</p><h4 id="c-提取数据">c.提取数据</h4><p>在可执行部分，按以下格式将游标工作区中的数据取到变量中。提取操作必须在打开游标之后进行。</p><pre><code>FETCH 游标名 INTO 变量名1[，变量名2...];</code></pre><p>或</p><pre><code>FETCH 游标名 INTO 记录变量;</code></pre><p>游标打开后有一个指针指向数据区，FETCH语句一次返回指针所指的一行数据，要返回多行需重复执行，可以使用循环语句来实现。控制循环可以通过判断游标的属性来进行。</p><p>定义记录变量的方法如下：</p><pre><code>     变量名 表名|游标名%ROWTYPE；</code></pre><h4 id="d-关闭游标">d.关闭游标</h4><pre><code>CLOSE 游标名;</code></pre><p>显式游标打开后，必须显式地关闭。游标一旦关闭，游标占用的资源就被释放，游标变成无效，必须重新打开才能使用。</p><p>【例1】 用游标提取emp表中7788雇员的名称和职务。</p><pre><code>SET SERVEROUTPUT ON     --在命令行界面是必须的，在第三方工具的SQL界面中无需此语句DECLARE    v_ename VARCHAR2(10);    v_job VARCHAR2(10);    CURSOR emp_cursor IS  SELECT ename,job FROM emp WHERE empno=7788;BEGIN    OPEN emp_cursor;    FETCH emp_cursor INTO v_ename,v_job;    DBMS_OUTPUT.PUT_LINE(v_ename||','||v_job);    CLOSE emp_cursor;END;</code></pre><p>【例2】  用游标提取emp表中7788雇员的姓名、职务和工资。</p><pre><code>SET SERVEROUTPUT ON     --在命令行界面是必须的，在第三方工具的SQL界面中无需此语句DECLARE CURSOR emp_cursor IS  SELECT ename,job,sal FROM emp WHERE empno=7788;emp_record emp_cursor%ROWTYPE;    --用游标定义记录变量BEGIN OPEN emp_cursor;FETCH emp_cursor INTO emp_record;   DBMS_OUTPUT.PUT_LINE(emp_record.ename||','|| emp_record.job||','|| emp_record.sal); CLOSE emp_cursor;END;</code></pre><p>【例3】  显示工资最高的前3名雇员的名称和工资。</p><pre><code>SET SERVEROUTPUT ON   --在命令行界面是必须的，在第三方工具的SQL界面中无需此语句DECLARE V_ename VARCHAR2(10);V_sal NUMBER(5);CURSOR emp_cursor IS  SELECT ename,sal FROM emp ORDER BY sal DESC;BEGIN OPEN emp_cursor; FOR I IN 1..3 LOOP   FETCH emp_cursor INTO v_ename,v_sal; DBMS_OUTPUT.PUT_LINE(v_ename||','||v_sal);  END LOOP;   CLOSE emp_cursor;END;</code></pre><h3 id="4-游标循环-重点">4、游标循环（重点）</h3><p>方法一：使用特殊的FOR循环形式显示全部雇员的编号和名称(省略掉定义记录变量、打开游标、提取数据、关闭游标)。</p><pre><code>SET SERVEROUTPUT ON     --在命令行界面是必须的，在第三方工具的SQL界面中无需此语句DECLARE CURSOR emp_cursor IS  SELECT empno, ename FROM emp;BEGINFOR Emp_record IN emp_cursor LOOP    DBMS_OUTPUT.PUT_LINE(Emp_record.empno|| Emp_record.ename);END LOOP;END;</code></pre><p>方法二：最简单方式</p><pre><code>SET SERVEROUTPUT ON      --在命令行界面是必须的，在第三方工具的SQL界面中无需此语句BEGIN FOR re IN (SELECT ename FROM EMP)  LOOPDBMS_OUTPUT.PUT_LINE(re.ename)END LOOP;END;</code></pre><h3 id="5-利用游标属性做循环条件">5、利用游标属性做循环条件</h3><p>【训练1】  使用游标的属性练习。</p><pre><code>SET SERVEROUTPUT ON         --在命令行界面是必须的，在第三方工具的SQL界面中无需此语句DECLARE  V_ename VARCHAR2(10);  CURSOR emp_cursor IS   SELECT ename FROM emp;BEGIN  OPEN emp_cursor;   IF emp_cursor%ISOPEN THEN  LOOP    FETCH emp_cursor INTO v_ename;            EXIT WHEN emp_cursor%NOTFOUND;    DBMS_OUTPUT.PUT_LINE(to_char(emp_cursor%ROWCOUNT)||'-'||v_ename);        END LOOP;  ELSE    DBMS_OUTPUT.PUT_LINE('用户信息：游标没有打开！');  END IF;           CLOSE  emp_cursor;END;</code></pre><h1>Oracle第十四章——游标、存储过程</h1><h2 id="游标参数的传递">游标参数的传递</h2><p>例：</p><pre><code>SET SERVEROUTPUT ONDECLARE    V_empno NUMBER(5);V_ename VARCHAR2(10);CURSOR emp_cursor(p_deptno NUMBER,p_job VARCHAR2) IS SELECT empno,ename FROM emp WHEREdeptno = p_deptno AND job = p_job;BEGIN OPEN emp_cursor(10, 'CLERK');  LOOP     FETCH emp_cursor INTO v_empno,v_ename;  EXIT WHEN emp_cursor%NOTFOUND;     DBMS_OUTPUT.PUT_LINE(v_empno||','||v_ename);END LOOP;END; </code></pre><h2 id="异常处理">异常处理</h2><p>错误处理的语法如下：</p><pre><code>     EXCEPTIONWHEN 错误1[OR 错误2] THEN 语句序列1;WHEN 错误3[OR 错误4] THEN 语句序列2;...WHEN OTHERS 语句序列n;      END;</code></pre><p>例：SET SERVEROUTPUT ON</p><pre><code>DECLARE    v_name VARCHAR2(10);BEGIN    SELECTename  INTO v_name  FROM emp  WHEREempno = 1234;DBMS_OUTPUT.PUT_LINE('该雇员名字为：'|| v_name);EXCEPTIONWHEN NO_DATA_FOUND THENDBMS_OUTPUT.PUT_LINE('编号错误，没有找到相应雇员！');WHEN OTHERS THEN DBMS_OUTPUT.PUT_LINE('发生其他错误！');END;</code></pre><h2 id="存储过程">存储过程</h2><h3 id="创建和删除存储过程">创建和删除存储过程</h3><p>格式：</p><pre><code> CREATE [OR REPLACE] PROCEDURE 存储过程名[(参数[IN|OUT|IN OUT] 数据类型...)]{AS|IS}[说明部分]   --定义需要使用的临时变量BEGIN语句集;[EXCEPTION]    [错误处理部分]END [过程名];</code></pre><p>删除：</p><pre><code>drop procedure 存储过程名;</code></pre><h3 id="调用存储过程">调用存储过程</h3><p>方法1：</p><pre><code>  EXECUTE 模式名.存储过程名[(参数...)];   (适用于命今行窗口及sql窗口)</code></pre><p>方法2： (适用于sql窗口)</p><pre><code>BEGIN      模式名.存储过程名[(参数...)];END;</code></pre><p>例：编写显示雇员信息的存储过程EMP_LIST，并引用EMP_COUNT存储过程(无参存储过程 )。</p><pre><code>CREATE OR REPLACE PROCEDURE EMP_LISTAS       CURSOR emp_cursor IS SELECT empno,ename FROM emp;BEGINFOR Emp_record IN emp_cursor LOOP   DBMS_OUTPUT.PUT_LINE(Emp_record.empno||Emp_record.ename);END LOOP;EMP_COUNT;END;调用：beginEMP_LIST;          end;</code></pre><h3 id="参数传递">参数传递</h3><p>a.输入参数: 参数名  IN 数据类型 DEFAULT 值；</p><pre><code>例：编写给雇员增加工资的存储过程CHANGE_SALARY，通过IN类型的参数传递要增加工资的雇员编号和增加的工资额。CREATE OR REPLACE PROCEDURE CHANGE_SALARY(P_EMPNO IN NUMBER DEFAULT 7788,P_RAISE NUMBER DEFAULT 10)  --形参P_EMPNO及P_RAISEAS   V_ENAME VARCHAR2(10);   V_SAL NUMBER(5);BEGIN   SELECT ENAME,SAL INTO V_ENAME,V_SAL FROM EMP WHERE EMPNO=P_EMPNO;   UPDATE EMP SET SAL=SAL+P_RAISE WHERE EMPNO=P_EMPNO;   DBMS_OUTPUT.PUT_LINE('雇员'||V_ENAME||'的工资被改为'||TO_CHAR(V_SAL+P_RAISE));   COMMIT;EXCEPTION  WHEN OTHERS THEN DBMS_OUTPUT.PUT_LINE('发生错误，修改失败！');  ROLLBACK; --如果出了异常则撤消END;调用：beginCHANGE_SALARY(7788,80)      end;</code></pre><p>b.输出参数: 参数名 OUT 数据类型 DEFAULT  值；</p><pre><code>    --例：统计雇员的人数       CREATE OR REPLACE PROCEDURE EMP_COUNT(P_TOTAL OUT NUMBER)   --P_TOTAL为输出参数AS   BEGINSELECT COUNT(*) INTO P_TOTAL FROM EMP;   END;调用：DECLARE V_EMPCOUNT NUMBER;   --定义变量接收过程求出的结果      BEGIN EMP_COUNT(V_EMPCOUNT); DBMS_OUTPUT.PUT_LINE('雇员总人数为：'||V_EMPCOUNT);      END;</code></pre><p>c.输入输出参数： 参数名  IN OUT   数据类型   DEFAULT   值；</p><pre><code>--例：使用IN OUT类型的参数，给电话号码增加区码。CREATE OR REPLACE PROCEDURE ADD_REGION(P_HPONE_NUM IN OUT VARCHAR2)ASBEGIN   P_HPONE_NUM:='024-'||P_HPONE_NUM;END;调用： DECLARE V_PHONE_NUM VARCHAR2(15);       BEGIN V_PHONE_NUM:='26731092'; ADD_REGION(V_PHONE_NUM); DBMS_OUTPUT.PUT_LINE('新的电话号码：'||V_PHONE_NUM);       END;</code></pre><p><br><br>本文链接： <a href="http://www.meng.uno/articles/33c755f8/">http://www.meng.uno/articles/33c755f8/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      Oracle第一章
 1. 首先打开Oracle服务
 2. 配置监听器（这个是因为教室的电脑Oracle安装有问题，没有配置好监听器）开始菜单中找到net configration assistant添加一个监听器
 3. 用system用户登录sqlplus
 4. 解锁scott用户 :（也是因为教室的Oracle安装问题导致scott账户未解锁）

1


alter user scott account unlock;


 5. 修改scott密码:

1


alter user scott identified by tiger;


 6. 使用scott登录sqlplus,
    
    </summary>
    
      <category term="Java开发Tips" scheme="http://www.meng.uno/categories/Java%E5%BC%80%E5%8F%91Tips/"/>
    
    
      <category term="Java" scheme="http://www.meng.uno/tags/Java/"/>
    
      <category term="Oracle" scheme="http://www.meng.uno/tags/Oracle/"/>
    
  </entry>
  
  <entry>
    <title>深度学习核心</title>
    <link href="http://www.meng.uno/articles/8193f764/"/>
    <id>http://www.meng.uno/articles/8193f764/</id>
    <published>2018-04-04T04:29:41.000Z</published>
    <updated>2018-08-16T05:44:13.000Z</updated>
    
    <content type="html"><![CDATA[<h1>1. 神经网络的结构</h1><p>内容：</p><ul>  <li>神经网络是什么？</li>  <li>神经网络的结构</li>  <li>神经网络的工作机制</li>  <li>深度学习中的“学习”指的是什么？</li>  <li>神经网络的不足</li></ul><p><strong>示例：一个用于数字手写识别的神经网络</strong></p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180701210407.png" alt=""></p><blockquote>  <p>这个示例相当于深度学习领域中的 “Hello World”.</p></blockquote><h2 id="1-1-神经元-隐藏单元-与隐藏层">1.1. 神经元（隐藏单元）与隐藏层</h2><p><strong>神经元（隐藏单元）</strong></p><ul>  <li>    <p>简单来说，神经元可以理解为一个用来装数字的容器，而这个数称为激活值</p>    <p>需要强调的是，激活值的值域取决于使用的激活函数，大多数激活函数的值域都是<strong>正值</strong></p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180701211429.png" alt=""></p>  </li></ul><blockquote>  <p>如果使用 sigmoid 激活函数，那么这个数字就在 0 到 1 之间；但通常来说，无论你使用哪种激活函数，这个数字都比较小</p></blockquote><ul>  <li>    <p>输入层也可以看做是一组神经元，它的激活值就是输入本身</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180701211850.png" alt="">      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180701214914.png" alt=""></p>    <blockquote>      <p>基本的神经网络只能处理向量型的输入，所以需要将这个 28*28 的像素图（矩阵），重排成长为 784 的向量</p>      <p>如果使用卷积神经网络，则可以直接处理矩阵型的输入</p>    </blockquote>  </li>  <li>    <p>对于分类问题，<strong>输出层</strong>中的激活值代表这个类别正确的可能性</p>    <blockquote>      <p>如果使用了 <code>softmax</code> 函数，那么整个输出层可以看作每个类别的概率分布</p>    </blockquote>  </li>  <li>    <p>所谓的“<strong>神经元被激活</strong>”实际上就是它获得了一个较大的激活值</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180701212340.png" alt=""></p>  </li></ul><p><strong>隐藏层</strong></p><ul>  <li>    <p>包含于输入层与输出层之间的网络层统称为“隐藏层”</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180701212657.png" alt=""></p>  </li></ul><blockquote>  <p>在这个简单的网络中，有两个隐藏层，每层有 16 个神经元</p>  <p>为什么是两层和 16 个？——层数的大小与问题的复杂度有关，而神经元的数量目前来看是随机的——网络的结构在实验时有很大的调整余地</p></blockquote><h2 id="1-2-神经网络的运作机制">1.2. 神经网络的运作机制</h2><ul>  <li>    <p>神经网络在运作的时候，隐藏层可以视为一个“黑箱”</p>  </li>  <li>    <p>每一层的激活值将通过某种方式计算出下一层的激活值——神经网络处理信息的核心机制</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180701213151.png" alt=""></p>    <blockquote>      <p>每一层被激活的神经元不同，（可能）会导致下一层被激活的神经元也不同</p>    </blockquote>  </li></ul><p><strong>为什么神经网络的分层结构能起作用？</strong></p><ul>  <li>    <p>人在初识数字时是如何区分的？——<strong>组合</strong>数字的各个部分</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702092532.png" alt=""></p>  </li>  <li>    <p><strong>在理想情况下</strong>，我们希望神经网络倒数第二层中的各隐藏单元能对应上每个<strong>基本笔画</strong>（pattern）</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702093050.png" alt=""></p>    <ul>      <li>当输入是 9 或 8 这种<strong>顶部带有圆圈</strong>的数字时，某个神经元将被激活（激活值接近 1）</li>      <li>不光是 9 和 8，所有顶部带有圆圈的图案都能激活这个隐藏单元</li>      <li>这样从倒数第二层到输出层，我们的问题就简化成了“学习哪些部件能组合哪些数字”</li>    </ul>  </li>  <li>    <p>类似的，基本笔画也可以由更基础的部件构成</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702094246.png" alt=""></p>  </li>  <li>    <p><strong>理想情况下</strong>，神经网络的处理过程</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702094455.png" alt=""></p>    <blockquote>      <p>从输入层到输出层，<strong>网络的抽象程度越来越高</strong></p>    </blockquote>  </li></ul><p><strong>深度学习的本质：通过组合简单的概念来表达复杂的事物</strong></p><ul>  <li>    <p>神经网络是不是这么做的，我们不得而知（所以是一个“黑箱”），但大量实验表明：神经网络确实在做类似的工作——<strong>通过组合简单的概念来表达复杂的事物</strong></p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702095428.png" alt=""></p>    <blockquote>      <p>语音识别：原始音频 → 音素 → 音节 → 单词</p>    </blockquote>  </li></ul><p><strong>隐藏单元是如何被激活的？</strong></p><ul>  <li>我们需要设计一个机制，这个机制能够把像素拼成边，把边拼成基本图像，把基本图像拼成数字</li>  <li>这个机制的基本处理方式是：通过上一层的单元激活下一层的单元</li></ul><p><strong>示例：如何使第二层的单个神经元识别出图像中的某块区域是否存在一条边</strong></p><ul>  <li>    <p>根据激活的含义，当激活值接近 1 时，表示该区域存在一条边，反之不存在</p>  </li>  <li>    <p><strong>怎样的数学公式能够表达出这个含义？</strong></p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702112135.png" alt="">      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702113031.png" alt="">      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702113631.png" alt=""></p>    <ul>      <li>        <p>考虑对所有输入单元加权求和</p>      </li>      <li>        <p>图中每条连线关联一个权值：绿色表示正值，红色表示负值，颜色越暗表示越接近 0</p>      </li>      <li>        <p>此时，只需将需要关注的像素区域对应的权值设为正，其余为 0</p>      </li>      <li>        <p>这样对所有像素的加权求和就只会累计我们关注区域的像素值</p>      </li>      <li>        <p>为了使隐藏单元真正被“激活”，加权和还需要经过某个<strong>非线性函数</strong>，也就是“激活函数”</p>      </li>      <li>        <p>早期最常用的激活函数是 <code>sigmoid</code> 函数（又称 logistic/逻辑斯蒂曲线）</p>        <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702114132.png" alt=""></p>        <blockquote>          <p>从 <code>sigmoid</code> 的角度看，它实际上在对加权和到底有多“正”进行打分</p>        </blockquote>      </li>      <li>        <p>但有时，可能加权和大于 10 时激活才有意义；</p>      </li>      <li>        <p>此时，需要加上“偏置”，保证不能随便激发，比如 -10。然后再传入激活函数</p>      </li>    </ul>  </li></ul><h3 id="1-2-1-权重和偏置">1.2.1. 权重和偏置</h3><ul>  <li>    <p>每个隐藏单元都会和<strong>上一层的所有单元</strong>相连，每条连线上都关联着一个<strong>权重</strong>；</p>  </li>  <li>    <p>每个隐藏单元又会各自带有一个<strong>偏置</strong></p>    <blockquote>      <p>偏置和权重统称为网络参数</p>    </blockquote>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702150309.png" alt="">      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702151423.png" alt=""></p>    <blockquote>      <p>每一层都带有自己的权重与偏置，这样一个小小的网络，就有 13002 个参数</p>    </blockquote>  </li></ul><p><strong>权重与偏置的实际意义</strong></p><ul>  <li>    <p>宏观来看，<strong>权重</strong>在告诉你当前神经元应该更关注来自上一层的哪些单元；或者说<strong>权重指示了连接的强弱</strong></p>  </li>  <li>    <p><strong>偏置</strong>则告诉你加权和应该多大才能使神经元的激发变得有意义；或者说<strong>当前神经元是否更容易被激活</strong></p>  </li></ul><p><strong>矢量化编程</strong></p><ul>  <li>    <p>把一层中所有的激活值作为一列<strong>向量</strong> <code>a</code></p>  </li>  <li>    <p>层与层之间的权重放在一个<strong>矩阵</strong> <code>W</code> 中：第 n 行就是上层所有神经元与下层第 n 个神经元的权重</p>  </li>  <li>    <p>类似的，所有偏置也作为一列<strong>向量</strong> <code>b</code></p>  </li>  <li>    <p>最后，将 <code>Wa + b</code> 一起传入激活函数</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702155142.png" alt=""></p>    <blockquote>      <p><code>sigmoid</code>会对结果向量中的每个值都取一次<code>sigmoid</code></p>    </blockquote>  </li>  <li>    <p>所谓“矢量化编程”，实际上就是将向量作为基本处理单元，避免使用 for 循环处理标量</p>  </li>  <li>    <p>通过定制处理单元（GPU运算），可以大幅加快计算速度</p>  </li></ul><p><strong>机器“学习”的实质</strong></p><p>当我们在讨论机器如何“学习”时，实际上指的是机器如何正确设置这些参数</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702152216.png" alt=""></p><h2 id="1-3-非线性激活函数">1.3. 非线性激活函数</h2><p><strong>神经网络本质上是一个函数</strong></p><ul>  <li>    <p>每个神经元可以看作是一个函数，其输入是上一层所有单元的输出，然后输出一个激活值</p>  </li>  <li>    <p>宏观来看，神经网络也是一个函数</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702151423.png" alt=""></p>    <blockquote>      <p>一个输入 784 个值，输出 10 个值的函数；其中有 13000 个参数</p>    </blockquote>  </li>  <li>    <p>早期最常用的激活函数是 <code>sigmoid</code> 函数，它是一个<strong>非线性函数</strong></p>  </li>  <li>    <p>暂不考虑它其他优秀的性质（使其长期作为激活函数的首选）以及缺点（使其逐渐被弃用）；</p>    <p>而只考虑其<strong>非线性</strong></p>  </li></ul><p><strong>为什么要使用非线性激活函数？——神经网络的万能近似定理</strong></p><blockquote>  <p>视频中没有提到为什么使用非线性激活函数，但这确实是神经网络能够具有如此强大<strong>表示能力</strong>的关键</p></blockquote><ul>  <li>使用<strong>非线性激活函数</strong>的目的是为了向网络中加入<strong>非线性因素</strong>，从而加强网络的表示能力</li></ul><p><strong>为什么加入非线性因素能够加强网络的表示能力？</strong></p><ul>  <li>    <p>首先要有这样一个认识，非线性函数具有比线性函数更强的表示能力。</p>  </li>  <li>    <p>如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合；</p>    <p>容易验证，此时无论有多少层，神经网络都只是一个线性函数。</p>  </li></ul><p><strong>万能近似定理</strong></p><ul>  <li>神经网络如果具有至少一个非线性输出层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。</li>  <li>这极大的扩展了神经网络的表示空间</li></ul><p><strong>新时代的激活函数——线性整流单元 ReLU</strong></p><p>这里简单说下 sigmoid 的问题：</p><ul>  <li>    <p><code>sigmoid</code> 函数在输入取绝对值非常大的正值或负值时会出现<strong>饱和现象</strong>，此时函数会对输入的微小改变会变得不敏感</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702114132.png" alt=""></p>    <blockquote>      <p>饱和现象：在图像上表现为函数值随自变量的变化区域平缓（斜率接近 0）</p>    </blockquote>  </li>  <li>    <p>饱和现象会导致<strong>基于梯度的学习</strong>变得困难，并在传播过程中丢失信息（<strong>梯度消失</strong>）</p>  </li></ul><p><strong>线性整流单元 ReLU</strong></p><p><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180702171411.png" alt=""></p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702171146.png" alt=""></p><ul>  <li>    <p><code>ReLU</code> 取代 <code>sigmoid</code> 的主要原因就是：使神经网络更容易训练（<strong>减缓梯度消失</strong>）</p>  </li>  <li>    <p>此外，一种玄学的说法是，早期引入 <code>sigmoid</code> 的原因之一就是为了模仿生物学上神经元的激发</p>    <p>而 <code>ReLU</code> 比 <code>sigmoid</code> 更接近这一过程。</p>  </li></ul><h1>2. 梯度下降法</h1><p>内容：</p><ul>  <li>梯度下降的思想</li>  <li>网络的能力分析</li>  <li>隐层神经元的真实目的</li></ul><p>网络示例依然是那个手写识别的例子：</p><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180701210407.png" alt=""></p><p><strong>神经网络是怎样学习的？</strong></p><ul>  <li>    <p>我们需要一种算法：通过喂给这个网络大量的<strong>训练数据</strong>——不同的手写数字图像以及对应的数字标签</p>    <p>算法会调整所有网络参数（权重和偏置）来提高网络对训练数据的表现</p>    <p>此外，我们还希望这种分层结构能够举一反三，识别训练数据之外的图像——<strong>泛化能力</strong></p>  </li>  <li>    <p>虽然使用了“学习”的说法，但实际上训练的过程更像在解一道<strong>微积分问题</strong></p>    <p>训练的过程实际上在寻找某个函数的（局部）最小值</p>  </li>  <li>    <p>在训练开始前，这些参数是随机初始化的</p>    <blockquote>      <p>确实存在一些随机初始化的策略，但目前来看，都只是“锦上添花”</p>    </blockquote>  </li></ul><h2 id="2-1-损失函数-loss-function">2.1. 损失函数（Loss Function）</h2><ul>  <li>    <p>显然，随机初始化不会有多好的表现</p>  </li>  <li>    <p>此时需要定义一个“<strong>损失函数</strong>”来告诉计算机：正确的输出应该只有标签对应的那个神经元是被激活的</p>  </li>  <li>    <p>比如这样定义单个样本的损失：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702194825.png" alt="">      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702195301.png" alt=""></p>    <ul>      <li>当网络分类正确时，这个值就越小</li>      <li>这里使用的损失函数为“均方误差”（mean-square error, MSE）</li>    </ul>  </li>  <li>    <p>现在，我们就可以用<strong>所有训练样本</strong>的平均损失来评价整个网络在这个任务上的“<strong>糟糕程度</strong>”</p>    <blockquote>      <p>在实践中，并不会每次都使用所有训练样本的平均损失来调整梯度，这样计算量太大了</p>      <p>随机梯度下降</p>    </blockquote>  </li></ul><p>实际上，<strong>神经网络学习的过程，就是最小化损失函数的过程</strong></p><p><strong>神经网络与损失函数的关系</strong></p><ul>  <li>    <p>神经网络本身相当于一个函数</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702195902.png" alt=""></p>    <blockquote>      <p>输入是一个向量，输出是一个向量，参数是所有权重和偏置</p>    </blockquote>  </li>  <li>    <p>损失函数在神经网络的基础上，还要再抽象一层：</p>    <p>所有权重和偏置作为它的输入，输出是单个数值，表示当前网络的性能；参数是所有训练样例（？）</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702201513.png" alt=""></p>  </li>  <li>    <p>从这个角度看，损失函数并不是神经网络的一部分，而是训练神经网络时需要用到的工具</p>  </li></ul><h2 id="2-2-梯度下降法-gradient-descent">2.2. 梯度下降法（Gradient Descent）</h2><p><strong>如何优化这些网络参数？</strong></p><ul>  <li>能够判断网络的“糟糕程度”并不重要，关键是如何利用它来<strong>优化</strong>网络参数</li></ul><p><strong>示例 1：考虑只有一个参数的情况</strong></p><ul>  <li>    <p>如果函数只有一个极值点，那么直接利用微积分即可</p>    <p>如果函数很复杂的话，问题就不那么直接了，更遑论上万个参数的情况</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702202810.png" alt="">      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702203041.png" alt=""></p>  </li>  <li>    <p><strong>一个启发式的思路是</strong>：先随机选择一个值，然后考虑向左还是向右，函数值会减小；</p>    <p>准确的说，如果你找到了函数在该点的斜率，<strong>斜率为正就向左移动一小步，反之向右</strong>；</p>    <p>然后每新到一个点就重复上述过程——计算新的斜率，根据斜率更新位置；</p>    <p>最后，就可能逼近函数的某个<strong>局部极小值</strong>点</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702203401.png" alt=""></p>  </li>  <li>    <p>这个想法最明显的问题是：由于无法预知最开始的值在什么位置，导致最终会到达不同的局部极小值；</p>    <p>关键是无法保证落入的局部极小值就是损失函数可能达到的全局最小值；</p>    <p>这也是神经网络最大的问题：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702204047.png" alt=""></p>  </li></ul><p><strong>示例 2：考虑两个参数的情况</strong></p><ul>  <li>    <p>输入空间是一个 XY 平面，代价函数是平面上方的曲面</p>    <p>此时的问题是：在输入空间沿哪个方向移动，能使输出结果下降得最快</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702210202.png" alt=""></p>  </li>  <li>    <p>如果你熟悉<strong>多元微积分</strong>，那么应该已经有了答案：</p>    <p>函数的<strong>梯度</strong>指出了函数的“<strong>最陡</strong>”增长方向，即沿着梯度的方向走，函数增长最快；</p>    <p>换言之，<strong>沿梯度的负方向走，函数值也就下降得最快</strong>；</p>    <p>此外，梯度向量的长度还代表了斜坡的“陡”的程度。</p>  </li>  <li>    <p>处理更多的参数也是同样的办法，</p>    <p>这种<strong>按照负梯度的倍数</strong>，不断调整函数输入值的过程，就叫作梯度下降法</p>  </li>  <li>    <p>直观来看，梯度下降法能够让函数值收敛到损失函数图像中的某一个“坑”中</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703102940.png" alt=""></p>  </li></ul><h3 id="2-2-1-理解梯度下降法的另一种思路">2.2.1. 理解梯度下降法的另一种思路</h3><ul>  <li>    <p>梯度下降法的一般处理方式：</p>    <ul>      <li>        <p>将所有网络参数放在一个列向量中，那么损失函数的负梯度也是一个向量</p>        <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703103735.png" alt=""></p>      </li>    </ul>  </li>  <li>    <p>负梯度中的每一项实际传达了两个信息</p>    <ol>      <li>        <p>正负号在告诉输入向量应该调大还是调小——因为是<strong>负梯度</strong>，所以正好对应调大，负号调小</p>      </li>      <li>        <p>每一项的相对大小表明每个输入值对函数值的影响程度；换言之，也就是调整各权重对于网络的影响</p>        <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703103757.png" alt="">          <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703104845.png" alt=""></p>      </li>    </ol>  </li>  <li>    <p>宏观来看，可以把梯度向量中的每个值理解为各参数（权重和偏置）的相对重要度，</p>    <p>同时指出了改变其中哪些参数的<strong>性价比</strong>最高</p>  </li>  <li>    <p>这个理解思路同样可以反馈到图像中</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703105249.png" alt=""></p>    <ul>      <li>        <p>一种思路是在点 <code>(1,1)</code> 处沿着 <code>[3,1]</code> 方向移动，函数值增长最快</p>      </li>      <li>        <p>另一种解读就是变量 <code>x</code> 的重要性是 <code>y</code> 的三倍；</p>        <p>也就是说，至少在这个点附近，改变 <code>x</code> 会造成更大的影响</p>      </li>    </ul>  </li></ul><p><strong>梯度下降法</strong>描述：</p><ol>  <li>计算损失函数对所有参数的（负）梯度</li>  <li>按梯度的负方向下降一定步长（直接加上负梯度）</li>  <li>重复以上步骤，直到满足精度要求</li></ol><ul>  <li>其中计算梯度的算法是整个神经网络的核心</li></ul><p><strong>梯度下降法</strong>与<strong>反向传播算法</strong></p><ul>  <li>    <p>梯度下降法是寻找局部最小值的一种<strong>策略</strong></p>    <p>其中最核心的部分利用<strong>损失函数 <code>L(θ)</code> 的梯度</strong>来更新所有参数 <code>θ</code></p>  </li>  <li>    <p><strong>反向传播算法</strong>是求解函数梯度的一种方法；</p>    <p>其本质上是利用<strong>链式法则</strong>对每个参数求偏导</p>  </li></ul><p><strong>损失函数的平滑性</strong></p><ul>  <li>    <p>为了能达到函数的局部最小值，损失函数有必要是<strong>平滑</strong>的</p>    <p>只有如此，损失函数才能基于梯度下降的方法，找到一个局部最小值</p>  </li>  <li>    <p>这也解释了为什么要求神经元的激活值是连续的</p>    <blockquote>      <p>生物学中的神经元是二元式的</p>    </blockquote>  </li></ul><h3 id="2-2-2-随机梯度下降-stochastic-gradient-descent">2.2.2. 随机梯度下降（Stochastic Gradient Descent）</h3><ul>  <li>    <p>基本的梯度下降法要求每次使用<strong>所有训练样本</strong>的平均损失来更新参数，也称为“<strong>批量梯度下降</strong>”</p>    <p>原则上是这样，但是为了<strong>计算效率</strong>，实践中并不会这么做</p>  </li>  <li>    <p>一种常用的方法是每次只随机选取<strong>单个样本</strong>的损失来计算梯度，该方法称为“<strong>随机梯度下降</strong>”（Stochastic Gradient Descent, SGD），它比批量梯度下降法要快得多</p>  </li>  <li>    <p>但更常用的方法是<strong>小批量梯度下降</strong>，它每次随机选取<strong>一批样本</strong>，然后基于它们的平均损失来更新参数</p>  </li>  <li>    <p>SGD 与 小批量梯度下降的优势在于：它们的计算复杂度与训练样本的数量无关</p>    <blockquote>      <p>很多情况下，并不区分 SGD 和小批量梯度下降；有时默认 SGD 就是小批量梯度下降，比如本视频</p>    </blockquote>  </li></ul><p><strong>批大小的影响</strong>：</p><ul>  <li>    <p><strong>较大的批能得到更精确的梯度估计</strong>，但回报是小于线性的。</p>  </li>  <li>    <p><strong>较小的批能带来更好的泛化误差</strong>，泛化误差通常在批大小为 1 时最好。但是，因为梯度估计的高方差，小批量训练需要<strong>较小的学习率</strong>以保持稳定性，这意味着<strong>更长的训练时间</strong>。</p>    <blockquote>      <p>原因可能是由于小批量在学习过程中加入了噪声，它们会有一些正则化效果 (Wilson and Martinez, 2003)</p>    </blockquote>  </li>  <li>    <p><strong>内存消耗和批的大小成正比</strong>，当批量处理中的所有样本可以并行处理时。</p>  </li>  <li>    <p>在某些硬件上使用特定大小可以减少运行时间。尤其是在使用 GPU 时，通常使用 <strong>2 的幂数</strong>作为批量大小可以获得更少的运行时间。一般，2 的幂数的<strong>取值范围是 32 到 256</strong>，16 有时在尝试大模型时使用。</p>  </li></ul><p><strong>神经网络的优化难题</strong></p><ul>  <li>    <p>有一个策略可以保证最终解<strong>至少</strong>能到达一个局部极小值点：使每次<strong>移动的步幅和斜率成正比</strong>；</p>    <p>因为在最小值附近的斜率会趋于平缓，这将导致每次移动步幅越来越小，防止跳出极值点</p>    <p><strong>但是</strong>，这对于现代各种巨大的神经网络而言，是一个<strong>负优化策略</strong>——它反而会<strong>限制网络的学习</strong>，导致其陷入某个局部极小值点</p>  </li>  <li>    <p>当参数数量非常庞大时，可能存在<strong>无数个极值点</strong>，而其中某些极值点的结果可能非常差。</p>    <blockquote>      <p>优化问题是深度学习最核心的两个问题之一，另一个是正则化</p>    </blockquote>  </li>  <li>    <p>也不要太过担心因为局部最小值点太多而无法优化；</p>    <p>事实上，只要你使用的数据不是完全随机，或者说有一定结构，那么最终网络倾向收敛到的各个局部最小值点，实际上都差不多</p>    <p>你可以认为如果数据集已经<strong>结构化</strong>了，那么你可以更轻松的找到局部最小值点</p>    <blockquote>      <p>[1412.0233] The Loss Surfaces of Multilayer Networks <a href="https://arxiv.org/abs/1412.0233" target="_blank" rel="noopener">https://arxiv.org/abs/1412.0233</a></p>    </blockquote>  </li></ul><h2 id="2-3-再谈神经网络的运作机制">2.3. 再谈神经网络的运作机制</h2><ul>  <li>    <p>我们对神经网络的<strong>期望</strong>是：</p>    <p>第一个隐藏层能够识别短边，第二个隐藏层能够将短边拼成圆圈等基本笔画，最后将这些部件拼成数字</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180702094455.png" alt=""></p>  </li>  <li>    <p>利用<strong>权重与所有输入像素对应相乘</strong>的方法，可以还原每个神经元对应的图案</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703115843.png" alt="">      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703115823.png" alt=""></p>    <blockquote>      <p>期望（左）与现实（右）</p>    </blockquote>  </li>  <li>    <p>事实上，与其说它们能识别各种散落的短边，它们不过都是一些松散的图案</p>    <p>就像“在如深海般的 13000 维参数空间中，找到了一个不错的局部最小值位置住了下来”</p>    <p>尽管它们能成功识别大部分手写数字，但它不像我们期望的能识别各种图案的基本部件</p>  </li>  <li>    <p>一个明显的例子：传入一张随机的图案</p>    <p>如果神经网络真的很“智能”，它应该对这个输入感到困惑——10个输出单元都没有明确的激活值</p>    <p>但实际上，它总会会给出一个确切的答案</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703121303.png" alt=""></p>    <blockquote>      <p>它能将一张真正的 “5” 识别成 5，也能把一张随机的噪声识别成 5</p>    </blockquote>  </li>  <li>    <p>或者说，这个神经网络<strong>知道怎么识别数字，但它不知道怎么写数字</strong></p>    <p>原因可能是，很大程度上它的训练被限制在了一个很窄的框架内；</p>    <p>“从神经网络的视角来看，整个世界都是由网络内清晰定义的静止数字组成的，它的损失函数只会促使它对最后的判断有绝对的自信。”</p>  </li>  <li>    <p>有一些观点认为，神经网络/深度学习并没有那么智能，它只是<strong>记住了所有正确分类的数据</strong>，然后<strong>尽量</strong>把那些跟训练数据类似的数据分类正确</p>  </li>  <li>    <p>但是，有些观点认为神经网络确实学到了某些更智能的东西：</p>    <p>如果训练时使用的是随机的数据，那么损失函数的下降会很慢，而且是接近线性的过程；也就是说网络很难找到可能存在的局部最小值点；</p>    <p>但如果你使用了<strong>结构化</strong>的数据，虽然一开始的损失会上下浮动，但接下来会快速下降；也就是很容易就找到了某个局部最小值——这表明神经网络能更快学习<strong>结构化的数据</strong>。</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703143800.png" alt=""></p>  </li></ul><h2 id="2-4-推荐阅读">2.4. 推荐阅读</h2><ul>  <li>Neural Networks and Deep Learning: <a href="http://neuralnetworksanddeeplearning.com/" target="_blank" rel="noopener">http://neuralnetworksanddeeplearning.com/</a></li>  <li>Chris Olah’s blog: <a href="http://colah.github.io/" target="_blank" rel="noopener">http://colah.github.io/</a>    <ul>      <li><a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" target="_blank" rel="noopener">Neural Networks, Manifolds, and Topology</a></li>      <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li>    </ul>  </li>  <li>Distill — Latest articles about machine learning <a href="https://distill.pub/" target="_blank" rel="noopener">https://distill.pub/</a></li>  <li>Videos on machine learning:    <ul>      <li>Learning To See [Part 1: Introduction] - YouTube <a href="https://www.youtube.com/watch?v=i8D90DkCLhI" target="_blank" rel="noopener">https://www.youtube.com/watch?v=i8D90DkCLhI</a></li>      <li>Neural Networks Demystified [Part 1: Data and Architecture] - YouTube <a href="https://www.youtube.com/watch?v=bxe2T-V8XRs" target="_blank" rel="noopener">https://www.youtube.com/watch?v=bxe2T-V8XRs</a></li>    </ul>  </li></ul><h1>3. 反向传播算法（Backpropagation Algorithm, BP）</h1><h2 id="3-1-反向传播的直观理解">3.1. 反向传播的直观理解</h2><ul>  <li>    <p>梯度下降法中需要用到损失函数的梯度来决定下降的方向，</p>    <p>其中 BP 算法正是用于求解这个复杂梯度的一种方法</p>  </li>  <li>    <p>在阐述 BP 算法之前，记住——</p>    <p><strong>梯度向量中的每一项不光告诉我们每个参数应该增大还是减小，还指示了调整每个参数的“性价比”</strong></p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703103757.png" alt=""></p>  </li></ul><p><strong>示例：单个训练样本对参数调整的影响</strong></p><ul>  <li>    <p>假设现在的网络还没有训练好，那么输出层的激活值看起来会很随机</p>    <p>而我们希望的输出应该是正确类别对应的输出值最大，其他尽可能接近 0</p>    <p>因此，我们希望正确类别对应的激活值应该增大，而其他都减小</p>    <blockquote>      <p>需要注意的是，激活值是由输入值和权重及偏置决定的；网络不会调整激活值本身，只能改变权重和偏置</p>    </blockquote>  </li>  <li>    <p>一个简单的<strong>直觉</strong>是：激活值<strong>变动的幅度</strong>应该与<strong>当前值和目标值之间的误差</strong>成正比</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703210401.png" alt="">      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703210941.png" alt=""></p>    <blockquote>      <p>我们希望第三个输出值增大，其他减小</p>      <p>显然，这里增加 “2” 的激活值比减少 “8” 的激活值更重要；因为后者已经接近它的目标了</p>    </blockquote>  </li>  <li>    <p>激活值的计算公式指明了调整的方向</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704095607.png" alt=""></p>  </li>  <li>    <p>以 “2” 对应的神经元为例，如果需要<strong>增大当前激活值</strong>，可以：</p>    <ol>      <li>增大偏置</li>      <li>增大权重</li>      <li>调整上一层的激活值</li>    </ol>    <blockquote>      <p>如果使用的是 <code>sigmoid</code> 或 <code>ReLU</code> 作为激活函数，那么激活值都 <code>≥ 0</code>；但无论激活值正负，都是类似的调整方法</p>    </blockquote>  </li></ul><p><strong>如何调整权重和偏置？</strong></p><ul>  <li>    <p>偏置只关注当前神经元，因此，可以正比于当前值和目标值之间的差来调整偏置</p>  </li>  <li>    <p>权重指示了连接的强弱；换言之，与之相连的上层激活值越大，那么该权重对当前神经元的影响也越大。显然，着重调整这个参数的性价比更高</p>    <p>因此，可以正比于与之关联的上层激活值来调整权重</p>  </li>  <li>    <p>这种学习的<strong>偏好</strong>将导致这样一个结果——<strong>一同激活的神经元被关联在一起</strong>；生物学中，称之为“赫布理论”（Hebbian theory）</p>    <p>这在深度学习中，并不是一个广泛的结论，只是一个粗略的对照；事实上，早期的神经网络确实在模仿生物学上大脑的工作；但深度学习兴起之后，其指导思想已经发生了重要转变——<strong>组合表示</strong></p>  </li></ul><p><strong>如何改变上层激活值？</strong></p><ul>  <li>    <p>因为权重带有正负，所以如果希望增大当前激活值，应该使所有通过<strong>正权</strong>连接的上层激活值增大，所有通过<strong>负权</strong>连接的上层激活值减小</p>  </li>  <li>    <p>类似的，与修改权重时类似，如果想造成更大的影响，应该对应权重的大小来对激活值做出<strong>成比例</strong>的改变；</p>    <p>但需要注意的是，上层激活值的大小也是由上层的权重和偏置决定的，</p>  </li>  <li>    <p>所以更准确的说法是，每层的权重和偏置会根据下一层的权重和偏置来做出成比例的改变，而最后一层的权重个偏置会根据<strong>当前值和目标值之间的误差</strong>来成比例调整</p>  </li></ul><p><strong>反向传播</strong></p><ul>  <li>    <p>除了需要增大正确的激活值，同时还要减小错误的。而这些神经元对于如何改变上一层的激活值都有各自的想法；</p>    <p>因此，需要将这些神经元的期待全部相加，作为改变上层神经元的指示；</p>    <p>这些期待变化，不仅对应权重的倍数，也是每个神经元激活值改变量的倍数。</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704111141.png" alt=""></p>  </li>  <li>    <p>这其实就是在实现“反向传播”的理念了——</p>    <p>将所有期待的改变相加，就得到了希望对上层改动的变化量；然后就可以重复这个过程，直到第一层</p>  </li>  <li>    <p>上面只是讨论了单个样本对所有参数的影响，实践时，需要同时考虑每个样本对权重与偏置的修改，然后将它们期望的平均作为每个参数的变化量；</p>    <p>不严格的来说，这就是梯度下降中的需要的“<strong>负梯度</strong>”</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704111911.png" alt="">      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704112743.png" alt=""></p>    <blockquote>      <p><code>η</code> 表示倍数</p>    </blockquote>  </li></ul><h3 id="3-1-1-bp-算法小结">3.1.1. BP 算法小结</h3><ul>  <li>    <p>反向传播算法计算的是<strong>单个训练样本</strong>对所有权重和偏置的调整——包括每个参数的正负变化以及<strong>变化的比例</strong>——使其能最快的降低损失。</p>  </li>  <li>    <p>真正的梯度下降需要对训练集中的每个样本都做一次反向传播，然后计算平均变化值，继而更新参数。</p>    <p>但这么操作会导致算法的复杂度与训练样本的数量相关。</p>  </li>  <li>    <p>实践时，会采用“<strong>随机梯度下降</strong>”的策略：</p>    <ol>      <li>首先打乱所有样本；</li>      <li>然后将所有样本分发到各个 mini-batch 中；</li>      <li>计算每个 mini-batch 的梯度，调整参数</li>      <li>循环至 Loss 值基本不再变化</li>    </ol>    <p>最终神经网络将会收敛到某个局部最小值上</p>  </li></ul><h3 id="3-1-2-相关代码">3.1.2. 相关代码</h3><ul>  <li>mnielsen/neural-networks-and-deep-learning/<a href="https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py" target="_blank" rel="noopener">network.py</a></li></ul><h2 id="3-2-反向传播的微积分原理">3.2. 反向传播的微积分原理</h2><p>从数学的角度看，反向传播本质上就是<strong>利用链式法则求导</strong>的过程；</p><p>本节的目标是展示机器学习领域是如何理解链式法则的。</p><h3 id="3-2-1-示例：每层只有一个神经元的网络">3.2.1. 示例：每层只有一个神经元的网络</h3><p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704161115.png" alt=""></p><ul>  <li>    <p>从最后两个神经元开始：</p>    <p>记最后一个神经元的激活值为 <code>a^(L)</code> 表示在第 L 层，上层的激活值为 <code>a^(L-1)</code>；</p>    <p>给定一个训练样本，其目标值记为 <code>y</code>；</p>    <p>则该网络对于单个训练样本的损失，可以表示为：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704193558.png" alt=""></p>    <!-- [![](http://www.meng.uno/images/assets/公式_20180704162743.png)](http://www.codecogs.com/eqnedit.php?latex=C_0(\theta)=(a^{(L)}-y)^2)>`θ` 为代价函数的参数，表示网络中所有权值和偏置 -->    <p>其中</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704193742.png" alt=""></p>    <!-- [![](http://www.meng.uno/images/assets/公式_20180704163428.png)](http://www.codecogs.com/eqnedit.php?latex=a^{(L)}=\sigma(w^{(L)}a^{(L-1)}&plus;b^{(L)})) -->    <p>为了方便，记加权和为 <code>z</code>，于是有</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704193955.png" alt=""></p>    <!-- [![](http://www.meng.uno/images/assets/公式_20180704163659.png)](http://www.codecogs.com/eqnedit.php?latex=z^{(L)}=w^{(L)}a^{(L-1)}&plus;b^{(L)}) -->  </li>  <li>    <p>整个流程可以概括为：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704164022.png" alt=""></p>    <ul>      <li>先使用前一个激活值和权重 <code>w</code> 以及偏置 <code>b</code> 计算出 <code>z</code></li>      <li>再将 <code>z</code> 传入激活函数计算出 <code>a</code></li>      <li>最后利用 <code>a</code> 和目标值 <code>y</code> 计算出损失</li>    </ul>  </li>  <li>    <p>我们的目标是理解<strong>代价函数对于参数的变化有多敏感</strong>；</p>    <p>从微积分的角度来看，这实际上就是在<strong>求损失函数对参数的导数</strong>。</p>  </li>  <li>    <p>以 <code>w^(L)</code> 为例，求 <code>C</code> 对 <code>w^(L)</code>的（偏）导数，记作：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704194111.png" alt=""></p>    <blockquote>      <p>把 <code>∂w^(L)</code> 看作对 <code>w</code> 的微小扰动，比如 0.001；把 <code>∂C</code> 看作“<strong>改变 <code>w</code> 对 <code>C</code> 造成的变化</strong>”</p>    </blockquote>    <!-- [![](http://www.meng.uno/images/assets/公式_20180704170039.png)](http://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;C_0}{\partial&space;w^{(L)}}) -->    <p>这实际上就是在计算 <code>C</code> 对 <code>w^(L)</code> 的微小变化有多敏感；</p>  </li>  <li>    <p>根据链式法则，有</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704194548.png" alt=""></p>    <!-- [![](http://www.meng.uno/images/assets/公式_20180704170404.png)](http://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;C_0}{\partial&space;w^{(L)}}=\frac{\partial&space;C_0}{\partial&space;a^{(L)}}\frac{\partial&space;a^{(L)}}{\partial&space;z^{(L)}}\frac{\partial&space;z^{(L)}}{\partial&space;w^{(L)}}) -->  </li>  <li>    <p>进一步分解到每个比值，有</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704194635.png" alt=""></p>    <!-- [![](http://www.meng.uno/images/assets/公式_20180704171334.png)](http://www.codecogs.com/eqnedit.php?latex=\begin&space;{aligned}&space;\frac{\partial&space;C_0}{\partial&space;w^{(L)}}&space;&=\frac{\partial&space;C_0}{\partial&space;a^{(L)}}\frac{\partial&space;a^{(L)}}{\partial&space;z^{(L)}}\frac{\partial&space;z^{(L)}}{\partial&space;w^{(L)}}\\&space;&=2(a^{(L)}-y)\cdot&space;\sigma%27(z^{(L)})\cdot&space;a^{(L-1)}&space;\end{aligned}) -->  </li>  <li>    <p>类似的，对偏置的偏导：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704194726.png" alt=""></p>    <p>对上层激活值的偏导：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704200743.png" alt=""></p>    <!-- [![](http://www.meng.uno/images/assets/公式_20180704173348.png)](http://www.codecogs.com/eqnedit.php?latex=\begin&space;{aligned}&space;\frac{\partial&space;C_0}{\partial&space;b^{(L)}}&space;&=\frac{\partial&space;C_0}{\partial&space;a^{(L)}}\frac{\partial&space;a^{(L)}}{\partial&space;z^{(L)}}\frac{\partial&space;z^{(L)}}{\partial&space;b^{(L)}}\\&space;&=2(a^{(L)}-y)\cdot&space;\sigma%27(z^{(L)})\cdot&space;1&space;\end{aligned}) -->    <!-- [![](http://www.meng.uno/images/assets/公式_20180704173511.png)](http://www.codecogs.com/eqnedit.php?latex=\begin&space;{aligned}&space;\frac{\partial&space;C_0}{\partial&space;a^{(L-1)}}&space;&=\frac{\partial&space;C_0}{\partial&space;a^{(L)}}\frac{\partial&space;a^{(L)}}{\partial&space;z^{(L)}}\frac{\partial&space;z^{(L)}}{\partial&space;a^{(L-1)}}\\&space;&=2(a^{(L)}-y)\cdot&space;\sigma%27(z^{(L)})\cdot&space;w^{(L)}&space;\end{aligned}) -->  </li>  <li>    <p>更上层的权重与偏置也是类似的方法，只是链的长度会更长而已</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704174134.png" alt=""></p>  </li>  <li>    <p>一个直观的理解，考虑将它们分别对应到一个数轴上；</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704165624.png" alt=""></p>    <ul>      <li><code>w^(L)</code> 的微小变化会导致 <code>z^(L)</code> 的微小变化</li>      <li><code>z^(L)</code> 的微小变化会导致 <code>a^(L)</code> 的微小变化</li>      <li><code>a^(L)</code> 的微小变化最终会影响到损失值 <code>C</code></li>      <li>反向传播的过程就是将 <code>C</code> 的微小变化传回去</li>    </ul>  </li>  <li>    <p>以上只是单个训练的损失对 <code>w^(L)</code> 的导数，实践中需要求出一个 mini-batch 中所有样本的平均损失</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704194944.png" alt=""></p>    <!-- [![](http://www.meng.uno/images/assets/公式_20180704171742.png)](http://www.codecogs.com/eqnedit.php?latex=\begin&space;{aligned}&space;\frac{\partial&space;C}{\partial&space;w^{(L)}}=\frac{1}{n}\sum^{n-1}_{k=0}\frac{\partial&space;C_k}{\partial&space;w^{(L)}}&space;\end{aligned}) -->  </li>  <li>    <p>进一步的，<code>∂C/∂w^(L)</code> 只是梯度向量 <code>▽C</code> 的一个分量；</p>    <p>而梯度向量本身有损失函数对每个参数的偏导构成：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704172524.png" alt=""></p>  </li></ul><blockquote>  <p>本系列视频称损失函数为“代价函数”，所以使用 <code>C</code> 表示代价（Cost）；更多会用 <code>L</code> 表示损失（Loss）；二者没有区别，但这里已经使用 L 表示网络的层数，所以使用 <code>C</code> 表示损失函数</p></blockquote><h3 id="3-2-2-更复杂的示例">3.2.2. 更复杂的示例</h3><ul>  <li>    <p>更复杂的神经网络在公式上并没有复杂很多，只有少量符号的变化</p>  </li>  <li>    <p>首先，利用<strong>下标</strong>来区分同一层不同的神经元和权重</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704195120.png" alt=""></p>    <p>以下的推导过程几乎只是符号的改变：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704195859.png" alt=""></p>    <p>然后求偏导：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704200316.png" alt=""></p>    <p>唯一改变的是，对 <code>(L-1)</code> 层的偏导：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704200453.png" alt=""></p>    <blockquote>      <p>此时激活值可以通过不同的途径影响损失函数</p>    </blockquote>    <p>只要计算出倒数第二层损失函数对激活值的敏感度，就可以重复以上过程，计算喂给倒数第二层的权重和偏置。</p>  </li>  <li>    <p>事实上，如果都使用矢量表示，那么整个推导公式跟单神经元的网络几乎是完全一样的</p>  </li>  <li>    <p>链式法则给出了决定梯度每个分量的偏导，使我们能不断下探，最小化神经网络的损失。</p>  </li></ul><h3 id="3-2-3-反向传播的-4-个基本公式">3.2.3. 反向传播的 4 个基本公式</h3><ul>  <li>    <p><strong>问题描述</strong>：</p>    <p>反向传播算法的目标是求出损失函数<strong>对所有参数的梯度</strong>，具体可分解为<strong>对每个权重和偏置的偏导</strong></p>    <p>可以用 4 个公式总结反向传播的过程</p>  </li>  <li>    <p><strong>标量形式</strong>：</p>    <p><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705190236.png" alt=""></p>    <p><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705134851.png" alt="">      <img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705154543.png" alt="">      <img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705154650.png" alt=""></p>    <blockquote>      <p>其中，上标 <code>(l)</code> 表示网络的层次，<code>(L)</code> 表示输出层（最后一层）；下标 <code>j</code> 和 <code>k</code> 指示神经元的位置；</p>      <p><code>w_jk</code> 表示 <code>l</code> 层的第 <code>j</code> 个神经元与<code>(l-1)</code>层第 <code>k</code> 个神经元连线上的权重</p>    </blockquote>    <p>以 <strong>均方误差（MSE）</strong> 损失函数为例，有</p>    <p><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705190536.png" alt=""><br> 根据以上公式，就可以反向求出所有损失函数对所有参数的偏导了    </p>  </li>  <li>    <p><strong>矢量形式</strong>：</p>    <p>在标量形式的基础上，修改下标即可</p>    <p><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705190657.png" alt=""></p>    <p><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705162428.png" alt=""></p>    <p><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705162521.png" alt=""></p>    <p><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705162633.png" alt=""></p>    <p><strong>注意</strong>：其中向量相乘都是以<strong>按元素相乘</strong>的形式，一般用 <code>⊙</code> 符号表示。</p>  </li>  <li>    <p>以上是纯微积分形式的表达，一些机器学习相关书籍上总结了更简洁的形式，比如：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180705162841.png" alt=""></p>    <blockquote>      <p><a href="http://neuralnetworksanddeeplearning.com/chap2.html#the_four_fundamental_equations_behind_backpropagation" target="_blank" rel="noopener">The four fundamental equations behind backpropagation</a></p>    </blockquote>    <p><strong>注意</strong>：前两个公式为<strong>矢量形式</strong>，后两个具体到单个参数的是<strong>标量形式</strong>。</p>    <p>其中，符号 <code>⊙</code> 按元素相乘：</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180705164018.png" alt=""></p>  </li></ul><p><br><br>本文链接： <a href="http://www.meng.uno/articles/8193f764/">http://www.meng.uno/articles/8193f764/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      1. 神经网络的结构
内容：

 * 神经网络是什么？
 * 神经网络的结构
 * 神经网络的工作机制
 * 深度学习中的“学习”指的是什么？
 * 神经网络的不足

示例：一个用于数字手写识别的神经网络



这个示例相当于深度学习领域中的 “Hello World”.

1.1. 神经元（隐藏单元）与隐藏层
神经元（隐藏单元）

 *  简单来说，神经元可以理解为一个用来装数字的容器，而这个数称为激活值
   
   需要强调的是，激活值的值域取决于使用的激活函数，大多数激活函数的值域都是正值
   
   
   
   

如果使用 sigmoid 激活函数，那么这个数字就在 0 到 
    
    </summary>
    
      <category term="DeepLearning" scheme="http://www.meng.uno/categories/DeepLearning/"/>
    
    
      <category term="深度学习" scheme="http://www.meng.uno/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://www.meng.uno/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>几个常见的社区推荐算法</title>
    <link href="http://www.meng.uno/articles/82a6b55c/"/>
    <id>http://www.meng.uno/articles/82a6b55c/</id>
    <published>2018-03-18T11:03:48.000Z</published>
    <updated>2018-03-18T13:39:36.000Z</updated>
    
    <content type="html"><![CDATA[<h1>PageRank算法</h1><p>PageRank算法预先给每个网页一个PR值（PR值指代PageRank值），PR值在物理意义上为一个网页被访问的概率，所以一般是1/N，其中N为网页总数。</p><p>另外，所有网页的PR值的和一般为1。（如果实在不为1也不是不行，最后算出来的不同网页之间PR值的大小关系仍然是正确的，只是这个数值不能直接地反映概率罢了。）</p><p>接着，运用下面的算法不断迭代计算，直至达到平稳分布为止。</p><h2 id="普通情况">普通情况</h2><p><img src="http://www.meng.uno/images/recommend/13.jpg" alt=""></p><p>互联网中的众多网页可以看成一个有向图，箭头的指向即为链接的链入，根据上图，我们得到A的PR值为：PR(A)=PR(B)/2+PR©/1。</p><h2 id="没有出链">没有出链</h2><p><img src="http://www.meng.uno/images/recommend/14.jpg" alt=""></p><p>网络中不乏一些没有出链的网页，如上图，其中，网页C没有出链，也就是说网页C对其他网页没有PR值的贡献，我们不喜欢这种“自私”的网页（其实是为了满足 Markov 链的收敛性），于是设定其对所有网页（包括它自己）都有出链，则此图中A的PR值表示为：PR(A)=PR(B)/2+PR©/4。</p><h2 id="出链循环圈">出链循环圈</h2><p><img src="http://www.meng.uno/images/recommend/15.jpg" alt=""></p><p>网络中还存在这样的网页：只对自己有出链，或者几个网页的出链形成一个循环圈。那么在不断迭代的过程中，这一个或几个网页的PR值将只增不减，这显然是不合理的。</p><p>那么如何解决这个问题呢？我们假设某人正在浏览网页C，显然他不会一直停留在网页C，他可能会随机地输入一个网址从而去往另一个网页，并且其跳转到每个网页的概率是一样的。于是此图中A的PR值表示为：PR(A)=∂(PR(B)/2)+(1-∂)/4。</p><p>综上，一般情况下，一个网页的PR值计算公式如下：</p><p><img src="http://www.meng.uno/images/recommend/16.png" alt=""></p><p>其中，Mpi是所有对pi网页有出链的网页集合，L(pj)是网页pj的出链数目，N是网页总数，α一般取0.85。</p><p>根据上面的公式，我们就可以计算出每个网页的PR值，在不断迭代并趋于平稳的时候，即为最终结果。</p><h1>HITS算法</h1><p>算法简介：</p><p>首先把那些根据关键相关返回网页作为根集合S，再由S集合网页节点的链入和链出网页节点派生出结合C，结合C包括S，链入和链出节点集合。</p><p>C中的每个节点分配一对权重&lt;h(s),a(s)&gt;, 节点h(s)权重由节点链出的节点的a(s)决定，a(s)由节点的链入节点的h(s)决定。</p><p>算法过程：</p><p><img src="http://www.meng.uno/images/recommend/7.jpg" alt=""></p><p>网页的a权重向量：</p><p><img src="http://www.meng.uno/images/recommend/8.jpg" alt=""></p><p>关于HITS算法收敛性，可以从如下变换形式来得出：</p><p><img src="http://www.meng.uno/images/recommend/9.jpg" alt=""></p><p>当算法收敛时候，a其实就是对应矩阵A那个最大特征值对应的特征向量的归一化形式，同样，h也是H矩阵那个最大特征值对应的特征向量的归一化形式。</p><p>算法实现：</p><figure class="highlight python">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre>      </td>      <td class="code">        <pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HITSIterator</span>:</span></span><br><span class="line">    __doc__ = <span class="string">'''计算一张图中的hub,authority值'''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dg)</span>:</span></span><br><span class="line">        self.max_iterations = <span class="number">100</span>  <span class="comment"># 最大迭代次数</span></span><br><span class="line">        self.min_delta = <span class="number">0.0001</span>  <span class="comment"># 确定迭代是否结束的参数</span></span><br><span class="line">        self.graph = dg</span><br><span class="line"></span><br><span class="line">        self.hub = &#123;&#125;</span><br><span class="line">        self.authority = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> self.graph.nodes():</span><br><span class="line">            self.hub[node] = <span class="number">1</span></span><br><span class="line">            self.authority[node] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hits</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        计算每个页面的hub,authority值</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.graph:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        flag = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.max_iterations):</span><br><span class="line">            change = <span class="number">0.0</span>  <span class="comment"># 记录每轮的变化值</span></span><br><span class="line">            norm = <span class="number">0</span>  <span class="comment"># 标准化系数</span></span><br><span class="line">            tmp = &#123;&#125;</span><br><span class="line">            <span class="comment"># 计算每个页面的authority值</span></span><br><span class="line">            tmp = self.authority.copy()</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> self.graph.nodes():</span><br><span class="line">                self.authority[node] = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> incident_page <span class="keyword">in</span> self.graph.incidents(node):  <span class="comment"># 遍历所有“入射”的页面</span></span><br><span class="line">                    self.authority[node] += self.hub[incident_page]</span><br><span class="line">                norm += pow(self.authority[node], <span class="number">2</span>)</span><br><span class="line">            <span class="comment"># 标准化</span></span><br><span class="line">            norm = sqrt(norm)</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> self.graph.nodes():</span><br><span class="line">                self.authority[node] /= norm</span><br><span class="line">                change += abs(tmp[node] - self.authority[node])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算每个页面的hub值</span></span><br><span class="line">            norm = <span class="number">0</span></span><br><span class="line">            tmp = self.hub.copy()</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> self.graph.nodes():</span><br><span class="line">                self.hub[node] = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> neighbor_page <span class="keyword">in</span> self.graph.neighbors(node):  <span class="comment"># 遍历所有“出射”的页面</span></span><br><span class="line">                    self.hub[node] += self.authority[neighbor_page]</span><br><span class="line">                norm += pow(self.hub[node], <span class="number">2</span>)</span><br><span class="line">            <span class="comment"># 标准化</span></span><br><span class="line">            norm = sqrt(norm)</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> self.graph.nodes():</span><br><span class="line">                self.hub[node] /= norm</span><br><span class="line">                change += abs(tmp[node] - self.hub[node])</span><br><span class="line"></span><br><span class="line">            print(<span class="string">"This is NO.%s iteration"</span> % (i + <span class="number">1</span>))</span><br><span class="line">            print(<span class="string">"authority"</span>, self.authority)</span><br><span class="line">            print(<span class="string">"hub"</span>, self.hub)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> change &lt; self.min_delta:</span><br><span class="line">                flag = <span class="keyword">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> flag:</span><br><span class="line">            print(<span class="string">"finished in %s iterations!"</span> % (i + <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"finished out of 100 iterations!"</span>)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"The best authority page: "</span>, max(self.authority.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>]))</span><br><span class="line">        print(<span class="string">"The best hub page: "</span>, max(self.hub.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    dg = digraph()</span><br><span class="line"></span><br><span class="line">    dg.add_nodes([<span class="string">"A"</span>, <span class="string">"B"</span>, <span class="string">"C"</span>, <span class="string">"D"</span>, <span class="string">"E"</span>])</span><br><span class="line"></span><br><span class="line">    dg.add_edge((<span class="string">"A"</span>, <span class="string">"C"</span>))</span><br><span class="line">    dg.add_edge((<span class="string">"A"</span>, <span class="string">"D"</span>))</span><br><span class="line">    dg.add_edge((<span class="string">"B"</span>, <span class="string">"D"</span>))</span><br><span class="line">    dg.add_edge((<span class="string">"C"</span>, <span class="string">"E"</span>))</span><br><span class="line">    dg.add_edge((<span class="string">"D"</span>, <span class="string">"E"</span>))</span><br><span class="line">    dg.add_edge((<span class="string">"B"</span>, <span class="string">"E"</span>))</span><br><span class="line">    dg.add_edge((<span class="string">"E"</span>, <span class="string">"A"</span>))</span><br><span class="line"></span><br><span class="line">    hits = HITSIterator(dg)</span><br><span class="line">    hits.hits()</span><br></pre>      </td>    </tr>  </table></figure><h1>SALSA算法</h1><p>SALSA算法和HITS算法初始部分一样，构建相同的集合集C和彼此的链接关系。</p><p>SALSA一种随机游走过程，但是不同经典的随机游走。它涉及到把一个网页节点看成2种不同类型节点：hub和authority，随机游走对应着这样两种不用类型的Markov链：hub链和authority链，状态转移为网页前向和后向。</p><p><img src="http://www.meng.uno/images/recommend/10.jpg" alt=""></p><p>首先是把构建一个无向图，原图节点分为2类，然后构建边。</p><p>这样从某个节点出发，进行两个方向的随机游走。h和a方向的状态转移矩阵：</p><p><img src="http://www.meng.uno/images/recommend/11.jpg" alt=""></p><p>对于以上的形式可以通过如下的矩阵相乘的方式展现：</p><p><img src="http://www.meng.uno/images/recommend/12.jpg" alt=""></p><p>有了H和A矩阵，就可以知道节点集合最终的h和a向量：和HITS一样，h和a对应H和A的最大特征值对应的归一化特征向量。其实，计算h和a可以参照HITS，进行迭代求解。</p><p><br><br>本文链接： <a href="http://www.meng.uno/articles/82a6b55c/">http://www.meng.uno/articles/82a6b55c/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      PageRank算法
PageRank算法预先给每个网页一个PR值（PR值指代PageRank值），PR值在物理意义上为一个网页被访问的概率，所以一般是1/N，其中N为网页总数。

另外，所有网页的PR值的和一般为1。（如果实在不为1也不是不行，最后算出来的不同网页之间PR值的大小关系仍然是正确的，只是这个数值不能直接地反映概率罢了。）

接着，运用下面的算法不断迭代计算，直至达到平稳分布为止。

普通情况


互联网中的众多网页可以看成一个有向图，箭头的指向即为链接的链入，根据上图，我们得到A的PR值为：PR(A)=PR(B)/2+PR©/1。

没有出链


网络中不乏一些没有出链的网页，
    
    </summary>
    
      <category term="机器学习" scheme="http://www.meng.uno/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="推荐" scheme="http://www.meng.uno/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="社交网络" scheme="http://www.meng.uno/tags/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>商品推荐：协同过滤</title>
    <link href="http://www.meng.uno/articles/6f93935a/"/>
    <id>http://www.meng.uno/articles/6f93935a/</id>
    <published>2018-03-17T06:05:52.000Z</published>
    <updated>2018-03-18T07:18:29.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>  <p>过去很长时间以及现今，电商都在蓬勃发展，支持电商越做越大的一个很重要的因素就是“商品推荐”。当我们打开天猫，我们发现不同的用户，一般而言，首页是不一样的，原因就是，它为不同的用户推荐了不同的商品。我们为什么要用某一个购物网站或者APP，我觉得很大程度上取决于其推荐的准确与否。本篇博客我将向大家介绍协同过滤在商品推荐上的应用。</p></blockquote><h1>关于推荐系统</h1><p>根据推荐引擎的数据源不同，一般而言，推荐系统可以分为如下三类：</p><ul>  <li>基于人口统计学的推荐机制（Demographic-based Recommendation）：根据系统用户的基本信息发现用户的相关程度。</li></ul><p><img src="http://www.meng.uno/images/recommend/6.jpg" alt=""></p><ul>  <li>基于内容的推荐机制（Content-based Recommendation）：根据推荐物品或内容的元数据，发现物品或者内容的相关性。</li></ul><p><img src="http://www.meng.uno/images/recommend/3.jpg" alt=""></p><ul>  <li>协同过滤的推荐机制（Collaborative Filtering-based Recommendation）：根据用户对物品或者信息的偏好，发现物品或者内容本身的相关性，或者是发现用户的相关性。</li></ul><p>根据推荐模型的建立方式不同，推荐系统可以分为这样三类：</p><ul>  <li>基于物品和用户本身。这种推荐引擎将每个用户和每个物品都当作独立的实体，预测每个用户对于每个物品的喜好程度，这些信息往往是用一个二维矩阵描述的。由于用户感兴趣的物品远远小于总物品的数目，这样的模型导致大量的数据空置，即我们得到的二维矩阵往往是一个很大的稀疏矩阵。同时为了减小计算量，我们可以对物品和用户进行聚类， 然后记录和计算一类用户对一类物品的喜好程度，但这样的模型又会在推荐的准确性上有损失。</li>  <li>基于关联规则的推荐（Rule-based Recommendation）。关联规则的挖掘已经是数据挖掘中的一个经典的问题，主要是挖掘一些数据的依赖关系，典型的场景就是“购物篮问题”，通过关联规则的挖掘，我们可以找到哪些物品经常被同时购买，或者用户购买了一些物品后通常会购买哪些其他的物品，当我们挖掘出这些关联规则之后，我们可以基于这些规则给用户进行推荐。</li>  <li>基于模型的推荐（Model-based Recommendation）。这是一个典型的机器学习的问题，可以将已有的用户喜好信息作为训练样本，训练出一个预测用户喜好的模型，这样以后用户在进入系统，可以基于此模型计算推荐。这种方法的问题在于如何将用户实时或者近期的喜好信息反馈给训练好的模型，从而提高推荐的准确度。</li></ul><h1>协同过滤</h1><p>关于协同过滤的一个最经典的例子就是看电影，有时候不知道哪一部电影是我们喜欢的或者评分比较高的，那么通常的做法就是问问周围的朋友，看看最近有什么好的电影推荐。在问的时候，都习惯于问跟自己口味差不多的朋友，这就是协同过滤的核心思想。</p><h2 id="步骤">步骤</h2><p>要实现协同过滤，一般需要这样三步：</p><ul>  <li>收集用户偏好</li>  <li>找到相似物或人</li>  <li>计算并推荐</li></ul><h3 id="收集用户偏好">收集用户偏好</h3><p>从用户的行为和偏好中发现规律，并基于此进行推荐，所以如何收集用户的偏好信息成为系统推荐效果最基础的决定因素。用户有很多种方式向系统提供自己的偏好信息，比如：评分，投票，转发，保存书签，购买，点击流，页面停留时间等等。</p><p>当然，得到原始数据之后，我们总是需要进行降噪、归一化等，在此不再赘述。</p><h3 id="找到相似物或人">找到相似物或人</h3><p>既然是找相似，我们就需要设定一个计算相似度的指标，一般而言，余弦相似度与皮尔逊相关系数是很好的选择。</p><h4 id="余弦相似度">余弦相似度</h4><p><img src="http://www.meng.uno/images/recommend/4.gif" alt=""></p><h4 id="皮尔逊相关系数">皮尔逊相关系数</h4><p>皮尔逊相关也称为积差相关（或积矩相关）是英国统计学家皮尔逊于20世纪提出的一种计算直线相关的方法。</p><p>假设有两个变量X、Y，那么两变量间的皮尔逊相关系数可通过以下公式计算：</p><p><img src="http://www.meng.uno/images/recommend/5.gif" alt=""></p><h3 id="计算并推荐">计算并推荐</h3><ul>  <li>基于用户的协同过滤推荐</li></ul><p>基于用户的协同过滤推荐的基本原理是，根据所有用户对物品或者信息的偏好，发现与当前用户口味和偏好相似的“邻居”用户群，在一般的应用中是采用计算“K- 邻居”的算法；然后，基于这 K 个邻居的历史偏好信息，为当前用户进行推荐。</p><p><img src="http://www.meng.uno/images/recommend/1.jpg" alt=""></p><p>上图示意出基于用户的协同过滤推荐机制的基本原理，假设用户 A 喜欢物品 A，物品 C，用户 B 喜欢物品 B，用户 C 喜欢物品 A ，物品 C 和物品 D；从这些用户的历史喜好信息中，我们可以发现用户 A 和用户 C 的口味和偏好是比较类似的，同时用户 C 还喜欢物品 D，那么我们可以推断用户 A 可能也喜欢物品 D，因此可以将物品 D 推荐给用户 A。</p><p>基于用户的协同过滤推荐机制和基于人口统计学的推荐机制都是计算用户的相似度，并基于“邻居”用户群计算推荐，但它们所不同的是如何计算用户的相似度，基于人口统计学的机制只考虑用户本身的特征，而基于用户的协同过滤机制可是在用户的历史偏好的数据上计算用户的相似度，它的基本假设是，喜欢类似物品的用户可能有相同或者相似的口味和偏好。</p><ul>  <li>基于项目的协同过滤推荐</li></ul><p>基于项目的协同过滤推荐的基本原理也是类似的，只是说它使用所有用户对物品或者信息的偏好，发现物品和物品之间的相似度，然后根据用户的历史偏好信息，将类似的物品推荐给用户。</p><p><img src="http://www.meng.uno/images/recommend/2.jpg" alt=""></p><p>假设用户 A 喜欢物品 A 和物品 C，用户 B 喜欢物品 A，物品 B 和物品 C，用户 C 喜欢物品 A，从这些用户的历史喜好可以分析出物品 A 和物品 C 时比较类似的，喜欢物品 A 的人都喜欢物品 C，基于这个数据可以推断用户 C 很有可能也喜欢物品 C，所以系统会将物品 C 推荐给用户 C。</p><p>与上面讲的类似，基于项目的协同过滤推荐和基于内容的推荐其实都是基于物品相似度预测推荐，只是相似度计算的方法不一样，前者是从用户历史的偏好推断，而后者是基于物品本身的属性特征信息。</p><ul>  <li>基于模型的协同过滤推荐</li></ul><p>基于模型的协同过滤推荐就是基于样本的用户喜好信息，训练一个推荐模型，然后根据实时的用户喜好的信息进行预测，计算推荐。</p><p>基于协同过滤的推荐机制是现今应用最为广泛的推荐机制，它有以下几个显著的优点：</p><p>它不需要对物品或者用户进行严格的建模，而且不要求物品的描述是机器可理解的，所以这种方法也是领域无关的。 这种方法计算出来的推荐是开放的，可以共用他人的经验，很好的支持用户发现潜在的兴趣偏好 而它也存在以下几个问题：</p><p>方法的核心是基于历史数据，所以对新物品和新用户都有“冷启动”的问题。 推荐的效果依赖于用户历史偏好数据的多少和准确性。 在大部分的实现中，用户历史偏好是用稀疏矩阵进行存储的，而稀疏矩阵上的计算有些明显的问题，包括可能少部分人的错误偏好会对推荐的准确度有很大的影响等等。 对于一些特殊品味的用户不能给予很好的推荐。 由于以历史数据为基础，抓取和建模用户的偏好后，很难修改或者根据用户的使用演变，从而导致这个方法不够灵活。</p><h2 id="代码实现">代码实现</h2><p>基于用户的CF：</p><figure class="highlight java">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre>      </td>      <td class="code">        <pre><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Map.Entry;</span><br><span class="line"><span class="keyword">import</span> java.util.Scanner;</span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment"> * 基于用户的协同过滤推荐算法实现 </span></span><br><span class="line"><span class="comment">A a b d </span></span><br><span class="line"><span class="comment">B a c </span></span><br><span class="line"><span class="comment">C b e </span></span><br><span class="line"><span class="comment">D c d e </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Administrator </span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserCF</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment">   * 输入用户--&gt;物品条目 一个用户对应多个物品 </span></span><br><span class="line"><span class="comment">   * 用户ID 物品ID集合 </span></span><br><span class="line"><span class="comment">   * A  a b d </span></span><br><span class="line"><span class="comment">   * B  a c </span></span><br><span class="line"><span class="comment">   * C  b e </span></span><br><span class="line"><span class="comment">   * D  c d e </span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">System.out.println(<span class="string">"Input the total users number:"</span>);</span><br><span class="line"><span class="comment">//输入用户总量 </span></span><br><span class="line"><span class="keyword">int</span> N = scanner.nextint();</span><br><span class="line"><span class="keyword">int</span>[][] sparseMatrix = <span class="keyword">new</span> <span class="keyword">int</span>[N][N];</span><br><span class="line"><span class="comment">//建立用户稀疏矩阵，用于用户相似度计算【相似度矩阵】 </span></span><br><span class="line">Map&lt;String, Integer&gt; userItemLength = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="comment">//存储每一个用户对应的不同物品总数 eg: A 3 </span></span><br><span class="line">Map&lt;String, Set&lt;String&gt;&gt; itemUserCollection = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="comment">//建立物品到用户的倒排表 eg: a A B </span></span><br><span class="line">Set&lt;String&gt; items = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"><span class="comment">//辅助存储物品集合 </span></span><br><span class="line">Map&lt;String, Integer&gt; userID = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="comment">//辅助存储每一个用户的用户ID映射 </span></span><br><span class="line">Map&lt;Integer, String&gt; idUser = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="comment">//辅助存储每一个ID对应的用户映射 </span></span><br><span class="line">System.out.println(<span class="string">"Input user--items maping infermation:&lt;eg:A a b d&gt;"</span>);</span><br><span class="line">scanner.nextLine();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N ; i++)&#123;</span><br><span class="line"><span class="comment">//依次处理N个用户 输入数据 以空格间隔 </span></span><br><span class="line">String[] user_item = scanner.nextLine().split(<span class="string">" "</span>);</span><br><span class="line"><span class="keyword">int</span> length = user_item.length;</span><br><span class="line">userItemLength.put(user_item[<span class="number">0</span>], length-<span class="number">1</span>);</span><br><span class="line"><span class="comment">//eg: A 3 </span></span><br><span class="line">userID.put(user_item[<span class="number">0</span>], i);</span><br><span class="line"><span class="comment">//用户ID与稀疏矩阵建立对应关系 </span></span><br><span class="line">idUser.put(i, user_item[<span class="number">0</span>]);</span><br><span class="line"><span class="comment">//建立物品--用户倒排表 </span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt; length; j ++)&#123;</span><br><span class="line"><span class="keyword">if</span>(items.contains(user_item[j]))&#123;</span><br><span class="line"><span class="comment">//如果已经包含对应的物品--用户映射，直接添加对应的用户 </span></span><br><span class="line">itemUserCollection.get(user_item[j]).add(user_item[<span class="number">0</span>]);</span><br><span class="line">&#125; <span class="keyword">else</span>&#123;</span><br><span class="line"><span class="comment">//否则创建对应物品--用户集合映射 </span></span><br><span class="line">items.add(user_item[j]);</span><br><span class="line">itemUserCollection.put(user_item[j], <span class="keyword">new</span> HashSet&lt;String&gt;());</span><br><span class="line"><span class="comment">//创建物品--用户倒排关系 </span></span><br><span class="line">itemUserCollection.get(user_item[j]).add(user_item[<span class="number">0</span>]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(itemUserCollection.toString());</span><br><span class="line"><span class="comment">//计算相似度矩阵【稀疏】 </span></span><br><span class="line">Set&lt;Entry&lt;String, Set&lt;String&gt;&gt;&gt; entrySet = itemUserCollection.entrySet();</span><br><span class="line">Iterator&lt;Entry&lt;String, Set&lt;String&gt;&gt;&gt; iterator = entrySet.iterator();</span><br><span class="line"><span class="keyword">while</span>(iterator.hasNext())&#123;</span><br><span class="line">Set&lt;String&gt; commonUsers = iterator.next().getValue();</span><br><span class="line"><span class="keyword">for</span> (String user_u : commonUsers) &#123;</span><br><span class="line"><span class="keyword">for</span> (String user_v : commonUsers) &#123;</span><br><span class="line"><span class="keyword">if</span>(user_u.equals(user_v))&#123;</span><br><span class="line"><span class="keyword">continue</span>;</span><br><span class="line">&#125;</span><br><span class="line">sparseMatrix[userID.get(user_u)][userID.get(user_v)] += <span class="number">1</span>;</span><br><span class="line"><span class="comment">//计算用户u与用户v都有正反馈的物品总数</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(userItemLength.toString());</span><br><span class="line">System.out.println(<span class="string">"Input the user for recommendation:&lt;eg:A&gt;"</span>);</span><br><span class="line">String recommendUser = scanner.nextLine();</span><br><span class="line">System.out.println(userID.get(recommendUser));</span><br><span class="line"><span class="comment">//计算用户之间的相似度【余弦相似性】 </span></span><br><span class="line"><span class="keyword">int</span> recommendUserId = userID.get(recommendUser);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>;j &lt; sparseMatrix.length; j++) &#123;</span><br><span class="line"><span class="keyword">if</span>(j != recommendUserId)&#123;</span><br><span class="line">System.out.println(idUser.get(recommendUserId)+<span class="string">"--"</span>+idUser.get(j)+<span class="string">"相似度:"</span>+sparseMatrix[recommendUserId][j]/Math.sqrt(userItemLength.get(idUser.get(recommendUserId))*userItemLength.get(idUser.get(j))));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//计算指定用户recommendUser的物品推荐度 </span></span><br><span class="line"><span class="keyword">for</span> (String item: items)&#123;</span><br><span class="line"><span class="comment">//遍历每一件物品 </span></span><br><span class="line">Set&lt;String&gt; users = itemUserCollection.get(item);</span><br><span class="line"><span class="comment">//得到购买当前物品的所有用户集合 </span></span><br><span class="line"><span class="keyword">if</span>(!users.contains(recommendUser))&#123;</span><br><span class="line"><span class="comment">//如果被推荐用户没有购买当前物品，则进行推荐度计算 </span></span><br><span class="line"><span class="keyword">double</span> itemRecommendDegree = <span class="number">0.0</span>;</span><br><span class="line"><span class="keyword">for</span> (String user: users)&#123;</span><br><span class="line">itemRecommendDegree += sparseMatrix[userID.get(recommendUser)][userID.get(user)]/Math.sqrt(userItemLength.get(recommendUser)*userItemLength.get(user));</span><br><span class="line"><span class="comment">//推荐度计算</span></span><br><span class="line">&#125;</span><br><span class="line">System.out.println(<span class="string">"The item "</span>+item+<span class="string">" for "</span>+recommendUser +<span class="string">"'s recommended degree:"</span>+itemRecommendDegree);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">scanner.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre>      </td>    </tr>  </table></figure><p>基于项目的CF：</p><figure class="highlight python">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre>      </td>      <td class="code">        <pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span>  itemgetter</span><br><span class="line">random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ItemBasedCF</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.trainset = &#123;&#125;</span><br><span class="line">        self.testset = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        self.n_sim_movie = <span class="number">20</span></span><br><span class="line">        self.n_rec_movie = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">        self.movie_sim_mat = &#123;&#125;</span><br><span class="line">        self.movie_popular = &#123;&#125;</span><br><span class="line">        self.movie_count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">'Similar movie number = %d'</span> % self.n_sim_movie, file = sys.stderr)</span><br><span class="line">        print(<span class="string">'Recommendend movie number = %d'</span> % self.n_rec_movie,file = sys.stderr)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loadfile</span><span class="params">(filename)</span>:</span></span><br><span class="line">        fp = open(filename, <span class="string">'r'</span>)</span><br><span class="line">        <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(fp):</span><br><span class="line">            <span class="keyword">yield</span> line.strip(<span class="string">'\r\n'</span>)</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">100000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">'load %s(%s)'</span> %(filename,i), file = sys.stderr)</span><br><span class="line">        fp.close()</span><br><span class="line">        print(<span class="string">'load %s succ'</span> %filename, file = sys.stderr)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_dataset</span><span class="params">(self, filename, pivot = <span class="number">0.7</span>)</span>:</span></span><br><span class="line">        trainset_len = <span class="number">0</span></span><br><span class="line">        testset_len = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> self.loadfile(filename):</span><br><span class="line">            user, movie, rating , _= line.split(<span class="string">'::'</span>)</span><br><span class="line">            <span class="keyword">if</span> random.random() &lt; pivot:</span><br><span class="line">                self.trainset.setdefault(user,&#123;&#125;)</span><br><span class="line">                self.trainset[user][movie] = int(rating)</span><br><span class="line">                trainset_len += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.testset.setdefault(user,&#123;&#125;)</span><br><span class="line">                self.testset[user][movie] = int(rating)</span><br><span class="line">                testset_len += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">'split succ , trainset is %d , testset is %d'</span>  %(trainset_len,testset_len) , file = sys.stderr)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_movie_sim</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> user, movies <span class="keyword">in</span> self.trainset.items():</span><br><span class="line">            <span class="keyword">for</span> movie <span class="keyword">in</span> movies:</span><br><span class="line">                <span class="keyword">if</span> movie <span class="keyword">not</span> <span class="keyword">in</span> self.movie_popular:</span><br><span class="line">                    self.movie_popular[movie] = <span class="number">0</span></span><br><span class="line">                self.movie_popular[movie] += <span class="number">1</span></span><br><span class="line">        print(<span class="string">'count movies number and pipularity succ'</span>,file = sys.stderr)</span><br><span class="line"></span><br><span class="line">        self.movie_count = len(self.movie_popular)</span><br><span class="line">        print(<span class="string">'total movie number = %d'</span> %self.movie_count, file = sys.stderr)</span><br><span class="line"></span><br><span class="line">        itemsim_mat = self.movie_sim_mat</span><br><span class="line">        print(<span class="string">'building co-rated users matrix'</span>, file = sys.stderr)</span><br><span class="line">        <span class="keyword">for</span> user, movies <span class="keyword">in</span> self.trainset.items():</span><br><span class="line">            <span class="keyword">for</span> m1 <span class="keyword">in</span> movies:</span><br><span class="line">                <span class="keyword">for</span> m2 <span class="keyword">in</span> movies:</span><br><span class="line">                    <span class="keyword">if</span> m1 == m2:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    itemsim_mat.setdefault(m1,&#123;&#125;)</span><br><span class="line">                    itemsim_mat[m1].setdefault(m2,<span class="number">0</span>)</span><br><span class="line">                    itemsim_mat[m1][m2] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">'build co-rated users matrix succ'</span>, file = sys.stderr)</span><br><span class="line">        print(<span class="string">'calculating movie similarity matrix'</span>, file = sys.stderr)</span><br><span class="line"></span><br><span class="line">        simfactor_count = <span class="number">0</span></span><br><span class="line">        PRINT_STEP = <span class="number">2000000</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m1, related_movies <span class="keyword">in</span> itemsim_mat.items():</span><br><span class="line">            <span class="keyword">for</span> m2, count <span class="keyword">in</span> related_movies.items():</span><br><span class="line">                itemsim_mat[m1][m2] = count / math.sqrt(self.movie_popular[m1] * self.movie_popular[m2])</span><br><span class="line">                simfactor_count += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> simfactor_count % PRINT_STEP == <span class="number">0</span>:</span><br><span class="line">                    print(<span class="string">'calcu movie similarity factor(%d)'</span> %simfactor_count, file = sys.stderr)</span><br><span class="line">        print(<span class="string">'calcu similiarity succ'</span>, file = sys.stderr)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recommend</span><span class="params">(self,user)</span>:</span></span><br><span class="line">        K = self.n_sim_movie</span><br><span class="line">        N = self.n_rec_movie</span><br><span class="line">        rank = &#123;&#125;</span><br><span class="line">        watched_movies = self.trainset[user]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> movie, rating <span class="keyword">in</span> watched_movies.items():</span><br><span class="line">            <span class="keyword">for</span> related_movie, similarity_factor <span class="keyword">in</span> sorted(self.movie_sim_mat[movie].items(), key=itemgetter(<span class="number">1</span>),</span><br><span class="line">                                                           reverse=<span class="keyword">True</span>)[<span class="number">0</span>:K]:</span><br><span class="line">                <span class="keyword">if</span> related_movie <span class="keyword">in</span> watched_movies:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                rank.setdefault(related_movie, <span class="number">0</span>)</span><br><span class="line">                rank[related_movie] += similarity_factor * rating</span><br><span class="line">        <span class="keyword">return</span> sorted(rank.items(), key=itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)[<span class="number">0</span>:N]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'evaluation start'</span>, file = sys.stderr)</span><br><span class="line"></span><br><span class="line">        N = self.n_rec_movie</span><br><span class="line"></span><br><span class="line">        hit = <span class="number">0</span></span><br><span class="line">        rec_count = <span class="number">0</span></span><br><span class="line">        test_count = <span class="number">0</span></span><br><span class="line">        all_rec_movies = set()</span><br><span class="line">        popular_sum = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, user <span class="keyword">in</span> enumerate(self.trainset):</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">'recommend for %d users '</span> %i , file = sys.stderr)</span><br><span class="line">            test_movies = self.testset.get(user,&#123;&#125;)</span><br><span class="line">            rec_movies = self.recommend(user)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> movie, _ <span class="keyword">in</span> rec_movies:</span><br><span class="line">                <span class="keyword">if</span> movie <span class="keyword">in</span> test_movies:</span><br><span class="line">                    hit += <span class="number">1</span></span><br><span class="line">                all_rec_movies.add(movie)</span><br><span class="line">                popular_sum += math.log(<span class="number">1</span> + self.movie_popular[movie])</span><br><span class="line"></span><br><span class="line">            rec_count += N</span><br><span class="line">            test_count += len(test_movies)</span><br><span class="line"></span><br><span class="line">            precision = hit / (<span class="number">1.0</span> * rec_count)</span><br><span class="line">            recall = hit / (<span class="number">1.0</span> * test_count)</span><br><span class="line">            coverage = len(all_rec_movies) / (<span class="number">1.0</span> * self.movie_count)</span><br><span class="line">            popularity = popular_sum / (<span class="number">1.0</span> * rec_count)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'precision is %.4f\t recall is %.4f \t coverage is %.4f \t popularity is %.4f'</span></span><br><span class="line">                  %(precision,recall,coverage,popularity), file = sys.stderr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    ratingfile = os.path.join(<span class="string">'ml-1m'</span>, <span class="string">'ratings.dat'</span>)</span><br><span class="line">    itemcf = ItemBasedCF()</span><br><span class="line">    itemcf.generate_dataset(ratingfile)</span><br><span class="line">    itemcf.calc_movie_sim()</span><br><span class="line">    itemcf.evaluate()</span><br></pre>      </td>    </tr>  </table></figure><p><br><br>本文链接： <a href="http://www.meng.uno/articles/6f93935a/">http://www.meng.uno/articles/6f93935a/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      过去很长时间以及现今，电商都在蓬勃发展，支持电商越做越大的一个很重要的因素就是“商品推荐”。当我们打开天猫，我们发现不同的用户，一般而言，首页是不一样的，原因就是，它为不同的用户推荐了不同的商品。我们为什么要用某一个购物网站或者APP，我觉得很大程度上取决于其推荐的准确与否。本篇博客我将向大家介绍协同过滤在商品推荐上的应用。

关于推荐系统
根据推荐引擎的数据源不同，一般而言，推荐系统可以分为如下三类：

 * 基于人口统计学的推荐机制（Demographic-based Recommendation）：根据系统用户的基本信息发现用户的相关程度。



 * 基于内容的推荐机制（Content
    
    </summary>
    
      <category term="机器学习" scheme="http://www.meng.uno/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="社会网络" scheme="http://www.meng.uno/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%A4%BE%E4%BC%9A%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="社会网络" scheme="http://www.meng.uno/tags/%E7%A4%BE%E4%BC%9A%E7%BD%91%E7%BB%9C/"/>
    
      <category term="商品推荐" scheme="http://www.meng.uno/tags/%E5%95%86%E5%93%81%E6%8E%A8%E8%8D%90/"/>
    
      <category term="协同过滤" scheme="http://www.meng.uno/tags/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"/>
    
  </entry>
  
  <entry>
    <title>JVM的垃圾回收机制</title>
    <link href="http://www.meng.uno/articles/dde60b3a/"/>
    <id>http://www.meng.uno/articles/dde60b3a/</id>
    <published>2018-03-09T07:14:32.000Z</published>
    <updated>2018-03-09T07:58:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1>简介</h1><p>Java GC（Garbage Collection，垃圾收集，垃圾回收）机制，是Java与C++/C的主要区别之一，作为Java开发者，一般不需要专门编写内存回收和垃圾清理代码，对内存泄露和溢出的问题，也不需要像C程序员那样战战兢兢。这是因为在Java虚拟机中，存在自动内存管理和垃圾清扫机制。概括地说，该机制对JVM（Java Virtual Machine）中的内存进行标记，并确定哪些内存需要回收，根据一定的回收策略，自动的回收内存，永不停息（Nerver Stop）的保证JVM中的内存空间，防止出现内存泄露和溢出问题。</p><p>关于JVM，需要说明一下的是，目前使用最多的Sun公司的JDK中，自从1999年的JDK1.2开始直至现在仍在广泛使用的JDK6，其中默认的虚拟机都是HotSpot。2009年，Oracle收购Sun，加上之前收购的EBA公司，Oracle拥有3大虚拟机中的两个：JRockit和HotSpot，Oracle也表明了想要整合两大虚拟机的意图，但是目前在新发布的JDK7中，默认的虚拟机仍然是HotSpot，因此本文中默认介绍的虚拟机都是HotSpot，相关机制也主要是指HotSpot的GC机制。</p><p>学习Java GC机制，可以帮助我们在日常工作中排查各种内存溢出或泄露问题，解决性能瓶颈，达到更高的并发量，写出更高效的程序。</p><h1>解决哪些问题</h1><p>既然是要进行自动GC，那必然会有相应的策略，而这些策略解决了哪些问题呢，粗略的来说，主要有以下几点。</p><ul>  <li>哪些对象可以被回收。</li>  <li>何时回收这些对象。</li>  <li>采用什么样的方式回收。</li></ul><p>说到垃圾回收（Garbage Collection，GC），很多人就会自然而然地把它和Java联系起来。在Java中，程序员不需要去关心内存动态分配和垃圾回收的问题，这一切都交给了JVM来处理。</p><p>顾名思义，垃圾回收就是释放垃圾占用的空间，那么在Java中，什么样的对象会被认定为“垃圾”？那么当一些对象被确定为垃圾之后，采用什么样的策略来进行回收（释放空间）？在目前的商业虚拟机中，有哪些典型的垃圾收集器？</p><h2 id="如何确定某个对象是-垃圾-？">如何确定某个对象是“垃圾”？</h2><p>既然垃圾收集器的任务是回收垃圾对象所占的空间供新的对象使用，那么垃圾收集器如何确定某个对象是“垃圾”？即通过什么方法判断一个对象可以被回收了。</p><p>在java中是通过引用来和对象进行关联的，也就是说如果要操作对象，必须通过引用来进行。那么很显然一个简单的办法就是通过引用计数来判断一个对象是否可以被回收。不失一般性，如果一个对象没有任何引用与之关联，则说明该对象基本不太可能在其他地方被使用到，那么这个对象就成为可被回收的对象了。这种方式成为引用计数法。</p><p>这种方式的特点是实现简单，而且效率较高，但是它无法解决循环引用的问题，因此在Java中并没有采用这种方式（Python采用的是引用计数法）。看下面这段代码：</p><figure class="highlight java">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre>      </td>      <td class="code">        <pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">       MyObject object1 = <span class="keyword">new</span> MyObject();</span><br><span class="line">       MyObject object2 = <span class="keyword">new</span> MyObject();</span><br><span class="line">        </span><br><span class="line">       object1.object = object2;</span><br><span class="line">       object2.object = object1;</span><br><span class="line">        </span><br><span class="line">       object1 = <span class="keyword">null</span>;</span><br><span class="line">       object2 = <span class="keyword">null</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyObject</span></span>&#123;</span><br><span class="line">   <span class="keyword">public</span> Object object = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre>      </td>    </tr>  </table></figure><p>最后面两句将object1和object2赋值为null，也就是说object1和object2指向的对象已经不可能再被访问，但是由于它们互相引用对方，导致它们的引用计数都不为0，那么垃圾收集器就永远不会回收它们。</p><p>为了解决这个问题，在Java中采取了 可达性分析法。该方法的基本思想是通过一系列的“GC Roots”对象作为起点进行搜索，如果在“GC Roots”和一个对象之间没有可达路径，则称该对象是不可达的，不过要注意的是被判定为不可达的对象不一定就会成为可回收对象。被判定为不可达的对象要成为可回收对象必须至少经历两次标记过程，如果在这两次标记过程中仍然没有逃脱成为可回收对象的可能性，则基本上就真的成为可回收对象了。</p><p>至于可达性分析法具体是如何操作的我暂时也没有看得很明白，如果有哪位朋友比较清楚的话请不吝指教。</p><p>下面来看个例子：</p><figure class="highlight java">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>      </td>      <td class="code">        <pre><span class="line">Object aobj = <span class="keyword">new</span> Object ( ) ;</span><br><span class="line">Object bobj = <span class="keyword">new</span> Object ( ) ;</span><br><span class="line">Object cobj = <span class="keyword">new</span> Object ( ) ;</span><br><span class="line">aobj = bobj;</span><br><span class="line">aobj = cobj;</span><br><span class="line">cobj = <span class="keyword">null</span>;</span><br><span class="line">aobj = <span class="keyword">null</span>;</span><br></pre>      </td>    </tr>  </table></figure><p>第几行有可能会使得某个对象成为可回收对象？第7行的代码会导致有对象会成为可回收对象。至于为什么留给读者自己思考。</p><p>再看一个例子：</p><figure class="highlight java">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>      </td>      <td class="code">        <pre><span class="line">String str = <span class="keyword">new</span> String(<span class="string">"hello"</span>);</span><br><span class="line">SoftReference&lt;String&gt; sr = </span><br><span class="line"><span class="keyword">new</span> SoftReference&lt;String&gt;(<span class="keyword">new</span> String(<span class="string">"java"</span>));</span><br><span class="line">WeakReference&lt;String&gt; wr = </span><br><span class="line"><span class="keyword">new</span> WeakReference&lt;String&gt;(<span class="keyword">new</span> String(<span class="string">"world"</span>));</span><br></pre>      </td>    </tr>  </table></figure><p>这三句哪句会使得String对象成为可回收对象？第2句和第3句，第2句在内存不足的情况下会将String对象判定为可回收对象，第3句无论什么情况下String对象都会被判定为可回收对象。</p><p>最后总结一下平常遇到的比较常见的将对象判定为可回收对象的情况：</p><ul>  <li>显示地将某个引用赋值为null或者将已经指向某个对象的引用指向新的对象，比如下面的代码：</li></ul><figure class="highlight java">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>      </td>      <td class="code">        <pre><span class="line">Object obj = <span class="keyword">new</span> Object();</span><br><span class="line">obj = <span class="keyword">null</span>;</span><br><span class="line">Object obj1 = <span class="keyword">new</span> Object();</span><br><span class="line">Object obj2 = <span class="keyword">new</span> Object();</span><br><span class="line">obj1 = obj2;</span><br></pre>      </td>    </tr>  </table></figure><ul>  <li>局部引用所指向的对象，比如下面这段代码：</li></ul><figure class="highlight java">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>      </td>      <td class="code">        <pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fun</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">.....</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++) &#123;</span><br><span class="line">       Object obj = <span class="keyword">new</span> Object();</span><br><span class="line">       System.out.println(obj.getClass());</span><br><span class="line">   &#125;   </span><br><span class="line">&#125;</span><br></pre>      </td>    </tr>  </table></figure><p>循环每执行完一次，生成的Object对象都会成为可回收的对象。</p><ul>  <li>只有弱引用与其关联的对象，比如：</li></ul><figure class="highlight java">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br></pre>      </td>      <td class="code">        <pre><span class="line">WeakReference&lt;String&gt; wr = <span class="keyword">new</span> WeakReference&lt;String&gt;(<span class="keyword">new</span> String(<span class="string">"world"</span>));</span><br></pre>      </td>    </tr>  </table></figure><h2 id="典型的垃圾收集算法">典型的垃圾收集算法</h2><p>在确定了哪些垃圾可以被回收后，垃圾收集器要做的事情就是开始进行垃圾回收，但是这里面涉及到一个问题是：如何高效地进行垃圾回收。由于Java虚拟机规范并没有对如何实现垃圾收集器做出明确的规定，因此各个厂商的虚拟机可以采用不同的方式来实现垃圾收集器，所以在此只讨论几种常见的垃圾收集算法的核心思想。</p><p>需要明确的一点是，这里谈到的垃圾回收算法针对的是JVM的堆内存，栈基本上不存在垃圾回收方面的困扰。</p><h3 id="mark-sweep-标记-清除-算法">Mark-Sweep（标记-清除）算法</h3><p>这是最基础的垃圾回收算法，之所以说它是最基础的是因为它最容易实现，思想也是最简单的。标记-清除算法分为两个阶段：标记阶段和清除阶段。标记阶段的任务是标记出所有需要被回收的对象，清除阶段就是回收被标记的对象所占用的空间。</p><p>标记—清除算法是最基础的收集算法，它分为“标记”和“清除”两个阶段：首先标记出所需回收的对象，在标记完成后统一回收掉所有被标记的对象，它的标记过程其实就是前面的根搜索算法中判定垃圾对象的标记过程。</p><p>最基础的收集算法是“标记-清除”（Mark-Sweep）算法，如同它的名字一样，算法分为“标记”和“清除”两个阶段。</p><p><img src="http://www.meng.uno/images/gc/3.png" alt=""></p><p><img src="http://www.meng.uno/images/gc/4.png" alt=""></p><p>标记-清除算法实现起来比较容易，但是有一个比较严重的问题就是容易产生内存碎片，碎片太多可能会导致后续过程中需要为大对象分配空间时无法找到足够的空间而提前触发新的一次垃圾收集动作。</p><p>该算法有如下缺点：</p><ul>  <li>标记和清除过程的效率都不高。</li>  <li>标记清除后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不触发另一次垃圾收集动作。</li></ul><p>我们在程序（程序也就是指我们运行在JVM上的JAVA程序）运行期间如果想进行垃圾回收，就必须让GC线程与程序当中的线程互相配合，才能在不影响程序运行的前提下，顺利的将垃圾进行回收。</p><p>为了达到这个目的，标记/清除算法就应运而生了。它的做法是当堆中的有效内存空间（available memory）被耗尽的时候，就会停止整个程序（也被成为stop the world），然后进行两项工作，第一项则是标记，第二项则是清除。</p><ul>  <li>标记：标记的过程其实就是，遍历所有的GC Roots，然后将所有GC Roots可达的对象标记为存活的对象。</li>  <li>清除：清除的过程将遍历堆中所有的对象，将没有标记的对象全部清除掉。</li></ul><p>就是当程序运行期间，若可以使用的内存被耗尽的时候，GC线程就会被触发并将程序暂停，随后将依旧存活的对象标记一遍，最终再将堆中所有没被标记的对象全部清除掉，接下来便让程序恢复运行。</p><h3 id="copying-复制-算法">Copying（复制）算法</h3><p>为了解决效率问题，一种称为“复制”（Copying）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。 当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。 只是这种算法的代价是将内存缩小为了原来的一半，未免太高了一点。</p><p>现在的商业虚拟机都采用这种收集算法来回收新生代，IBM公司的专门研究表明，新生代中的对象98%是“朝生夕死”的，所以并不需要按照1:1的比例来划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor[1]。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。 HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，也就是每次新生代中可用内存空间为整个新生代容量的90%（80%+10%），只有10%的内存会被“浪费”。  当然，98%的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。</p><p>内存的分配担保就好比我们去银行借款，如果我们信誉很好，在98%的情况下都能按时偿还，于是银行可能会默认我们下一次也能按时按量地偿还贷款，只需要有一个担保人能保证如果我不能还款时，可以从他的账户扣钱，那银行就认为没有风险了。 内存的分配担保也一样，如果另外一块Survivor空间没有足够空间存放上一次新生代收集下来的存活对象时，这些对象将直接通过分配担保机制进入老年代。</p><p>复制算法是针对标记—清除算法的缺点，在其基础上进行改进而得到的，它讲课用内存按容量分为大小相等的两块，每次只使用其中的一块，当这一块的内存用完了，就将还存活着的对象复制到另外一块内存上面，然后再把已使用过的内存空间一次清理掉。复制算法有如下优点：</p><ul>  <li>每次只对一块内存进行回收，运行高效。</li>  <li>只需移动栈顶指针，按顺序分配内存即可，实现简单。</li>  <li>内存回收时不用考虑内存碎片的出现。</li></ul><p>它的缺点是：可一次性分配的最大内存缩小了一半。</p><p><img src="http://www.meng.uno/images/gc/1.png" alt=""></p><p>这种算法虽然实现简单，运行高效且不容易产生内存碎片，但是却对内存空间的使用做出了高昂的代价，因为能够使用的内存缩减到原来的一半。</p><p>很显然，Copying算法的效率跟存活对象的数目多少有很大的关系，如果存活对象很多，那么Copying算法的效率将会大大降低。</p><p>我们首先一起来看一下复制算法的做法，复制算法将内存划分为两个区间，在任意时间点，所有动态分配的对象都只能分配在其中一个区间（称为活动区间），而另外一个区间（称为空闲区间）则是空闲的。</p><p>当有效内存空间耗尽时，JVM将暂停程序运行，开启复制算法GC线程。接下来GC线程会将活动区间内的存活对象，全部复制到空闲区间，且严格按照内存地址依次排列，与此同时，GC线程将更新存活对象的内存引用地址指向新的内存地址。</p><p>此时，空闲区间已经与活动区间交换，而垃圾对象现在已经全部留在了原来的活动区间，也就是现在的空闲区间。事实上，在活动区间转换为空间区间的同时，垃圾对象已经被一次性全部回收。</p><p>很明显，复制算法弥补了标记/清除算法中，内存布局混乱的缺点。不过与此同时，它的缺点也是相当明显的。</p><ol>  <li>它浪费了一半的内存，这太要命了。</li>  <li>如果对象的存活率很高，我们可以极端一点，假设是100%存活，那么我们需要将所有对象都复制一遍，并将所有引用地址重置一遍。复制这一工作所花费的时间，在对象存活率达到一定程度时，将会变的不可忽视。</li></ol><p>所以从以上描述不难看出，复制算法要想使用，最起码对象的存活率要非常低才行，而且最重要的是，我们必须要克服50%内存的浪费。</p><h3 id="mark-compact-标记-整理-算法">Mark-Compact（标记-整理）算法</h3><p>为了解决Copying算法的缺陷，充分利用内存空间，提出了Mark-Compact算法。该算法标记阶段和Mark-Sweep一样，但是在完成标记之后，它不是直接清理可回收对象，而是将存活对象都向一端移动，然后清理掉端边界以外的内存。</p><p>复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。 更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。</p><p>根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。</p><p>标记/整理算法与标记/清除算法非常相似，它也是分为两个阶段：标记和整理。下面LZ给各位介绍一下这两个阶段都做了什么。</p><ul>  <li>标记：它的第一个阶段与标记/清除算法是一模一样的，均是遍历GC Roots，然后将存活的对象标记。</li>  <li>整理：移动所有存活的对象，且按照内存地址次序依次排列，然后将末端内存地址以后的内存全部回收。因此，第二阶段才称为整理阶段。</li></ul><p>复制算法比较适合于新生代，在老年代中，对象存活率比较高，如果执行较多的复制操作，效率将会变低，所以老年代一般会选用其他算法，如标记—整理算法。该算法标记的过程与标记—清除算法中的标记过程一样，但对标记后出的垃圾对象的处理情况有所不同，它不是直接对可回收对象进行清理，而是让所有的对象都向一端移动，然后直接清理掉端边界以外的内存。</p><p><img src="http://www.meng.uno/images/gc/2.png" alt=""></p><p>不难看出，标记/整理算法不仅可以弥补标记/清除算法当中，内存区域分散的缺点，也消除了复制算法当中，内存减半的高额代价，可谓是一举两得，一箭双雕，一石两鸟。</p><p>不过任何算法都会有其缺点，标记/整理算法唯一的缺点就是效率也不高，不仅要标记所有存活对象，还要整理所有存活对象的引用地址。从效率上来说，标记/整理算法要低于复制算法。</p><h3 id="generational-collection-分代收集-算法">Generational Collection（分代收集）算法</h3><p>分代收集算法是目前大部分JVM的垃圾收集器采用的算法。它的核心思想是根据对象存活的生命周期将内存划分为若干个不同的区域。一般情况下将堆区划分为老年代（Tenured Generation）和新生代（Young Generation），老年代的特点是每次垃圾收集时只有少量对象需要被回收，而新生代的特点是每次垃圾回收时都有大量的对象需要被回收，那么就可以根据不同代的特点采取最适合的收集算法。</p><p>目前大部分垃圾收集器对于新生代都采取Copying算法，因为新生代中每次垃圾回收都要回收大部分对象，也就是说需要复制的操作次数较少，但是实际中并不是按照1：1的比例来划分新生代的空间的。一般来说是将新生代划分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden空间和其中的一块Survivor空间，当进行回收时，将Eden和Survivor中还存活的对象复制到另一块Survivor空间中，然后清理掉Eden和刚才使用过的Survivor空间。</p><p>而由于老年代的特点是每次回收都只回收少量对象，一般使用的是Mark-Compact算法。</p><p>注意，在堆区之外还有一个代就是永久代（Permanet Generation），它用来存储class类、常量、方法描述等。对永久代的回收主要回收两部分内容：废弃常量和无用的类。</p><p>当前商业虚拟机的垃圾收集 都采用分代收集，它根据对象的存活周期的不同将内存划分为几块，一般是把Java堆分为新生代和老年代。在新生代中，每次垃圾收集时都会发现有大量对象死去，只有少量存活，因此可选用复制算法来完成收集，而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用标记—清除算法或标记—整理算法来进行回收。</p><h2 id="典型的垃圾收集器">典型的垃圾收集器</h2><p>垃圾收集算法是 内存回收的理论基础，而垃圾收集器就是内存回收的具体实现。下面介绍一下HotSpot（JDK 7)虚拟机提供的几种垃圾收集器，用户可以根据自己的需求组合出各个年代使用的收集器。</p><ul>  <li>Serial/Serial Old</li></ul><p>Serial/Serial Old收集器是最基本最古老的收集器，它是一个单线程收集器，并且在它进行垃圾收集时，必须暂停所有用户线程。Serial收集器是针对新生代的收集器，采用的是Copying算法，Serial Old收集器是针对老年代的收集器，采用的是Mark-Compact算法。它的优点是实现简单高效，但是缺点是会给用户带来停顿。</p><ul>  <li>ParNew</li></ul><p>ParNew收集器是Serial收集器的多线程版本，使用多个线程进行垃圾收集。</p><ul>  <li>Parallel Scavenge</li></ul><p>Parallel Scavenge收集器是一个新生代的多线程收集器（并行收集器），它在回收期间不需要暂停其他用户线程，其采用的是Copying算法，该收集器与前两个收集器有所不同，它主要是为了达到一个可控的吞吐量。</p><ul>  <li>Parallel Old</li></ul><p>Parallel Old是Parallel Scavenge收集器的老年代版本（并行收集器），使用多线程和Mark-Compact算法。</p><ul>  <li>CMS</li></ul><p>CMS（Current Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器，它是一种并发收集器，采用的是Mark-Sweep算法。</p><ul>  <li>G1</li></ul><p>G1收集器是当今收集器技术发展最前沿的成果，它是一款面向服务端应用的收集器，它能充分利用多CPU、多核环境。因此它是一款并行与并发收集器，并且它能建立可预测的停顿时间模型。</p><p><br><br>本文链接： <a href="http://www.meng.uno/articles/dde60b3a/">http://www.meng.uno/articles/dde60b3a/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      简介
Java GC（Garbage Collection，垃圾收集，垃圾回收）机制，是Java与C++/C的主要区别之一，作为Java开发者，一般不需要专门编写内存回收和垃圾清理代码，对内存泄露和溢出的问题，也不需要像C程序员那样战战兢兢。这是因为在Java虚拟机中，存在自动内存管理和垃圾清扫机制。概括地说，该机制对JVM（Java Virtual Machine）中的内存进行标记，并确定哪些内存需要回收，根据一定的回收策略，自动的回收内存，永不停息（Nerver Stop）的保证JVM中的内存空间，防止出现内存泄露和溢出问题。

关于JVM，需要说明一下的是，目前使用最多的Sun公司的JD
    
    </summary>
    
      <category term="Java开发Tips" scheme="http://www.meng.uno/categories/Java%E5%BC%80%E5%8F%91Tips/"/>
    
    
      <category term="JVM" scheme="http://www.meng.uno/tags/JVM/"/>
    
      <category term="GC" scheme="http://www.meng.uno/tags/GC/"/>
    
      <category term="垃圾回收" scheme="http://www.meng.uno/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"/>
    
  </entry>
  
  <entry>
    <title>H5网页失去焦点Title改变的方法</title>
    <link href="http://www.meng.uno/articles/7794c7e7/"/>
    <id>http://www.meng.uno/articles/7794c7e7/</id>
    <published>2018-03-08T06:33:11.000Z</published>
    <updated>2018-03-08T06:47:36.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>  <p>今天要讲的其实是一个API：<a href="https://developer.mozilla.org/zh-CN/docs/Web/Events/visibilitychange" target="_blank" rel="noopener">visibilitychange</a></p></blockquote><p>这个 API 本身非常简单，由以下三部分组成。</p><p>document.hidden：表示页面是否隐藏的布尔值。页面隐藏包括 页面在后台标签页中 或者 浏览器最小化 （注意，页面被其他软件遮盖并不算隐藏，比如打开的 sublime 遮住了浏览器）。</p><p>document.visibilityState：表示下面 4 个可能状态的值</p><p>hidden：页面在后台标签页中或者浏览器最小化</p><p>visible：页面在前台标签页中</p><p>prerender：页面在屏幕外执行预渲染处理 document.hidden 的值为 true</p><p>unloaded：页面正在从内存中卸载</p><p>Visibilitychange事件：当文档从可见变为不可见或者从不可见变为可见时，会触发该事件。</p><p>这样，我们可以监听 Visibilitychange 事件，当该事件触发时，获取 document.hidden 的值，根据该值进行页面一些事件的处理。</p><figure class="highlight html">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre>      </td>      <td class="code">        <pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"zh-CN"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">"Content-Type"</span> <span class="attr">content</span>=<span class="string">"text/html; charset=utf-8"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span>&gt;</span>这是原来的title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"viewport"</span> <span class="attr">content</span>=<span class="string">"width=device-width, initial-scale=1, maximum-scale=1"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="javascript">        <span class="keyword">var</span> tmptitle = <span class="built_in">document</span>.title;</span></span><br><span class="line"><span class="javascript">        <span class="built_in">document</span>.addEventListener(<span class="string">'visibilitychange'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">          <span class="keyword">var</span> isHidden = <span class="built_in">document</span>.hidden;</span></span><br><span class="line"><span class="javascript">          <span class="keyword">if</span> (isHidden) &#123;</span></span><br><span class="line"><span class="javascript">            <span class="built_in">document</span>.title = <span class="string">'当焦点不在当前窗口时的网页标题'</span>;</span></span><br><span class="line"><span class="javascript">          &#125; <span class="keyword">else</span> &#123;</span></span><br><span class="line"><span class="javascript">            <span class="built_in">document</span>.title = tmptitle;</span></span><br><span class="line"><span class="undefined">          &#125;</span></span><br><span class="line"><span class="undefined">        &#125;);</span></span><br><span class="line"><span class="undefined">        </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre>      </td>    </tr>  </table></figure><p><br><br>本文链接： <a href="http://www.meng.uno/articles/7794c7e7/">http://www.meng.uno/articles/7794c7e7/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      今天要讲的其实是一个API：visibilitychange

这个 API 本身非常简单，由以下三部分组成。

document.hidden：表示页面是否隐藏的布尔值。页面隐藏包括 页面在后台标签页中 或者 浏览器最小化 （注意，页面被其他软件遮盖并不算隐藏，比如打开的 sublime 遮住了浏览器）。

document.visibilityState：表示下面 4 个可能状态的值

hidden：页面在后台标签页中或者浏览器最小化

visible：页面在前台标签页中

prerender：页面在屏幕外执行预渲染处理 document.hidden 的值为 true

unloaded
    
    </summary>
    
      <category term="随笔" scheme="http://www.meng.uno/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="H5" scheme="http://www.meng.uno/tags/H5/"/>
    
      <category term="Title" scheme="http://www.meng.uno/tags/Title/"/>
    
      <category term="焦点" scheme="http://www.meng.uno/tags/%E7%84%A6%E7%82%B9/"/>
    
  </entry>
  
  <entry>
    <title>Unix进程的那些事</title>
    <link href="http://www.meng.uno/articles/aeaab565/"/>
    <id>http://www.meng.uno/articles/aeaab565/</id>
    <published>2018-03-04T12:06:22.000Z</published>
    <updated>2018-03-04T12:56:59.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>  <p>不知道你们有没有这样的疑惑，每次在看资料时遇到<code>fork(2)</code>，我都不理解，为什么<code>fork()</code>函数需要<code>2</code>做参数？还只能是<code>2</code>。</p></blockquote><blockquote>  <p>本篇博客在阅读了《Working with Unix Processes》之后总结而成。</p></blockquote><h1>回答疑问</h1><p>首先，我就来解释一下之前留下的那个疑问，用过Mac或者Linux的同学都知道电脑中有很多<code>man</code>文件夹，我也不知道怎么回事，莫非是因为我是个<code>man</code>？后来我知道了，<code>man</code>是<code>manpages</code>的意思，中文译作“Unix手册页”，和我们现实中使用的手册一样，这个手册也是分节的，其中比较重要的几节：</p><ul>  <li>节1：一般命令</li>  <li>节2：系统调用</li>  <li>节3：C库函数</li>  <li>节4：特殊文件</li></ul><p>那么我们该如何使用这个手册呢？</p><p>很简单 我们只需要：<code>man [节号] 命令名</code>就可以了。</p><p>例如：<code>man 2 fork</code></p><figure class="highlight stylus">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre>      </td>      <td class="code">        <pre><span class="line">     fork() will fail and no child process will be created <span class="keyword">if</span>:</span><br><span class="line"></span><br><span class="line">     [EAGAIN]           The system-imposed limit on the total number of processes under</span><br><span class="line">                        execution would be exceeded.  This limit is configuration-depen-</span><br><span class="line">                        dent.</span><br><span class="line"></span><br><span class="line">     [EAGAIN]           The system-imposed limit MAXUPRC (&lt;sys/param.h&gt;) on the total num-</span><br><span class="line">                        ber of processes under execution by <span class="selector-tag">a</span> single user would be</span><br><span class="line">                        exceeded.</span><br><span class="line"></span><br><span class="line">     [ENOMEM]           There is insufficient swap space <span class="keyword">for</span> the new process.</span><br><span class="line"></span><br><span class="line">LEGACY SYNOPSIS</span><br><span class="line">     <span class="selector-id">#include</span> &lt;sys/types.h&gt;</span><br><span class="line">     <span class="selector-id">#include</span> &lt;unistd.h&gt;</span><br><span class="line"></span><br><span class="line">     The include file &lt;sys/types.h&gt; is necessary.</span><br><span class="line"></span><br><span class="line">SEE ALSO</span><br><span class="line">     execve(<span class="number">2</span>), sigaction(<span class="number">2</span>), wait(<span class="number">2</span>), compat(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">HISTORY</span><br><span class="line">     A fork() function call appeared <span class="keyword">in</span> Version <span class="number">6</span> AT&amp;T UNIX.</span><br><span class="line"></span><br><span class="line">CAVEATS</span><br><span class="line">     There are limits to what you can do <span class="keyword">in</span> the child process.  To be totally safe you</span><br><span class="line">     should restrict yourself to only executing async-signal safe operations until such</span><br><span class="line">     <span class="selector-tag">time</span> as one of the exec functions is called.  All APIs, including global data sym-</span><br><span class="line">     bols, <span class="keyword">in</span> any framework or library should be assumed to be unsafe after <span class="selector-tag">a</span> fork()</span><br><span class="line">     unless explicitly documented to be safe or async-signal safe.  If you need to use</span><br><span class="line">     these frameworks <span class="keyword">in</span> the child process, you must exec.  In this situation it is rea-</span><br><span class="line">     sonable to exec yourself.</span><br><span class="line"></span><br><span class="line"><span class="number">4</span>th Berkeley Distribution        June <span class="number">4</span>, <span class="number">1993</span>        <span class="number">4</span>th Berkeley Distribution</span><br><span class="line">(END)</span><br></pre>      </td>    </tr>  </table></figure><p>这就是完整的对fork(2)的解释。</p><h1>Unix进程</h1><h2 id="提示">提示</h2><p>所有<code>Ruby</code>代码皆需要在<code>irb</code>环境下运行，如何安装<code>Ruby</code>，可以百度。</p><h2 id="进程标识">进程标识</h2><p>每个人都有一个唯一的身份证号，进程也是如此，这个唯一的标识符叫做<code>pid</code>。我们输入：<code>puts Process.pid</code>就可以得到当前进程的pid了。</p><p>pid并不传达关于进程本身的任何信息，它仅仅是一个顺序标识符。在内核眼中进程只是一个数字而已。</p><p>pid是对进程的一种简单通用的描述，至于用途之一，比如我们常常会在日志文件中发现pid，当有多个进程向一个日志文件写入日志的时候，在每一行加入pid就可以知道哪一行日志是由哪个进程写入的。</p><h2 id="父进程">父进程</h2><p>系统中运行的每一个进程都有对应的父进程，每一个进程都知道其父进程的标识符(ppid)。多数情况下特定进程的父进程就是调用它的那个进程。比如启动终端并进入bash提示符，此时新创建的bash进程的父进程就是终端进程。如果在bash中调用ls等命令，那么bash进程便是ls进程的父进程。</p><p>父进程对于检测守护进程有比较重要的作用。</p><p>我们输入：<code>puts Process.ppid</code>就可以得到当前进程的父进程pid了。</p><h2 id="文件描述符">文件描述符</h2><p>我们讨论进程，怎么突然说道“文件”？其实，在Unix眼中，一切皆为“文件”！设备是文件，套接字是文件，文件也是文件。当然为了避免误解，一般将文件称为文件，其他称为资源。</p><p>我们使用这样的语句打印某“文件”的描述符：<code>puts 文件名.fileno</code>。</p><p>例如，STDIN的描述符：<code>puts STDIN.fileno</code>，结果是不是很吃惊？因为居然是<code>0</code>！！！</p><p>同理，排在其后的分别是STDOUT与STDERR，他们三者也被称为标准流。</p><h2 id="资源限制">资源限制</h2><p>文件描述符代表已经打开的资源，当资源没有被关闭时，该资源的文件描述符编号会一直被占用，文件描述符编号一直处于递增状态，而内核为每个进程设置了最大文件描述符号，即施加了一些资源限制。对于文件描述符编号的限制有软限制和硬限制。软限制一般可以比较小而硬限制一般数值比较大而且可以修改。如果超出限制则会报错。</p><p>资源限制除了允许打开的最大资源数以外，还包括可创建的最大文件长度和进程最大段的大小等。对于用户内核会限制其最大并发进程数。</p><p>我们使用<code>p Process.getrlimit(:NOFILE)</code>来获取当前进程的资源限制，在我的电脑上结果是：<code>[256, 9223372036854775807]</code>。</p><p>可能我们觉得软限制256有点少，那好，我们尝试给他设置一个大的。使用如下命令：</p><p><code>Process.setrlimit(:NOFILE,4096)</code></p><h2 id="环境变量">环境变量</h2><p>我们每个人都设置过环境变量，环境变量是包含进程数据的键值对。所有进程都从其父进程继承环境变量，它们由父进程设置并被子进程所继承。每一个进程都有环境变量，环境变量对于特定进程而言是全局性的。比如环境变量PWD对应的值为当前的工作目录等等。环境变量经常作为一种将输入传递到命令行程序中的方法。</p><h2 id="参数">参数</h2><p>所有进程都可以访问名为ARGV的特殊数组（<code>p ARGV</code>），它是一个参数向量或数组。保存了在命令行中传递给当前进程的参数。有些像C语言中main函数中第二个参数：char** argv。</p><h2 id="进程名">进程名</h2><p>系统中每一个进程都有名称，进程名可以在运行期间被修改并作为一种通信手段。一般都会有一个全局变量来存储当前进程的名称。可以通过给这个全局变量赋值来修改当前进程的名称。</p><p>我们可以用<code>puts $PROGRAM_NAME</code>来打印当前进程的进程名。</p><h2 id="退出码">退出码</h2><p>我们写C程序的时候，总是默认加上<code>return 0</code>，可能大家也遇到过其他的返回值，例如<code>exit(1)</code>等，这里的0、1就是退出码。</p><p>所有进程在退出时都带有数字退出码(0-255)用于指明进程是否顺利结束。一般退出码为0的进程被认为是顺利结束，其他的退出码则表明出现了错误，不同的退出码代表不同的错误。</p><p>尽管退出码通常用来表明不同的错误，它们其实是一种通信手段。作为程序员的你可以以适合自己程序的方式来处理各种进程退出码。</p><ul>  <li>exit</li></ul><p>默认进程退出码为<code>0</code>，可以传递指定的退出码。<code>exit 22</code>代表定制进程退出码为<code>22</code>，不指定数字则默认为<code>0</code>，而且指定退出码在<code>0-255</code>之间的数值才是有效的。</p><ul>  <li>exit!</li></ul><p>默认进程退出码为<code>1</code>，可以传递指定的退出码。<code>exit!33</code>代表定制进程退出码为<code>33</code>，不指定数字时默认为<code>1</code>，而且指定退出码在<code>0-255</code>之间的数值才是有效的。</p><ul>  <li>abort</li></ul><p>会将当前进程的退出码设置为<code>1</code>，而且可以传递一条消息给STDERR。例如<code>abort “Something went wrong!”</code>，则进程退出码为<code>1</code>且会在<code>STDERR</code>中打印<code>“Something went wrong”</code>。注意该方法不能指定退出码。</p><ul>  <li>raise</li></ul><p><code>raise</code>方法不会立即结束进程，它只是抛出一个异常，该异常会沿着调用栈向上传递并可能会得到处理。如果没有代码对其进程处理，那么这个未处理的异常将会终结该进程。类似于<code>abort</code>方法，一个未处理的异常会将退出码设置为<code>1</code>。也可以传递一条消息给<code>STDERR</code>。例如<code>raise “Something went wrong!”</code>，则进程退出码为<code>1</code>且会在<code>STDERR</code>中打印<code>“Something went wrong”</code>。<strong>注意该方法也不能指定退出码。</strong></p><h2 id="fork-与友好进程">fork()与友好进程</h2><p>fork()系统调用允许运行中的进程以编程的形式创建新的进程，这个心进程和原始进程一模一样。调用fork()的进程被称为父进程，新创建的进程被称为子进程。因子进程是一个全新的进程，所以它拥有自己唯一的进程id。</p><p>子进程从父进程处继承了其所占用内存中的所有内容，以及所有属于父进程的已打开的文件描述符的编号。这样，两个进程就可以共享打开的文件、套接字等。因子进程会复制父进程在内存中的所有内容，所以子进程可以随意更改其内存内容的副本，而不会对父进程造成任何影响(后面会介绍COW写时复制技术)。</p><p>对于fork()方法的一次调用实际上会返回两次。fork方法创造了一个新进程，在调用进程(父进程)中返回一次，且会返回子进程的pid；在新创建的进程(子进程)中又返回一次，返回0。</p><p>fork创建了一个和旧进程一模一样的新进程。所以试想一个使用了500MB内存的进程进行了衍生，那么就有1GB的内存被占用了。重复同样的操作十次，很快就会耗尽内存，这通常被称为“fork炸弹”。</p><p>所以现代的Unix/Linux操作系统采用写时复制(copy-on-write, COW)的方法来克服这个问题。COW将实际的内存复制操作推迟到了真正需要写入的时候。所以说父进程和子进程实际上是在共享内存中的数据，直到它们其中一个需要对数据进行修改，届时才会进行内存复制，使得两个进程保持适当的隔离。</p><p>这里多补充点COW的知识，自己在面试中也被问到这个问题，当时并不了解这个知识点，所以对这个知识点印象比较深刻。当采用COW技术时，子进程并不完全复制父进程的数据，只是以只读的方式共享父进程的页表，并将符进程的页表项也标记为只读。当父子进程中任何一个进程试图修改这些地址空间时，就会引发系统的页错误异常。异常错误处理程序将会生成该页的一份复制，并修改进程的页表项，指向新生成的页面，并将该页标记为已修改。</p><p>除了修改的数据和页面之外，其余的部分依然可以共享。</p><p>在一些语言当中，比如ruby中，会通过block代码块来使用fork。将一个block代码块传递给fork方法，那么这个block代码块将在新的子进程中执行，而父进程会跳过block中的内容。而且子进程执行完block之后就会退出，并不会像父进程那样指向随后的代码。</p><h2 id="孤儿进程">孤儿进程</h2><p>当父进程结束后而子进程没有结束时，子进程会照常继续运行，此时子进程被称为孤儿进程。孤儿进程会被系统当中的守护进程所收养，该进程是一种长期运行的进程，而且是有意作为孤儿进程存在。</p><h2 id="进程等待与僵尸进程">进程等待与僵尸进程</h2><p>wait是一个阻塞调用，该调用使得父进程一直等到它的某个子进程退出以后才继续执行。wait会返回其等待子进程的pid。wait2会返回两个值(pid, status)。除了pid之外还包括status，该变量存储有大量关于子进程的有用的信息，可让我们获知某个进程是怎样退出的。</p><p>wait/wait2是等待任意子进程的退出，而waitpid/waitpid2则是等待特定的由pid指定的子进程退出。</p><p>内核将退出的进程信息加入到队列，这样以来父进程就总是能够依照子进程退出的顺序接收到信息。就是说，即使子进程退出而父进程还没有准备妥当的时候，父进程也总能够通过队列获取到每个子进程的退出信息。注意，如果不存在子进程，调用wait的任一变体都会抛出ERRNO::ECHILD异常。所以最好让调用wait的数量和创建的子进程的数量相等才不会抛出异常。</p><p>一些服务器会使用看护进程这一模式：有一个衍生出多个并发子进程的进程，这个进程看管这些子进程，确保它们能够保持响应，并对子进程的退出做出响应，这个进程就是看护进程。</p><p>内核会将已退出的子进程的状态信息加入队列，所以即便父进程在子进程退出很久之后才调用wait，依然可以获取它的状态信息。内核会一直保留已退出的子进程的状态信息直到父进程调用wait请求这些消息。如果父进程一直不发出请求，那么状态信息就会被内核一直保留着，因此创建一个即发即弃的子进程却不去请求状态信息，便是在浪费内核资源，比如pid，要知道内核可创建的pid和进程控制块PCB是有限的，如果一直创建进程其父进程却不去请求它的退出信息，那么pid和PCB有可能会被耗尽而使得系统无法继续产生新进程。此时的子进程就被称为僵尸进程，所以说僵尸进程是有害的。</p><p>任何应结束的进程，如果它的状态信息一直未能读取，那么它就是一个僵尸进程，任何子进程在结束之时其父进程仍在运行，那么这个子进程很快就会称为僵尸进程。一旦父进程读取了僵尸进程的状态信息，那么它就不复存在，也就不再消耗内核资源。</p><p>有一种避免僵尸进程出现的方法就是分离父子进程，当父进程新创建一个子进程以后，如果不打算调用wait去等待和读取子进程的退出信息，可以使用detach方法。detach方法核心就是生成一个新线程，这个线程唯一的工作就是等待有pid所指定的那个进程退出并获取进程退出信息，从而确保内核不会一直保留进程的状态信息造成僵尸进程的出现和内核资源的浪费。</p><p>那么怎么识别僵尸进程呢？</p><p>很简答，我们使用如下指令：<code>pid = fork{ sleep 1} ; puts pid; sleep</code>的方式，发现结果为：<code>z</code>。</p><h2 id="信号量">信号量</h2><p>wait为父进程提供了一种很好方式来监管子进程。但它是一个阻塞调用：直到子进程结束，调用才会返回，任何一行代码都可能被信号中断。信号投递时不可靠的。如果你的代码正在处理CHLD信号，这时候另一个子进程结束了，那么你未必能收到第二个CHLD信号(CHLD信号：提醒父进程子进程退出的信号)。如果同一个信号在极短间隔内被多次收到，就会出现这种情况。这时可以考虑使用wait的非阻塞方法，形如<code>wait(-1, Process::WNOHANG)</code>。当获得一个信号并返回值以后就继续等待信号的产生。</p><p>信号是一种异步通信，当进程从内核接收到一个信号时，它可以执行下列某一个操作：</p><ul>  <li>忽略该信号；</li>  <li>执行特定操作；</li>  <li>执行默认操作。</li></ul><p>信号有内核发出，信号是由一个进程发送给另一个进程，不过内核作为中介而已。下表为常用信号介绍，大部分信号的默认行为都是终止进程，其中dump动作表示进程会立即结束并进行核心转储(栈跟踪)，而且比较特殊信号有SIGKILL和SIGSTOP信号不能被捕获、阻塞或忽略，SIGSR1和SIGSR2两个信号对应的操作由你的进程来定义。</p><p>信号是一个了不起的工具，不过捕获一个信号有点像使用全局变量，有可能把其他程序锁依赖的东西给修改了，不过和全局变量不同的是信号处理程序并没有命名空间。从最佳事件角度来说，个人代码不应该定义任何信号处理程序，除非它是服务器。正如一个从命令行启动的长期运行的进程，库代码极少会捕获信号。</p><p>进程可以在任何时候接收到信号，这就是信号的美所在！而且信号是异步的。有了信号，一旦知道了对方的pid，系统中的进程便可以彼此通信，使得信号成为一种极其强大的通信工具，常见的用法是使用kill方法来发送信号。实践当中，信号多是由长期运行的进程响应和使用，例如服务器和守护进程。而多数情况下，发送信号的都是人类用户而非自动化程序。</p><h2 id="进程通讯">进程通讯</h2><p>进程间通信(IPC)两个常见的实用方法是管道和套接字对(socket pairs)。</p><p>管道是一个单向数据流。打开一个管道，一个进程拥有管道的一段，另一个进程拥有另一端。然后数据就沿着管道单向传递。因此如果某个进程将自己作为一个管道的reader，而非writer，那么它就无法向管道中写入数据，反之亦然。例如在ruby脚本程序中：</p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>      </td>      <td class="code">        <pre><span class="line">reader，writer = IO.pipe</span><br><span class="line">writer.write(&quot;I am writing something..&quot;)</span><br><span class="line">writer.close</span><br><span class="line">puts reader.read</span><br></pre>      </td>    </tr>  </table></figure><p>结果为：</p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br></pre>      </td>      <td class="code">        <pre><span class="line">I am writing something..</span><br></pre>      </td>    </tr>  </table></figure><p>pipe返回一个包含两个元素的数组，第一个元素为reader的信息，第二个元素为writer的信息。</p><p>向管道写完信息就关闭writer，是因为reader调用read方法时，会不停地试图从管道中读取数据，直到读到一个EOF(文件结束标志)。这个标志告诉reader已经读完管道中所有的数据了。只要writer保持打开，那么reader就可能读到更多的数据，因此它就会一直等待。在读取之前关闭writer，将一个EOF放入管道中，这样一来，reader获得原始数据之后就会停止读取。要是忘记或者省去关闭writer这一步，那么reader就会被阻塞并不停地试图读取数据。</p><p>因为管道是单向的，所以再上诉程序中，reader只能读取，writer只能写入。</p><p>当某个进程衍生出一个子进程的时候，会与子进程共享打开的资源，管道也被认为是一种资源，它有自己的文件描述符等，因此可以与子进程共享。</p><p>当使用诸如管道或TCP套接字这样的IO流时，将数据写入流中，之后跟着一些特定协议的分隔符，随后从IO流中读取数据时，一次读取一块(chuck)，遇到分隔符就停止读取。</p><p>Unix套接字是一种只能用于在同一台物理主机中进行通信的套接字，它比TCP套接字快很多，非常适合用于IPC。</p><p>管道和套接字都是对进程间通信的有益抽象。它们即快速有简单，多被用作通信通道，来代替更为原始的方法，如共享数据库或日志文件。使用哪种方法取决于自己的需要，不过记得管道提供的是单向通信，套接字提供的是双向通信。</p><h2 id="终端进程">终端进程</h2><p>我们在终端执行每一条命令，其实都是创建了一个终端进程。</p><p>exec()系统调用非常简单，它允许使用另一个进程来替换当前进程，exec()这种转变是有去无回的，一旦你将当前进程转变为另外一个别的进程，那就再也变不回来了。</p><p>在要生成新进程的时候，fork()+exec()的组合是常见的一种用法，使用fork()创建一个新进程，然后用exec()把这个进程变成自己想要的进程，你的当前进程仍像从前一样运行，也仍可以根据需要生成其他进程。如果程序依赖于exec()调用的输出结果，可用wait方法来确保你的程序一直等到子进程完成它的工作，这样就可取回结果。exec()在默认情况下不会关闭任何打开的文件描述符或进行内存清理。</p><p>把字符串传递给exec实际上会启动一个shell进程，然后shell进程对这个字符串进行解释，传递一个数组的话，它会跳过shell，直接将此数组作为新进程的ARGV-参数数组，除非真的需要，一般尽可能地传递数组。</p><p>fork()是有成本的，记住这点有益无害，有时候它会成为性能瓶颈，主要是因为fork()的新子进程的两个独特属性：</p><ul>  <li>获得了一份父进程在内存中所有内容的副本；</li>  <li>获得了父进程已打开的所有文件描述符的副本。</li></ul><p>有一个系统调用posix_spawn，子保留了第2条，没有保留第1条。posix_spawn所生成的子进程可以访问父进程打开的所有文件描述符，却无法与父进程共享内存。这也是为什么posix_spawn比fork快、更有效率的原因。但事务都有两面性，也会因此而缺乏灵活性。</p><h2 id="守护进程">守护进程</h2><p>守护进程是在后台运行的进程，不受终端用户控制。Web服务器或数据库服务器都属于常见的守护进程，它们一直在后台运行响应请求。守护进程也是操作系统的核心功能，有很多进程一直在后台运行以保证系统的正常运行，任何进程都可变成守护进程。</p><p>当内核被引导时会产生一个叫做init的进程。该进程的pid是1，而ppid是0，作为所有进程的祖父。它是首个进程，没有祖先。一个孤儿进程会被init进程收养，孤儿进程的ppid始终是1，这是内核能够确保一直运行的唯一进程。</p><p>每一个进程都属于某个组，每一个组都有唯一的整数id，称为进程组id。进程组是一个相关进程的集合，通常是父进程与子进程。但是也可以按照需要将进程分组，可以通过setpgrp(new_group_ip)方法来设置进程组id。通常情况下，进程组id和进程组组长的id是相同的。进程组组长是终端命令的发起进程。也就是说，如果在终端启动一个进程，那么它就会成为一个新进程组的组长，它所创建的子进程就成为同一个进程组的组员。</p><p>这里进一步说明一下，之前讲过孤儿进程，子进程在父进程退出后会被init进程收养而继续运行，这是父进程退出的行为，但是如果父进程由终端控制并被信号终止的话，孤儿进程也会被终止的。这是因为父子进程属于同一个进程组，而父进程由终端控制，当父进程收到来自终端的终止信号时，与父进程属于同一个进程组的子进程也会收到终止信号而被终止。</p><p>会话组是更高一级的抽象，它是进程组的集合。一个会话组可以依附于一个终端，也可以不依附与任何终端，比如守护进程。终端用一种特殊的方法来处理会话组：发送给会话领导的信号会被转发到该会话中的所有进程组内，然后再转发到这些进程组中的所有进程。系统调用getsid()可用来检索当前的会话组id。</p><p>以下是创建一个守护进程的过程：</p><ul>  <li>首先在终端创建一个进程，并在进程中衍生出一个子进程，然后作为父进程的自己退出。启动该进程的终端察觉到进程退出后，将控制返回给用户，但是衍生出的子进程仍然拥有从父进程中继承而来的组id和会话组id，此时这个衍生进程既非会话领导也非进程组组长。因终端与衍生进程之间仍有牵连，如果终端发送信号到衍生进程的会话组，衍生进程会接收到这个信号，但我们想要的是完全脱离终端。</li>  <li>setsid方法可使得衍生进程成为一个新进程组的组长和新会话组的领导，而且此时新的会话组并没有控制终端。注意，如果在某个已经是进程组组长的进程中调用setsid方法，则会失败，它只能从子进程中调用。</li>  <li>已经成为进程组和会话组组长的衍生进程再次进行衍生，然后自己退出。新衍生出的进程不再是进程组和会话组组长，由于之前会话领导并没有相应的控制终端，且此进程也不是会话领导，因此该进程绝对不会有相应的控制终端存在，如此就可以确保进程现在是完全脱离了控制终端并且可以独立运行。</li>  <li>将进程的工作目录更改为系统的根目录，可避免进程的启动进程出于个各种问题被删除或者卸载。</li>  <li>将所有标准流重定向到“/dev/null”，也就是将其忽略，主要是因为守护进程已不再依附于某个终端会话，所以标准流也就无用了，但是不能简单的关闭，因为一些进程可能还指望它们随时可用。</li></ul><p>以下是ruby语言创建一个守护进程的完整程序：</p><figure class="highlight plain">  <table>    <tr>      <td class="gutter">        <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre>      </td>      <td class="code">        <pre><span class="line">exit if fork</span><br><span class="line">Process.setsid</span><br><span class="line">exit if fork</span><br><span class="line"></span><br><span class="line">Dir.chdir  &quot;/&quot;</span><br><span class="line">STDIN.reopen   &quot;/dev/null&quot;</span><br><span class="line">STDOUT.reopen   &quot;/dev/null&quot;, &quot;a&quot;</span><br><span class="line">STDERR.reopen  &quot;/dev/null&quot;, &quot;a&quot;</span><br></pre>      </td>    </tr>  </table></figure><p>对于是否需要创建一个守护进程，就应该问自己一个基本问题：这个进程是否需要一直保持响应？如果答案为否，那么你也许可以考虑定时任务或后台作业系统，如果答案是肯定的，那就去创建，不用犹豫。</p><p><br><br>本文链接： <a href="http://www.meng.uno/articles/aeaab565/">http://www.meng.uno/articles/aeaab565/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      不知道你们有没有这样的疑惑，每次在看资料时遇到fork(2)，我都不理解，为什么fork()函数需要2做参数？还只能是2。

本篇博客在阅读了《Working with Unix Processes》之后总结而成。

回答疑问
首先，我就来解释一下之前留下的那个疑问，用过Mac或者Linux的同学都知道电脑中有很多man文件夹，我也不知道怎么回事，莫非是因为我是个man？后来我知道了，man是manpages的意思，中文译作“Unix手册页”，和我们现实中使用的手册一样，这个手册也是分节的，其中比较重要的几节：

 * 节1：一般命令
 * 节2：系统调用
 * 节3：C库函数
 * 节4：特
    
    </summary>
    
      <category term="操作系统" scheme="http://www.meng.uno/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Unix" scheme="http://www.meng.uno/tags/Unix/"/>
    
      <category term="进程" scheme="http://www.meng.uno/tags/%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础</title>
    <link href="http://www.meng.uno/articles/c0b3d81d/"/>
    <id>http://www.meng.uno/articles/c0b3d81d/</id>
    <published>2018-03-04T04:29:41.000Z</published>
    <updated>2018-08-16T04:32:06.000Z</updated>
    
    <content type="html"><![CDATA[<h1>梯度下降法</h1><h2 id="梯度下降法的作用-目的-本质">梯度下降法的作用/目的/本质</h2><ul>  <li>    <p>参数<strong>优化</strong>的一种策略，用于寻找<strong>局部最小值</strong></p>  </li>  <li>    <p>微积分中使用<strong>梯度</strong>表示函数增长最快的方向；相应的，神经网络中使用<strong>负梯度</strong>来指示损失函数下降最快的方向</p>  </li>  <li>    <p><strong>梯度</strong>实际上是损失函数对网络中每个参数的偏导所组成的向量，其中计算各偏导的方法即<strong>反向传播算法</strong></p>  </li>  <li>    <p>从另一个角度来理解<strong>方向</strong>这个概念，可以认为<strong>负梯度</strong>中的每一项实际传达了两个信息：</p>    <ol>      <li>        <p>正负号在告诉输入向量应该调大还是调小——正调大，负调小</p>      </li>      <li>        <p>每一项的相对大小表明每个输入值对函数值的影响程度；换言之，也就是调整各权重对于网络的影响</p>        <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180703103757.png" alt=""></p>      </li>    </ol>  </li></ul><h2 id="随机梯度下降">随机梯度下降</h2><ul>  <li>基本的梯度下降法要求每次使用<strong>所有训练样本</strong>的平均损失来更新参数</li>  <li>为了<strong>加快计算效率</strong>，一般的做法会首先<strong>打乱</strong>所有训练样本，每次计算梯度时会<strong>随机</strong>抽取其中一<strong>批</strong>(<strong>Batch</strong>)来计算平均损失——这就是“<strong>随机梯度下降</strong>”。    <blockquote>      <p>也有地方将使用<strong>全部</strong>、<strong>一个</strong>、<strong>一批</strong>样本的方法分别称为“<strong>批量梯度下降</strong>”、“<strong>随机梯度下降</strong>”、“<strong>小批量梯度下降</strong>”</p>    </blockquote>  </li></ul><h2 id="随机梯度下降中-批-的大小对优化效果的影响">随机梯度下降中“批”的大小对优化效果的影响</h2><blockquote>  <p>《深度学习》 8.1.3 批量算法和小批量算法</p></blockquote><ul>  <li><strong>较大的批能得到更精确的梯度估计</strong>，但回报是小于线性的。</li>  <li><strong>较小的批能带来更好的泛化误差</strong>，泛化误差通常在批大小为 1 时最好。但是，因为梯度估计的高方差，小批量训练需要<strong>较小的学习率</strong>以保持稳定性，这意味着<strong>更长的训练时间</strong>。    <blockquote>      <p>原因可能是由于小批量在学习过程中带来了<strong>噪声</strong>，使产生了一些正则化效果 (Wilson and Martinez, 2003)</p>    </blockquote>  </li>  <li><strong>内存消耗和批的大小成正比</strong>，当批量处理中的所有样本可以并行处理时。</li>  <li>在某些硬件上使用特定大小可以减少运行时间。尤其是在使用 GPU 时，通常使用 <strong>2 的幂数</strong>作为批量大小可以获得更少的运行时间。一般，2 的幂数的<strong>取值范围是 32 到 256</strong>，16 有时在尝试大模型时使用。</li></ul><h1>反向传播算法</h1><h2 id="反向传播的作用-目的-本质">反向传播的作用/目的/本质</h2><ul>  <li>    <p><strong>反向传播概述</strong>：</p>    <p><strong>梯度下降法</strong>中需要利用损失函数对所有参数的梯度来寻找局部最小值点；</p>    <p>而<strong>反向传播算法</strong>就是用于计算该梯度的具体方法，其本质是利用<strong>链式法则</strong>对每个参数求偏导。</p>  </li></ul><h2 id="反向传播的公式推导">反向传播的公式推导</h2><ul>  <li>    <p>可以用 4 个公式总结反向传播的过程</p>    <p><strong>标量形式</strong>：</p>    <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705190236.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=%5Cbegin%7Baligned%7D&amp;space;%5Cfrac%7B%5Cpartial&amp;space;C%7D%7B%5Cpartial&amp;space;%7B%5Ccolor%7BRed%7D&amp;space;a_j" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&amp;space;\frac{\partial&amp;space;C}{\partial&amp;space;{\color{Red}&amp;space;a_j</a><sup>{(L)}}}=\frac{\partial&amp;space;C({\color{Red}&amp;space;a_j</sup>{(L)}},y_j)}{\partial&amp;space;{\color{Red}&amp;space;a_j^{(L)}}}&amp;space;\end{aligned})</p>    <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cbegin%7Baligned%7D&amp;space;%5Cfrac%7B%5Cpartial&amp;space;C%7D%3C!--%EF%BF%BC1--%3E%7D=%7B%5Ccolor%7BTeal%7D%5Csum_%7Bk=0%7D%5E%7Bn_l-1%7D%7D&amp;space;%5Cfrac%7B%5Cpartial&amp;space;z_k%5E%7B(l+1)%7D%7D%3C!--%EF%BF%BC2--%3E%7D&amp;space;%5Cfrac%3C!--%EF%BF%BC3--%3E%7D%7B%5Cpartial&amp;space;z_k%5E%7B(l+1)%7D%7D&amp;space;%5Cfrac%7B%5Cpartial&amp;space;C%7D%3C!--%EF%BF%BC4--%3E%7D&amp;space;%5Cend%7Baligned%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705134851.png" alt=""></a></p>    <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cbegin%7Baligned%7D&amp;space;%5Cfrac%7B%5Cpartial&amp;space;C%7D%3C!--%EF%BF%BC5--%3E%7D=%5Cfrac%7B%5Cpartial&amp;space;z_j%5E%7B(l)%7D%7D%3C!--%EF%BF%BC6--%3E%7D%5Cfrac%3C!--%EF%BF%BC7--%3E%7D%7B%5Cpartial&amp;space;z_j%5E%7B(l)%7D%7D%5Cfrac%7B%5Cpartial&amp;space;C%7D%3C!--%EF%BF%BC8--%3E%7D&amp;space;%5Cend%7Baligned%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705154543.png" alt=""></a></p>    <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Cbegin%7Baligned%7D&amp;space;%5Cfrac%7B%5Cpartial&amp;space;C%7D%3C!--%EF%BF%BC9--%3E%7D=%5Cfrac%7B%5Cpartial&amp;space;z_j%5E%7B(l)%7D%7D%3C!--%EF%BF%BC10--%3E%7D%5Cfrac%3C!--%EF%BF%BC11--%3E%7D%7B%5Cpartial&amp;space;z_j%5E%7B(l)%7D%7D%5Cfrac%7B%5Cpartial&amp;space;C%7D%3C!--%EF%BF%BC12--%3E%7D&amp;space;%5Cend%7Baligned%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705154650.png" alt=""></a></p>    <blockquote>      <p>上标 <code>(l)</code> 表示网络的层，<code>(L)</code> 表示输出层（最后一层）；下标 <code>j</code> 和 <code>k</code> 指示神经元的位置；<code>w_jk</code> 表示 <code>l</code> 层的第 <code>j</code> 个神经元与<code>(l-1)</code>层第 <code>k</code> 个神经元连线上的权重</p>    </blockquote>  </li>  <li>    <p><strong>符号说明</strong>，其中：</p>    <ul>      <li>        <p><code>(w,b)</code> 为网络参数：权值和偏置</p>      </li>      <li>        <p><code>z</code> 表示上一层激活值的线性组合</p>      </li>      <li>        <p><code>a</code> 即 “activation”，表示每一层的激活值，上标<code>(l)</code>表示所在隐藏层，<code>(L)</code>表示输出层</p>      </li>      <li>        <p><code>C</code> 表示激活函数，其参数为神经网络输出层的激活值<code>a^(L)</code>，与样本的标签<code>y</code></p>        <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180704193955.png" alt=""></p>      </li>    </ul>  </li>  <li>    <p>以 <strong>均方误差（MSE）</strong> 损失函数为例，有</p>    <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180705190536.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=%5Cbegin%7Baligned%7D&amp;space;%5Cfrac%7B%5Cpartial&amp;space;C%7D%7B%5Cpartial&amp;space;%7B%5Ccolor%7BRed%7D&amp;space;a_j" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=\begin{aligned}&amp;space;\frac{\partial&amp;space;C}{\partial&amp;space;{\color{Red}&amp;space;a_j</a><sup>{(L)}}}&amp;=\frac{\partial&amp;space;C({\color{Red}&amp;space;a_j</sup>{(L)}},y_j)}{\partial&amp;space;{\color{Red}&amp;space;a_j<sup>{(L)}}}&amp;space;\&amp;space;&amp;=\frac{\partial&amp;space;\left&amp;space;(&amp;space;\frac{1}{2}({\color{Red}a_j</sup>{(L)}}-y_j)<sup>2&amp;space;\right&amp;space;)&amp;space;}{\partial&amp;space;{\color{Red}a_j</sup>{(L)}}}={\color{Red}a_j^{(L)}}-y&amp;space;\end{aligned})</p>  </li>  <li>    <p>Nielsen 的课程中提供了另一种更利于计算的表述，本质上是一样的。</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180705162841.png" alt=""></p>    <blockquote>      <p><a href="http://neuralnetworksanddeeplearning.com/chap2.html#the_four_fundamental_equations_behind_backpropagation" target="_blank" rel="noopener">The four fundamental equations behind backpropagation</a></p>    </blockquote>  </li></ul><h1>激活函数</h1><h2 id="激活函数的作用-为什么要使用非线性激活函数？">激活函数的作用——为什么要使用非线性激活函数？</h2><ul>  <li>    <p>使用<strong>激活函数</strong>的目的是为了向网络中加入<strong>非线性因素</strong>；</p>    <p>从而加强网络的表示能力，解决<strong>线性模型</strong>无法解决的问题</p>    <blockquote>      <p><a href="https://www.zhihu.com/question/22334626" target="_blank" rel="noopener">神经网络激励函数的作用是什么？有没有形象的解释？</a> - 知乎</p>    </blockquote>  </li></ul><p><strong>为什么加入非线性因素能够加强网络的表示能力？——神经网络的万能近似定理</strong></p><ul>  <li>    <p>神经网络的万能近似定理认为主要神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何<strong>从一个有限维空间到另一个有限维空间</strong>的函数。</p>  </li>  <li>    <p>如果不使用非线性激活函数，那么每一层输出都是上层输入的<strong>线性组合</strong>；</p>    <p>此时无论网络有多少层，其整体也将是线性的，这会导致失去万能近似的性质</p>    <blockquote>      <p>《深度学习》 6.4.1 万能近似性质和深度；</p>    </blockquote>  </li>  <li>    <p>但仅<strong>部分层是纯线性</strong>是可以接受的，这有助于<strong>减少网络中的参数</strong>。</p>    <blockquote>      <p>《深度学习》 6.3.3 其他隐藏单元</p>    </blockquote>  </li></ul><h2 id="常见的激活函数">常见的激活函数</h2><blockquote>  <p>《深度学习》 6.3 隐藏单元</p></blockquote><h3 id="整流线性单元-relu">整流线性单元 <code>ReLU</code></h3><ul>  <li>    <p>公式与图像</p>    <p><a href="http://www.codecogs.com/eqnedit.php?latex=g(z)=%5Cmax(0,z)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610213451.png" alt=""></a></p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608212808.png" alt=""></p>  </li>  <li>    <p>ReLU 通常是激活函数较好的默认选择</p>  </li></ul><h4 id="relu-的拓展"><code>ReLU</code> 的拓展</h4><ul>  <li>    <p><code>ReLU</code> 及其扩展都基于以下公式：</p>    <p><a href="http://www.codecogs.com/eqnedit.php?latex=g(z;%5Calpha)&amp;space;=%5Cmax(0,z)+%5Calpha%5Cmin(0,z)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214123.png" alt=""></a></p>    <p>当 <code>α=0</code> 时，即标准的线性整流单元</p>  </li>  <li>    <p><strong>绝对值整流</strong>（absolute value rectification）</p>    <p>固定 <code>α = -1</code>，此时整流函数即<strong>绝对值函数</strong> <code>g(z)=|z|</code></p>  </li>  <li>    <p><strong>渗漏整流线性单元</strong>（Leaky ReLU, Maas et al., 2013）</p>    <p>固定 <code>α</code> 为一个小值，比如 0.01</p>  </li>  <li>    <p><strong>参数化整流线性单元</strong>（parametric ReLU, PReLU, He et al., 2015）</p>    <p>将 <code>α</code> 作为一个可学习的参数</p>  </li>  <li>    <p><strong><code>maxout</code> 单元</strong> (Goodfellow et al., 2013a)</p>    <p><code>maxout</code> 单元 进一步扩展了 <code>ReLU</code>，它是一个可学习的 <code>k</code> 段函数</p>    <p><strong>Keras 简单实现</strong></p>    <figure class="highlight plain">      <table>        <tr>          <td class="gutter">            <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>          </td>          <td class="code">            <pre><span class="line"># input shape:  [n, input_dim]</span><br><span class="line"># output shape: [n, output_dim]</span><br><span class="line">W = init(shape=[k, input_dim, output_dim])</span><br><span class="line">b = zeros(shape=[k, output_dim])</span><br><span class="line">output = K.max(K.dot(x, W) + b, axis=1)</span><br></pre>          </td>        </tr>      </table>    </figure>    <blockquote>      <p>参数数量是普通全连接层的 k 倍</p>      <p><a href="https://blog.csdn.net/hjimce/article/details/50414467" target="_blank" rel="noopener">深度学习（二十三）Maxout网络学习</a> - CSDN博客</p>    </blockquote>  </li></ul><h3 id="sigmoid-与-tanh"><code>sigmoid</code> 与 <code>tanh</code></h3><ul>  <li>    <p><code>sigmoid(z)</code>，常记作 <code>σ(z)</code>:</p>    <p><a href="http://www.codecogs.com/eqnedit.php?latex=%5Csigma(z)=%5Cfrac%7B1%7D%7B1+%5Cexp(-z)%7D" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610214846.png" alt=""></a></p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608195851.png" alt=""></p>  </li>  <li>    <p><code>tanh(z)</code> 的图像与 <code>sigmoid(z)</code> 大致相同，区别是<strong>值域</strong>为 <code>(-1, 1)</code></p>  </li></ul><h3 id="其他激活函数">其他激活函数</h3><blockquote>  <p>很多未发布的非线性激活函数也能表现的很好，但没有比流行的激活函数表现的更好。比如使用 <code>cos</code> 也能在 MNIST 任务上得到小于 1% 的误差。通常新的隐藏单元类型只有在被明确证明能够提供显著改进时才会被发布。</p></blockquote><ul>  <li>    <p><strong>线性激活函数</strong>：</p>    <p>如果神经网络的每一层都由线性变换组成，那么网络作为一个整体也将是线性的，这会导致失去万能近似的性质。但是，仅<strong>部分层是纯线性</strong>是可以接受的，这可以帮助<strong>减少网络中的参数</strong>。</p>  </li>  <li>    <p><strong>softmax</strong>：</p>    <p>softmax 单元常作为网络的输出层，它很自然地表示了具有 k 个可能值的离散型随机变量的概率分布。</p>  </li>  <li>    <p><strong>径向基函数（radial basis function, RBF）</strong>：</p>    <p><a href="http://www.codecogs.com/eqnedit.php?latex=h_i=%5Cexp(-%5Cfrac%7B1%7D%7B%5Csigma_i%5E2%7D%5Cleft&amp;space;%7C&amp;space;W_%7B:,i%7D-x&amp;space;%5Cright&amp;space;%7C%5E2)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215150.png" alt=""></a></p>    <p>在神经网络中很少使用 RBF 作为激活函数，因为它对大部分 x 都饱和到 0，所以很难优化。</p>  </li>  <li>    <p><strong>softplus</strong>：</p>    <p><code>softplus</code> 是 <code>ReLU</code> 的平滑版本。</p>    <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215222.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=g(z)=%5Czeta(z)=%5Clog(1+%5Cexp(z)" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=g(z)=\zeta(z)=\log(1+\exp(z)</a>))</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608204913.png" alt=""></p>    <p>通常不鼓励使用 softplus 函数，大家可能希望它具有优于整流线性单元的性质，但根据经验来看，它并没有。</p>    <blockquote>      <p>(Glorot et al., 2011a) 比较了这两者，发现 ReLU 的结果更好。</p>    </blockquote>  </li>  <li>    <p><strong>硬双曲正切函数（hard tanh）</strong>：</p>    <p>[<img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180610215308.png" alt="">](<a href="http://www.codecogs.com/eqnedit.php?latex=g(a)=%5Cmax(-1,%5Cmin(1,a)" target="_blank" rel="noopener">http://www.codecogs.com/eqnedit.php?latex=g(a)=\max(-1,\min(1,a)</a>))</p>    <p>它的形状和 tanh 以及整流线性单元类似，但是不同于后者，它是有界的。</p>  </li></ul><h2 id="relu-相比-sigmoid-的优势-3"><code>ReLU</code> 相比 <code>sigmoid</code> 的优势 (3)</h2><ol>  <li><strong>避免梯度消失</strong>***</li></ol><ul>  <li><code>sigmoid</code>函数在输入取绝对值非常大的正值或负值时会出现<strong>饱和</strong>现象——在图像上表现为变得很平，此时函数会对输入的微小变化不敏感——从而造成梯度消失；</li>  <li><code>ReLU</code> 的导数始终是一个常数——负半区为 0，正半区为 1——所以不会发生梯度消失现象</li></ul><ol start="2">  <li><strong>减缓过拟合</strong>**</li></ol><ul>  <li><code>ReLU</code> 在负半区的输出为 0。一旦神经元的激活值进入负半区，那么该激活值就不会产生梯度/不会被训练，造成了网络的稀疏性——<strong>稀疏激活</strong></li>  <li>这有助于减少参数的相互依赖，缓解过拟合问题的发生</li></ul><ol start="3">  <li><strong>加速计算</strong>*</li></ol><ul>  <li><code>ReLU</code> 的求导不涉及浮点运算，所以速度更快</li></ul><blockquote>  <p>总结自知乎两个答案 <a href="https://www.zhihu.com/question/52020211/answer/152378276" target="_blank" rel="noopener">Ans1</a> &amp; <a href="https://www.zhihu.com/question/29021768/answer/43488153" target="_blank" rel="noopener">Ans2</a></p></blockquote><p><strong>为什么 ReLU 不是全程可微/可导也能用于基于梯度的学习？</strong></p><ul>  <li>虽然从数学的角度看 ReLU 在 0 点不可导，因为它的左导数和右导数不相等；</li>  <li>但是在实现时通常会返回左导数或右导数的其中一个，而不是报告一个导数不存在的错误。从而避免了这个问题</li></ul><h1>正则化</h1><h2 id="l1-l2-范数正则化">L1/L2 范数正则化</h2><blockquote>  <p>《深度学习》 7.1.1 L2 参数正则化 &amp; 7.1.2 - L1 参数正则化</p>  <p><a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">机器学习中正则化项L1和L2的直观理解</a> - CSDN博客</p></blockquote><h3 id="l1-l2-范数的作用-异同">L1/L2 范数的作用、异同</h3><p><strong>相同点</strong></p><ul>  <li>限制模型的学习能力——通过限制参数的规模，使模型偏好于<strong>权值较小</strong>的目标函数，防止过拟合。</li></ul><p><strong>不同点</strong></p><ul>  <li><strong>L1 正则化</strong>可以产生更<strong>稀疏</strong>的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；<strong>L2 正则化</strong>主要用于防止模型过拟合</li>  <li><strong>L1 正则化</strong>适用于特征之间有关联的情况；<strong>L2 正则化</strong>适用于特征之间没有关联的情况。</li></ul><h3 id="为什么-l1-和-l2-正则化可以防止过拟合？">为什么 L1 和 L2 正则化可以防止过拟合？</h3><ul>  <li>L1 &amp; L2 正则化会使模型偏好于更小的权值。</li>  <li>更小的权值意味着<strong>更低的模型复杂度</strong>；添加 L1 &amp; L2 正则化相当于为模型添加了某种<strong>先验</strong>，限制了参数的分布，从而降低了模型的复杂度。</li>  <li>模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。——直观来说，就是对训练数据的拟合刚刚好，不会过分拟合训练数据（比如异常点，噪声）——<strong>奥卡姆剃刀原理</strong></li></ul><h3 id="为什么-l1-正则化可以产生稀疏权值-而-l2-不会？">为什么 L1 正则化可以产生稀疏权值，而 L2 不会？</h3><ul>  <li>    <p>对目标函数添加范数正则化，训练时相当于在范数的约束下求目标函数 <code>J</code> 的最小值</p>  </li>  <li>    <p>带有<strong>L1 范数</strong>（左）和<strong>L2 范数</strong>（右）约束的二维图示</p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608171710.png" alt="">      <img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180608172312.png" alt=""></p>    <ul>      <li>图中 <code>J</code> 与 <code>L1</code> 首次相交的点即是最优解。<code>L1</code> 在和每个坐标轴相交的地方都会有“<strong>顶点</strong>”出现，多维的情况下，这些顶点会更多；在顶点的位置就会产生稀疏的解。而 <code>J</code> 与这些“顶点”相交的机会远大于其他点，因此 <code>L1</code> 正则化会产生稀疏的解。</li>      <li><code>L2</code> 不会产生“<strong>顶点</strong>”，因此 <code>J</code> 与 <code>L2</code> 相交的点具有稀疏性的概率就会变得非常小。</li>    </ul>  </li></ul><h2 id="dropout-与-bagging-集成方法">Dropout 与 Bagging 集成方法</h2><blockquote>  <p>《深度学习》 7.12 Dropout</p></blockquote><h3 id="bagging-集成方法">Bagging 集成方法</h3><ul>  <li>    <p><strong>集成方法</strong>的主要想法是分别训练不同的模型，然后让所有模型<strong>表决</strong>最终的输出。</p>    <p>集成方法奏效的原因是不同的模型<strong>通常不会</strong>在测试集上产生相同的误差。</p>    <p>集成模型能至少与它的任一成员表现得一样好。<strong>如果成员的误差是独立的</strong>，集成将显著提升模型的性能。</p>  </li>  <li>    <p><strong>Bagging</strong> 是一种集成策略——具体来说，Bagging 涉及构造 k 个<strong>不同的数据集</strong>。</p>    <p>每个数据集从原始数据集中<strong>重复采样</strong>构成，和原始数据集具有<strong>相同数量</strong>的样例——这意味着，每个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的例子</p>    <blockquote>      <p>更具体的，如果采样所得的训练集与原始数据集大小相同，那所得数据集中大概有原始数据集 <code>2/3</code> 的实例</p>    </blockquote>  </li></ul><p><strong>集成方法与神经网络</strong>：</p><ul>  <li>    <p>神经网络能找到足够多的不同的解，意味着他们可以从模型平均中受益——即使所有模型都在同一数据集上训练。</p>    <p>神经网络中<strong>随机初始化</strong>的差异、<strong>批训练数据</strong>的随机选择、<strong>超参数</strong>的差异等<strong>非确定性</strong>实现往往足以使得集成中的不同成员具有<strong>部分独立的误差</strong>。</p>  </li></ul><h3 id="dropout-策略">Dropout 策略</h3><ul>  <li>    <p>简单来说，Dropout 通过<strong>参数共享</strong>提供了一种廉价的 Bagging 集成近似—— Dropout 策略相当于集成了包括所有从基础网络除去部分单元后形成的子网络。</p>  </li>  <li>    <p>通常，<strong>隐藏层</strong>的采样概率为 <code>0.5</code>，<strong>输入</strong>的采样概率为 <code>0.8</code>；超参数也可以采样，但其采样概率一般为 <code>1</code></p>    <p><img src="http://www.meng.uno/images/assets/TIM%E6%88%AA%E5%9B%BE20180611152559.png" alt=""></p>  </li></ul><p><strong>权重比例推断规则</strong></p><ul>  <li>权重比例推断规则的目的是确保在测试时一个单元的期望总输入与在训练时该单元的期望总输入大致相同。</li>  <li>实践时，如果使用 <code>0.5</code> 的采样概率，<strong>权重比例规则</strong>相当于在训练结束后<strong>将权重除 2</strong>，然后像平常一样使用模型；等价的，另一种方法是在训练期间将单元的状态乘 2。</li></ul><h4 id="dropout-与-bagging-的不同">Dropout 与 Bagging 的不同</h4><ul>  <li>在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型<strong>共享参数</strong>，其中每个模型继承父神经网络参数的不同子集。</li>  <li>在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。</li></ul><h1>深度学习实践</h1><h2 id="参数初始化">参数初始化</h2><ul>  <li>一般使用服从的<strong>高斯分布</strong>（<code>mean=0, stddev=1</code>）或<strong>均匀分布</strong>的随机值作为<strong>权重</strong>的初始化参数；使用 <code>0</code> 作为<strong>偏置</strong>的初始化参数</li>  <li>一些<strong>启发式</strong>方法会根据<strong>输入与输出的单元数</strong>来决定初始值的范围    <ul>      <li>        <p>比如 <code>glorot_uniform</code> 方法 (Glorot and Bengio, 2010)</p>        <p><a href="http://www.codecogs.com/eqnedit.php?latex=W_%7Bi,j%7D%5Csim&amp;space;U%5Cleft&amp;space;(&amp;space;-%5Csqrt&amp;space;%5Cfrac%7B6%7D%7Bn_%7Bin%7D+n_%7Bout%7D%7D,&amp;space;%5Csqrt&amp;space;%5Cfrac%7B6%7D%7Bn_%7Bin%7D+n_%7Bout%7D%7D&amp;space;%5Cright&amp;space;)" target="_blank" rel="noopener"><img src="http://www.meng.uno/images/assets/%E5%85%AC%E5%BC%8F_20180706115540.png" alt=""></a></p>        <blockquote>          <p>Keras 全连接层默认的<strong>权重</strong>初始化方法</p>        </blockquote>      </li>    </ul>  </li>  <li><strong>其他初始化方法</strong>    <ul>      <li>随机正交矩阵（Orthogonal）</li>      <li>截断高斯分布（Truncated normal distribution）</li>    </ul>  </li></ul><blockquote>  <p>Keras 提供的所有参数初始化方法：Keras/<a href="https://keras.io/initializers/" target="_blank" rel="noopener">Initializers</a></p></blockquote><h1>CNN 卷积神经网络</h1><h2 id="cnn-与-lstm-的区别">CNN 与 LSTM 的区别</h2><ul>  <li>CNN更像视觉，天然具有二维整体性；而LSTM更像听觉和语音，总是通过串行的方式来理解整体。    <blockquote>      <p><a href="http://www.dataguru.cn/article-10314-1.html" target="_blank" rel="noopener">首次超越LSTM : Facebook 门卷积网络新模型能否取代递归模型？</a></p>    </blockquote>  </li></ul><p><br><br>本文链接： <a href="http://www.meng.uno/articles/c0b3d81d/">http://www.meng.uno/articles/c0b3d81d/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      梯度下降法
梯度下降法的作用/目的/本质
 *  参数优化的一种策略，用于寻找局部最小值
   
   
 *  微积分中使用梯度表示函数增长最快的方向；相应的，神经网络中使用负梯度来指示损失函数下降最快的方向
   
   
 *  梯度实际上是损失函数对网络中每个参数的偏导所组成的向量，其中计算各偏导的方法即反向传播算法
   
   
 *  从另一个角度来理解方向这个概念，可以认为负梯度中的每一项实际传达了两个信息：
   
    1.  正负号在告诉输入向量应该调大还是调小——正调大，负调小
       
       
    2.  每一项的相对大小表明每个输入值对函数值的影
    
    </summary>
    
      <category term="DeepLearning" scheme="http://www.meng.uno/categories/DeepLearning/"/>
    
    
      <category term="深度学习" scheme="http://www.meng.uno/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AI" scheme="http://www.meng.uno/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>自制简单搜索引擎及Wiser的使用</title>
    <link href="http://www.meng.uno/articles/c49b2caf/"/>
    <id>http://www.meng.uno/articles/c49b2caf/</id>
    <published>2018-03-03T06:10:07.000Z</published>
    <updated>2018-03-03T06:53:43.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>  <p>自己开发一个搜索引擎，可能是每一个计算机爱好者的梦想，但是当我们看到网上开源搜索引擎那么庞大时，未免有点害怕。那么开发一个搜索引擎真的很难么？</p></blockquote><blockquote>  <p>本博文在阅读了《How to Develop a Search Engineer》之后，总结而成。</p></blockquote><blockquote>  <p>代码下载：<a href="http://www.meng.uno/codes/wiser.zip">Wiser</a></p></blockquote><h1>搜索引擎简介</h1><p>搜索引擎（Search Engine）是指根据一定的策略、运用计算机技术从互联网上搜集信息，在对信息进行组织和处理后，为用户提供检索服务。在日常生活中，可以看到 Google 等 Web 检索网站，还有邮件检索和专利检索等各种应用程序。</p><h2 id="背景知识">背景知识</h2><p>在自己写一个搜索引擎之前，需要先了解基本的原理和概念。比如分词，倒排索引，排序算法（BM25、PageRank）等。</p><p>搜索引擎工作步骤分为这几步：</p><ul>  <li>爬虫模块 Crawler 在网页上抓取感兴趣的网页数据存储为 Cached pages</li>  <li>索引构造器 Indexer 对 Cached pages 处理生成倒排索引(Inverted Index)</li>  <li>对查询词 Query 在倒排索引中查找对应的文档 Document</li>  <li>计算 Query 和 Document 的关联度，返回给用户 TopK 个结果</li>  <li>根据用户点击 TopK 的行为去修正用户查询的 Query，形成反馈闭环。</li></ul><p>搜索引擎的四大组件：</p><ul>  <li>文档管理器(Document Manager)</li>  <li>索引构建器(Indexer)</li>  <li>索引管理器(Index Manager)</li>  <li>索引检索器(Index Searcher)</li></ul><p>组件关系图：</p><p><img src="http://www.meng.uno/images/se/1.png" alt=""></p><h1>Wiser使用</h1><h3 id="编译运行">编译运行</h3><p>下载好<code>wiser.zip</code>文件，并解压缩到相应位置，进入文件夹，运行<code>make wiser</code>，稍待片刻，即可完成编译。</p><p><img src="http://www.meng.uno/images/se/4.png" alt=""></p><h3 id="收集数据">收集数据</h3><p>在本次使用wiser的实验中，直接从<code>https://dumps.wikimedia.org/zhwiki/latest/</code>下载相应的<code>xml</code>文件即可（省去了实际的爬虫过程）。</p><p><strong>使用wiser存入sqlite使用命令：<code>wiser -x XXX.xml -m 100 wiki.db</code></strong></p><p><img src="http://www.meng.uno/images/se/2.png" alt=""></p><p>此时，我们已经将10条数据存入<code>.db</code>文件中了。</p><h3 id="构建倒排索引">构建倒排索引</h3><p>*在上一步已经完成。</p><h3 id="检索文档">检索文档</h3><p><strong>使用wiser搜索一个关键词使用命令：<code>wiser -q &quot;XXX&quot; wiki.db</code></strong></p><h3 id="排序并呈现">排序并呈现</h3><p><img src="http://www.meng.uno/images/se/3.png" alt=""></p><p>从截图中可见，score代表匹配指数，已经计算好，并返回给我们。</p><h1>Wiser代码剖析</h1><p>在此先简单的介绍各个主要的<code>.c</code>文件实现的功能：</p><ul>  <li><code>wiser.c</code>: 主程序，接收命令行输入，并相应的调用其他函数；</li>  <li><code>database.c</code>: 操作sqlite，包括增，查等功能；</li>  <li><code>search.c</code>: 全文检索，TF-IDF求相关度；</li>  <li><code>postings.c</code>: 倒排索引压缩与解压缩；</li>  <li><code>token.c</code>: 创建倒排索引，N-gram分词；</li>  <li><code>wikiload.c</code>: 加载wikipedia上下载的<code>xml</code>文件；</li>  <li><code>util.c</code>: 编码相关的杂项。</li></ul><p>其他详情，还请实际使用啊！</p><p><br><br>本文链接： <a href="http://www.meng.uno/articles/c49b2caf/">http://www.meng.uno/articles/c49b2caf/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      自己开发一个搜索引擎，可能是每一个计算机爱好者的梦想，但是当我们看到网上开源搜索引擎那么庞大时，未免有点害怕。那么开发一个搜索引擎真的很难么？

本博文在阅读了《How to Develop a Search Engineer》之后，总结而成。

代码下载：Wiser

搜索引擎简介
搜索引擎（Search Engine）是指根据一定的策略、运用计算机技术从互联网上搜集信息，在对信息进行组织和处理后，为用户提供检索服务。在日常生活中，可以看到 Google 等 Web 检索网站，还有邮件检索和专利检索等各种应用程序。

背景知识
在自己写一个搜索引擎之前，需要先了解基本的原理和概念。比如分词，
    
    </summary>
    
      <category term="信息检索" scheme="http://www.meng.uno/categories/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
    
      <category term="搜索引擎" scheme="http://www.meng.uno/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"/>
    
      <category term="Wiser" scheme="http://www.meng.uno/tags/Wiser/"/>
    
      <category term="倒排文件" scheme="http://www.meng.uno/tags/%E5%80%92%E6%8E%92%E6%96%87%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Eclipse的阿里巴巴代码规范配置</title>
    <link href="http://www.meng.uno/articles/6e79ab7a/"/>
    <id>http://www.meng.uno/articles/6e79ab7a/</id>
    <published>2018-03-01T15:25:33.000Z</published>
    <updated>2018-03-01T15:25:51.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="插件安装">插件安装</h2><p>环境：JDK1.8，Eclipse4+。有同学遇到过这样的情况，安装插件重启后，发现没有对应的菜单项，从日志上也看不到相关的异常信息，最后把JDK从1.6升级到1.8解决问题。</p><p>Help -&gt; Install New Software…</p><p><img src="https://gw.alicdn.com/tfscom/TB1LOyPifJNTKJjSspoXXc6mpXa.png" alt=""></p><p>输入Update Site地址：<a href="https://p3c.alibaba.com/plugin/eclipse/update" target="_blank" rel="noopener">https://p3c.alibaba.com/plugin/eclipse/update</a> 回车，然后勾选Ali-CodeAnalysis，再一直点Next Next…按提示走下去就好。 然后就是提示重启了，安装完毕。</p><p><img src="https://gw.alicdn.com/tfscom/TB1Ud5kifBNTKJjSszcXXbO2VXa.png" alt=""></p><p>注意：有同学反映插件扫描会触发很多 “JPA Java Change Event Handler (Waiting)” 的任务，这个是Eclipse的一个<a href="https://bugs.eclipse.org/bugs/show_bug.cgi?id=387455" target="_blank" rel="noopener">bug</a>，因为插件在扫描的时候会对文件进行标记，所以触发了JPA的任务。卸载JPA插件，或者尝试升级到最新版的Eclipse。附：<a href="https://my.oschina.net/cimu/blog/278724" target="_blank" rel="noopener">JPA project Change Event Handler问题解决</a></p><h2 id="插件使用">插件使用</h2><p>目前插件实现了开发手册中的53条规则，大部分基于PMD实现，其中有4条规则基于Eclipse实现，支持4条规则的QuickFix功能。</p><pre><code>* 所有的覆写方法，必须加@Override注解， * if/for/while/switch/do等保留字与左右括号之间都必须加空格,* long或者Long初始赋值时，必须使用大写的L，不能是小写的l）* Object的equals方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals。</code></pre><p>目前不支持代码实时检测，需要手动触发，希望更多的人加入进来一起把咱们的插件做得越来越好，尽量提升研发的使用体验。</p><h3 id="代码扫描">代码扫描</h3><p>可以通过右键菜单、Toolbar按钮两种方式手动触发代码检测。同时结果面板中可以对部分实现了QuickFix功能的规则进行快速修复。</p><h4 id="触发扫描">触发扫描</h4><p>在当前编辑的文件中点击右键，可以在弹出的菜单中触发对该文件的检测。</p><p><img src="https://gw.alicdn.com/tfscom/TB1XGo8iPihSKJjy0FeXXbJtpXa.png" alt=""></p><p>在左侧的Project目录树种点击右键，可以触发对整个工程或者选择的某个目录、文件进行检测。</p><p><img src="https://gw.alicdn.com/tfscom/TB18UsJi2NZWeJjSZFpXXXjBFXa.png" alt=""></p><p>也可以通过Toolbar中的按钮来触发检测，目前Toolbar的按钮触发的检测范围与您IDE当时的焦点有关，如当前编辑的文件或者是Project目录树选中的项，是不是感觉与右键菜单的检测范围类似呢。</p><p><img src="https://gw.alicdn.com/tfscom/TB1vt1oifBNTKJjSszcXXbO2VXa.png" alt=""></p><h4 id="扫描结果">扫描结果</h4><p>简洁的结果面板，按规则等级分类，等级-&gt;规则-&gt;文件-&gt;违规项。同时还提供一个查看规则详情的界面。</p><p>清除结果标记更方便，支持上面提到的4条规则QuickFix。</p><p><img src="https://gw.alicdn.com/tfscom/TB1_uFJi6ihSKJjy0FlXXadEXXa.png" alt=""></p><h4 id="查看所有规则">查看所有规则</h4><p><img src="https://gw.alicdn.com/tfscom/TB1UNTnmYsTMeJjSszhXXcGCFXa.png" alt="">  <img src="https://gw.alicdn.com/tfscom/TB1_rf7sOAKL1JjSZFoXXagCFXa.png" alt=""></p><h4 id="国际化">国际化</h4><p><img src="https://gw.alicdn.com/tfscom/TB1KsyYsiFTMKJjSZFAXXckJpXa.png" alt=""></p><p><img src="https://gw.alicdn.com/tfscom/TB19bzdm3oQMeJjy1XaXXcSsFXa.png" alt=""></p><p><br><br>本文链接： <a href="http://www.meng.uno/articles/6e79ab7a/">http://www.meng.uno/articles/6e79ab7a/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      插件安装
环境：JDK1.8，Eclipse4+。有同学遇到过这样的情况，安装插件重启后，发现没有对应的菜单项，从日志上也看不到相关的异常信息，最后把JDK从1.6升级到1.8解决问题。

Help -&gt; Install New Software…



输入Update Site地址：https://p3c.alibaba.com/plugin/eclipse/update 回车，然后勾选Ali-CodeAnalysis，再一直点Next Next…按提示走下去就好。 然后就是提示重启了，安装完毕。



注意：有同学反映插件扫描会触发很多 “JPA Java Change Event Ha
    
    </summary>
    
      <category term="Java开发Tips" scheme="http://www.meng.uno/categories/Java%E5%BC%80%E5%8F%91Tips/"/>
    
      <category term="代码规范" scheme="http://www.meng.uno/categories/Java%E5%BC%80%E5%8F%91Tips/%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/"/>
    
    
      <category term="代码规范" scheme="http://www.meng.uno/tags/%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/"/>
    
      <category term="Alibaba Format" scheme="http://www.meng.uno/tags/Alibaba-Format/"/>
    
      <category term="Eclipse" scheme="http://www.meng.uno/tags/Eclipse/"/>
    
  </entry>
  
  <entry>
    <title>Eclipse的Google样式Java代码自动规范配置</title>
    <link href="http://www.meng.uno/articles/548d5dfd/"/>
    <id>http://www.meng.uno/articles/548d5dfd/</id>
    <published>2018-03-01T12:59:33.000Z</published>
    <updated>2018-03-01T15:24:23.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>  <p>不知道大家在用Eclipse的时候有没有感觉到，当代码量一大就看不懂自己代码了呢？有人可能要说了，eclipse不是自己带有格式化代码工具吗？确实如此，但是我们可能想使用更高级的自动化工具，例如Google—Java-Style。这篇博文，我将展示如何使用eclipse自带的自动化代码规范工具，以及怎么配置Google-Java-Style。</p></blockquote><h1>准备</h1><p>文件下载：</p><ul>  <li>Eclipse: <a href="http://www.eclipse.org/" target="_blank" rel="noopener">进入官网</a></li>  <li>Google Java Format File: <a href="http://meng.uno/utils/eclipse-java-google-style.xml" target="_blank" rel="noopener">点击下载</a></li></ul><h1>使用Eclipse自带</h1><ul>  <li>快捷键： <code>Ctrl/Command + Shift + F</code></li>  <li>鼠标：    <ul>      <li>单个文件：进入文件/对着文件名<code>点右键</code> &gt; 找到<code>Source</code> &gt; 点击<code>Format</code> (其实就是快捷键的作用！)</li>      <li>项目：对着项目名/包名<code>点右键</code> &gt; 找到<code>Source</code> &gt; 点击<code>Format</code></li>    </ul>  </li></ul><p>如下截图：</p><p><img src="http://www.meng.uno/images/format/3.png" alt="右键"></p><h1>更换成Google Style</h1><p>当我们下载了本博客提供的<code>eclipse-java-google-style.xml</code>，就可以开始为formatter改风格了。</p><ul>  <li>打开eclipse的<code>Preferences</code>找到<code>Java</code>，再展开<code>Code Style</code>，找到<code>Formatter</code>。</li></ul><p><img src="http://www.meng.uno/images/format/1.jpg" alt="Code Style"></p><p>点击<code>Import</code>，在弹出窗口里选择我们下载的文件，确定即可。</p><p><img src="http://www.meng.uno/images/format/2.jpg" alt="Code Style"></p><p>再次进入项目，对着想要格式化的对象进行格式化操作，在进度条走完，我们就得到一份Google Java Style的代码了。</p><h1>后记</h1><ul>  <li>按照相似的步骤，我们也可以<code>Import</code>其他风格的代码规范；</li>  <li>Google不仅提供了eclipse上Java的代码规范，还有其他很多规范，详见<a href="https://github.com/google/styleguide" target="_blank" rel="noopener">Goole Style Guile</a></li>  <li>如果任何代码规范都不和心意，也可以打开某个代码规范，自己做相应的改动。</li></ul><p><img src="http://www.meng.uno/images/format/4.png" alt="Code Style"></p><p><br><br>本文链接： <a href="http://www.meng.uno/articles/548d5dfd/">http://www.meng.uno/articles/548d5dfd/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      不知道大家在用Eclipse的时候有没有感觉到，当代码量一大就看不懂自己代码了呢？有人可能要说了，eclipse不是自己带有格式化代码工具吗？确实如此，但是我们可能想使用更高级的自动化工具，例如Google—Java-Style。这篇博文，我将展示如何使用eclipse自带的自动化代码规范工具，以及怎么配置Google-Java-Style。

准备
文件下载：

 * Eclipse: 进入官网
 * Google Java Format File: 点击下载

使用Eclipse自带
 * 快捷键： Ctrl/Command + Shift + F
 * 鼠标：  * 单个文件：进入文件/
    
    </summary>
    
      <category term="Java开发Tips" scheme="http://www.meng.uno/categories/Java%E5%BC%80%E5%8F%91Tips/"/>
    
      <category term="代码规范" scheme="http://www.meng.uno/categories/Java%E5%BC%80%E5%8F%91Tips/%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/"/>
    
    
      <category term="代码规范" scheme="http://www.meng.uno/tags/%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/"/>
    
      <category term="Eclipse" scheme="http://www.meng.uno/tags/Eclipse/"/>
    
      <category term="Google Format" scheme="http://www.meng.uno/tags/Google-Format/"/>
    
  </entry>
  
  <entry>
    <title>What are Human Genome Project and ENCODE Project?</title>
    <link href="http://www.meng.uno/articles/32469d52/"/>
    <id>http://www.meng.uno/articles/32469d52/</id>
    <published>2018-02-18T11:54:31.000Z</published>
    <updated>2018-02-18T13:25:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Human Genome Project</h1><h2 id="the-profile-of-the-project">The Profile of the Project</h2><p>人类基因组计划(Human Genome Project,简称HGP)是由美国科学家于1985年率先提出，又于1990年正式启动的。来自美国、英国、法国、德国、日本和中国科学家共同参与了这一预算达30亿美元的人类基因组计划。按照这个计划的设想，在2005年，要把人体内约10万个基因的密码全部解开，同时绘制出人类基因的谱图。换句话说，就是要揭开组成人体4万个基因的30亿个碱基对的秘密。</p><p>这一计划的最终目的是测定人类基因组30亿个基本化学组成（称为碱基对或核苷酸），进而揭开与人类的生老病死有关的数万个基因的相互关系。1984年，美国能源部开会，开始酝酿“人类基因组计划”。1989年，美国能源部和美国国家卫生研究所提出了人类基因图谱工程。美国在1990年10月1日率先启动人类基因组计划。美国人类基因组顾问委员会委员梅纳德•奥尔森是人类基因组计划最早的推动者之一，另外美国一个测序中心的主任罗伯特•沃特斯顿以及英国的人类基因组总负责人均表示支持。美国完成人类基因组计划近54%的工作量，为人类基因组计划最大的贡献国。英国是人类基因组计划的第二大贡献国，共34%的贡献都是由Wellcome基金会资助的Sanger中心完成的。日本、法国、德国对人类基因组计划的贡献分别为6.8%、2.8%与2.2%。中国承担了3号染色体区域短臂端粒侧约30  cM，约占人类整个基因组1% 的测序工作。中国的华大基因、国家自然科学基金会、中科院遗传所南方基因中心、北方人类基因组中心等单位及于军、杨焕明、汪建、刘斯奇、吴旻、强伯勤、陈竺等也给予人类基因组计划大力的推动。</p><h2 id="the-importance-of-the-project">The Importance of the Project</h2><h3 id="目的">目的</h3><p>人类是在“进化”历程上最高级的生物，对人类基因的研究有助于认识自身、掌握生老病死规律、疾病的诊断和治疗、了解生命的起源。 测出人类基因组DNA的30亿个碱基对的序列，发现所有人类基因，找出它们在染色体上的位置，破译人类全部遗传信息。</p><p>在人类基因组计划中，还包括对五种生物基因组的研究：大肠杆菌、酵母、线虫、果蝇和小鼠，称之为人类的五种“模式生物”。</p><p>HGP的目的是解码生命、了解生命的起源、了解生命体生长发育的规律、认识种属之间和个体之间存在差异的起因、认识疾病产生的机制以及长寿与衰老等生命现象、为疾病的诊治提供科学依据。</p><h3 id="意义">意义</h3><p>人类基因组计划是一项规模宏大，跨国跨学科的科学探索工程。其宗旨在于测定组成人类染色体(指单倍体)中所包含的30亿个碱基对组成的核苷酸序列，从而绘制人类基因组图谱，并且辨识其载有的基因及其序列，达到破译人类遗传信息的最终目的。基因组计划是人类为了探索自身的奥秘所迈出的重要一步。</p><p>“人类基因组计划”与”曼哈顿原子弹计划”和”阿波罗计划”并称为二十世纪三大科学计划。</p><h2 id="the-achievement-of-the-project">The Achievement of the Project</h2><p>2000年6月26日，美国总统克林顿与英国首相布莱尔共同宣布人类基因组计划工作草图完成；次年2月，工作草图的具体序列信息、测序所采用的方法以及序列的分析结果被国际人类基因组测序联盟和塞雷拉基因组的科学家分别公开发表于《自然》与《科学》杂志。这一工作草图覆盖了基因组序列的83％，包括常染色质区域的90％（带有150,000个空缺，且许多片断的顺序和方位并没有得到确定）。</p><p>2001年2月12日，美国Celera公司与人类基因组计划分别在《科学》和《自然》杂志上公布了人类基因组精细图谱及其初步分析结果。</p><p>2003年，发现了新的方法通过检测另外的库来关闭Gaps。使用FISH技术或其他方法来分析没有闭合的Gaps大小。22，21条染色体就是用这种方式。</p><p>1999年至2006年，完成了全部23条染色体的测序工作，具体如下：</p><p>1999年12月，22号染色体测序完成；</p><p>2000年5月，21号染色体测序完成；</p><p>2001年12月，20号染色体测序完成；</p><p>2003年2月，14号染色体测序完成；</p><p>2003年6月，男性特有的Y染色体测序完成；</p><p>2003年5月和7月，7号染色体测序完成；</p><p>2003年10月，6号染色体测序完成；</p><p>2004年4月，13号和19号染色体测序完成；</p><p>2004年5月，9号和10号染色体测序完成；</p><p>2004年9月，5号染色体测序完成；</p><p>2004年12月，16号染色体测序完成；</p><p>2005年3月，X染色体测序完成；</p><p>2005年4月，2号和4号染色体测序完成；</p><p>2005年9月，18号染色体测序完成；</p><p>2006年1月，8号染色体测序完成；</p><p>2006年3月，11号,12号和15号染色体测序完成；</p><p>2006年4月，17号和3号染色体测序完成；Human Genome Project Information</p><p>2006年5月，1号染色体测序完成；Human Genome Project Information</p><p>2004年，国际人类基因组测序联盟的研究者宣布，人类基因组中所含基因的预计数目从先前的30,000至40,000（在计划初期的预计数目则高达2,000,000）调整为20,000至25,000。预期还需要多年的时间来确定人类基因组中所含基因的精确数目。</p><p>截止到2005年，人类基因组计划的测序工作已经完成。</p><h2 id="the-research-contents-of-the-project">The Research Contents of the Project</h2><h3 id="遗传图谱">遗传图谱</h3><p>遗传图谱又称连锁图谱（linkage map），它是以具有遗传多态性（在一个遗传位点上具有一个以上的等位基因，在群体中的出现频率皆高于1%）的遗传标记为“路标”，以遗传学距离（在减数分裂事件中两个位点之间进行交换、重组的百分率，1%的重组率称为1cM）为图距的基因组图。遗传图谱的建立为基因识别和完成基因定位创造了条件。意义：6000多个遗传标记已经能够把人的基因组分成6000多个区域，使得连锁分析法可以找到某一致病的或表现型的基因与某一标记邻近（紧密连锁）的证据，这样可把这一基因定位于这一已知区域，再对基因进行分离和研究。对于疾病而言，找基因和分析基因是个关键。</p><h3 id="物理图谱">物理图谱</h3><p>物理图谱是指有关构成基因组的全部基因的排列和间距的信息，它是通过对构成基因组的DNA分子进行测定而绘制的。绘制物理图谱的目的是把有关基因的遗传信息及其在每条染色体上的相对位置线性而系统地排列出来。DNA物理图谱是指DNA链的限制性酶切片段的排列顺序，即酶切片段在DNA链上的定位。因限制性内切酶在DNA链上的切口是以特异序列为基础的，核苷酸序列不同的DNA，经酶切后就会产生不同长度的DNA片段，由此而构成独特的酶切图谱。因此，DNA物理图谱是DNA分子结构的特征之一。DNA是很大的分子，由限制酶产生的用于测序反应的DNA片段只是其中的极小部分，这些片段在DNA链中所处的位置关系是应该首先解决的问题，故DNA物理图谱是顺序测定的基础，也可理解为指导DNA测序的蓝图。广义地说，DNA测序从物理图谱制作开始，它是测序工作的第一步。制作DNA物理图谱的方法有多种，这里选择一种常用的简便方法──标记片段的部分酶解法，来说明图谱制作原理。</p><h3 id="序列图谱">序列图谱</h3><p>随着遗传图谱和物理图谱的完成，测序就成为重中之重的工作。DNA序列分析技术是一个包括制备DNA片段化及碱基分析、DNA信息翻译的多阶段的过程。通过测序得到基因组的序列图谱。</p><h3 id="基因图谱">基因图谱</h3><h4 id="简介">简介</h4><p>基因图谱是在识别基因组所包含的蛋白质编码序列的基础上绘制的结合有关基因序列、位置及表达模式等信息的图谱。在人类基因组中鉴别出占具2%~5%长度的全部基因的位置、结构与功能，最主要的方法是通过基因的表达产物mRNA反追到染色体的位置。</p><h4 id="意义-v2">意义</h4><p>它能有效地反应在正常或受控条件中表达的全基因的时空图。通过这张图可以了解某一基因在不同时间不同组织、不同水平的表达；也可以了解一种组织中不同时间、不同基因中不同水平的表达，还可以了解某一特定时间、不同组织中的不同基因不同水平的表达。人类基因组是一个国际合作项目：表征人类基因组，选择的模式生物的DNA测序和作图，发展基因组研究的新技术，完善人类基因组研究涉及的伦理、法律和社会问题，培训能利用HGP发展起来的这些技术和资源进行生物学研究的科学家，促进人类健康。</p><h2 id="the-contributions-of-the-project">The Contributions of the Project</h2><h3 id="对人类疾病的贡献">对人类疾病的贡献</h3><p>人类疾病相关的基因是人类基因组中结构和功能完整性至关重要的信息。对于单基因病，采用“定位克隆”和“定位候选克隆”的全新思路，导致了亨廷顿氏舞蹈症、遗传性结肠癌和乳腺癌等一大批单基因遗传病致病基因的发现，为这些疾病的基因诊断和基因治疗奠定了基础。对于心血管疾病、肿瘤、糖尿病、神经精神类疾病（老年性痴呆、精神分裂症）、自身免疫性疾病等多基因疾病是疾病基因研究的重点。健康相关研究是HGP的重要组成部分，1997年相继提出：“肿瘤基因组解剖计划”“环境基因组学计划”。</p><h3 id="对医学的贡献">对医学的贡献</h3><p>基因诊断、基因治疗和基于基因组知识的治疗、基于基因组信息的疾病预防、疾病易感基因的识别、风险人群生活方式、环境因子的干预。</p><h3 id="对生物技术的贡献">对生物技术的贡献</h3><h4 id="基因工程药物">基因工程药物</h4><p>分泌蛋白（多肽激素，生长因子，趋化因子，凝血和抗凝血因子等）及其受体。</p><h4 id="诊断和研究试剂">诊断和研究试剂</h4><p>基因和抗体试剂盒、诊断和研究用生物芯片、疾病和筛药模型。</p><h4 id="细胞工程">细胞工程</h4><p>胚胎和成年期干细胞、克隆技术、器官再造技术。</p><h2 id="the-project-with-china">The Project with China</h2><p>作为继美、英、法、德、日6个成员国之后中唯一的发展中国家，中国对人类基因组的的贡献不只是工作量，在这个划时代的里程碑上，已经刻上了中国人的名字，中国在生物组学的发展上占有一席之地，通过参与这一计划，我们可以分享数据、资源、技术与发言权，最终来开发我国自己的基因资源。中国的加入改变了国际人类基因组计划原有的组织格局，提高其国际合作的形象，带来了国际社会对“国际人类基因组计划精神”的支持，联合国教科文组织关于人类基因组基本信息免费共享的声明，就是在中国代表的直接努力下促成的。可以说，中国需要人类基因组计划，而基因组计划也使我国的基因测序能力进人世界前列，在中国本土成长起来的作为我国基因组学的典型代表、创新型机构——华大基因已经成为全球最大的基因组学中心。</p><p>因此，人类基因组计划对华大基因的影响力也是举足轻重的，华大基因也因此而“生”的伟大。华大基因随着“国际人类基因组计划1%项目”的正式启动而诞生。华大基因自成立之日起就站在世界同步的轨迹上，使得中国的基因组学研究位于跟踪——参与——同步的国际地位。为后期的华大基因在基因组上的引领及跨越式发展奠定了基础。</p><p>在人类基因组计划之后，人类基因研究开始朝着与人类生育健康、肿瘤个体化治疗、病原微生物、遗传性疾病、血液病等的相关疾病的基因检测方向发展，未来，医疗技术将从末端的疾病治疗，逐步走向前端的基因诊断和预防，个性化医疗及精准医疗。人类将通过基因检测技术、通过个性化医疗以更精确的诊断，预测潜在疾病的风险，提供更有效、更有针对性的治疗，预防某种疾病的发生，比“治有病”更节约治疗成本。</p><p>华大基因希望凭借全球领先的基因组学技术，华大基因将千万家庭远离遗传性出生缺陷，肿瘤能早期检测和诊断并能全景式、定期监控个人健康动态，人人做到“我的基因我知道，我的健康我做主”。其研究方向主要涉及遗传性出生缺陷、肿瘤、心脑血管疾病、精准医疗 # The ENCODE Project</p><h2 id="the-profile-of-the-project-v2">The Profile of the Project</h2><p>The ENCODE Project（即Encyclopedia Of DNA Elements，中文译作DNA元件百科全书计划），是美国国立人类基因组研究院（US National Human Genome Research Institute，NHGRI）在2003年9月启动的跨国研究项目。该项目旨在解析人类基因组中的所有功能性元件，它是人类基因组计划完成之后，又一重要的跨国基因组学研究项目。该项目联合了来自美国，英国，西班牙，新加坡和日本的32个实验室的422名科学家的努力，获得了迄今最详细的人类基因组分析数据（他们获得并分析了超过15兆兆字节的原始数据）。研究花费了约300年的计算机时间，对147个组织类型进行了分析，以确定哪些能打开和关闭特定的基因，以及不同类型细胞之间的“开关”存在什么差异。</p><h2 id="the-achievement-of-the-project-v2">The Achievement of the Project</h2><p>近年来基因研究已经取得巨大进展。不过，迄今为止，这些研究主要还集中在编码蛋白的特定基因上，而它们所佔的比例不到整个人类基因组的2%。ENCODE计划首次系统地研究了所有类型的功能元件的位点和组织方式。</p><p>迄今为止，ENCODE计划主要集中研究了44个靶标共3000万个DNA硷基对。负责该计划数据整合和分析工作的欧洲分子生物学实验室主任Ewan Birney说：“我们的结论揭示了有关DNA功能元件构成的重要原理，为从DNA转录到哺乳动物进化的一切过程提供了新的认识。”</p><p>研究发现，人类基因组中的大多数DNA都会转录成RNA，这些副本会普遍交叠。因此，人类基因组实际上是一个非常复杂的网络，所谓的无用基因实际上非常少。基因只不过是众多具有特定功能的DNA序列类型之一。科学家们在基因之外的调控区域新发现了4491个转录启动位点，这一数字超过了已知基因的10倍。这些都挑战了长期以来的一个观点，即基因组中的基因是孤立的，同时，新的发现也支持了人类基因数量应该超过3万个的看法。</p><p>ENCODE计划的另一个巨大成就就是对哺乳动物基因组进化的认识。传统理论认为，与生理功能相关的重要DNA序列往往位于基因组中的“进化限制”区域，它们在物种进化过程中更容易保存下来。但是，最新的研究表明，大约一半人类基因组中的功能元件在进化过程中不会受到很大限制。科学家认为，哺乳动物缺乏“进化限制”这一点，很可能意味著许多物种的基因组都囊括了大量包括RNA转录副本在内的功能元件，在进化过程中，这些功能元件成了基因“仓库”。</p><p>此次ENCODE计划的成果亮点还包括：确定了许多之前不为人知的DNA转录启动位点；推翻了传统观点的认识，调控区域也有可能位于DNA转录启动位点的下游；确定了组蛋白变化的特定标记；加深了人们对组蛋白改变协调DNA复制的理解。</p><p>2012年9月5日，ENCODE项目的阶段性研究结果被整理成30篇论文发表于《自然》（6篇），《基因组研究》（6篇）和《基因组生物学》（18篇）上。</p><p>研究结果显示，人类基因组内的非编码DNA至少80%是有生物活性的，而并非之前认为的“垃圾” DNA （junk DNA）。这些新的发现有望帮助研究人员理解基因受到控制的途径，以及澄清某些疾病的遗传学风险因子。 ENCODE是人类基因组计划之后国际科学界在基因组学研究领域取得的又一重大进展。</p><p>2012年12月21日，ENCODE项目被《科学》杂志评为本年度十大科学突破之一。</p><h2 id="the-research-contents-of-the-project-v2">The Research Contents of the Project</h2><h3 id="试点研究的内容">试点研究的内容</h3><p>对编码的功能DNA进行鉴定和分类；对已存在的几种方法进行测试和比较，严格分析了人类基因组序列中已被定义的序列。</p><p>阐明人类生物学和疾病之间的关系。</p><p>对大量鉴定基因特征的方法、技术和手段进行检测和评估。</p><h3 id="研究对象">研究对象</h3><p>编码蛋白基因</p><p>非编码蛋白基因</p><p>调控区域</p><p>染色体结构维持和调节染色体复制能力的DNA元件</p><h3 id="研究特点">研究特点</h3><p>采用综合性研究策略</p><p>重视新技术的研发</p><p>将计划向学术界和公司开放</p><h2 id="the-contributions-of-the-project-v2">The Contributions of the Project</h2><h3 id="人细胞转录全景图">人细胞转录全景图</h3><p>通过ENCODE项目，人们知道RNA是基因组编码的遗传信息的直接输出。细胞的大部分调节功能都集中在RNA的合成、加工和运输、修饰和翻译之中。研究人员证实，75%的人基因组能够发生转录，并且观察到几乎所有当前已标注的RNA和上千个之前未标注的RNA的表达范围与水平、定位、加工命运、调节区和修饰。总之，这些观察结果表明人们需要重新定义基因的概念。</p><h3 id="人基因组中可访问的染色质全景图">人基因组中可访问的染色质全景图</h3><p>DNase I超敏感位点(DNase I hypersensitive sites, DHSs)是调节性DNA序列的标记物。研究人员通过对125个不同的细胞和组织类型进行全基因组谱分析而鉴定出大约290万个人DHSs，并且首次大范围地绘制出人DHSs图谱。</p><h3 id="基因启动子的远距离相互作用全景图">基因启动子的远距离相互作用全景图</h3><p>在ENCODE项目中，研究人员选择1%的基因组作为项目试点区域，并且利用染色体构象捕获碳拷贝(chromosome conformation capture carbon copy, 简称为5C)技术来综合性地分析了这个区域中转录起始位点和远端序列元件之间的相互作用。他们获得GM12878、K562和HeLa-S3细胞的5C图谱。在每个细胞系，他们发现启动子和远端序列元件之间存在1000多个远距离相互作用。</p><h3 id="gencode：encode项目的人基因组参照标注">GENCODE：ENCODE项目的人基因组参照标注</h3><p>GENCODE项目旨在利用计算分析、人工标注和实验验证来鉴定出人基因组中所有的基因特征。GENCODE第七版(GENCODE v7)公开发布了基因组标注数据集，包含了20687个蛋白编码的RNA基因座位、9640个长链非编码RNA基因座位，并且拥有33977个在UCSC基因数据库和RefSeq数据库中不存在的编码性转录本。它还对公开获得的长链非编码RNA(long noncoding RNA, lncRNA)进行最全面的标注。</p><h1>我的认识</h1><p>在上这门课之前，我从没认真想过这个问题，到底研究基因有什么用？通过这几天的学习，以及对文章所提的两个项目的检索、认识，我对基因测序这一工作，有了更深层次的认识。</p><p>虽然外界关于基因测序有不同的看法，例如有人支持，因为它可以为医学做贡献；有人反对，因为这样做相当于为基因做了一次曝光，这样一来，就有优劣基因之分。在我看来，这一任务还是利大于弊的，毕竟现在看来是这样。科学家可以通过对已有的基因测序结果的分析，总结出基因的“中心法则”，使我们对自身有了更进一步的了解。再者，基因分析有很多好的应用，通过对胎儿基因分析可以达到优生的目的，以及对有基因缺陷、先天性遗传病患者可以提供治标治本的治疗方案。</p><p>当然，要了解所有基因的功能还有很长的一段路要走。例如以前人们所认为的垃圾DNA实际上并不“垃圾”，它们在基因组的进化、每个个体的差异性以及许多其他方面扮演着重要角色，是世界上许多实验室着力研究的目标。</p><p>即使已经过了将近30年，人类基因组也没有完成“完全”测序，不过我们了解到了基因并不是静态的，而是处在复杂的变化之中，所以对人类基因的研究也是对人类自身的研究，这一研究将会一直进行下去，永无终点。 虽然人类基因组目前也只是一张初步的蓝图，需要经过更多的研究和分析。但是人类已经通过对基因组的学习，进入了医学的新纪元，为预防、诊断和治疗疾病带来了新的方法。所以对基因组的研究势必将成为人类新的曙光。</p><p>总之，我对基因组计划以及ENCODE计划充满期待与支持。</p><h1>参考资料</h1><ol>  <li>HGP计划百度百科：<a href="http://dwz.cn/3ITVf3" target="_blank" rel="noopener">http://dwz.cn/3ITVf3</a></li>  <li>人类基因组计划- 维基百科http://dwz.cn/3JHOap</li>  <li>科学松鼠会之人类基因组计划 <a href="http://dwz.cn/3JHOXZ" target="_blank" rel="noopener">http://dwz.cn/3JHOXZ</a></li>  <li>ENCODE项目百度百科：<a href="http://dwz.cn/3ITSPr" target="_blank" rel="noopener">http://dwz.cn/3ITSPr</a></li>  <li>Genome网 <a href="https://www.genome.gov/10005107/encode-project" target="_blank" rel="noopener">https://www.genome.gov/10005107/encode-project</a></li>  <li>ENCODE项目官网：<a href="https://www.encodeproject.org" target="_blank" rel="noopener">https://www.encodeproject.org</a></li>  <li>“DNA元件百科全书”首批成果出炉，链接：<a href="http://big5.cas.cn/xw/kjsm/gjdt/200706/t20070619_1011212.shtml" target="_blank" rel="noopener">http://big5.cas.cn/xw/kjsm/gjdt/200706/t20070619_1011212.shtml</a></li></ol><p><br><br>本文链接： <a href="http://www.meng.uno/articles/32469d52/">http://www.meng.uno/articles/32469d52/</a> 欢迎转载！</p>]]></content>
    
    <summary type="html">
    
      Human Genome Project
The Profile of the Project
人类基因组计划(Human Genome Project,简称HGP)是由美国科学家于1985年率先提出，又于1990年正式启动的。来自美国、英国、法国、德国、日本和中国科学家共同参与了这一预算达30亿美元的人类基因组计划。按照这个计划的设想，在2005年，要把人体内约10万个基因的密码全部解开，同时绘制出人类基因的谱图。换句话说，就是要揭开组成人体4万个基因的30亿个碱基对的秘密。

这一计划的最终目的是测定人类基因组30亿个基本化学组成（称为碱基对或核苷酸），进而揭开与人类的生老病死有关的数万个
    
    </summary>
    
      <category term="生物信息" scheme="http://www.meng.uno/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"/>
    
    
      <category term="生物信息" scheme="http://www.meng.uno/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"/>
    
      <category term="Genome" scheme="http://www.meng.uno/tags/Genome/"/>
    
      <category term="ENCODE" scheme="http://www.meng.uno/tags/ENCODE/"/>
    
  </entry>
  
</feed>
